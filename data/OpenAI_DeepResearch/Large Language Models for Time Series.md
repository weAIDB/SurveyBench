# Large Language Models for Time Series: A Comprehensive Survey

## 1 Introduction

Time series data - sequences of observations ordered in time - are ubiquitous across domains, from finance and economics to healthcare, climate science, and manufacturing. Accurate time series analysis (forecasting future values, detecting anomalies, classifying patterns, etc.) is crucial for informed decisionmaking in these fields. Over the past decade, advances in deep learning have spurred new methods for time series modeling. However, traditional neural architectures often struggle to capture long- range temporal dependencies and handle the irregularities of real- world time series. In parallel, Large Language Models (LLMs) have revolutionized natural language processing by learning from massive text corpora and demonstrating surprising generalization and reasoning abilities. This raises a natural question: Can the power of LLMs be brought to bear on time series tasks?

This survey explores the emerging convergence of large language models and time series analysis. We review how LLM techniques are being adapted for time series data, including formal problem definitions, methodological innovations, historical milestones, open challenges, and applications across domains. We aim to make this survey accessible to newcomers while providing depth and rigor for experts. Key contributions of this survey include: (1) formal definitions of time series tasks and LLM concepts; (2) a historical timeline of major developments bridging NLP and time series; (3) a taxonomy of methods applying LLMs or their architectures to time series (from prompt- based zero- shot forecasting to pre- trained foundation models for time series); (4) discussion of open challenges (data scarcity, nonstationarity, interpretability, etc.); and (5) an overview of applications in finance, healthcare, IoT, climate, and beyond. We construct this survey from fundamental principles and original research findings - rather than simply summarizing existing surveys - to provide an independent and up- to- date synthesis of this rapidly evolving interdisciplinary field.

## 2 Background: Time Series and Language Models

Time Series Definition: Formally, a time series is a sequence of observations \(\) x_{- }1,x_{- }2,\(dots,x_{- }T\)S indexed by time (assumed here to be uniformly spaced for simplicity) 1 . Time series can be univariate (a single variable changing over time) or multivariate (multiple variables observed in parallel). Key characteristics include temporal dependencies (autocorrelation), trends (long- term progression), seasonality (periodic patterns), and noise/irregularities. Time series analysis encompasses multiple tasks: - Forecasting: Predicting future values \(\) x_{- }T+1\(,dots,x_{- }T+1)S given past observations. Forecasts can be point predictions or probabilistic (distributions over future values). - Anomaly Detection: Identifying time points or segments that deviate significantly from expected patterns. - Classification & Clustering: Assigning labels to entire series or grouping similar series, e.g., classifying activity from wearable sensor signals. - Imputation: Filling in missing values in a time series. - Change Point Detection: Detecting times where the generative process changes (distribution shifts).

Each task has unique challenges, yet all require modeling the temporal structure in data. Classical statistical methods (ARIMA, exponential smoothing, state- space models, etc.) model each time series independently (a "local" model per series) and often assume stationarity or specific seasonal periods

2 . Modern deep learning models instead often train global models on many series, leveraging cross- series patterns 3 . Deep neural networks like RNNs, CNNs, and Transformers have been applied to time series, sometimes with specialized architectural tweaks (e.g. temporal attention mechanisms, seasonal decompositions). However, deep models can be data- hungry and may struggle with non- stationarity (changing statistics over time) and with providing uncertainty estimates 4 5 .

Large Language Models (LLMs): LLMs are a class of deep neural networks (typically based on the Transformer architecture) trained on enormous text corpora to predict the next token in a sequence 6 . Given a sequence of tokens \(\mathbb{S}(\mathbb{W}_{- }\mathbb{1},\mathbb{W}_{- }\mathbb{2},\mathbb{\backslash}d o t s,\mathbb{W}_{- }\mathbb{n})\mathbb{S}\) (which could be characters, subwords, or words), an LLM models the probability of the next token Sw \(\{n + 1\} S\) . effectively learning a distribution \(\mathbb{S}\mathbb{P}(\mathbb{W}\{n + 1\}\) \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \) \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid \mid\) . Modern LLMs like GPT- 3, GPT- 4, LLaMA, PaLM, etc., contain hundreds of millions to hundreds of billions of parameters and are pre- trained on diverse text data 8 . They have demonstrated emergent capabilities in natural language generation, question answering, reasoning, and few- shot learning - i.e. they can adapt to new tasks described in prompts without additional training. Fundamentally, LLMs are powerful sequence models: they learn complex patterns and long- range dependencies in sequences of discrete tokens. This sequential prediction capability is not specific to language - it is a general sequence modeling ability.

Bridging Time Series and Language Models: At first glance, time series data (real- valued, often continuous signals) seem quite different from text. However, both are sequences exhibiting temporal order. The key challenges in applying LLMs to time series are: (1) Representation - how to represent continuous time series in a way an LLM can ingest (since LLMs expect discrete token sequences); and (2) Alignment - how to align the learned knowledge of an LLM (mostly linguistic or general world knowledge) to the patterns of time series data. Despite these challenges, the analogy "time series as language" has motivated multiple approaches. One straightforward idea is to discretize or tokenize time series values into symbols, so that forecasting becomes a next- token prediction problem on a "pseudo- language" of time series 9 . Another idea is to leverage the strong sequence modeling of transformers but train them on time series data (with appropriate architectural modifications or pre- training regimes) to create foundation models for time series. Before diving into methods, we outline a brief history of developments at this intersection.

## 3 Historical Timeline of Key Developments

The intersection of time series modeling and advances in language models has accelerated in recent years. Below is a timeline of major milestones leading to the current landscape of LLMs for time series:

- 2015-2017: Early deep learning for time series forecasting emerges. Recurrent neural networks (RNNs) and Long Short- Term Memory (LSTM) networks are applied to forecasting and seq-to-seq prediction, but with mixed results. DeepAR (2017) by Amazon introduces a global RNN-based probabilistic forecasting model trained across many time series 10, outperforming naive statistical baselines on several datasets. This demonstrated the benefit of shared sequence models for time-oriented data.

- 2017: The Transformer architecture is introduced in the seminal paper "Attention is All You Need" by Vaswani et al. 11. Transformers, with their self-attention mechanism, achieve state-of-the-art in machine translation and quickly become the default for NLP sequence modeling. Their potential for capturing long-range dependencies sparks interest in using Transformers for time series.

- 2018–2019: Researchers adapt Transformers to time series tasks. Temporal Fusion Transformer (TFT) (2019) introduces an attention-based architecture tailored for multi-horizon forecasting with interpretable attention on temporal features 12. Other efforts (e.g. LogTrans, 2019) explore modifications like sparse attention for long sequences 13. Meanwhile, non-transformer deep models like N-BEATS (2019) show that purely architecture-driven approaches can rival statistical models by learning generic basis expansions for time series.

- 2020–2021: A proliferation of transformer-based forecasting models occurs. Informer (2021) employs probabilistic sparse self-attention for long-term series forecasting, aiming to reduce Transformers'  $\{0(n^{\wedge}2)\}$  time complexity. Autoformer (NeurIPS 2021) and FEDformer (ICML 2022) introduce decomposition of series into trend/seasonality and frequency domain attention, respectively, to improve long-horizon forecasts. Despite these innovations, a provocative finding emerges: Are Transformers actually effective for time series? In a 2022 study, Zeng et al. show that on several benchmarks, simple linear models surprisingly outperform complex transformer architectures for long-term forecasting 14 15. This prompted reflection on how to better exploit transformers' strengths for time series.

- 2022: The concept of prompting LLMs for time series is first explored. Xue and Salim propose PromptCast (2022) as a prompt-based forecasting paradigm 16. They transform a numerical forecast problem into a text-to-text task: the historical numbers are inserted into a textual template prompt (like a fill-in-the-blank question), and a pre-trained language model is asked to generate the continuation, which is parsed back to numbers. They introduce a dataset (PISA) for evaluating this approach 17. Around the same time, Wu et al. develop TimesNet (2023) as a task-general time series foundation model not by using an LLM, but by designing a novel Temporal 2D-Variation block to capture multi-periodic patterns in time series 18 19. TimesNet achieved state-of-the-art across five tasks (forecasting, imputation, classification, anomaly detection, and regression) without task-specific tuning 20, showcasing the promise of a single neural architecture for diverse time series tasks.

- 2023: The year when Large Language Models meet Time Series in full force. Multiple independent efforts demonstrate that LLMs can indeed be applied to time series forecasting:

- LLMTime (Gruver et al., NeurIPS 2023): Shows that by encoding time series as strings of digits, off-the-shelf LLMs like GPT-3 and LLaMA-2 can zero-shot extrapolate time series patterns. Remarkably, GPT-style models matched or exceeded the accuracy of specialized time series models on several benchmarks 9. They handled seasonality and multimodal predictive distributions naturally, and even managed missing data by using special tokens 21.

- One-Fits-All / GPT-4TS (Zhou et al., NeurIPS 2023): Introduces a unified model for general time series analysis (forecasting, classification, etc.) by fine-tuning a pre-trained GPT-2 model 22. They freeze most of GPT-2's weights and only learn lightweight adapters (like new positional embeddings and normalization layers) for each task 23. Instead of textual tokenization, they feed time series patches as continuous embeddings (inspired by PatchTST) to the model, avoiding the need to discretize values 24.

- Prompt-based Multimodal Models: GPT4MTS (AAAI 2023) and others incorporate additional context like text descriptors or metadata along with numeric series in prompts 25. Chung et al. develop Auto-TTE (2023) for clinical applications, which generates realistic 12-lead ECG time series conditioned on textual cardiology reports 26 27 - effectively using an LLM to translate text to time-series (a fascinating cross-modal generation task).

- Time-LLM (Jin et al., ICLR 2024): Proposes a reprogramming framework to align time series data with a frozen language model 28. The idea is to prepend and append text prototypes to pieces

of time series, encoding numeric patches in a format that the LLM can process 29 . A Prompt- asPrefix (PaP) is used - a textual prompt at the beginning to guide the LLM' s reasoning on the timeseries input 30 . The LLM' s output (still in token space) is then projected back to numeric forecasts. Time- LLM demonstrated state- of- the- art results on standard forecasting benchmarks, even outperforming specialized models, all while the core language model (e.g. GPT- 2 or GPTNeo) remains intact (not fine- tuned) 31 .

- Foundation Models for Time Series: Several groups independently pursue large-scale pretraining of transformer models on broad collections of time series data:

- ForecastPFN (Dooley et al., NeurIPS 2023): A transformer trained entirely on synthetically generated time series (with predefined trend/seasonality patterns) and then applied zero-shot to real data 32 . This showed the viability of learning a generic time series model from simulation.  
- Chronos (Ansari et al., 2024): The first large-scale pre-trained language model for time series, trained on a "language of time series" 33 . Chronos uses minimal modifications: raw time series values are scaled and quantized into tokens, and an existing Transformer LM (based on T5 architecture) is trained on sequences of those tokens via next-token prediction 34 . Figure 1 illustrates this approach. Chronos was trained on a corpus of 42 diverse datasets (plus additional synthetic series for augmentation) and achieved impressive results: on datasets it was trained on, it significantly outperformed classical and deep learning models; and on new unseen datasets, it achieved comparable or superior zero-shot forecasts relative to specialized models trained on those datasets 35 36 . This was a breakthrough showing that foundation models can be learned for time series, analogous to GPT for text.

Figure: High- level depiction of the Chronos framework 37. Time series values are mean- scaled and quantized into a sequence of discrete tokens. A Transformer language model (encoder- decoder or decoder- only) is trained on these token sequences via a next- token prediction objective (cross- entropy loss). At inference, the model autoregressively generates forecast tokens, which are mapped back to numeric values (multiple sample trajectories yield a probabilistic forecast distribution).

- **TimeGPT (Garza et al., 2023):** Introduced as a commercial foundation model for time series by Nixtla, TimeGPT is an encoder-decoder Transformer pre-trained on a wide array of time series (including finance, retail, energy, IoT, etc.) [49 + L127-L134] . It was designed for *zero-shot* forecasting as a service. Reports indicated TimeGPT achieved strong performance across various benchmarks, often outperforming both classical methods and other deep learning models without needing domain-specific tuning [49 + L129-L134] . Notably, TimeGPT allows *exogenous variables* (covariates) and probabilistic predictions out-of-the-box [37 + L165-L173] [37 + L185-L193] , making it practical for real-world use.

- **MOMENT (Goswami et al., ICML 2024):** An open-source family of *general-purpose time series foundation models* [49 + L135-L142] . MOMENT models (up to 385 million parameters, based on a T5 Transformer) are pre-trained on a massive curated dataset dubbed the "Time Series Pile" [49 + L135-L142] . Uniquely, MOMENT is evaluated not only on forecasting, but also classification and anomaly detection, demonstrating the ability of one pre-trained model to be fine-tuned (or used zero-shot) for multiple types of time series tasks.

- **Lag-LLaMA (Rasul et al., 2024):** A foundation model specifically for *univariate probabilistic forecasting* [49 + L141-L148] . Lag-LLaMA uses a GPT-style decoder-only Transformer, augmented with a notion of *lagged covariates* (i.e., incorporating a window of past values as known inputs) to help predict the future [35 + L61-L70] . It was pretrained on a large corpus of univariate series from diverse domains. Lag-LLaMA achieved state-of-the-art average accuracy on several univariate forecasting benchmarks, with strong zero-shot generalization and the ability to fine-tune on small

data to further improve  $[35 + \mathsf{L}63 - \mathsf{L}71]$  . Its design highlights the importance of retaining domain- specific concepts (like lag features for seasonality) in foundation models. - \*\*SimMTM (Dong et al., NeurIPS 2023):\*\* Although not an LLM per se, SimMTM is notable as a \*selfsupervised representation learning\* approach for time series. It pre- trains a model by \*\*masked time- series modeling\*\* - masking out portions of a time series and training the network to reconstruct them  $[31 + \mathsf{L}9 - \mathsf{L}17]$  . This is analogous to BERT' s masked language modeling, but applied to continuous time series. Such pre- trained representations can then fine- tuned for forecasting or classification  $[30 + \mathsf{L}241 - \mathsf{L}250]$  , contributing to the toolbox of foundation modeling techniques in the time series domain.

- 2024: Reflection and Next Steps. With numerous methods proposed, researchers began critically evaluating the efficacy of LLMs for time series. Tan et al. (NeurIPS 2024) ask "Are Language Models Actually Useful for Time Series Forecasting?" 43. In an extensive ablation study of three recent LLM-based forecasters, they found that removing the LLM component often did not hurt performance – sometimes it even improved 44. Moreover, replacing the LLM with a simple attention mechanism or training a smaller model from scratch gave similar accuracy at far lower computational cost 45. This suggests that in some cases the LLM might be learning little more than what a well-tuned time series model already captures, prompting the need for more principled integration of LLMs. Despite this sobering result, other work shows nuance: when appropriately adapted, LLMs can still shine. The Time-LLM authors, for example, demonstrated strong gains from carefully aligning time-series inputs with an LLM's understanding 31. As of 2025, the community is synthesizing these insights. The first "Time Series in the Age of Large Models" workshop (NeurIPS 2024) has been organized 46, indicating an active forum to debate progress and chart directions.

This timeline illustrates a rapid shift from early explorations to sophisticated models and even products, followed by introspection about when and how LLMs truly add value. Next, we categorize the major methodological approaches for leveraging LLMs in time series analysis, detailing representative techniques in each category.

## 4 Methodologies for LLM-Based Time Series Modeling

Researchers have pursued multiple approaches to bring together large language models and time series. We organize these methods into several broad categories: (A) Direct Prompting and Zero- Shot Forecasting, (B) Discrete Tokenization and Language Modeling, (C) Alignment and Reprogramming Strategies, and (D) Time- Series- Specific Foundation Models. These are not mutually exclusive – some methods combine elements of each – but this taxonomy helps clarify the design choices made to apply LLMs to time series.

### 4.1 Direct Prompting Approaches

One straightforward way to use an LLM for time series is to pose the time series problem as a text prompt and utilize the LLM's zero- shot or few- shot capabilities. In these approaches, time series values are embedded into textual templates, and the LLM's output (text) is parsed as the answer.

PromptCast (2022) – The pioneering work in this vein, PromptCast, reformulates forecasting as a sentence- to- sentence problem 16. For example, given past numerical values, one might create a prompt like: "Given a series: 10, 12, 15, 20, 18, what comes next?" and feed this string to a language model. PromptCast introduced hand- crafted templates for different datasets (ensuring the prompt provides necessary context like time units or seasonality) 47 48. It then uses a pre- trained language model (e.g.,

T5 or GPT- 2) to generate a continuation, which is interpreted as the forecast. The authors showed that with careful prompt design and even very small LMs, this method achieved reasonable accuracy 49. More interestingly, it showed much better zero- shot generalization than standard numerical models - when trained on one domain, it could generalize to others via prompting, whereas numerical models could not 50.

Large Language Models as Zero- Shot Forecasters (LLMTime, 2023) - Grover et al. took the direct approach to the extreme by using no fine- tuning and minimal templating. They represented every time series value in plain decimal notation and concatenated them with delimiters, essentially treating the series as a "sentence" of numbers 51. A large pre- trained LM (like GPT- 3) was then prompted to continue this sequence. Amazingly, GPT- 3 and LLaMA- 2 (70B) demonstrated the ability to extrapolate complex patterns in a zero- shot manner 52. For instance, given a historical seasonal pattern, GPT- 3 often continued with an appropriate seasonal value. The authors attribute this to biases in LMs: language models have a propensity for repeating periodic structures and favoring simple trends, which align with common time series features like seasonality and smooth trajectories 53. Additionally, LLMs naturally produce a distribution over next tokens; LLMTime leveraged this to construct flexible continuous predictive densities by mapping token probabilities to numeric values 54. They also showed LLMs can handle missing data by inserting a special token (like a "[missing]" text) and still producing coherent outputs 55. The takeaway is that off- the- shelf LLMs are surprisingly competent time series predictors in a zero- shot setting, often rivaling purpose- built models 56. However, this approach can be constrained by tokenization issues - e.g., GPT- 4's subword tokenization of digits was found to degrade its performance relative to GPT- 3 57 - and by the expense of querying huge models for large volumes of data.

Multimodal Prompting - Some approaches incorporate textual context or output along with series data. For example, consider forecasting demand for a product: one could provide an LLM with not only the sales figures but also a text description of recent promotions or events. A recent method called GPT4MTS (2023) allows an LLM to attend to a time series and a related text simultaneously 25. In a different direction, Auto- TTE (Chung et al.) flips the problem to text- to- time- series: it trains a model to generate an ECG signal conditioned on a textual medical report 58. These highlight the flexibility of prompt- based paradigms - because LLMs are fluent in language, we can incorporate domain knowledge via text, or even obtain explanations for forecasts by asking the model to justify its prediction in words 59. Indeed, LLMTime demonstrated that, after forecasting, one can prompt the model with follow- up questions like "Why will the value increase?" and get plausible explanations 55. This ability to converse about a time series (explain patterns, answer what- if questions) is a unique strength of LLMs over standard models.

However, direct prompting also has limitations. It often requires careful template design (to encode numbers in a way the LLM understands magnitude and timing) 47. There is also no guarantee the LLM's "knowledge" is relevant to the numeric data - an LLM might output a number that is grammatically sensible but numerically unreasonable (e.g., a value far outside any plausible range). Ensuring format correctness (like outputting exactly h numbers for an h- step forecast) can also be tricky, sometimes needing constrained decoding or post- processing. Nonetheless, prompting techniques opened the door for LLMs to be used directly as time series analysts, without retraining models from scratch.

### 4.2 Discrete Tokenization and Language Modeling

A major branch of work focuses on how to represent time series for input to a language model. The core idea is to transform continuous time series into a sequence of discrete tokens (words), so that one can train or use a language model on those tokens. This can be done either by tokenizing raw values or

by structuring the sequence in segments/patches. Once in token form, either a pre- trained LM can be applied, or a new model can be trained using the same objectives (next- token prediction).

Value Quantization: The simplest tokenization is to discretize each numeric value. Chronos (2024) follows this approach: for each time series, it applies mean scaling (divide by the mean absolute value in the history) to normalize scale, then quantization to map each real value to one of \(\) K\\( discrete levels (tokens)\) 60 61 . For example, with \(\) K=1024\\( tokens, each observation is replaced by the nearest token representing a quantized bin. The entire sequence thus becomes something like [TOKEN\)5(512)\(, TOKEN\) S, ...] which can be fed into a Transformer. The model is then trained with a standard language modeling objective to predict the next token (i.e., approximate the time series distribution) 37 . The authors of Chronos deliberately kept \(\) K\\( modest and the procedure simple (uniform bins after scaling) to test whether an unmodified language modeling approach would work 33 34 . Indeed, it performed strongly, suggesting that time series can be learned as "a new language" by Transformers with minimal changes. A drawback of hard quantization is the introduction of discretization error (especially for real values with high precision or outliers). Chronos mitigated this by also outputting multiple sample paths to form a predictive interval, which partially recovers continuous uncertainty 62 63 . Another discretization approach is via VQ- VAE (Vector- Quantized Variational Autoencoders), which learn a codebook of representative vectors. Chung et al. applied VQ- VAE to ECG time series, yielding tokens that an LLM can then model (Auto- TTE used this for text- conditioned ECG generation) 26 64 .

Patching and Embeddings: An alternative to quantizing every point is to chunk the time series into segments or "patches" and treat each patch as a higher- level token. This is analogous to how Vision Transformers patch images. For time series, a patch might be a short sub- sequence of length \(\) m\\(, which can be represented by its raw values or some features (e.g., by a small embedding network). Methods like PatchTST (2023) showed that patching can improve long - horizon forecast accuracy by allowing a model to attend to coarse segments rather than every timestamp 65 . One - Fits- All (GPT4TS) took inspiration from this: it feeds patch embeddings directly into a GPT - 2 model 23 . In their approach, the continuous embedding (a learned vector representing a window of time series) plays the role of a "word embedding" in the language model. This bypasses the need for fixed token dictionaries and allows the model to operate on continuous data internally. However, the model still learns to predict the next patch embedding in sequence, using the same transformer machinery. A small linear layer then projects the final embeddings to actual numeric outputs 66 . The authors found this patch - based GPT model to be effective across forecasting, classification, and anomaly detection tasks when fine- tuned, illustrating that language model architectures can consume time series in forms other than text tokens 67 68 .

Between these two extremes (full quantization vs. full continuous patches) are hybrid strategies. For instance, LLMTime (Gruver) quantizes each digit of a number into tokens (so "123.45" becomes tokens "1", "2", "3", "4", "5") 69 . This retains high precision and avoids out- of- vocabulary issues for rare values, at the expense of longer token sequences. They then add special formatting tokens (like start/end of series markers). The Aligner (Chang et al. 2023, LLM4TS) takes another approach: it first supervises a smaller model to learn the time series, and then generates pseudo- text from that model's output to fine- tune an LLM - effectively aligning the LLM's discrete predictions with the original series values 70 71 .

Overall, discrete tokenization enables training language models from scratch on time series. Several works in 2023- 2024 have trained Transformers on large corpora of time series tokens: - Chronos, as discussed, was trained on real + synthetic data and achieved strong generalization 62 . - MOMENT (2024) used masked token prediction on time series - randomly masking some tokenized values and training the model to fill them in 72 . This is akin to BERT pre- training, capturing contextual relations in series. The resulting 385M model was then adapted to various tasks with minimal fine- tuning, showing the

benefit of a self- supervised pre- training on time series 73 . - ForecastPFN trained on purely synthetic token sequences representing known seasonal patterns 74 . Interestingly, Chronos found that incorporating a mix of synthetic and real data during training gave the best results, as synthetic data alone may not capture all real- world nuances 75 . - Lag- LLaMA did not tokenize values per se, but it did treat lags (e.g. the value 24 hours ago, 7 days ago, etc.) as special covariate tokens that are concatenated to the input sequence 76 . This is more a feature- engineering approach, but highlights that domain knowledge (like seasonal lags) can be encoded as part of the sequence for an LLM to utilize.

One must be mindful of vocabulary design in these methods. A too- small vocabulary leads to high quantization error; too large a vocabulary leads to data sparsity (many tokens rarely seen) 77 . Chronos experimented with vocabulary sizes and found increasing the vocab improved point accuracy up to a point, after which it hurt probabilistic calibration 78 79 . The optimal encoding likely varies by dataset - e.g. integer- valued series vs. floating- point series may benefit from different schemes (there are works exploring integer encoding with custom numeral systems to shorten sequences 80 ). Despite these challenges, tokenization is a powerful tool: it enables the direct use of language modeling advances (architectures, pre- training tricks, etc.) for time series. In effect, it reduces time series analysis to a special case of sequence modeling that LLMs excel at.

### 4.3 Alignment and Reprogramming Strategies

A crucial insight is that off- the- shelf LLMs are trained on text, not on numeric data - so to use their full capabilities, one might seek to align time series with the LLM's native modality (language). Reprogramming refers to techniques that embed or transform non- language inputs into a form that a pre- trained LLM can understand, without retraining the LLM's weights. This typically involves adding learned prompts, adapters, or otherwise guiding the LLM to interpret the input correctly.

Time- LLM (2024) - This method is a prime example of alignment. Time- LLM keeps a large pre- trained language model frozen (e.g. a 7B parameter LLaMA) and prepends a learned textual embedding to the input time series patches 29 . Concretely, it creates text prototypes - essentially, sequences of pseudowords - that correspond to positions in a time series window 81 . Imagine something like: "<TS> a b c ... </TS>" where a, b, c are trainable vectors (treated as word embeddings) representing the numerical patch. These are concatenated to a natural language prompt that tells the LLM what to do (e.g., "Given the above time series data, forecast the next values" ). This Prompt- as- Prefix (PaP) approach provides two things: it gives the LLM an instruction in plain language, and it provides a slot where the numeric data goes in an encoded form 30 . The LLM then processes this combined sequence as if it were processing a paragraph of text. The output is a sequence of embeddings which are finally projected to numeric predictions by a lightweight regression head 82 31 . During training, only the small components (the prototype embeddings, the prefix prompt, and the projection head) are learned - the LLM's weights stay fixed. The results from Time- LLM are compelling: despite not updating the massive language model, the system achieved top- tier accuracy on forecasting benchmarks and was notably effective in few- shot settings 31 (where only a few examples of a new dataset are provided, mimicking how LLMs are used with prompts). The success indicates that LLMs have strong pattern- recognition capabilities that can be "unlocked" by feeding them properly encoded time series 82 . The frozen LLM in Time- LLM presumably contributes its robust sequence processing and reasoning ability, while the trainable front- end handles the modality gap. This approach echoes earlier "reprogramming" works in other domains (e.g. feeding images to LLMs by describing them in text, or feeding tabular data via crafted sentences).

Adapter and Fine- Tuning Techniques: Other alignment methods involve fine- tuning small parts of an LLM on time series data. The One- Fits- All model (GPT4TS) mentioned earlier is one such approach: it

freezes most of GPT- 2, and only fine- tunes the positional embeddings and layer norms for each distinct task 23 . This effectively teaches the model how to read time series positions (since time positions might not correspond to natural language positions) and how to scale its internal representations, without disturbing its language- learned weights too much. This approach achieved good performance on multiple tasks with minimal trainable parameters, showing that lightweight fine- tuning can adapt an LLM to new data modalities. Another example is LLM4TS (Chang et al. 2023) which proposed a two- stage fine- tuning: first, do a supervised pre- training on a large collection of time series (to give the model some time- series awareness), then apply parameter- efficient fine- tuning (like LoRA or adapters) on target forecasting tasks 83 71 . They reported improved data efficiency - i.e., the pre- trained LLM needed far fewer samples from a new dataset to achieve strong performance, compared to training a model from scratch 70 .

Vision or Tool Integration: A very different alignment idea is using other modalities as intermediate bridges. Zhang et al. (2024) survey pointed out a category termed "vision as a bridge" 84 - for example, one could convert a time series plot into an image and then use a vision- language model to interpret it, or use an LLM to output a description which is then parsed by a separate model. While not mainstream, there have been attempts like treating an ECG as an image of a heartbeat waveform and feeding it to an LLM that can consume both text and images (via multi- modal training). Additionally, "tool integration" refers to allowing an LLM to call external time series algorithms (acting like a controller). An illustration of this is ToolLLM (2023) which gave the LLM access to a toolbox of classical forecasting models - the LLM, given a task, could decide which tool to call (ARIMA, Prophet, etc.) and how to ensemble their outputs 85 . This leverages the LLM' s strength in reasoning and high- level decision making, while offloading actual number- crunching to specialized models. It was reported to produce interpretable results and the flexibility to incorporate domain knowledge (the LLM can, for instance, read a description "sales spike on holidays" and decide to use a model that accounts for occasional spikes) 86 .

In summary, alignment strategies seek to get the best of both worlds: harness the general intelligence of LLMs without requiring orders of magnitude more time series data to train them. These methods are especially useful when one already has a powerful pre- trained model (like GPT- 3 or LLaMA) and limited time series data. By clever encoding, prompting, or slight fine- tuning, the LLM can be repurposed for time series tasks 28 . The risk, of course, is that if the alignment is imperfect, the LLM might misinterpret the data (for instance, think a number token is a year or an age rather than a value). Careful prompt engineering and validation is needed to ensure the LLM' s outputs remain consistent and numerically valid.

### 4.4 Time-Series-Specific Foundation Models

The ultimate vision for many researchers is to develop foundation models for time series - analogous to GPT- style models, but trained on vast amounts of time series data (and possibly multimodal data) to serve as general- purpose predictors and representation learners 86 . Unlike using an existing LLM, this approach involves training (or pre- training) a model predominantly or entirely on time series data, possibly with architectures inspired by LLMs. We have already encountered some in previous sections; here we summarize their key features and contributions:

General- Purpose vs. Specialized: Foundation models for time series fall on a spectrum from broad to focused. On the broad end, models like MOMENT (2024) and TimesNet (2023) aim to handle any time series task or at least multiple tasks. TimesNet' s design, using 2D patterns, was task- agnostic and showed strong results in forecasting, classification, and anomaly detection without retraining 20 . MOMENT similarly demonstrated one model' s ability to be fine- tuned for various tasks across domains 41 . The benefit of general- purpose models is clear: practitioners could use a single pretrained model

and apply it to different problems (similar to how one can use GPT- 4 for translation, coding, or writing with only prompt changes). However, achieving excellence across all tasks is challenging. Some models thus focus on a narrower but still large scope. For example, Lag- LLaMA is specialized for forecasting (and specifically probabilistic forecasting) of univariate series 42. By narrowing scope, it can incorporate domain- specific features like lags and uncertainty estimation into its architecture 76. Another specialized foundation model is one by Wu et al. (2023) for spatiotemporal data - extending foundation modeling to data that has time and space (e.g. traffic flow on networks, climate data on grids). Such models incorporate not just time but also spatial graph or grid structure in a large model, indicating how LLM concepts inspire even non- language domains.

Architecture Choices: Most foundation models for time series still rely on Transformers at their core (hence they are often called "transformer- based foundation models" 87). But they often include custom components: - Encoder- Decoder vs. Decoder- Only: Language models come in both varieties; for time series, encoder- decoders (like T5- based Chronos, MOMENT) are popular because forecasting naturally maps to an encoder- decoder setup (history as input, future as output). Decoder- only (GPT style) models are used too (Lag- LLaMA, and LLMTime essentially used GPT- 3 in decoder mode). Decoder- only models generate one step at a time and can naturally produce probabilistic forecasts by sampling. Encoder- decoder models can ingest the entire history context and then generate the whole future in one go. Both have merits and are being explored. - Position and Time Encoding: Unlike text which has a linear position, time series may have calendar features (hour of day, day of week, etc.). Foundation models often embed time stamps or seasonal markers into the input. Models like TimeGPT explicitly allow date/time features to be input alongside the series 40. Others, like One- Fits- All, had to learn positional embeddings that correspond to time indices 23. Getting the temporal context right is crucial for generalization - e.g., a model should ideally know the difference between a series sampled hourly vs daily (if it's general purpose). - Probabilistic Output Heads: Traditional LLMs output a categorical distribution over tokens. For time series, especially forecasting, we often want a numeric distribution. Some foundation models adapt the output layer accordingly. Chronos and others effectively perform regression via classification by tokenizing values 88. Lag- LLaMA outputs parameters of a probabilistic model (like quantiles or mixture components) - so it is trained to minimize a likelihood (such as negative log- likelihood of the true value under the predicted distribution) 76. This brings the model closer to the realm of statistical forecasting, marrying it with the flexibility of transformers. - Data Augmentation and Synthetic Data: A pragmatic detail: since real- world time series data is relatively scarce (certainly compared to text), foundation models have embraced augmentation. Chronos integrated TSMix (mixing time series segments) and KernelSynth (GP- based synthetic data generation) during training 89. ForecastPFN and others rely on fully synthetic training 32. This helps teach models basic concepts of trend, seasonality, noise, etc. The risk is over- reliance on synthetic patterns that may not cover all real cases, but moderate use has shown to boost zero- shot robustness 90 75.

Performance and Findings: The early results from these foundation models are promising. Chronos set a new standard by performing well without any per- dataset tuning - a trained Chronos model could zeroshot forecast on entirely new datasets and often beat those datasets' own specialized models 62. TimeGPT similarly showed strong out- of- the- box accuracy across domains 39. MOMENT' s multi- task abilities hint that a single model can encode features useful for detection, classification, and forecasting simultaneously 91. On the other hand, the critique by Tan et al. suggests that not all "LLM- based" models were truly leveraging the LLM: in their analysis, some models that included large pre- trained transformers did not significantly outperform (or even underperformed) much simpler models after controlling for other factors 44 . This underscores that simply inserting an LLM is not a magic bullet - the surrounding design, training regimen, and data are critical. It' s likely that future foundation models will hybridize ideas (e.g., combining transformer backbones with state- space model components to capture very long- range memory, as hinted by the success of S4 and Mamba state- space models for long sequences 92 ).

The table below summarizes representative methods and their characteristics:  

<table><tr><td>Method (Year)</td><td>Approach Category</td><td>Key Idea and Highlights</td></tr><tr><td>PromptCast 
(2023) 16 48</td><td>Prompting (Zero-shot)</td><td>Templates numerical input/output as text prompts, uses pre-trained LM to generate forecasts. Demonstrated better zero-shot generalization than numeric models, but requires careful prompt design.</td></tr><tr><td>LLMTime 
(2023) 9 21</td><td>Direct Token Prompting</td><td>Encodes time series as a string of digits, leverages GPT-3/LLaMA zero-shot to continue sequence. Matched or beat dedicated models; naturally handles seasonality and missing data via text tokens.</td></tr><tr><td>GPT4TS / One-Fits-All (2023) 23 67</td><td>Fine-tune LLM (Adapter)</td><td>Uses pre-trained GPT-2 as backbone; feeds continuous patch embeddings instead of tokens. Only fine-tunes minimal parameters (positional embeddings &amp;amp; norms) per task. Achieved strong results on forecasting, classification, anomaly detection with one model per task.</td></tr><tr><td>Time-LLM 
(2024) 29 31</td><td>Reprogramming (Frozen LM)</td><td>Aligns time series to language modality by reprogramming inputs as learned text prototypes + natural language prefix. Keeps LLM frozen. Outperforms state-of-art specialized forecasters and excels in few-shot/zero-shot scenarios, proving effective modality alignment.</td></tr><tr><td>Chronos (2024) 34 35</td><td>Pre-trained TS LM</td><td>Tokenizes values (scaling + quantization) and trains a Transformer from scratch on 42 datasets + synthetics. Achieved (a) SOTA on training-domain data, and (b) competitive zero-shot performance on unseen data 35. Simple design (no TS-specific architecture) validated the LM approach for TS.</td></tr><tr><td>ForecastPFN 
(2023) 74</td><td>Pre-trained (Synthetic)</td><td>Trains a transformer entirely on synthetic time series with known patterns, then applies zero-shot to real series. Showed viability of learning from simulation – captured basic seasonal/trend patterns, though limited in variability.</td></tr><tr><td>TimeGPT 
(2023) 38</td><td>Pre-trained (Hybrid)</td><td>Proprietary foundation model (Transformer) trained on diverse real time series. Provides zero-shot forecasting and anomaly detection via API. Incorporates exogenous variables, and an encoder-decoder architecture for multi-series forecasts. Reported to outperform both classical and deep baselines on multiple datasets 39.</td></tr><tr><td>MOMENT 
(2024) 41</td><td>Pre-trained (Multi-task)</td><td>Open-source T5-based model (385M) pre-trained with masked token modeling on a large corpus (“Time Series Pile”). Designed for general-purpose use – demonstrates strong performance on forecasting, classification, and anomaly detection after fine-tuning, highlighting cross-task transfer.</td></tr></table>

<table><tr><td>Method (Year)</td><td>Approach Category</td><td>Key Idea and Highlights</td></tr><tr><td>Lag-LLaMA 
(2024) 42 76</td><td>Pre-trained 
(Univariate)</td><td>Decoder-only model focused on univariate probabilistic forecasting. Uses lagged inputs as covariate tokens and outputs probabilistic predictions (e.g. via likelihood loss). Achieved state-of-art average accuracy across domains, illustrating specialized foundation model success.</td></tr><tr><td>TimesNet 
(2023) 19 20</td><td>Task-General Arch.</td><td>Does not use an LLM, but a novel 2D convolution-based “Inception” block to capture multi-periodicity. Trained as a unified model, it attained SOTA in five different time series tasks, foreshadowing the promise of a single architecture for all TS tasks.</td></tr></table>

Table: Representative methods at the intersection of LLMs and time series, spanning prompt- based zero- shot usage, adaptation of pre- trained LMs, and training large foundation models for time series. 23 9 29 62

Each approach has its own trade- offs. Prompt methods are simple and leverage existing models but might fall short on accuracy or require complex prompt engineering for each new scenario. Full pretraining from scratch requires huge data but yields models tailored to time series specifics (units, temporal semantics, etc.). Hybrid approaches like reprogramming and lightweight fine- tuning aim to use the strength of pre- trained LLMs while bridging the modality gap efficiently. The rapid progress in this area, as evidenced by the flurry of papers in 2023- 2024, indicates an active search for the "sweet spot" that maximizes performance and generality with reasonable data and compute.

## 5 Applications Across Domains

Time series are central in many domains, and LLM- based techniques have begun to make inroads into various application areas:

- **Finance and Economics:** These fields abound with time series (stock prices, economic indicators, cryptocurrency transactions, etc.). Foundation models like TimeGPT have been applied to financial time series forecasting with success, enabling zero-shot predictions for new assets or markets 38. LLMs can also assist in generating narratives from financial time series 
- e.g., translating a stock price chart into a textual summary (useful for automated reporting). An interesting application is using LLMs to detect anomalies or fraud by analyzing patterns in transaction sequences 39. Some works use prompt-based approaches for scenario analysis, e.g. "Given the GDP and inflation trends, how will unemployment change?", leveraging LLMs to combine economic reasoning with data.

- **Retail and Supply Chain:** Demand forecasting for products, inventory optimization, and sales analytics involve time series with seasonality (weekly cycles, holidays) and external factors (promotions, events). LLM-based models pre-trained on many such series can generalize to new products or stores quickly. The ability to incorporate external text (news about a product, descriptions of promotions) via prompts or multi-modality is a boon. For instance, an LLM might take as input: "Product: ACME Widget, Last 12 months sales: [numbers], Description: newly launched eco-friendly line" and forecast the next quarter's sales, combining numeric pattern learning with language understanding of "newly launched" effects.

- Healthcare: Medical and biological data often come as time series 
- vital signs, EEG/ECG signals, patient monitoring data over time. LLMs have been used to generate and interpret medical time series. Auto-TTE' s text-to-ECG is one example where an LLM generates plausible ECG waveforms from a doctor' s textual notes 58, potentially useful for data augmentation or training. Conversely, one could imagine an LLM-based tool that explains an ICU patient' s vital sign trends in plain language for doctors. Foundation models trained on hospital data (e.g. MOMENT including classification tasks) can help detect anomalies like cardiac arrhythmias or classify signals as normal/abnormal. Privacy and data scarcity are concerns here; thus, techniques like masked modeling (SemMTM) that learn from unlabeled data are valuable to utilize the troves of medical sensor data without requiring manual labels.

- Energy and Climate: Power grid load forecasting, solar/wind energy output, weather measurements 
- these are classic time series use cases. Traditionally, domain-specific models and physical models dominate here. LLM-based models are being explored to improve forecasts by learning from vast historical data and capturing unusual patterns (like how an LLM might learn the effect of holidays on electricity demand across regions). In climate science, one intriguing idea is to have LLMs analyze time series data of climate indicators alongside textual climate reports or research papers, to see if the model can align scientific knowledge with data patterns (a form of multi-modal reasoning). While this is in early stages, the ability of LLMs to incorporate context (e.g. "a volcanic eruption happened at time  $X^{\prime \prime}$  as a text prompt with the time series of temperatures) could enhance understanding of causal factors in climate time series.

- Internet of Things (IoT) and Sensors: IoT devices generate streams of data (temperature, humidity, usage metrics, etc.). Anomaly detection for predictive maintenance (e.g. detecting when a machine is behaving abnormally from sensor readings) is a key task. Foundation models like MOMENT, which was trained on heterogeneous time series, explicitly target such scenarios 41. They can potentially recognize anomalies in one context by learning from examples in other contexts. Also, LLMs can serve as a unifying layer to manage multi-modal sensor data: for example, a robot might have vision (images) and time series from accelerometers 
- a large model that can handle both sequential sensor data and textual commands could reason about events (some recent work uses LLMs in robotics to interpret sensor time series in terms of high-level events, by prompt descriptors).

- Operations Research and Management: Many decision processes rely on forecasts (staffing requirements, call volumes, supply chain lead times). LLM-based time series models can integrate with decision support systems. An exciting direction is decision-focused prompts 
- asking an LLM not just "forecast demand" but "given this demand forecast, what is the optimal ordering policy?" letting the LLM perform a chain-of-thought reasoning on top of its forecast. Preliminary research shows LLMs can solve small optimization problems stated in text; combining that with time series understanding could lead to more autonomous operational AI systems.

It is worth noting that while LLM- based approaches bring flexibility, domain- specific validation remains crucial. For high- stakes domains (finance, healthcare), LLM outputs must be thoroughly evaluated for reliability. In some early tests, domain experts found that LLMs sometimes produce forecasts that look plausible but violate known physical constraints or commonsense (a reminder of the "hallucination" issue in LMs). Thus, a recurring theme is the integration of domain knowledge and constraints into LLM- based models - whether via constrained decoding, adding physics- based components, or hybridizing with traditional models as "tools." In applications like energy where physical laws govern the data, purely data- driven LLM models might be augmented with physics- informed layers or by fine- tuning on simulation data that encodes those laws.

## 6 Open Challenges and Future Directions

The marriage of large language models and time series analysis is still in its early days. As the preceding sections illustrate, there have been impressive successes but also sobering contradictions. We now discuss some of the key open challenges and research directions moving forward:

1. Data Scarcity and Quality: Unlike text or images, there aren't enormous public repositories of diverse time series covering everything we might want to model 94. Time series data often reside in silos (e.g., each company's sales, each hospital's patients) due to privacy or proprietary concerns. Moreover, time series datasets can be small and short, limiting the direct applicability of data-hungry LLM techniques. Approaches like synthetic data generation (used in ForecastPFN and Chronos) and self-supervised pre-training (SimMTM, masked modeling) are attempts to overcome limited real data. A future direction is to create large-scale collaborative datasets (similar to "The Pile" for text) - initiatives like the "Time Series Pile" in MOMENT 41 are a start. Ensuring high quality and representativeness in such data is crucial; otherwise, a foundation model might overfit to quirks of the data it sees. Federated learning for time series foundation models is another frontier: how to train a giant model on data spread across many parties without centralizing the data (respecting privacy)? This could unlock healthcare and personal device data for modeling without violating privacy laws.

2. Non-Stationarity and Concept Drift: Time series often violate the i.i.d. assumption - their statistical properties can change over time. An LLM or transformer may struggle if the data distribution at inference is different from training (for instance, a sudden pandemic causes patterns unlike anything seen before). Traditional forecasting addresses this via model retraining or adaptive filters. For LLM-based models, one challenge is efficient adaptation: fine-tuning a huge model frequently is not feasible for most users. Techniques like online learning, lightweight adaptation (e.g., adjusting normalization layers on recent data), or prompts that inform the model of regime changes (e.g., "note: after 2020, pattern may shift due to event X") could be explored. Models like TimeGPT allow fine-tuning on user data 95 - but doing so while preserving the model's general knowledge is tricky. One idea is to have hierarchical models: a fixed global model that provides general trends and a small local model that adjusts to recent deviations (combining them for final output). This remains an open research problem. The concept drift issue also ties to robustness: LLM-based models should be tested against scenarios of structural change (new seasonality, different volatility, etc.) to ensure they don't break catastrophically.

3. Tokenization and Numerical Reasoning: While tokenization enables LMs to handle time series, there is an inherent tension: numbers in time series carry magnitudes and continuous relationships that are not naturally captured by discrete tokens. LLMs are known to have difficulty with precise arithmetic or handling very large/small quantities - they can approximate patterns but might not extrapolate well beyond the range of training data. Advancements in numerical reasoning for LLMs (an active area in NLP) will benefit time series applications. For example, some research injects computational abilities into LLMs (like a calculator tool) for better arithmetic accuracy. In forecasting, an analogous approach might have the model explicitly calculate trends or averages as part of a "chain-of-thought" (just as humans do intermediate calculations). Neural ODEs or continuous-time models could also be combined with LLMs to better represent underlying continuous dynamics. Designing tokenization schemes that preserve more information (e.g., piecewise linear segments as tokens instead of single points) might improve the model's ability to understand slopes and accelerations, not just values.

4. Evaluation and Benchmarks: The field currently lacks consensus on how to best evaluate large models on time series. Traditional metrics (MSE, MAPE, CRPS for probabilistic forecasts, etc.) are used, but these don't capture the full picture when a model is intended to be general-purpose. For instance, how do we assess an LLM's reasoning about a time series or its ability to handle multiple tasks

simultaneously? New benchmarks are needed that test a model's transfer learning: e.g., train on one kind of time series task and test on another. The NeurIPS 2024 workshop is likely to foster benchmark development 96. Additionally, evaluation should include efficiency metrics - if an LLM- based approach uses  $100x$  more compute than a simpler model for a  $0.1\%$  accuracy gain, is it worth it? Tan et al.'s study 44 raises this concern: the practical utility of LLMs in time series might be limited if they incur huge computational or latency costs. Future evaluations should consider memory and speed, especially for real- time applications (e.g., streaming sensor data where decisions must be made quickly).

5. Interpretability and Trustworthiness: In sensitive applications, users often require explanations for forecasts or anomaly alerts. One advantage of prompting LLMs is the possibility of natural language explanations. We can ask the model to justify a forecast, as done in some research 55. However, there's a risk: the model might give a plausible-sounding explanation that isn't the true reason (post-hoc rationalization). Ensuring faithful explanations is an open area. Perhaps a model could be constrained to output which past points it attended to (attention weights as explanation) or which external factors it considered (via prompt engineering like "because the last 3 values increased steadily, I predict..."). There's also the matter of uncertainty communication: probabilistic forecasts are one way, but LLMs could also output confidence in words ("I'm not very certain because the pattern changed recently"). Aligning this with rigorous uncertainty quantification is a challenge. Moreover, safety and fairness issues that plague LLMs in NLP (bias, adversarial inputs) can appear with time series too. An example: an LLM might have learned biases from training data (maybe underpredicting rare spikes or overreacting to trends) that systematically err in certain conditions. Identifying and correcting such biases will be important, especially if these models start informing policy or major decisions.

6. Combining Model-Driven and Data-Driven Approaches: A promising future direction is hybrid models that integrate the strengths of classical time series models (which encode domain knowledge or physical laws) with LLMs (which excel at learning from data). For example, in energy load forecasting, one could imbue the model with the known effect of temperature (via a physics-based equation) and let the LLM handle the residual patterns. Some initial work in physics-informed neural nets could translate here. The blog discussion 92 of state-space models (SSMs) is relevant - SSMs like S4 provide a way to capture long-term dependencies efficiently and have some nice stability properties 92. Integrating SSM components into a transformer (some recent papers do this by replacing certain attention sublayers with SSM layers) might yield models that handle long sequences and irregular sampling better than vanilla transformers. Similarly, auto-regressive statistical models (AR, ARIMA) can be seen as 1-layer networks; LLMs could take their outputs as features or enforce their structure for long-term consistency (preventing the kind of drifting sometimes seen in pure neural nets).

7. Domain-Specific Customization: While foundation models promise generality, domain-by-domain customization will likely remain important. An LLM forecasting weather might need to respect conservation laws (mass, energy) - one could enforce those via constraints or by multi-task training with simulation data. In finance, regulatory requirements might demand that models not use certain data or be explainable in specific ways. So, adaptation of foundation models to particular industries (perhaps via fine-tuning on domain data with relevant context) will be a practical necessity. The concept of prompt libraries for different domains (like a repository of prompts that are known to work well for energy vs. retail) could help practitioners leverage LLMs without starting from scratch each time.

8. Scaling and Efficiency: Finally, the size of models is a double-edged sword. Large models can capture more complex patterns but are hard to deploy. Research into efficient transformers (e.g., Linformers, LongT5) and compression (distilling a giant model into a smaller one) will be crucial for real-world adoption. Perhaps a workflow where a giant model is used offline to generate forecasts or augment data, and a distilled model runs online for real-time predictions, could combine accuracy and speed. Also,

hardware- friendly models (quantized weights, etc.) are needed at edge devices if one wants to run an LLM- based anomaly detector on, say, a sensor hub in a factory.

In summary, while LLM- for- Time- Series has made great strides, there is much work ahead to fully realize its potential. The intersection is rich with possibilities: every advance in language models (like better context handling or reasoning) could inspire new time series methods, and every unique challenge of time series (like concept drift or irregular sampling) might spur innovations that feedback into the broader machine learning field.

## 7 Conclusion

The convergence of large language models and time series analysis represents an exciting new chapter in machine learning. We have surveyed how concepts born in NLP - from transformers and self- attention to prompting and few- shot learning - are being adapted to model the dynamic, complex behaviors found in time series data. This survey covered the evolution from early attempts (applying RNNs and basic attention to time series) through to the latest foundation models that treat time series as a "language" to be learned. We discussed key methodologies, including direct prompting of pretrained LLMs for forecasting 52, encoding time series into discrete token sequences for language model training 34, aligning time series with LLMs via reprogramming and prompts 29, and building large pre- trained models dedicated to time series tasks 88. Throughout, we highlighted successes - such as LLMs demonstrating zero- shot prediction capabilities on novel series 56 and foundation models achieving broad cross- domain performance 97 - as well as challenges - such as cases where removing the LLM component did not reduce accuracy 44, emphasizing the need for careful validation of when LLMs truly help.

It is clear that large models bring new abilities to time series analysis: the ability to transfer learning across diverse series, to incorporate context in versatile ways, and to output not just predictions but also narrative insights. The historical trend suggests an increasing unification of approaches - the lines between time series models, NLP models, and even vision models are blurring as transformers become a general- purpose architecture for sequence data. We are also witnessing a re- thinking of longstanding problems: concepts like zero- shot forecasting 32 or prompt- based anomaly explanation were practically unheard of a few years ago in the time series community.

However, this is not the end of the road; it is the beginning. Future progress will likely come from interdisciplinary efforts that bring together time series experts, NLP researchers, and domain specialists. The roadmap ahead includes creating richer datasets and benchmarks, developing models that are not just large but also smart about the unique characteristics of temporal data, and ensuring that these models can be trusted and understood by human decision- makers. If these challenges are met, large language models (and their descendants) might become standard tools in the time series toolkit - not replacing traditional methods outright, but augmenting them and opening up new frontiers (like conversational analytics of time series, or automated insight generation).

In conclusion, large language models offer a powerful new lens to view time series problems. They encourage us to think of time series not just as points on a timeline, but as information streams that can be read, interpreted, and generated much like language. By leveraging this analogy while respecting the differences, the community can move towards more general, robust, and intelligent time series systems. The synergy between LLMs and time series is a prime example of how progress in one field of AI can catalyze breakthroughs in another - a trend that will likely continue as AI systems become more unified and general. The coming years will reveal whether foundation models truly herald a paradigm shift in time series analysis, or whether they will be one tool among many. Either way, the journey of combining

LLMs with time series has undoubtedly broadened our horizons and will continue to spur innovative research at this fascinating intersection of sequences, semantics, and time.

Acknowledgements: This survey was informed by numerous original research works in both the time series and NLP communities. We endeavored to cite primary sources to give credit to foundational developments and recent breakthroughs in this domain. Due to space, we could not cover every relevant paper; the omission of any specific work is not a sign of its insignificance. We encourage readers to explore the cited literature for deeper technical details and to follow emerging papers and benchmarks (e.g., from recent conferences and the NeurIPS 2024 workshop) as this field evolves rapidly.

## References

1 2 3 6 7 8 10 11 22 23 24 32 34 35 36 37 60 61 62 63 65 67 68 74 75 77 78 79 89 90 [2403.07815] Chronos: Learning the Language of Time Series https://ar5iv.labs.arxiv.org/html/2403.07815

4 5 80 Empowering Time Series Analysis with Large Language Models: A Survey https://www.ijcai.org/proceedings/2024/0895. pdf 9 21 51 52 53 54 55 56 57 59 69 94 openreview.net https://openreview.net/pdf?id=md68e8iZK1

12 Temporal Fusion Transformers for interpretable multi- horizon time ... https://www.sciencedirect.com/science/article/pii/S0169207021000637

13 Enhancing the Locality and Breaking the Memory Bottleneck of ... https://papers.nips.cc/paper/8766- enhancing- the- locality- and- breaking- the- memory- bottleneck- of- transformer- on- time- series- forecasting

14 15 33 38 39 41 42 46 84 86 88 91 92 96 97 The Rise of Foundation Models in Time Series: A Paradigm Shift or Just Another Hype?

https://www.videns.ai/en- ca/blog/lessor- des- modeles- fondamentaux- dans- les- series- temporelles- un- changement- de- paradigm- e- ou- juste- un- autre- engouement

16 17 47 48 49 50 [2210.08964] PromptCast: A New Prompt- based Learning Paradigm for Time Series Forecasting https://arxiv.org/abs/2210.08964

18 19 20 [2210.02186] TimesNet: Temporal 2D- Variation Modeling for General Time Series Analysis https://arxiv.org/abs/2210.02186

25 [PDF] Prompt- based Large Language Model for Multimodal Time- series ... https://ojs.aiai.org/index.php/AAAI/article/view/30383/32447

26 58 Multi- Modal Forecaster: Jointly Predicting Time Series and Textual ... https://arxiv.org/html/2411.06735v1

27 [PDF] arXiv:2402.01801v3 [cs.LG] 6 May 2024 http://dar.ucsd.edu/pubs/Xiyuan_JCAI2024_LLMTIME.pdf

28 29 30 31 81 82 [2310.01728] Time- LLM: Time Series Forecasting by Reprogramming Large Language Models https://arxiv.org/abs/2310.01728

40 95 About TimeGPT - TimeGPT Foundational model for time series forecasting and anomaly detection https://www.nixtla.io/docs/introduction/about_timegpt

43 44 45 [2406.16964] Are Language Models Actually Useful for Time Series Forecasting? https://arxiv.org/abs/2406.16964

64 [PDF] Large Language Models for Time Series: A Survey - IJCAI https://www.ijcai.org/proceedings/2024/0921. pdf

66 [PDF] Are Language Models Actually Useful for Time Series Forecasting? https://proceedings.neurips.cc/papers/files/paper/2024/file/6ed5bf446f59e2c6646d23058c86434b- Paper- Conference.pdf

70 83 Aligning Pre- Trained LLMs as Data- Efficient Time- Series Forecasters https://www.semanticscholar.org/paper/LLM4T5%3A- Aligning- Pre- Trained- LLMs- as- Data- Efficient- Chang- Peng/ 003de4944abc1e1d3c63f9f34a0c513033d16

71 A novel LLM time series forecasting method based on integer ... https://www.nature.com/articles/s41598- 025- 06581- x

72 MOMENT: A Family of Open Time- series Foundation Models - arXiv https://arxiv.org/html/2402.03885v2

73 [PDF] MOMENT: A Family of Open Time- series Foundation Models - GitHub https://raw.githubusercontent.com/mlresearch/v235/main/assets/goswami24a/goswami24a.pdf

76 [2310.08278] Lag- Llama: Towards Foundation Models for Probabilistic Time Series Forecasting https://arxiv.org/abs/2310.08278

85 LLMs for Time Series Forecasting - Medium https://medium.com/@kyle- t- jones/lms- for- time- series- forecasting- 59ae5f5ceecc

87 Foundation Models for Time Series: A Survey https://arxiv.org/html/2504.04011v1

93 Large Language Model Performance in Time Series Analysis https://medium.com/data- science/large- language- model- performance- in- time- series- analysis- 4d274b480e24