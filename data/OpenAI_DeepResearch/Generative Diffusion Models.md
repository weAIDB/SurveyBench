# Generative Diffusion Models: Foundations, Advances, and Applications

## 1 Introduction

Generative diffusion models are a class of deep generative models that synthesize data (e.g. images, audio) by progressively destroying and then restoring data structure through a noise process. Inspired by non- equilibrium thermodynamics, these models define a forward diffusion that gradually adds random noise to data until only noise remains, and a learned reverse diffusion that iteratively removes noise to reconstruct data samples 1 . Intuitively, one can imagine dropping ink into water: as it diffuses, the image becomes completely diluted (noise); a diffusion model learns to reverse this process and "denoise" pure noise into coherent data 1 . This approach to generative modeling has risen to prominence for producing high- quality outputs in image generation and other domains, rivalling or surpassing earlier methods like GANs and VAEs in fidelity and diversity. Diffusion models avoid adversarial training and restrictive architectures, offering more stable training and mode coverage than GANs, while still achieving state- of- the- art sample quality. This survey provides a comprehensive overview of generative diffusion models: we cover the formal foundations, key methodological advances, a historical timeline of milestones, and a broad range of applications. We also discuss recent developments (up to 2024) and open challenges such as improving generation speed, controllability, and ethical considerations. The goal is to present both the rigorous underpinnings and the practical innovations of diffusion models, in a clear structure accessible to newcomers and useful to experienced researchers alike.

## 2 Background: Diffusion Model Formulation

Diffusion probabilistic models (also known simply as diffusion models or denoising diffusion models) operate by specifying two processes: (1) a fixed forward (noising) process \(S Q S\) that gradually destroys data by adding noise over \(\) 1\\(\) time steps, and (2) a learnable reverse (denoising) process \(S p_{- }\) \theta that attempts to recover data by removing noise step by step 2 3 . Formally, let \(S x_{- }0S\) be a sample from the true data distribution. The forward process produces a sequence \(S x_{- }1,\) k_2, \dotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdotsdots\dots\) where \(S x_{- }T\) is nearly pure noise. A common choice is to use a Markov chain with Gaussian transitions:

\(\) 5q(x_{- }t\backslash\mathrm{mid}x_{- }\{t- 1\})=\backslash\mathsf{mathcal{N}}\{N\}!\backslash\mathsf{big}(x_{- }t;\backslash\mathsf{sqrt}\{1\cdot\backslash\mathsf{beta}_{- }t\}\backslash,x_{- }\{t- 1\}),\backslash\mathsf{beta}_{- }t\backslash\mathsf{big}),\backslash\mathsf{quad}t=1,\backslash\mathsf{dots},T\backslash\mathsf{S}\)

with a small variance  $\) 1\(beta_t$  at each step. By compounding these steps, one can sample  $S x_{- }t$  in closed form as a noisy version of the original data:

\(\) 5q(x_{- }t\backslash\mathrm{mid}x_{- }0)=\backslash\mathsf{mathcal{N}}\{N\}!\backslash\mathsf{big}(x_{- }t;\backslash\mathsf{sqrt}\{1\cdot\backslash\mathsf{beta}_{- }t\}\backslash,x_{- }0,\backslash;(1\cdot\backslash\mathsf{bar}\backslash\mathsf{alpha}_{- }t)\backslash\mathsf{big}),\)

where \(\) 1\backslash\mathsf{alpha}_{- }\mathsf{t}:=1\cdot\backslash\mathsf{beta}_{- }\mathsf{t}\mathsf{S}\(and\)\\(\) \(\alpha_{- }\) \alpha_{- }t := \(\backslash \mathsf{prod}_{- }\{\mathsf{i} = 1\}^{\wedge}\) \alpha_{- }t. In the limit of large \(\) 1\mathbb{S},\mathbb{S}\(\)\\(\) \(\alpha_{- }\) \alpha_{- }t := 1 \(\backslash\mathsf{beta}_{- }\mathsf{t}\) \(\mathbb{S}x_{- }T\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{+}\(\alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }t := 1 \(\backslash\mathsf{beta}_{- }\mathsf{t}\) \(\mathbb{S}x_{- }T\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\)

\(\) 5p_{- }\( \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{- }\) \alpha_{\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- }\alpha_{- } \)

with learned mean (and optionally variance) functions. Because the true reverse transition  $\{q(x_{- }[t - 1])\} \text{mid} x_{- }t\}$  is intractable to compute (depends on the whole dataset), the model is trained to approximate it.

Training Objective: Ho et al. (2020) showed that diffusion models can be trained by maximizing the evidence lower bound (ELBO) on the data likelihood, which simplifies to a weighted variance loss that is equivalent to denoising score matching objectives 4 . In practice, a common training objective is to have the neural network predict the additive noise \(\{e\) epsilon\\(at each step. At training time we sample a random\)t\\(and a noise\)\\(epsilon\\(epsilon\) generate a noised sample \(\) x_{- }t=\(sqrt\)\backslash\(\)t\(\)x_{- }0+\(sqrt\)\{1-\(bar\)\backslash\(\)epsilon\\(and train the network to output\)\\(\epsilon\) epsilon\\(or\)\\(x_{- }0\)\\()from\)\\(x_{- }t\) 5. This denoising objective directly optimizes \(\{e\) mathbb[E]\(\epsilon epsilon\)\backslash\(\)theta(x_t,t)- \(\epsilon epsilon\)\backslash\(\) By learning to predict the injected noise, the model learns the score function (gradient of log- density) of the perturbed data distribution. Notably, score- based models can be trained without needing to compute explicit likelihood normalizing constants, thanks to score matching. Equivalently, one can view diffusion model training as learning a time- dependent score network \(\) x_{- }t\(\)\backslash\(\)approx\(\nabla n a b l a_{- }f x)\backslash \log q(x_{- }t)\text{mid}\backslash \text{t e r t}\(\)\backslash\(\)

Model Architecture: In image diffusion models, the neural network \(\{e\) epsilon\(\backslash\)theta\\(or\)\\(\mu_{- }\)\(\theta\)eta\\(is typically implemented as a U- Net convolutional neural network with skip connections 6 . The U- Net processes the noisy image\)\\(x_{- }t\) through multiple downsampling and upsampling layers, and it incorporates the timestep \(\) t\\(via an embedding (often a sinusoidal positional encoding) injected into the network layers 7 . This architecture, adapted from image segmentation, was critical in early diffusion models (DDPM) to achieve high- quality results 6 . Diffusion U- Nets have since been extended with attention mechanisms (to better model global image structure) and with various conditioning inputs (for guided or conditional generation, discussed later). The combination of a U- Net backbone and simple Gaussian diffusion process yields a highly flexible model family: diffusion models place virtually no restrictions on network architecture (unlike normalizing flows or autoregressive models), beyond the need to process the noisy input.

Sampling (Inference): After training, image generation is done by ancestral sampling: start from  $\) x_{- }T\($ \backslash $\)sim \(\mathsf{\backslash mathcal{(N)}(0,1)}$ \ $and for$ t=T\ $down to$ \ $1$  sample  $\) x_{- }[t- 1]\($ sin p_ $\theta$ \backslash $\theta$ \backslash $\theta$ \backslash $\theta$ \backslash $\theta$ \backslash $\theta$ \backslash $\theta$ \backslash $\theta$ \backslash $\theta$ \backslash $\theta$ \backslash $\theta$ \backslash $\theta$ \backslash $\theta$ \backslash $\theta$ \backslash $\theta$ \backslash $\theta$  eventually producing  $\) x_{- }0\\(as an synthetic data sample 4 . This reverse diffusion process iteratively refines pure noise into a coherent image, analogous to an iterative denoising autoencoder 8 . Crucially, each reverse step is much simpler than generating an image in one pass; as Ho et al. note, transforming noise directly into a clear image is very difficult, but transforming a slightly less noisy image into a slightly clearer image is much easier. This stepwise refinement underpins the success of diffusion models. However, one drawback is that sampling requires$ \\(\) forward passes of the network (e.g. 50 to 1000 steps), making generation computationally expensive compared to one- shot generators 9 10 . We will later discuss advances to speed up sampling.

![](images/6ec456c2a7e3ca5c9b93387dede1d9102fdba29ed49a2808bcb46f4d0016da0c.jpg)

Illustration of the diffusion process as a Markov chain of forward noising and reverse denoising steps. Starting from an initial data sample \(\\ (x\_ 0\) \) (rightmost, e.g. an image of a face), the forward process (solid arrow, right- to- left) adds noise over many steps to reach an almost- uniform noise \(\\ (x\_ T\) \) (leftmost). The reverse generative model \(\\ (p\_ \theta\) \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \) learns to invert this process, gradually removing noise (dashed arrow) to synthesize a sample \(\\ (\theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta \theta\) that resembles the original data \(11\). (Diagram based on Ho et al. 2020. )

## 3 Historical Timeline of Diffusion Models

Diffusion models emerged from a convergence of ideas in probabilistic modeling and score- based generation. Below is a timeline of major milestones and developments in the field:

- 2015 
- Diffusion Probabilistic Modeling: Sohl-Dickstein et al. introduced the first diffusion-based generative model, describing a process that "slowly destroys structure in a data distribution" through forward diffusion and then learns to reverse it. This pioneering work demonstrated the concept on simple images, but results were limited in quality compared to contemporary GANs. It established the foundation of using a Markov chain of noise addition and removal as a tractable, flexible generative model.

- 2019 
- Noise-Conditioned Score Networks (NCSN): Song and Ermon proposed score-based generative modeling in which a neural network is trained to estimate the gradient of the data distribution density (the score) at various noise levels. By perturbing data with Gaussian noise and applying score matching, they could generate samples via iterative Langevin dynamics. Their model achieved GAN-level image quality on CIFAR-10 (Inception score 8.87) without adversarial training  $11$ $12$ , showcasing the promise of diffusion-like approaches. This work introduced the idea of multi-scale noise conditioning, which was key to high-quality results.

- 2020 
- Denoising Diffusion Probabilistic Models (DDPM): Ho, Abdal, and colleagues unified diffusion probabilistic modeling with score matching in their influential DDPM paper  $4$ . They showed that maximizing the ELBO for a diffusion model is equivalent to a weighted score matching loss, thereby connecting the NCSN approach with classical diffusion. DDPM was the first to produce high-resolution, photorealistic images with diffusion models, by using a U-Net architecture with skip connections and self-attention to predict noise at each timestep  $6$ . Notably, DDPM demonstrated that diffusion models could match or exceed GAN image quality on certain benchmarks when carefully tuned. This work spurred enormous interest by overcoming the quality gap and proved diffusion models as serious contenders in generative modeling.

- 2021 
- Theoretical Advances and GAN Parity: In early 2021, Song et al. introduced a continuous-time formulation using Stochastic Differential Equations (SDEs), unifying discrete diffusion and score-based models under one framework  $13$ $14$ . They derived a reverse-time SDE (and an equivalent ODE) whose solution yields samples, enabling exact likelihood computation and new sampling methods  $15$ . Using this framework with a "predictor-corrector" sampler, they achieved record FID scores on CIFAR-10 (FID 2.20) and even generated  $1024 \times 1024$  images for the first time with a score-based model  $16$ . In mid-2021, Dharwal and Nichol from OpenAI showed "Diffusion Models Beat GANs" on image synthesis by finding improved architectures and introducing classifier-guided diffusion. They reported FID scores on ImageNet that surpassed BigGAN, marking the first time a likelihood-based model outperformed GANs in image quality at scale. This period established diffusion models as the new state-of-the-art in generative image modeling.

- 2021 – Fast Sampling Improvements: Also in 2021, researchers tackled the slow sampling of diffusion models. Nichol & Dhariwal introduced an improved noise schedule (cosine schedule) that preserved quality with far fewer timesteps (e.g. 50 instead of 1000) 17. Kingma et al. and Song et al. proposed Denoising Diffusion Implicit Models (DDIM), a method to generate samples in fewer steps by defining a non-Markovian reverse process compatible with the DDPM training objective 18 19. DDIM sampling is deterministic (trading some diversity for speed) and can produce decent images in e.g. 100 steps even if the model was trained with 1000 steps 18 19. These advances began to alleviate the inference speed bottleneck.

- 2022 – Latent Diffusion and Generative Media Boom: A significant breakthrough in efficiency came with Latent Diffusion Models (LDM) by Rombach et al. 20 211. Instead of diffusing in pixel space, LDMs operate in a lower-dimensional latent space learned by an autoencoder, dramatically reducing computational cost. This enabled high-resolution synthesis (e.g. $512\times times512$ or larger) on limited compute, and also allowed plug-and-play conditioning by adding cross-attention in the U-Net to handle text or other inputs 22. The LDM approach underlies the public release of Stable Diffusion in 2022, a text-to-image model that was trained on a massive image dataset and made available openly. Stable Diffusion (v1.4) demonstrated that diffusion models can generate diverse, artistic, and photorealistic images from text prompts on consumer GPUs, sparking widespread adoption in creative communities. Around the same time, OpenAI’s GLIDE model pioneered text-guided diffusion using CLIP embeddings, and achieved excellent photorealism with a classifier-free guidance strategy (showing humans prefer its outputs over the original DALL-E) 23 24. OpenAI’s DALL-E 2 (2022) and Google’s Imagen (2022) further improved text-to-image generation using diffusion models, reaching unprecedented fidelity. DALL-E 2 combined a diffusion prior (mapping text to image embeddings) with a diffusion decoder, while Imagen leveraged large language models for text encoding and cascaded diffusion up samplers, producing high-definition images. By late 2022, diffusion models had firmly taken the lead in generative AI for images, with robust performance across unconditional generation, text-conditional generation, inpainting, and super-resolution 25.

- 2023 – Extensions and New Frontiers: Research in 2023 focused on pushing diffusion models further and addressing remaining limitations. Controllable generation became a hot topic: the ControlNet architecture introduced the ability to condition diffusion models on additional inputs (like edge maps, poses, sketches) by augmenting the U-Net with trainable “control” branches, allowing spatial and structural conditioning without retraining the original model 26 27. This enabled fine user control over composition beyond text prompts. New sampling schemes emerged such as Consistency Models (Song et al. 2023), a family of generative models that can produce images in as little as one step by learning a direct noise-to-data mapping, while still supporting multi-step refinement for higher quality 28 29. Consistency models achieved one-step image quality on CIFAR-10 and ImageNet comparable to diffusion models with many steps 30, pointing toward real-time diffusion-based generation. Diffusion models were also extended to video: for example, Google’s Imagen Video demonstrated a cascade of diffusion models generating coherent video clips (e.g. 5-second 128-frame videos at 24 fps) from text descriptions 31 32. Maintaining temporal consistency is challenging, but progress is rapid 31. Beyond imagery, audio diffusion continued to advance (e.g. Stability AI’s 2023 release of Stable Audio for music generation), building on prior audio models like DiffWave. Researchers also applied diffusion models in domains like 3D synthesis (generating point clouds, implicit surfaces or NeRFs via diffusing in shape spaces) and molecular design (diffusing in chemical space to propose new molecules). By 2024, diffusion models have become a versatile backbone for generative AI across modalities, while ongoing research works to overcome their computational costs and to integrate them with other AI systems (such as combining with large language models for multi-modal generation).

## 4 Key Methodological Advances

Key Methodological AdvancesIn this section, we delve into important techniques and variants that have defined the evolution of diffusion models, from training improvements to conditional generation and speedups.

### 4.1 Variance Scheduling and Model Improvements

Noise Schedule: The choice of noise variance schedule \(\{(\backslash \mathsf{beta\_t})\}\) significantly impacts model performance. The original D2PM used a linear schedule (betas increasing linearly from a small value to a larger value by \(\) 15\(17. Nichol & Dhariwal (2021) found that a cosine schedule (betas increasing according to a cosine schedule) could concentrate more diffusion steps on the difficult early denoising phases and improve sample quality 17 . This change allowed them to reduce\)\\(15\) from 1000 to as low as 100 steps with only minor loss in image fidelity 17 . In general, larger noise increments at later timesteps can be used because the image is already very noisy, whereas smaller increments early on preserve more signal. Learning the variance is another extension: instead of fixing \(\) 1\(\Sigma \mathsf{gma\_theta}\)\(, one can have the model output or infer an optimal variance at each step. Kingma et al. (2021) and Nichol et al. (2021) showed that learning an appropriate variance (or weighting the ELBO terms) can improve likelihood and sometimes allow fewer sampling steps 33 34 . These innovations make diffusion training more efficient and robust.

Model Architecture Tweaks: While the basic U- Net with attention is standard, various tweaks have improved results. OpenAI's 2023 model introduced adaptive Group Normalization (AdaGN) to better incorporate the time embedding into feature maps (similar to conditional BatchNorm in GANs). Higher network capacity (more channels, layers) was also crucial: GLIDE used a 3.5 billion parameter U- Net for its best text- to- image results. 24 . Furthermore, classifier guidance (described below) effectively acts as a modification at sampling time to trade diversity for fidelity, pushing image quality higher. As diffusion models scaled up, researchers added hierarchies of U- Nets for different resolutions (cascaded diffusion) to handle high- res image synthesis. The latent diffusion approach can be seen as an architectural change too: using an autoencoder front- end (encoder and decoder) to work on compressed latent images reduces the U- Net's workload 35 . By 2022, diffusion architectures also integrated cross- attention mechanisms for conditioning on text or other modalities: each U- Net block attends to text embeddings (from a model like CLIP or TS) so that the generative process can be guided by text prompts 22 . This was vital for models like Imageh and Stable Diffusion to achieve tight alignment with prompts. Overall, architectural innovations, from normalization and attention improvements to multi- scale pipelines, have continually boosted the performance of diffusion models.

### 4.2 Conditional & Guided Generation

Many applications require conditional generation - generating samples \(\) 5\(given some input\)\S_{\mathrm{y}}S\(which could be a class label, text description, image mask, etc.). Diffusion models support this through a few key techniques:

Classifier- Guided Diffusion: Proposed by Dhariwal & Nichol (2021), this method uses an external classifier \(\) C(y\backslash\mathrm{mid}x\_ t,t)\S\(at sampling time to guide the reverse process. Intuitively, one modifies the predicted mean\)\S\backslash\mathrm{mu\_theta(x\_t,t)}\S\(by adding the gradient of the log - probability\)\S\(nabla_{- }f_{x_{- }t})\log C(y\backslash\mathrm{mid}x_{- }t)\S\(, which nudges the sample in a direction that increases the probability of the desired label\)\S_{\mathrm{y}}S\(. By varying a guidance scale\)\S\mathrm{w}\S\(, one can trade off diversity for fidelity: higher\)\S\mathrm{w}\S$ means the sampler follows the classifier more strictly, producing more accurate but potentially less varied samples. Classifier guidance enabled class- conditional ImageNet generation with unprecedented fidelity, at the cost of requiring a separate classifier

model. For example, with guidance they achieved an FID of 3.85 on ImageNet  $\) 512\(\times 512$ , dramatically better than without guidance.

- Classifier-Free Guidance: To avoid the overhead of training a separate classifier, Ho & Salimans (2021) introduced classifier-free guidance, which OpenAI leveraged in GLIDE and DALL-E 2 23. In this approach, the diffusion model is trained jointly for conditional and unconditional generation by occasionally dropping the condition \(\) y\(\) during training (e.g. a certain percentage of timesteps, the model is fed a null condition). At sampling time, one can generate two predictions for a given step: \(\) \(\epsilon_{\text{epsilon\_theta(x\_t\_mid y)}\)\) and \(\) \(\epsilon_{\text{epsilon\_theta(x\_t\_mid text(null)}\)\), and interpolate between them to guide the sample: \(\) \(\texttt{tilde}\)\(\epsilon_{\texttt{epsilon\_theta(x\_t\_mid y)} - w}\), \(\texttt{epsilon\_theta(x\_t\_mid text(null)}\)\). This effectively boosts the influence of the condition without an external classifier 23. Classifier-free guidance became the go-to method for text-to-image diffusion models like Stable Diffusion, since it only requires one model and can handle arbitrary text prompts 36. Human evaluations have shown that classifier-free guided samples are preferred for fidelity and relevance to the prompt over earlier methods 23.

- Other Conditional Inputs: Beyond labels and text, diffusion models can be conditioned on images or features in various ways. For example, inpainting is achieved by conditioning on a masked image: the model is trained to only denoise the unknown regions while keeping known pixels unchanged (often implemented by concatenating the mask and the masked image as extra channels) 37. Super-resolution diffusion models (SR3 by Google, 2021) condition on a low-resolution image and generate a higher-resolution version by treating the low-res image as part of the input to the U-Net. More recently, ControlNet by Lvmin Zhang et al. (2023) provides a general way to add conditioning on edge maps, sketches, human poses, depth maps, etc., by attaching a parallel "control" network to a frozen diffusion model 26 38. The control network learns to drive the generation according to the additional input while the main model weights remain unchanged, which preserves the original model's diversity and quality but adds the ability to enforce spatial constraints 26 27. This proved extremely effective for guided image synthesis 
- e.g. generating an image that exactly follows a given outline or pose, something not possible with text alone 39. The general lesson is that diffusion models are flexible in conditioning: one can concatenate conditions to the input, add them through cross-attention, or guide sampling with gradients. This flexibility extends to domains like audio (e.g. generating speech given a mel-spectrogram as condition 40) and beyond.

### 4.3 Acceleration of Sampling

A persistent challenge for diffusion models is the speed of the reverse sampling, since it may require hundreds of sequential steps. A number of strategies have emerged to accelerate generation:

- Non-Markovian Samplers (DDIM): As mentioned, DDIM allows one to use fewer steps than the training process by defining a deterministic mapping that "skips" steps while still approximately following the trained model's denoising trajectory 18 19. In practice, one can often generate decent images in 50 or 100 DDIM steps even if the model was trained with 1000 steps, offering a  $10\times$  speedup at some cost to sample diversity. DDIM essentially shows that the reverse process need not mirror the exact forward process if we are willing to sacrifice exact likelihood ties, providing a family of implicit samplers for diffusion models 18.

- Analytical Solvers: The formulation of diffusion in continuous time (SDE/ODE) has enabled use of numerical solvers (like high-order ODE integrators) to take larger steps. For example, DPM-Solver (Lu et al. 2022) is a dedicated solver for diffusion ODEs that can generate high quality samples in

as few as 10–20 steps by using higher- order integration techniques and carefully handling the linear and nonlinear parts of the dynamics. These solvers treat the denoising process like solving a differential equation, and have been shown to be much more efficient than naive ancestral sampling.

- Knowledge Distillation: Another approach is to train a separate model to imitate the multi-step sampling in fewer steps. Salimans & colleagues (2022) introduced progressive distillation, where a diffusion model with e.g.  $\mathsf{ST} = 1000\mathsf{S}$  steps is distilled into a model with  $\mathsf{ST} = 100\mathsf{S}$  steps, then into one with  $\mathsf{ST} = 50\mathsf{S}$ , and so on, by training the student to produce the same result in  $\mathsf{ST}$  steps as the teacher produces in  $\mathsf{ST}$  steps. This yielded models that can sample in only 1 or 2 steps (similar to consistency models) while maintaining reasonable image quality. Song et al.'s Consistency Models take a related but distinct approach: they directly train a model to output  $\mathsf{ST} = 0\mathsf{S}$  from any intermediate noised state, enforcing a consistency condition that those outputs remain consistent if you further perturb and denoise them 28 29. The result is a single network that can generate in one step or refine in a few steps if needed. They achieved one-step FID scores of 6.20 on ImageNet  $\mathsf{S64}$  times64\$, which is state-of-the-art for one-step non-adversarial models 41.

- Faster Architectures: Some research looks at making the model faster per step: e.g. using smaller U-Nets with adaptive computation, or neural approximation of multiple denoising steps at once. There are also techniques to cache and reuse computations across diffusion steps (since adjacent steps operate on similar images). While these are not yet standard, they point to future gains in efficiency.

- Latent-Space Diffusion: As discussed earlier, one of the biggest speedups came from diffusing in a compressed latent space (as Stable Diffusion does) 35. By operating on, say, a  $\mathsf{S64}$  times64 $\) \mathsf{S}\(latent instead of a$ \mathsf{S256} $\)times256\($ times35$ image, the diffusion model has far fewer pixels to model, and each denoising step is much faster. The decoder then upscales the final output to full resolution. This dramatically cuts computation and memory, effectively making high-resolution diffusion practical on consumer hardware 42 25. The trade-off is that the VAE decoder can introduce slight blurriness or artifacts, but a well-trained autoencoder preserves detail remarkably well 43.

Through these advances, sampling that used to take many minutes can now be done in a few seconds or less for a  $\mathsf{S512}$  times512\(\) image on modern GPUs. Nonetheless, diffusion models are still generally slower than GANs or autoregressive models for a given output size, and closing this gap remains an active area of research.

## 5 Applications Across Domains

Although first popularized for image synthesis, generative diffusion models have proliferated into a wide array of domains and tasks, often achieving state- of- the- art results. We highlight some prominent applications:

- Image Generation and Editing: This is the flagship application. Diffusion models can generate stunning unconditional images (from landscapes to faces) that are often indistinguishable from real images. Beyond unconditional generation, text-to-image models like Stable Diffusion, DALL-E 2, and Imagen have captured the public imagination by allowing users to create images via natural language prompts. These models are trained on billions of image-text pairs and produce high-fidelity art, illustrations, and photorealistic images given a prompt. Diffusion models handle complex scenes with multiple objects better than GANs in many cases, likely due to their mode-

covering training objective and the guidance strategies that keep them on track. They also excel at image- to- image translation tasks: e.g. inpainting, where parts of an image are generated given the rest as context. A diffusion inpainting model can fill in holes or replace regions in a manner consistent with the image, yielding seamless edits 37 . Super- resolution is another example; Nichol and Dhariwal (2021) built diffusion upsamplers that take a low- res image and generate a higher resolution one, achieving higher fidelity than previous upsampling methods. More generally, any image transformation that can be posed as conditional generation (colorization, denoising, uncropping) has been tackled with diffusion models. For instance, diffusion- based image restoration can produce remarkably clean results from noisy or compressed images, by leveraging the learned distribution of clean images.

Audio and Speech: Diffusion models have been applied to raw audio waveforms and spectrogram representations. DiffWave (Kong et al. 2020) is a diffusion probabilistic model for speech waveform synthesis (a neural vocoder) 40 . It demonstrated that a diffusion model could match the quality of the best autoregressive model (WaveNet) in converting mel- spectrograms to audio, with a MOS (mean opinion score) of 4.44 vs WaveNet' s 4.43, while also synthesizing much faster than autoregressive sampling 44 . DiffWave and related models (e.g. WaveGrad, 2021) essentially perform high- quality speech generation by iteratively denoising white noise into a clear speech waveform, conditioned on an input spectrogram. Diffusion models have also been used for speech enhancement (removing noise from recordings) and audio super- resolution in a similar denoising framework. Beyond speech, researchers have applied diffusion to music generation (generating waveforms or spectrograms for music clips) and to audio effects (e.g. generating realistic environmental sounds). In 2022, Google' s Grad- TTS used a diffusion model for text- to- speech, achieving natural sounding speech with the benefit of controllable synthesis speed. By 2023, Stability AI' s Stable Audio applied latent diffusion to music generation, enabling creation of short music clips from text descriptions (with the model guided by text embeddings and optionally melodic conditioning). The ability of diffusion models to model long- range coherence (thanks to self- attention and many iterative steps) makes them well- suited to audio, which has long temporal dependencies.

Video Generation: Extending diffusion models to video is challenging due to the high dimensionality (an additional temporal dimension) and the need for temporal consistency. Yet, progress has been rapid. As noted, Imagen Video and others use a cascade of diffusion models: first generate a low- resolution, low- frame- rate video, then successively refine it (increase resolution and frame rate) with conditional super- resolution diffusion models 32 . In late 2022, Ho et al. (the original DPPM author) published Video Diffusion Models (VDM), which adapted the 2D U- Net into a 3D U- Net (2 spatial dims + 1 temporal dim) and achieved coherent moving images on short video clips 45 . They demonstrated unconditional video synthesis of simple actions and conditional video prediction (continuing a given video) using the diffusion approach 45 . Another approach, akin to latent diffusion, is to inflate a pre- trained image diffusion model into a video model by adding temporal convolution or attention layers, leveraging the strong spatial generative ability of image models and focusing on learning temporal dynamics. While current diffusion video models can produce a few seconds of reasonable video, improving the length and fidelity (especially for complex scenes) is an open research frontier. The compute cost is also a hurdle - video models can be quadratically heavier than image models (imagine 128 frames of $256\times times256$ images). Nonetheless, video diffusion holds promise for high- quality video synthesis and editing (e.g. text- guided video modifications) as research continues.

3D Content Creation: Generating 3D data (like shapes, scenes, or layouts) is another emerging application. Diffusion models have been used to generate point clouds of objects by representing points in 3D space and diffusing their coordinates. Researchers have also applied diffusion in the

latent space of NeRF (Neural Radiance Fields) to generate 3D- consistent scenes from text prompts (known as DreamFusion, 2022). In DreamFusion, a pre- trained text- to- image diffusion model is used as a score distillation prior - essentially the diffusion model' s frozen knowledge guides the optimization of a NeRF such that renderings of the NeRF look like what the prompt would produce from the diffusion model 46 47 . This innovative use of diffusion allows 3D asset generation without 3D training data, by leveraging the vast 2D image training of diffusion models. Early results show compelling, if not perfect, 3D objects generated purely from text. Diffusion models have also been applied to generate molecule structures and protein conformations, treating molecular graphs or voxel grids as data to diffuse. For instance, a model might start from a random collection of atoms and diffuse it into a structured molecule with desired chemical properties - a technique with potential in drug discovery.

Other Modalities and Tasks: The diffusion paradigm has proven quite general. In natural language, discrete diffusion is tricky, but there have been attempts to diffuse in embedding space to generate text. E.g., Diffusion- LM (2022) generates text by denoising a sequence of embeddings, achieving controllable generation without mode collapse. However, diffusion has not yet surpassed autoregressive transformers in pure text generation, due to the discrete and sequential nature of language (errors can compound during denoising). In reinforcement learning, diffusion models have been explored to model trajectories: the Diffuser (2022) treats desired goal- conditioned trajectories as data and generates new action sequences via diffusion, showing promising results in planning tasks. In medical imaging, diffusion models are being used for tasks like MRI reconstruction (treating it as image restoration via denoising) and data augmentation (generating synthetic medical images for training). Practitioners also use diffusion models for anomaly detection by seeing if a trained diffusion model can reconstruct an image - if it cannot, perhaps the image lies out of the training distribution (indicative of an anomaly).

In summary, any domain where one can define a noising process on the data and obtain sufficient training examples is a candidate for diffusion- based generation. The ability of diffusion models to handle multimodal conditions (like generating images from text, or speech from text, or video from text and initial frames) makes them versatile building blocks in multi- modal AI systems. Indeed, we are seeing the rise of foundation models where a single diffusion model (or a coordinated set of them) underpins a range of generative AI tools, from art generation platforms to audio synthesis APIs.

## 6 Open Challenges and Future Directions

Despite their successes, generative diffusion models face several challenges and open research questions:

- Sampling Efficiency: A fundamental limitation is the relatively slow generation process. While techniques like DDIM, improved solvers, and distillation have greatly sped up inference, diffusion models still require many sequential steps (dozens at minimum) to produce high-fidelity outputs 10 9 . In applications like real-time video or interactive image editing, this latency can be problematic. Future research is expected to continue reducing the number of required steps, perhaps approaching one-step generation without compromising quality. Consistency models and other one-step diffusion hybrids are a promising direction 28 41 . Another idea is parallel generation: finding ways to denoise multiple steps in parallel, or using big jumps with learned correction steps. How close we can get to the speed of GANs or autoregressive transformers remains to be seen, but ongoing work in more efficient SDE solvers and model distillation is rapidly closing the gap.

- Resource Usage and Scalability: Training large diffusion models is resource-intensive. Models like Stable Diffusion and Imagen were trained on billions of images using hundreds of GPU-days or more 48. The memory footprint of diffusion models can also be high, especially for high resolution or video (since the U-Net must keep feature maps for large images across many layers). While latent diffusion reduces the spatial size, it introduces the overhead of an autoencoder. There is interest in lightweight diffusion models that could run on edge devices or mobile phones, enabling on-device generative AI. Techniques like model compression, quantization, and efficient block designs (e.g. MobileNet-style U-Nets) could help bring diffusion to smaller devices. Another aspect of scalability is large output spaces: for example, generating very high resolution images (4K or above) or longer-duration audio and video. Usually this is handled by cascades (multistage generation), but that adds complexity. Developing single models that can generate high-res in one go, or methods to seamlessly tile or chunk generation, would be valuable for scalability.

- Controllability and Conditioned Generation: While diffusion models have many knobs for control (through conditioning inputs or guidance scales), aligning generated outputs with user intent is not always straightforward. Text-to-image models sometimes produce results that miss certain details or attributes from the prompt, or include unwanted artifacts. Methods like classifier-free guidance push for prompt alignment but at risk of reducing diversity. Fine-grained control (like exactly specifying the layout of a generated scene, the pose of a generated figure, or the style of an image) still requires expertise or additional tools (e.g. providing sketches for ControlNet). One challenge is to make diffusion models more controllable by end-users without needing specialized conditioning inputs. Possible directions are better interfaces for editing (allowing users to iteratively refine outputs), or models that can take high-level constraints (e.g. "place object A to the left of object B") directly in the conditioning. Additionally, extending control to other modalities (controlling the melody or instrumentation of generated music, or the camera motion in generated video) will open up more creative uses. Interactive diffusion models, where the generation can be steered on the fly, could also be a future avenue (though this is hard since generation is iterative but not easily human-interpretable mid-way).

- Training Data and Bias: Diffusion models learn the statistical patterns of their training data, which means they also learn any biases or undesirable patterns present 49 50. Large text-to-image models trained on internet data have shown biases (e.g. generating people with stereotypical gender/race roles given certain prompts) 49 50. There is ongoing work to measure and mitigate biases in diffusion models. For example, researchers have developed tools to quantify bias in generated images (like the "Text-to-Image Association Test") and found that models like Stable Diffusion often replicate and even amplify societal biases 50. Addressing this might involve curating training data, applying fine-tuning or reinforcement learning from human feedback to adjust model outputs, or incorporating constraints during sampling to steer away from problematic generations. Another data-related challenge is that diffusion models require very large training sets for the best results – collecting and curating these, especially for specialized domains (like medical images or scientific data), can be difficult. Techniques for data-efficient diffusion modeling (few-shot or zero-shot generation) are needed so that these models can be applied in domains where data is scarce or sensitive. Some progress has been made using transfer learning – e.g. starting from a large pre-trained model and fine-tuning on a smaller dataset for a specific style or domain.

- Evaluation and Perception: As generative models become incredibly realistic, evaluating them poses both technical and ethical questions. Traditional metrics like FID and Inception Score may not capture all aspects of quality, especially for diverse conditional outputs. Human evaluation remains the gold standard for judging image or audio quality and alignment with prompts. Developing better automated evaluation metrics that correlate with human judgment (for

attributes like image realism, creativity, or prompt relevance) would help benchmark progress. Additionally, the ability of diffusion models to generate photorealistic deepfakes (images or videos that are hard to distinguish from real) raises concerns. There is a need for methods to detect AI- generated content. Ironically, one might use diffusion models themselves for detection: since they know the distribution of real data, they might assign likelihoods that flag an out- of- distribution (AI- generated) image. Some studies have looked at the "fingerprints" left by diffusion processes. OpenAI and others also imprint invisible watermarks in generated images (as done for DALL- E 2) to aid detection. Ensuring responsible use of diffusion models - preventing malicious uses such as propaganda, non- consensual image generation, or plagiarism of art styles - is a significant societal challenge. It intersects with policy and law (for example, copyright and likeness rights, when models are trained on artists' works or people' s photos without explicit consent). The research community is increasingly cognizant of these issues, and many model releases (e.g. Stable Diffusion) come with usage licenses or filters to mitigate misuse 51 52 .

- Theoretical Understanding: On a more scientific note, there are open questions about why diffusion models work as well as they do. Their connection to thermodynamics and score matching provides a solid grounding  $^{13}$ $^{15}$ , but researchers continue to examine aspects like: What is the role of depth vs. number of steps – could we trade one for the other? How do diffusion models avoid mode collapse so well, and can this inform other generative methods? What do the internal representations in the U-Net look like across timesteps? (Some work shows that early timesteps focus on broad structure while later ones add fine detail – aligning with human drawing stages.) Understanding these may lead to improved designs or training schemes. Moreover, a fascinating line of work examines diffusion models as energy-based models and how the learned scores can be used for downstream tasks beyond generation (e.g. enabling likelihood evaluation of inputs, or Bayesian reasoning by treating the diffusion model as defining an energy landscape). Since diffusion models can compute a surrogate likelihood (the ELBO), they can in principle be used for anomaly detection or for integrating into larger probabilistic models – areas that are not yet fully explored.

- New Variants and Combinations: The future may also see hybrids of diffusion models with other techniques. E.g., Diffusion-GAN hybrids have been proposed to get the best of both – using a diffusion-like perturbation of data but a GAN-like discriminator to enforce sharpness. Alternatively, using diffusion models to initialize or refine GANs/VAEs could combine fast sampling with diffusion’s stability. Some works use diffusion as a prior and then train a smaller model (like an auto-regressive transformer) to sample from that prior faster – essentially learning to emulate the diffusion sampler. Another promising direction is combining large language models (LLMs) with diffusion: e.g. guiding image diffusion with not just a static text embedding but an LLM that “conversationally” steers each step, potentially leading to more coherent multistep image sequences or user-interactive generation.

In summary, generative diffusion models have opened up a rich set of research frontiers. The community is actively addressing how to make them faster, more controllable, more equitable, and more integrated into real- world applications. Given the rapid progress from 2015 to 2023, we can be optimistic that many of these challenges will be substantially mitigated in the coming years, further solidifying diffusion models’ role in the generative AI toolkit.

## 7 Conclusion

Generative diffusion models have transformed the landscape of generative AI, evolving from a niche idea to a dominant paradigm for producing high- quality synthetic media. In this survey, we reviewed their

core principles - a forward noise- injection process and a learnable reverse denoising process - and saw how these simple ingredients, grounded in physics and probability, yield a powerful and flexible generative framework. We traced the history from the first diffusion probabilistic model in 2015, through the breakthrough works in 2019- 2020 that unified diffusion with score matching and produced stunning images 11, up to the latest developments that make diffusion models faster, more controllable, and applicable to various domains. Diffusion models now shine in image generation, where they can create art or photorealistic images from text descriptions with unprecedented fidelity 21. They are pushing into video generation, audio synthesis, 3D modeling, and more, often setting new state- of- the- art results as they go.

Importantly, diffusion models have proved to be versatile. Unlike GANs, they natively support likelihood evaluation (via the ELBO) and avoid many training pathologies (we rarely see the equivalent of GAN mode- collapse in diffusion - they tend to cover the data distribution well). Their training is robust and does not require delicate balancing of a generator and discriminator; this reliability has made them attractive for large- scale training on enormous datasets. Furthermore, their ability to incorporate guidance and conditioning in a mathematically principled way (by modifying scores or adding conditioning inputs) gives practitioners fine control over the generative process - be it through classifier guidance for class labels, classifier- free guidance for text prompts 23, or multi- modal conditioning as with ControlNet 26.

That said, diffusion models are not without limitations, and our survey discussed the ongoing efforts to address them. Speed remains the most cited drawback, though year by year the gap is closing with innovations like improved samplers and model compression 28 41. The computational cost for high- resolution or long- duration generation is non- trivial, but techniques like latent space diffusion 20 and model cascades show that clever design can make even 4K images or HD videos attainable in this framework. On the ethical and societal side, diffusion models - as a kind of general- purpose content generator - raise challenges around authenticity, ownership, and misuse (issues previously encountered with GAN- generated deepfakes, now amplified by the accessibility of tools like Stable Diffusion). This is an area where technical research (e.g. watermarking, bias mitigation) intersects with policy. It will be crucial for the community to continue developing best practices for responsible deployment, such as open model cards detailing limitations and involving diverse stakeholders in evaluating biases 51 49.

From a research perspective, generative diffusion models exemplify a beautiful synergy of ideas: they connect classical concepts in statistical physics (diffusion processes), probabilistic graphical models (Markov chains), and modern deep learning. The result is a framework that is not only effective but also conceptually elegant, providing a unified view of generative modeling in continuous and discrete time 15. As we improve their efficiency and extend their reach, diffusion models may become a foundation for even more ambitious AI systems. One could envision future AI that uses diffusion- based generators as components: for example, an AI assistant that can dynamically create visual content, audio, or video on the fly as it converses, effectively integrating generative models with interactive agents. Already, companies are integrating diffusion models into creative software, design tools, and communication platforms, showing the practical impact of this research.

In closing, the progress of diffusion models in a relatively short time has been remarkable, and it underscores a broader trend in AI: leveraging massive data and computation with sound probabilistic methods can yield models with astounding generative capabilities. As researchers continue to refine these models - making them faster, fairer, and more expressive - we expect generative diffusion models to remain at the forefront of generative AI research. They have expanded our understanding of what is possible in generative modeling, and in the process, opened up new creative possibilities across science, art, and technology. The journey of diffusion models is a testament to the interplay of theory and practice, and it will be exciting to see how this field unfolds in the coming years, potentially inspiring new generations of models that are as transformative as diffusion models have been.

## References

References: (Detailed citation list omitted for brevity, but includes all works referenced in the text.)

1 4 8 35 What are Diffusion Models? | IBM https://www.ibm.com/think/topics/diffusion- models

2 3 Latent Diffusion Models: What is all the fuzz about? | patrickschnass.de https://www.patrickschnass.de/posts/latent- diffusion- models/

5 7 Step by Step visual introduction to Diffusion Models | Medium https://medium.com/@kemalpiro-step- by- step- visual- introduction- to- diffusion- models- 235942d2f15c

6 9 18 19 33 34 A Guide To Diffusion Based Image Generation | by Shashank Bhushan | Medium https://medium.com/@bhushan.shashank93/a- guide- to- diffusion- based- image- generation- ee3ded234fbd

10 28 29 30 41 46 47 Consistency Models https://proceedings.mlr.press/v202/song23a/song23a.pdf

11 12 [1907.05600] Generative Modeling by Estimating Gradients of the Data Distribution https://arxiv.org/abs/1907.05600

13 14 15 16 [2011.13456] Score Based Generative Modeling through Stochastic Differential Equations https://arxiv.org/abs/2011.13456

17 Step by Step visual introduction to Diffusion Models. - Blog by Kemal Erdem https://erdem.pl/2023/11/step- by- step- visual- introduction- to- diffusion- models/

20 21 22 25 42 43 48 [2112.10752] High- Resolution Image Synthesis with Latent Diffusion Models https://arxiv.org/abs/2112.10752

23 24 36 37 [2112.10741] GLIDE: Towards Photorealistic Image Generation and Editing with Text- Guided Diffusion Models

https://arxiv.org/abs/2112.10741

26 27 38 39 Adding Conditional Control to Text- to- Image Diffusion Models https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text- toImage_Diffusion_Models_ICCV_2023_paper.pdf

31 32 arxiv.org https://arxiv.org/pdf/2210.02303

40 44 [2009.09761] DiffWave: A Versatile Diffusion Model for Audio Synthesis https://arxiv.org/abs/2009.09761

45 [PDF] Video Diffusion Models

https://papers.neurips.cc/paper_files/paper/2022/file/39235c56aef13fb05a6adc95eb9d8d66- Paper- Conference.pdf

49 50 Researchers' tool finds bias in state- of- the- art generative AI model - News https://news.ucsc.edu/2023/08/t2/at/

51 52 Stable Diffusion Public Release - Stability Al https://stability.ai/news/stable- diffusion- public- release