# Hallucination in Large Language Models: A Comprehensive Survey

## 1 Introduction

Large Language Models (LLMs) have revolutionized natural language processing, demonstrating remarkable abilities in tasks from open- ended dialogue to complex question answering. However, alongside their impressive fluency and versatility, LLMs are plagued by a critical limitation: hallucinations  $1$ . In the context of AI, hallucination denotes instances where an LLM produces text that is plausible- sounding but incorrect, nonsensical, or ungrounded in the provided input or reality  $3$ . For example, a chatbot might confidently fabricate a historical "fact" or cite a non- existent legal case. Such outputs undermine trust and can have serious real- world consequences. In one notable case, ChatGPT falsely accused a radio host of misdeeds in a summarized court case, leading to a defamation lawsuit  $5$ . In domains like law, medicine, or finance, LLM hallucinations risk misinformation that could result in legal disputes, health risks, or economic losses  $6$ . Reducing hallucinations has therefore become paramount for deploying LLMs in high- stakes applications.

The term hallucination in Al is borrowed from psychology (where it originally described perceiving nonexistent stimuli) and first appeared in computer vision literature in a rather positive sense (e.g. "image hallucination" for super- resolution)  $7$ . In natural language generation (NLG), however, it took on a negative connotation by around 2017, describing neural machine translation outputs that were fluent but unrelated to the source text  $8$ . By 2020, researchers in summarization explicitly characterized models "hallucinating content that is unfaithful to the input"  $9$ , bringing the term into the mainstream of NLP. Today, LLM hallucination broadly refers to any confident error of an AI text generator - a failure of factuality or fidelity that often lurks beneath an otherwise coherent surface.

This survey provides a comprehensive overview of hallucination in LLMs. We begin by formally defining key concepts and taxonomies of hallucinations (§2). Next, we present a historical timeline of major milestones in understanding and addressing this phenomenon (§3). We then discuss the foundations and causes of hallucinations, from data and modeling issues to fundamental theoretical limits (§4). Section 5 reviews techniques for detecting and mitigating hallucinations, including recent methods like retrieval- augmentation, self- consistency, and alignment training. In §6, we examine hallucinations across different subfields and applications - from summarization and translation to dialogue, code generation, and multimodal systems - highlighting domain- specific challenges. Finally, we outline open challenges and future directions (§7), charting the path toward more truthful and trustworthy LLMs. We aim for the survey to balance technical depth with clarity, making it accessible to newcomers while providing rigorous insight for experts. Throughout, we emphasize the most recent developments (through 2024- 2025) in this rapidly evolving area, constructing our analysis from original research findings and fundamental principles.

## 2 Defining Hallucinations in LLMs

Hallucination in LLMs lacks a single authoritative definition, but most descriptions converge on the idea of unwarranted, incorrect content generation. A frequently cited definition is that an LLM "generates content that is nonsensical or unfaithful to the provided source"  $3$ . In other words, the model "makes

stuff up” – producing text that may appear fluent and plausible to a human reader but is factually inaccurate or not supported by the input context  $^{10}$  4. Crucially, hallucinations are not simple random errors; they often come in the form of confidently stated falsehoods or irrelevant elaborations that the model has no grounds to output.

Formally, one line of work has attempted to pin down a rigorous definition. Xu et al. (2024) define hallucination in a formal world as any output where an LLM’s response diverges from the ground truth function for the query  $^{11}$ $^{12}$ . In essence, if we consider an LLM as a (possibly probabilistic) function mapping inputs to ideal outputs, a hallucination occurs whenever the model’s output is not the correct (truthful or relevant) output. Under this formalization, they prove that hallucination is inevitable for general- purpose LLMs – no model can compute all functions or possess omniscient knowledge, so for some inputs the model will inevitably produce an incorrect result  $^{13}$ $^{14}$ . Similarly, Banerjee et al. (2025) argue that hallucinations stem from the fundamental limitations of algorithmic systems (drawing connections to undecidability and Gödel’s incompleteness) and introduce the notion of “structural hallucinations” as errors that cannot be fully eliminated without changing the nature of the model  $^{15}$ $^{16}$ . These theoretical perspectives reinforce the practical view that hallucinations are an inherent risk in LLM outputs, though they can be mitigated in frequency and severity.

### 2.1 Faithfulness vs. Factuality

It is useful to distinguish between two related criteria for correct LLM output: faithfulness and factuality. Faithfulness (or consistency with source) means that an LLM’s output does not introduce information unsupported by the input or context. Factuality means that the content of the output is true in the real- world sense (conforms to facts and knowledge). Hallucinations can violate one or both of these criteria  $^{17}$ $^{18}$ . This leads to a coarse division of hallucinations into two broad classes:

- Unfaithful (Extrinsic) Hallucination: Any content generated by the model that cannot be verified against the source or context, implying the model has introduced extraneous information. For example, in abstractive summarization, if the summary contains a detail not present in the source document, it is unfaithful to the input. These are called extrinsic hallucinations in literature  $^{19}$ . An extrinsic hallucination might still be factually correct (e.g. adding a well-known fact that was not in the source) or it might be incorrect – the key is that the information is out-of-source.

- Unfactual Hallucination: Any content that is factually incorrect or logically nonsensical, regardless of whether it was derived from the source. This corresponds to violating factuality. For instance, stating that “Thomas Edison invented the internet” is a factually incorrect statement and hence a hallucination  $^{20}$ . Even if the prompt did not supply the correct information (so the model had to guess), providing a wrong answer is a hallucination in terms of truth. We often simply call these factual errors or fabrications  $^{21}$ .

Notably, an output can be unfaithful without being factually false – a phenomenon sometimes termed factual hallucination in summarization. Maynez et al. (2020) define that “a summary contains a factual hallucination if it includes information not found in the source document that is nonetheless factually correct.”  $^{22}$ . In their analysis, abstractive summarizers often pulled in outside knowledge or plausible details; if those details were correct in reality but just not in the input, they count as factual (or benign) hallucinations. On the other hand, non- factual hallucinations refer to additions that are not only unsupported by the source but also false  $^{22}$ . Whether any hallucination is acceptable depends on the application – for instance, a news summary should ideally be strictly faithful, but one could argue a bit of external background (if true) might enrich a sports recap  $^{23}$ . In general, however, the prevailing view is

that LLMs should avoid introducing information beyond the given context unless explicitly asked to do so (and even then remain truthful).

### 2.2 Intrinsic vs. Extrinsic Hallucinations

A complementary way to categorize hallucinations, especially in tasks with a source text (like translation or summarization), is by source dependence 24 25 :

Intrinsic Hallucination: The model's output distorts or contradicts information present in the input. Here the model is using the source content but misrepresents it 26 . For example, if a document says "Alice was born in 1980 in Paris" and the summary states "Alice was born in 1990 in London," this is an intrinsic hallucination - the output actively contradicts the source on key details. Intrinsic hallucinations often arise from failures in reasoning or text understanding, causing internal inconsistencies. They are akin to context- conflicting errors in dialogue, where the model's response contradicts earlier parts of the conversation 27 .

Extrinsic Hallucination: The model's output introduces new information that is not present in the input at all 24 28 . In summarization, this means adding details, names, or events not found in the source document. In translation, it might mean injecting a whole extra phrase or sentence unrelated to the source. Extrinsic hallucinations indicate the model is ignoring the source and reverting to unsupported text generation (effectively behaving like an open- ended language model rather than a conditioned one 29 ). This often occurs when the model "goes off script" due to biases or noise - for instance, early neural translators were noted to sometimes output fluent sentences completely unrelated to the input when encountering unfamiliar inputs 30 .

Intrinsic vs. extrinsic classification was formally introduced by Maynez et al. (2020) for summarization 25 , and has since been widely adopted. In practice, a hallucinated output can contain elements of both. For instance, a summary might misuse a name from the source (intrinsic) and also invent a detail (extrinsic) in the same sentence. Nonetheless, this categorization helps in analysis: intrinsic errors suggest a need for better comprehension or consistency enforcement, whereas extrinsic errors suggest a need to better ground the model to the input or to limit its open- ended tendencies.

### 2.3 Fine-Grained Taxonomies

Recent research has proposed more fine- grained taxonomies of LLM hallucinations. For example, one taxonomy breaks hallucinations into input- conflicting, context- conflicting, and fact- conflicting categories 27 :

Input- conflicting hallucinations are essentially the same as intrinsic - deviations from the userprovided input or source. Context- conflicting hallucinations refer to contradictions with the model' s own prior outputs or context in a multi- turn interaction (the model "forgets" or contradicts what it said earlier). Fact- conflicting hallucinations are statements that conflict with established world knowledge or facts (broadly corresponding to factual errors).

Another framework (Rawte et al., 2024) uses colorful labels like "factual mirage" vs. "silver lining" to denote hallucinations based on whether the source itself was incorrect 31 . While terminologies differ, the central idea is to capture the nature of the hallucinated content (false facts, contradictions, irrelevancies, etc.) and the source of truth it conflicts with (the input, prior dialogue, the real world, or the model' s own knowledge). For the purposes of this survey, we will primarily use the terms hallucination or hallucinatory output in the broad sense (any untruthful or unfaithful generation). When needed, we

will specify the subtype (e.g., "extrinsic hallucination" for content that is fabricated beyond the source, or "factual hallucination" for externally correct but out- of- source additions).

## 3 Historical Timeline and Milestones

Hallucination in generative AI has been recognized as a problem for several years, but its prominence has surged with the advent of powerful LLMs. Here we chronicle major milestones in understanding and addressing hallucinations:

- 2015-2016 (Early Observations): Some of the first observations of bizarre or ungrounded outputs came from neural dialogue systems. Vinyals and Le (2015) noted that a seq2seq conversational model sometimes gave nonsensical answers when it ran out of learned context 8. Around the same time, researchers in machine translation reported that neural MT systems could produce "spurious translations unrelated to the source text", especially in low-resource settings or when confronted with anomalous inputs 32. The term hallucination wasn't yet standard in NLP, but the phenomenon was being documented.

- 2017 (Term Introduction in NMT): Koehn and Knowles (2017) formally highlighted hallucinations as a key challenge in neural machine translation 30. They described how an NMT model might translate a sentence that is mostly gibberish or entirely unrelated to the input when faced with difficult or out-of-domain content. This brought the term into the NLP lexicon, framing it as an analogy to a system "seeing" something that isn't there in the input.

- 2018-2019 (Task-Specific Analyses): Research in abstractive summarization and data-to-text generation started to grapple with unfaithful outputs. Studies showed that models often diverged from source documents, inventing details - for instance, a CNN article summary that introduced a wrong name or event. Concurrently, image captioning researchers used hallucination to describe captions that mentioned objects not present in the image 7. In 2019, an influential analysis of NMT (Lee et al., 2018) termed it "the curious case of hallucinations", studying conditions under which MT systems hallucinate and ways to induce or reduce it 33.

- 2020 (Hallucinations in Summarization and Metrics): The issue gained mainstream attention with Maynez et al. (2020) "On Faithfulness and Factuality in Abstractive Summarization", which systematically quantified hallucinated content in summarization systems 9. They found that over 70% of one-sentence summaries from state-of-the-art models contained some hallucination, and introduced the intrinsic/extrinsic distinction 25. This work underscored that progress in ROUGE scores had masked a fidelity problem 35. In response, researchers proposed new automatic evaluation metrics for factual consistency: for example, the FactCC metric used a trained classifier to detect factual errors in summaries 35, and QAGS (2020) proposed using question-answering to probe whether summary content was supported by the source 36. These efforts marked a shift toward fidelity-aware evaluation in NLG.

- 2021 (Truthfulness in Open-Ended Generation): As very large language models like GPT-3 were released (2020), their tendency to confidently generate false information became a widely noted problem. In 2021, the TruthfulQA benchmark was introduced to measure how truthful a model is in answering questions that may prompt misconceptions 37. Interestingly, models often mimicked human falsehoods - for instance, perpetuating common myths - showing that bigger models were not automatically truthful. Efforts by OpenAI and others on reinforcement learning from human feedback (RLHF) for alignment also included reducing blatant inaccuracies. Ouyang et al. (2022) reported that instruct-tuned models (like InstructGPT) were preferred by users in part

because they were less often wildly wrong; however, RLHF only partially mitigated hallucinations, and models would still fabricate when they lacked knowledge 38.

- 2022 (ChatGPT and Public Awareness): The release of ChatGPT (Nov 2022) brought the term AI hallucination into popular discourse. Millions of users experienced first-hand how an LLM could instantly generate a very convincing answer that was completely false – for example, inventing sources, fictional legal citations, or incorrect explanations. Hallucination became recognized as one of the primary barriers to relying on AI for factual tasks. This period also saw initial deployments of retrieval-augmented LLMs, such as Google's LaMDA and OpenAI's WebGPT, which use web search or databases to ground their answers in retrieved text, aiming to curb hallucinations. Early studies indicated that providing retrieval evidence can reduce factual errors (WebGPT answers were rated as having fewer inaccuracies, though not eliminated).

- 2023 (Mitigation Strategies Proliferate): With the boom in LLM applications, research on hallucination exploded. Multiple surveys and position papers on hallucinations were published 39 15, reflecting the community's recognition of the issue's importance. Key developments included:

- Chain-of-Thought & Self-Consistency: Prompting techniques like chain-of-thought (asking the model to reason step-by-step) were found to sometimes improve factual accuracy, especially in multi-step reasoning problems. The self-consistency approach (generating multiple answers and choosing the most common) helped reduce random hallucinations in arithmetic or commonsense QA by averaging out noise.

- Open-Source LLM Benchmarks: New benchmarks for factuality and faithfulness emerged. For example, HaluEval and HaluQA were introduced to systematically evaluate hallucinations in GPT-style models 40. Domain-specific benchmarks (e.g. for medical or scientific truthfulness) also came forth, highlighting the varying performance of models across knowledge areas.

- Model Improvements: Proprietary models like GPT-4 (2023) and Google's PaLM 2 showed improved factuality compared to their predecessors. GPT-4, in particular, was significantly better on TruthfulQA and factual question answering than GPT-3.5, illustrating that increasing model size and incorporating feedback can make a difference (yet GPT-4 still hallucinated on difficult or niche queries). Concurrently, GPT-4's tool-use abilities (such as the browsing/search plugin) indicated a future direction: having the model automatically fetch information to avoid guessing. Microsoft's Bing Chat (which combines an LLM with web search) became a prominent example of an LLM aimed at minimizing hallucinations by grounding answers in retrieved web content.

- Theoretical Insights: Researchers also turned to theory. One paper formalized the problem and proved an LLM can't be a perfect truth-teller for all inputs 11 12. Another argued that overconfidence is incentivized – models tend to guess rather than abstain because training and evaluation rarely reward saying "I don't know" 41. These insights began to inform more nuanced mitigation strategies (e.g. encourage models to express uncertainty).

- 2024-2025 (Current State): Hallucination remains an active and unsolved challenge. Latest works focus on hallucination detection – for instance, using an entropy-based uncertainty measure to detect when an answer is likely a "confabulation," meaning essentially an arbitrary guess the model made under uncertainty 42 3. Detection can enable a system to refuse to answer or invoke a backup plan (like ask a human or do a database lookup) when it suspects a hallucination. Other research delves into fine-grained hallucinations in new modalities (vision-language models can hallucinate image details, etc.), and into user-facing implications (for trust, users must be educated that AI outputs can be wrong). There is also a burgeoning conversation on "living with hallucinations" – acknowledging they may never fully go away, so how do we

design AI that is still useful and safe despite that? This includes ideas like graceful failure (the AI knows when to say it' s unsure) and human- in- the- loop oversight. We will discuss many of these developments in depth in later sections.

In summary, over the past few years the community' s understanding of hallucinations has evolved from isolated anecdotes to a structured field of study. Table 1 below highlights some key milestones in this timeline:

<table><tr><td>Year</td><td>Milestone</td><td>Significance</td></tr><tr><td>2017</td><td>Hallucination identified in NMT 30</td><td>Neural translators sometimes generate fluent but unrelated text.</td></tr><tr><td>2020</td><td>Maynez et al. on Summarization 9 25</td><td>Over 70% of model summaries have unfaithful content; intrinsic vs extrinsic defined.</td></tr><tr><td>2021</td><td>TruthfulQA benchmark 37</td><td>First benchmark to explicitly test truthfulness of LLMs on tricky queries.</td></tr><tr><td>2022</td><td>ChatGPT + RLHF (OpenAI)</td><td>Alignment reduces some hallucinations but issue persists; public learns term &quot;AI hallucination.&quot;</td></tr><tr><td>2023</td><td>GPT-4, Bing Chat with retrieval</td><td>New LLMs improve factual accuracy; retrieval-augmented generation (RAG) deployed to counteract hallucinations.</td></tr><tr><td>2024</td><td>Entropy-based detectors 42, Theoretical proofs 13</td><td>Advances in automatically flagging likely hallucinations; proof that some hallucination is unavoidable.</td></tr></table>

Table 1: Selected milestones in the development of understanding and combating hallucinations in LLMs.

## 4 Causes and Foundations of Hallucination

Why do large language models hallucinate? The reasons are multifaceted, spanning the entire lifecycle of an LLM from training data to inference decoding. We break down the major contributing factors and the foundational insights explaining hallucinations:

### 4.1 Training Data Limitations

LLMs learn from vast datasets that are inevitably incomplete, imperfect, or biased with respect to the space of all possible queries. This knowledge incompleteness is a fundamental cause of hallucination. If an LLM is asked about an obscure fact that never appeared in its training data, it has two choices: admit ignorance or guess. Today' s models often do the latter, because they are trained to always produce an answer: in formal terms, any fact or pattern not reflected sufficiently in the training distribution is not learnable by the model, so when prompted for it the model will extrapolate in an unsupported way 43 . Kalai and Vempala (2024) showed that if a certain fraction of facts appear only once in the training set, a base language model will hallucinate at least that fraction of the time on queries about those facts 44 . Intuitively, if  $20\%$  of factual associations were seen only one time, the model cannot confidently generalize them and will likely produce wrong answers  $\sim 20\%$  of the time for those associations 45 .

Beyond missing facts, the training data may contain erroneous information. LLMs can and do internalize common misconceptions or inaccuracies present in internet text 46 . For example, if the web has many instances of a false statement (say, a conspiracy theory or an incorrect medical tip), a model might repeat

it as factual. Such behavior is a form of hallucination rooted in data quality issues - the model is "faithful" to its flawed training data but not to the truth. This blurs the line between a lie and an honest mistake from the model' s perspective. Either way, the output is a hallucination to the user.

Another data- related cause is distributional shift. If the input prompt is unlike anything the model saw during training, the model is effectively extrapolating to a new distribution. Early NMT models, for instance, would produce unrelated output when given a badly garbled source sentence or a sentence in a different genre than they were trained on 47. The model "hallucinates" because it has no reliable pattern to follow. This is analogous to a person asked a question in a foreign language they barely know - they might respond with some phrases that come to mind, but it could be nonsense.

Finally, models are trained with a next- word prediction objective that does not explicitly teach them to stick to factuality. They optimize for plausibility and linguistic probability, not for truth. As Maynez et al. note, maximizing likelihood of reference text "does not necessarily reward models for being faithful" to a source 48. Training data may also contain inconsistent pairs (e.g., a noisy summarization dataset where references include extraneous info), which can confuse the model about how strictly it should copy the source 49. All these factors in the data stage sow the seeds for hallucinations at inference.

### 4.2 Model and Architecture Factors

Certain properties of the model and learning process can exacerbate hallucinations:

- Parametric Knowledge vs. Reasoning: LLMs store an immense amount of "knowledge" in their parameters, but they do not have a reliable mechanism to truly understand or verify facts. They are essentially very sophisticated predictive text engines 50 51. As one survey poignantly stated, "the model has not learned to think and has no concept of truth; it has learned to mimic the products of thought." 51. This means an LLM has no built-in preference for a true sentence over a false but fluent sentence – unless the false sentence was rarer in training. The concept of truth for an LLM is statistical. Thus, if asked something like "Who won the 1972 Olympic gold in marathon?", a model might generate a plausible name or mix of names (because it sounds like a reasonable answer) even if it's not sure. It does not truly know the fact in a robust way that would allow it to refrain from answering.

- Exposure Bias and Decoding Strategies: During training, models are fed ground-truth sequences token by token (teacher forcing), but at inference they generate tokens one by one based on their own previous outputs. This mismatch (known as exposure bias) can lead to the model drifting off course. Once a few incorrect tokens are generated, the subsequent context is now off-distribution, and the model may then compound errors – a single hallucinated detail can spiral into a larger fabrication. How we decode the model also matters. Beam search, for example, has been observed to sometimes amplify the likelihood of generic but unsupported continuations, causing more extrinsic hallucination in summarization if the beam tries too hard to maximize likelihood 28. In contrast, sampling-based decoding (like nucleus sampling) injects randomness that can sometimes actually avoid certain deterministic errors, but it can also produce wilder outputs in other cases. Tuning decoding parameters is thus a trade-off between coherence and the risk of the model "running away" with an idea.

- Model Size and Generalization: Larger models tend to hallucinate less blatantly than smaller ones, especially on factual queries 52. They have seen more data and often have better internal representations, which helps them in correctness. For instance, GPT-4 (with hundreds of billions of parameters, possibly over a trillion) significantly outperforms GPT-3.5 in factual QA and rarely falls

for basic traps or contradictions that smaller models do. However, scale is not a panacea - even the biggest models hallucinate on sufficiently challenging or novel prompts. Size also introduces new failure modes like the ability to produce very intricate, convincing falsehoods. So while increasing model capacity and pretraining data yields a generally more knowledgeable model (reducing knowledge gaps that lead to hallucination), it does not eliminate the issue. There are diminishing returns: beyond a point, further scaling might yield only minor gains in truthfulness relative to the cost.

- Lack of Grounding Mechanisms: Unlike systems explicitly connected to knowledge bases or the world, pure LLMs have no grounding beyond text correlations. They don't have an internal module that cross-checks facts against a database or the real world. This lack of a verification component means the model's outputs aren't automatically checked against reality. Humans writing an article can go verify a detail in an encyclopedia; current LLMs cannot (unless we equip them with tools, as we will discuss in mitigation). Some researchers point out that LLMs lack a "citation" or evidence retrieval mechanism by design 53. Therefore, when an LLM states a fact, it's often just regurgitating a training pattern or synthesizing one - it doesn't "know what it doesn't know."

- Overconfidence Incentivized: A subtle but important cause identified recently is that the training and reinforcement processes penalize uncertainty in model responses. In many training setups (supervised fine-tuning or RLHF), if a model says "I don't know" or expresses doubt, that response is not preferred because it looks less informative or less helpful. As a result, models learn to state something as if it were certain, even when unsure – effectively learning to bluff in the face of uncertainty 41. Analogously, students in a classroom might get credit for attempting an answer rather than leaving it blank, so they guess; LLMs are in a similar "always answer" regime. OpenAI researchers have argued that this is a key reason why models do not abstain or indicate lack of knowledge – they are rarely or inconsistently rewarded for doing so 54. The outcome is overconfident hallucinations: the model would rather fabricate a specific answer (with full fluency) than output a low-confidence indicator. This is a training alignment issue – current metrics don't align well with truthfulness because they don't account for a model's calibrated uncertainty.

### 4.3 Inference-Time Triggers

Even with a well- trained model, certain inputs or contexts at inference time can trigger hallucinations:

- Ambiguous or Underspecified Prompts: If the user query is vague, the model may hallucinate details to fill the gap. For example, asking "Write a story about a scientist" might lead the model to make up a name and background for the scientist. That's expected in a creative task, but in a factual setting, an underspecified question like "Tell me about the new vaccine" might lead the model to guess details if it's not sure which vaccine is meant. The model will produce a fluent answer about some plausible "new vaccine" which could be partly or wholly invented.

- Adversarial or Trick Prompts: Users can intentionally or unintentionally provoke hallucinations. Prompting a model with contradictory instructions or false premises (e.g., "Based on the following false claim, explain why it's true...") can confuse it into producing wrong information. There is a line of research on adversarial attacks that shows how adding seemingly irrelevant sentences or typos to a prompt can cause models to err or change outputs drastically 52. For instance, minor input perturbations or typos have been shown to induce hallucinations in some LLMs where none occur with clean input 52. This suggests some fragility in how models parse

inputs - they may mis- read a corrupted input and then latch onto some hallucinated interpretation.

- Long Contexts and Forgetfulness: In long conversations or documents, an LLM may lose track of earlier details and inadvertently contradict itself or make things up to bridge gaps. Context-window limitations mean the model might not explicitly "remember" some pieces of context if the prompt is very large. This can cause context-conflicting hallucinations (the model states something inconsistent with what it said 2 pages ago because that content scrolled out of its attention). Moreover, as the generated text grows long, errors can accumulate and the model might stray further from factual grounding, particularly if the user does not correct it.

- High Creativity Modes: Some LLM applications encourage maximum creativity (for example, telling the model to "imagine" or produce fiction). In these modes, the line between acceptable creative invention and unwanted hallucination can blur. The model might slip into a less factual style even if the user later asks a factual question, because it's in a "creative" frame. Conversely, highly restrictive instructions can also backfire - if a model is forced to always give an answer even when uncertain (e.g., via a system prompt that says "never say you don't know"), it will hallucinate when it has no knowledge. Striking the right balance in prompt instructions is thus important to manage hallucination tendencies.

In summary, hallucinations stem from a complex interplay of data gaps, model architecture, training objectives, and inference conditions. The root cause can be as deep as theoretical limits of algorithmic learning or as practical as a poorly phrased question. This multifaceted causality explains why hallucinations are so persistent: even if we patch one source (say, add more facts to the training data), other sources (like uncertain reasoning or decoding artifacts) can still lead to errors. Recognizing these causes helps inform the development of countermeasures, which we turn to next.

## 5 Detection and Mitigation Strategies

Given the prevalence of hallucinations in LLMs, a rich area of research (and engineering) has emerged to detect when a model is hallucinating and to mitigate or prevent such errors. Approaches range from dataset- level solutions to architectural changes to post- processing techniques. We organize the discussion into several themes:

### 5.1 Data and Training-Level Approaches

Improving Training Data: Since data issues are a major cause, one straightforward approach is to curate higher- quality and more comprehensive training corpora. This includes filtering out known false information, removing or down- weighting noisy references that might teach a model to be unfaithful (e.g., cases where summaries contain info not in sources), and augmenting the data with factual sources (like Wikipedia or structured knowledge) to cover more truths. Some studies have created specialized datasets of fact- checked statements or added an "I don't know" option in training to teach models that it's acceptable not to hallucinate an answer. For instance, reinforcement learning can incorporate a penalty for outputs that conflict with known facts: a factuality reward model can judge the output and guide the training to reduce factual errors 55 56. There have also been efforts to fine- tune models on dialogues where refusing to answer when unsure is the desired behavior, thus aligning them to not answer rather than answer incorrectly. However, these methods are limited by the difficulty of covering the vast space of possible queries - no training set can include the correct answer to every obscure question, and no set of filtered data can remove all subtle inaccuracies. So while careful training data

design can reduce some hallucinations (especially glaring ones from bad data), it cannot alone solve the issue.

Controlled Generation and Constraints: Researchers have experimented with modifying the training objective to more directly enforce faithfulness. One example is incorporating textual entailment or consistency checks into the loss: models get rewarded for generating outputs entailed by the source (to discourage divergence) 57 Another idea is to train models to produce justifications or citations alongside answers, which inherently forces them to focus on verifiable content. If a model must cite a source for every factual claim, it might be less inclined to make a claim for which it finds no source (in theory). Some prototypes of this idea exist, such as training LLMs on an augmented dataset where each statement is paired with evidence; the model then learns a style of answering with referenced facts. This overlaps with retrieval- based methods (discussed below). Additionally, contrastive training has been used (e.g., a model is trained to distinguish a correct continuation from a hallucinated one), thereby teaching it implicitly what kind of continuations are invalid 49. All these approaches aim to bake in a sort of "guardrail" during the training phase so that the model's generation process avoids known pitfalls.

Model Architecture Enhancements: Some proposals involve changing the model architecture to better handle knowledge. For example, there are neural architectures that integrate symbolic knowledge bases or use modular components (like a separate module for factual recall). These can help by giving the language model an explicit memory or reference it can query, rather than relying purely on parametric knowledge. One instantiation is the Retriever- Reader models, where the architecture is split into a retrieval part and a generation part. During training, such models learn to consult an external source (e.g., Wikipedia) and then generate. If trained end- to- end, the model can learn when it should consult external info versus when it "knows" the answer. Early versions like RAG (2020) were still mostly about inference- time retrieval, but newer research attempts to more tightly couple retrieval within the model's layers. Another architectural idea is to have uncertainty estimation built into the model - e.g., a parallel head that predicts whether the model's answer is likely to be correct or not. If such a head is trained from human feedback or from consistency signals, the model could essentially know when it is hallucinating and adjust or abstain accordingly. This is still an emerging area, as it requires ground truth signals of correctness to train on, which are not always available.

Theoretical Limits Acknowledgment: No matter the training improvements, some works emphasize that we must acknowledge hallucination as a fact of life (or of AI life). Banerjee et al. (2025) argue that rather than trying in vain to eliminate hallucinations, we should focus on managing them - for instance, always pair the LLM with a verification system or ensure a human can review outputs in critical cases 15 58 . This is more of a philosophical or strategic approach than a specific technique, but it influences how we might design systems: e.g., default to a mode where the model's outputs are treated as suggestions that need checking rather than final answers. Training the users and developers to understand the model' s limitations is as important as training the model.

### 5.2 Retrieval-Augmented Generation (RAG) and External Knowledge

One of the most promising strategies to reduce hallucinations is to supply the LLM with relevant ground truth information at inference time so that it doesn't have to rely solely on its internal parameters. This is known as retrieval- augmented generation (RAG) or more broadly, knowledge- augmented LLMs. The idea is simple: when a question comes in, first use a search or database lookup to fetch documents (web pages, Wikipedia articles, corporate knowledge base entries, etc.) that likely contain the answer, then feed those documents into the LLM's context before having it generate a response 59 60. By doing so, the model's probability distribution for next words will be conditioned on actual reference text, drastically reducing the need (or temptation) to invent facts.

Mechanism: A typical RAG system consists of a retriever and a reader (the LLM). The retriever might be a vector search over an index of text chunks, or a more classical information retrieval system, which given the user's query, returns a set of relevant passages. These passages are then concatenated (perhaps with some prompt template) and given to the generation model. The model then produces an answer ideally by drawing from and summarizing the retrieved information. Because the relevant facts are right there in the context, the model doesn't have to recall them from memory or fill in gaps - it can copy or rephrase them. If the retrieval finds correct info, the result is a much more grounded answer.

![](images/195802347008016f23a16ef503b1a17a02dce8e907f065d6959cffecefd7d895.jpg)  
Figure 1: A high-level illustration of Retrieval-Augmented Generation (RAG). The LLM (generation model) is augmented with a retrieval module that pulls in relevant external documents or data based on the user's query. The model's prompt is then expanded to include this retrieved context, grounding its response in the provided evidence. By injecting authoritative information at runtime, RAG aims to reduce the model's propensity to hallucinate facts. 61 62

RAG has been shown to significantly reduce factual hallucinations in many scenarios. For example, if an LLM is asked a question about a specific legal statute or a detailed scientific fact, a vanilla model might only know part of the answer and hallucinate the rest. A RAG- enabled model can search the law text or scientific papers, find the exact relevant snippet, and then its answer will be directly supported by that snippet. Empirically, systems like Bing Chat or the new wave of LLM- based search engines use this approach to good effect - the answers come with references, and users can verify each claim against a source. This directly addresses the citation gap identified in base LLMs 53.

However, RAG is not a panacea. Its effectiveness depends on retrieving the right information. If the knowledge base is outdated or the retriever fails (retrieves irrelevant or wrong documents), the LLM might still hallucinate or even be fed astray by incorrect retrieved info. Also, combining retrieved info with model knowledge can sometimes produce conflicts - the model might mix a retrieved fact with a hallucinated detail. There's ongoing work on how to make the model defer completely to the retrieval (perhaps by training it to copy more and freestyle less when context is present). Nonetheless, RAG addresses one core cause of hallucination: the model not knowing something. It essentially extends the model's knowledge beyond training, and as observed, "augments the model with facts and details on which to base its response" 63 64, thereby improving reliability.

We should note that retrieval is not limited to text; it could also be a database query, a calculator (for math problems), or an API call. All these can be seen as forms of grounding the model in a tool or external source that provides authoritative outputs. For example, if asked to compute something, a non- tool LLM might hallucinate a calculation, whereas one with a calculator tool will give the correct result. Integrating such tools via plugins or specialized prompting (e.g., Program- of- Thought which allows calling external

APIs) is a growing practice to combat hallucinations in specific areas like arithmetic, dates and times, or factual lookups.

In summary, RAG and tool use represents a major advance in mitigating hallucinations: it turns the open- ended generative problem into more of a closed- book exam into an open- book exam. The model can refer to a "book" (the retrieved documents) instead of relying purely on memory, which dramatically increases accuracy on factual queries and reduces fabrication [65, 66]. The trade- off is complexity and dependency on the retrieval system, but for many applications that is a worthwhile price for reliable information. Indeed, Gartner's 2024 report suggests organizations prioritize RAG for deploying GenAI on their private data [67], precisely to ensure factuality and compliance.

### 5.3 Prompting Techniques and Inference-Time Control

Another class of mitigation techniques works at inference time by crafting the prompts or using multiple passes of generation to reduce hallucinations:

Chain- of- Thought Prompting: Prompting the model to "think step by step" or outline its reasoning can help it avoid leaps that lead to hallucinations. When an LLM is forced to break down a problem (especially logical or multi- hop questions), it may be more likely to catch contradictions or realize it lacks certain knowledge and thus avoid asserting falsehoods. In tasks like math word problems and factual QA, chain- of- thought has improved correctness. The model might generate a reasoning trace that includes checking a fact, which can prevent hallucinating a wrong fact. However, chain- of- thought is not foolproof - the model can just as easily hallucinate a multi- step reasoning that is internally consistent but wrong in conclusion. Some research has combined chain- of- thought with external verification, e.g., after each step, query a knowledge source, or at least at the end, verify the final answer.

Self- Consistency and Voting: This involves generating multiple answers or reasoning paths from the model and then using a voting or ranking mechanism to select the most consistent answer. The intuition is that a hallucination might be more arbitrary and thus less likely to appear repeatedly. If you sample an LLM several times for a question and get answers A, B, C, etc., and if A appears 6 out of 10 times while others are varied, A is likely a more robust answer grounded in the model's strongest knowledge, whereas a one- off weird answer might only appear once. This has been shown to improve accuracy in reasoning tasks (e.g., arithmetic) [68]. For factual questions, if the model has uncertainty, its answers might vary - seeing that variance and perhaps cross- checking the answers can alert one to a likely hallucination. Some implementations use the model to evaluate its own different answers (a kind of internal debate) and come to a final answer. The downside is increased computational cost, but for critical queries it can be worthwhile.

Prompt Instruction and Style: How you ask the model to behave can influence hallucination. For instance, instructing the model explicitly not to fabricate information and to say "I don't know" if unsure can reduce hallucinations. Many system designers now include directives like "If you are not fully confident or the information is not available, do not invent an answer." This doesn't always work - the model might still guess - but it provides a bias against hallucination. Another trick is asking the model to give an answer and an explanation or to show its sources. When a model knows it has to show a source, it may either avoid giving an unsupported claim or it will make up a fake source (which is another kind of hallucination!). The latter is actually a known issue: models often hallucinate references - e.g., a nonexistent paper citation - when asked to provide sources. To mitigate that, some approaches have the model generate the answer first, then separately verify or fetch sources for each statement in a second pass (and if it fails, it flags that part). Ensuring the model output is in a format that is easier to check (like bullet points of facts rather than one long narrative) can also help a human or automated checker verify the content.

Fact- checking and Post- hoc Verification: After the model generates an output, one can run a verification step. This could be another LLM prompt like "Check the above answer against a knowledge base and list any inaccuracies." Or it could be an automated fact- check: extract claims from the output and search for them, see if reputable sources confirm or contradict. There are automated fact- checking systems (some using NLI - natural language inference - to see if the source entails the claim) that can be applied to the model's output 35 69. If any claim is flagged as likely incorrect, the system can either ask the LLM to try again or attach a warning to the user. For instance, a pipeline might be: LLM drafts an answer  $\rightarrow$  pass each sentence to a fact- checker (could be a smaller model fine- tuned for this, or an API)  $\rightarrow$  have the LLM revise sentences that were marked inconsistent with known facts. This kind of approach is promising but challenging: fact- checkers are also imperfect, and integrating them seamlessly is nontrivial. Still, OpenAI's "moderation" for factual issues might one day be as robust as their moderation for toxicity - imagine a system that refuses or double- checks answers that sound like they could be hallucinations.

Uncertainty Quantification: We touched on this, but at inference one can have the model produce some measure of confidence. Some research uses the entropy of the model's output distribution as a signal - high entropy (model uncertain, many plausible continuations) often correlates with higher chance of hallucination 42 . In Farquhar et al. (2024) terminology, they focus on confabulations - arbitrary answers that are sensitive to random seeds, indicating the model is essentially guessing rather than recalling a stable fact 3 . They find ways to detect those by measuring how an answer changes with slight prompt variations or by looking at probability concentrations. In practice, a system could say: "The model is not very confident in this answer." Some current LLM apps do output a confidence estimate. But calibrating these is hard - language models are notoriously miscalibrated (they might be overconfident in wrong answers and underconfident in correct ones). Work is being done on calibration, such as fine- tuning the logits to better reflect true correctness likelihoods, or training a separate model to predict correctness from the answer content (like analyzing if the answer has certain telltale signs of BS).

In summary, inference- time techniques are about either guiding the model to avoid hallucinations during generation or catching hallucinations after generation. They are comparatively easier to implement on top of existing models (since they don't require retraining) and can yield significant improvements. The combination of chain- of- thought prompting and self- critique is an especially active area: e.g., a method might prompt the model: "Give your answer, then analyze whether each part of your answer is fully supported by given context or common knowledge." Such an approach essentially makes the model do a bit of what a human fact- checker would do. Indeed, one recent strategy named SelfCheckGPT had the model generate multiple continuations and use the variability (uncertainty) among them to self- diagnose hallucination 70 . This showed promising ability to flag its own potentially hallucinated statements without external data.

### 5.4 Evaluation and Detection Benchmarks

A crucial aspect of mitigating hallucinations is measuring them accurately. Traditional metrics like BLEU or ROUGE (which compare to a reference text) are insufficient to detect hallucinations because a hallucinated answer can be linguistically fine and even overlap with reference in some n- grams but still contain a critical error. Hence, a line of work has been devoted to creating benchmarks and metrics specifically targeting hallucination detection 71 72 . Some notable ones:

- TruthfulQA (2021): As mentioned, a benchmark of adversarial questions that humans often answer falsely. This measures if the model outputs falsehoods (hallucinations) even when the truth is actually often counter-intuitive. It's hand-crafted and evaluations show most models struggle to be consistently truthful here, highlighting the hallucination issue in zero-shot Q&A.

- Hallucination Evaluation Datasets: There are datasets for summarization (like XSumFaith, SummEval) with human annotations of which parts of summaries are hallucinated. For dialog, there's FaithDial, where model responses are annotated for being consistent with knowledge. In 2023, HalluEval was introduced as a unified evaluation where models have to generate and then are scored on factual consistency across tasks 71. Also, HalluQA (2023) specifically provides a Q&A setting with some questions answerable from context and some not, testing if model will hallucinate an answer or correctly say it cannot answer 73.

- Metrics: Many automatic metrics have been proposed. They generally fall into two types: reference-based and reference-free. Reference-based metrics (used in tasks like summarization) assume a ground-truth or source text. These include:

- Entailment-based metrics: Use an NLI model to check if the model's output is entailed by the source text 35. If not, there's likely hallucination. E.g., FEQA, FactCC.  
- Question generation + answering: Generate questions from the model's output and try to answer them using the source. If the answers don't match, the output had unsupported info 74. (This was the QAGS approach and others like QuestEval).  
- Overlap with knowledge base: For factual outputs, compare the statements to a database or Wikipedia text via information retrieval; use precision at retrieving known relevant facts as a metric (this is like an automated open-book exam for the output).

Reference- free metrics don't assume a source text (for when an LLM generates something from scratch, like a story or open- ended question). These are trickier: they might rely on multiple model outputs (to see consistency) or use another large model to judge factuality. One approach is using GPT- 4 itself as a evaluator - ask it "Is this answer factual and does it make logical sense?" - and use that score. Some evaluations have found GPT- 4 can rate factuality pretty well, but we are basically using a nonhallucinating instance of a model to judge another instance.

Newer metrics are focusing on fine- grained detection - e.g., identifying which specific sentence or entity in the output is hallucinated. This is useful for partial credit: maybe  $90\%$  of an answer is correct and one sentence is wrong. A good metric would highlight that. The Atomicity Score (2023) breaks text into atomic facts and assesses each 75. This granular evaluation can guide model developers to the exact types of mistakes being made.

Human Evaluation: Ultimately, because automatic metrics can miss subtleties, human evaluation remains critical. In research, it's common to have crowdworkers or domain experts evaluate outputs for factual correctness and faithfulness. For example, the Holistic Evaluation of Language Models (HELM) initiative includes human assessment of factuality on various tasks. Human eval is expensive and slow, but for high- stakes deployment, human oversight is often included as a safety check (e.g., a human editor reviewing an AI- generated summary of a medical article for accuracy).

By improving how we detect and evaluate hallucinations, we indirectly mitigate them: better evaluation metrics and detectors can be used as feedback signals to train better models or as runtime filters. For instance, if a detector raises an alarm for a certain response, the system can automatically trigger a regeneration or notify a human. In an RLHF scheme, a detector's judgment could be part of the reward function (penalizing hallucinations). In this way, detection and mitigation go hand in hand.

## 6 Hallucinations Across Domains and Applications

Hallucination manifests differently across various tasks and application domains. Here we survey how it appears and is handled in some key areas related to LLMs:

### 6.1 Abstractive Summarization

We have already touched on summarization, as it was one of the first areas to face hallucination issues head- on. Abstractive summarizers (especially on datasets like XSum) commonly generate content not in the source document. In news summarization, this might mean adding a wrong age for a person, or misreporting a quote, or conflating names - all of which are serious errors for journalism. Studies found a large fraction of model- generated summaries contained at least one hallucinated fact 9. Summarization hallucinations are especially problematic because the whole point of a summary is to faithfully convey the source. An unfaithful summary can misinform readers even more than if they hadn't read the summary.

To combat this, research introduced metrics like we discussed, and also dataset construction changes. For example, some summarization datasets were inherently biased toward hallucination (XSum' s human- written "summaries" often included background info not in the article, basically requiring models to add info). As a result, if one doesn' t want any hallucination, one might choose or create datasets where summaries are strictly extractive/faithful. Some researchers even framed a new task "faithful summarization" and built data for it.

On the model side, attention mechanisms and copy mechanisms (that allow the model to directly copy spans from the source text) were employed to keep summaries grounded 26 28 . A notable finding was that pretrained language models (like BART, T5) hallucinate less than earlier RNN- based models 34 . Pretraining seems to instil a bit more world knowledge and maybe a better sense of language consistency. Yet, even they hallucinate. By 2022- 2023, state- of- the- art summarization models combined pretrained LMs with factually rewards or post- hoc corrections. For instance, there were attempts to finetune summarizers with a loss function that penalizes unfaithful statements detected by a classifier 57 . Others used reinforcement learning, where the reward is high if the summary is factual and low if not - effectively teaching the model through trial and error to avoid hallucinations 57 .

Open challenges in summarization include entity hallucination (the model makes up an entity or confuses entities) and structure hallucination (model outputs an entirely unrelated sentence, which sometimes happens when it' s summarizing very complex or lengthy inputs). Multi- document summarization adds another layer - if sources conflict, the model might hallucinate a compromise or mix up sources. Summarization is a good testing ground for hallucination mitigation, and many methods developed here (like NLI checks, QA- based verification) have been ported to other tasks.

### 6.2 Machine Translation

Neural machine translation (NMT) systems can hallucinate in a couple of ways. The most notorious is when translating between very low- resource languages or when given an input that it can't parse - the model might output a fluent sentence in the target language that is completely unrelated to the source. For example, researchers demonstrated an English- to- French model given an English sentence with some unusual proper names started outputting a French sentence about politics that had nothing to do with the input 33. Essentially, the model falls back to prior training knowledge when it cannot align to the source well, producing a kind of free- form translation (often biased to common themes in the target language corpus).

Another scenario is partial hallucination: the model translates the first part of a sentence correctly, then for a phrase it doesn't know, it might drop it or replace it with something made- up, then continue translating the rest. This can be dangerous in something like translating instructions - omitting a negation or a clause could invert meaning or lose critical info.

The causes in MT were diagnosed as including: overly strong language modeling component (the decoder of an NMT has a prior that might overwhelm the source signal), exposure bias with long sentences (once they go off track, they continue), and beam search issues. One interesting finding was that using smaller beam widths or even pure sampling sometimes reduced crazy hallucinations - because a large beam would prefer to generate something generic but not in the source, whereas a smaller beam might "give up" less. Also, coverage mechanisms (ensuring every source word is translated at least once) were introduced to reduce hallucination and also omission (which is the flip side - model might also drop content, considered another kind of fidelity error). Some NMT systems have an explicit "confidence" and if low, they might resort to copying the source or indicating they're unsure.

Modern large translation models (like those underlying Google Translate or Meta's NLLB) have much fewer obvious hallucinations, partly due to training on massive parallel data and better architectures. But even so, for very difficult or long inputs, hallucination can occur. A known technique now is to break inputs into smaller chunks or sentences, translate those, and then re- compose - rather than giving the model a full paragraph which increases the risk of hallucination in low- resource cases.

### 6.3 Dialogue and Conversational Agents

In open- domain dialogue (chatbots like ChatGPT, etc.), hallucination often takes the form of making up facts in responses to user questions. If a user asks, "What is the population of X city?", a honest failure would be to say "I'm sorry, I don't know." A hallucination is providing a random number. ChatGPT and similar are known to sometimes invent statistics, dates, names, or sources to keep the conversation flowing. Dialogue systems are optimized to be helpful and engaging, which historically meant they were more likely to improvise an answer than to refuse. Only with the recent focus on correctness do we see them sometimes saying, "I'm not sure about that."

A specific type of hallucination in dialogue is the persona or memory hallucination: if a chatbot has a persona (like role- playing as a character), it might hallucinate memories or facts about that persona that were never mentioned. Or if it conversed for a long time, it might forget earlier context and inadvertently hallucinate what was said before (context- conflicting hallucination). This is seen in some multi- turn chats where the bot contradicts itself or invents something that the user supposedly said ("As you mentioned earlier, ...") when the user never did.

Safety- related hallucinations in dialogue are a big concern - for example, if asked medical or legal advice, a chatbot might hallucinate a convincing but incorrect answer that could be harmful. Thus, some safety mitigations intentionally cause the bot to refuse answers in those domains or only provide very vetted info.

One interesting approach in dialogue is the concept of retrieval- enabled chat (like BlenderBot 2 from Facebook) which will search the internet for an answer and then ground its response. This significantly cuts down on factual hallucination, though it introduces new risks (like the bot quoting inaccurate web content - but at least it's not purely self- fabricated).

Also, companies have begun to deploy tools that monitor a live chatbot for hallucinations. For instance, after an answer is produced, they might run a second pass that highlights all named entities and factual

claims and checks them. If something looks dubious, the bot might append a clarification like, “(I’m not entirely sure about that detail.)”

### 6.4 Question-Answering and Knowledge Bases

LLMs used for question- answering (especially in a closed- book setting) illustrate the core hallucination issue: when asked a factual question and not knowing the answer, an LLM will often hallucinate. For example, ask a model “Who discovered element 115?” and if it doesn’t truly know, it might output a plausible scientist name. One study termed these “confabulations” – answers that are not just wrong, but essentially unjustified guesses that vary with random seeds 76. They distinguished these from cases where the model is consistently wrong due to training on some misconception (in which case it will always give the same wrong answer – arguably less of a hallucination and more of a learned error).

The QA community has moved toward open- book QA (the model can search or be given documents) precisely to avoid hallucination. Datasets like Natural Questions or TriviaQA expect the system to pull evidence text. LLMs can do a form of this via prompt (like the ReAct prompting: first “think” which yields a search query, then get info, then answer). This merges into the retrieval discussion earlier.

There’s also a connection with knowledge graphs: Some approaches try to inject knowledge graph info into LLMs during training or via prompts so that factual QA is improved. If the model had a way to query a structured knowledge base (like Wikidata) when faced with a factual question, it could get authoritative answers.

Another concept is calibration – ideally, if a model doesn’t know the answer, it should say it doesn’t know. In QA evaluations, one now measures not just accuracy but also something called selective accuracy: can the model abstain on questions it would get wrong? A perfectly calibrated model would answer only when likely correct, and say nothing otherwise, achieving high accuracy on the ones it does answer. Current LLMs are not great at this out- of- the- box (they answer almost everything). But with some fine- tuning or prompting to allow refusals, modest improvements in calibration are seen. This is important for real applications: for example, a “AI assistant lawyer” should not fabricate case law – it should either recall it correctly or tell the user it needs to look it up. Products might combine an LLM with a retrieval step from a database of legal cases to ensure no hallucination of citations (something that has famously occurred when lawyers used ChatGPT and it made up cases).

### 6.5 Code Generation

Large language models (like OpenAI’s Codex, or AlphaCode, or Code Llama) are also used to generate source code from natural language. Hallucination in code generation usually means producing code that looks plausible but is incorrect or non- functional. For instance, the model might call a function that doesn’t exist or use an API incorrectly. It might also invent an algorithm that superficially seems right but doesn’t actually solve the problem. Because code can be compiled/run, hallucinations are often quickly exposed by errors or failing tests.

One particular hallucination is non- existent libraries or functions: a user asks to do X, the model “recalls” a library that does X, but it’s actually mixing two names or hallucinating an API that isn’t real. Novice programmers can be misled if they trust that code. A well- known example: a model invented a function `parse_datetime()` in Python’s standard library which does not exist. It was hallucinating a plausible name based on `datetime.strptime` or such.

Mitigation in code is partly easier: you can unit- test the output or at least compile it to see if it runs. Some coding assistance tools now do an iteration: generate code  $\Rightarrow$  run tests or type- checker  $\Rightarrow$  if errors, have the model debug its output. This is effectively catching hallucinations (like if it hallucinated a function, the compiler error "function not defined" will prompt it to fix by using an actual function).

Another angle is that code LLMs are often fine- tuned with an execution- based reward. For example, the model might be trained with many test cases such that producing output that passes tests is rewarded, failing is penalized. This encourages the model to output correct, non- hallucinated solutions. There's also the technique of retrieving relevant documentation: when asked a coding question, some systems fetch the official docs for the APIs in question and give that to the model, grounding its output. That way it's less likely to make up usage or functions since it can see the real ones.

Despite these, code models do still hallucinate especially for rare tasks or when asked in natural language that's slightly ambiguous. They might produce syntactically correct but logically wrong code. In those cases, formal verification or human review is needed - which is analogous to fact- checking for text.

### 6.6 Multimodal Applications

As LLMs extend to multimodal inputs/outputs (like image+text models, or audio+text), hallucination takes on new forms. For instance, image captioning models sometimes exhibit object hallucination: they confidently caption an object that isn't actually in the image (e.g., seeing "a dog by the beach" when there is no dog) 7. This is similar in concept to text hallucination - the model's prior learned bias for co- occurring objects can insert an object that is "likely" but not truly present. In fact, the term hallucination was used in vision before NLP in exactly this way: early image models "hallucinated" missing details or objects to create plausible images 7.

For vision- language models (like describing an image or answering questions about an image), a common hallucination is giving a plausible answer that isn't supported by the visual evidence. For example, if shown a blurry photo of a person and asked "What is this person's profession?", a model might say "He is a firefighter" with no real basis. It's guessing from contextual cues (maybe he wears something red that it misinterprets as a uniform). Detection here can involve checking consistency - e.g., if multiple descriptions of the same image conflict, some must be hallucinated.

In text- to- image generation (like DALL- E, Stable Diffusion), hallucination might be considered a different problem (the images might contain artifacts not in the prompt, etc.), but that's beyond our focus which is mainly textual hallucination. However, interestingly, if you have an image+LLM system (like GPT- 4 with vision), it might hallucinate interpretations of an image that aren't real - e.g., describing unseen parts of an image.

Audio: If an LLM- based system is doing things like transcribing and summarizing audio, it might mis- hear (ASR error) and then hallucinate part of summary. In any generation from raw data, there's potential to add things not in input.

Cross- checking between modalities can mitigate hallucination: e.g., forcing alignment between described image and text. Some works impose that any detail said in text must be grounded in some part of the image features. If not, it's considered hallucinated and either removed or not generated.

### 6.7 High-Stakes Domains (Medicine, Law, Finance)

Hallucinations are particularly concerning in domains where accuracy is paramount:

Medicine: An LLM doctor assistant might hallucinate a treatment recommendation or a drug dosage. Even small errors can be life- threatening. For example, advising a patient to take a certain medication that doesn' s actually treat their condition, or mixing up symptoms. Ensuring fidelity here is critical. That' s why there are efforts on fine- tuning medical LLMs with strict factual supervision and using retrieval (e.g., have the LLM cite medical literature). Some papers talk about "medical hallucinations" when a model says something not backed by clinical evidence 77 Often, these models are instructed to always provide a source or say they can' t answer without one. Law: Legal Al tools that summarize cases or suggest arguments have to avoid fabricating case law or statutes. The infamous example where an attorney submitted a brief containing fake case citations generated by ChatGPT has become a cautionary tale - the AI had made up cases that sounded real 5 . The solution being pursued is to limit models to a closed database of verified legal documents and force them to only draw from there (and provide the citations). If a query is outside that knowledge, the model should say "no found precedent" rather than making one up. Finance: In finance, making up a number or mis- stating a trend can lead to huge decisions being made wrongly. If a financial assistant Al hallucinated a company' s earnings or a stock' s performance, it could mislead investors. These systems thus incorporate rigorous checks or only allow generation from actual data (like it has to pull the number from a database or not give it). Any statements like "Company X' s revenue grew  $20\%$  last quarter" should be directly linked to known data.

Across these domains, a pattern is emerging: constraint and verification. The LLM is not trusted to freeform generate as much; instead, it' s constrained to sources, and everything it says is supposed to be traceable. Researchers in these fields often deploy a human- in- the- loop, at least for now, where the model' s output is reviewed by a professional.

### 6.8 Benefits of Hallucination?

One might ask: are there cases where "hallucination" is not entirely bad? In creative writing, what we call hallucination is essentially creativity. If you tell an LLM to write a short story or a poem, you actually want it to invent details, characters, metaphors - none of which are grounded in a single source of truth. Here, factual accuracy is not the goal. The model drawing on its imagination (learned from tons of text) is a feature, not a bug. This is why any blanket attempt to eliminate hallucination must be nuanced: we want zero factual hallucinations in factual tasks, but we want the model to still be imaginative and generative when appropriate. The term "hallucination" wouldn't be used in a purely fictional context as a pejorative, but the line can blur. For instance, if asked to "tell me a bedtime story with dinosaurs and astronauts," the output is all "hallucinated" in a sense, but it's desired. The key difference is user intent and expectation. If the user expects truth and the model delivers fiction, that's bad. If the user expects fiction, that's fine.

Thus, some systems are exploring mode switching: an LLM might have a mode where it' s strictly factual (and it will refuse to answer or be very cautious if unsure) versus a mode where it' s allowed to be more fanciful. Indicating this to the user (like "This is a creative generation, not factual" ) is also important to set expectations.

## 7 Open Challenges and Future Directions

Despite significant progress, several open challenges remain in the quest to handle hallucinations in LLMs:

Robust and Generalizable Detection: We have various detectors and metrics, but none are perfect. Models can still produce very subtle falsehoods that slip past automated checks. For example, a statement might be partially true and partially false - current NLI- based metrics might classify it as "somewhat" entailed and not flag it. Also, detection systems themselves can make mistakes and even be gamed (an LLM might learn to produce outputs that feel a particular detector). Research is ongoing to create detectors that are generalizable - able to flag hallucinations in any domain or style. This is hard because what counts as a hallucination can be context- dependent. Future detectors might use more world models or cross- verify from multiple perspectives to ensure a statement' s truthfulness.

Knowledge Boundaries & Uncertainty: As highlighted, a big challenge is getting models to know their own limits. Estimating the knowledge boundary - what the model does vs. doesn' t know - is an unsolved problem 78 . Humans have a sense of when they don' t know something (though not always accurate, at least better than current AIs). Endowing models with a similar sense is tricky. Some ideas include training models to internalize a calibrated score for each output token (like a probability of being correct) or to generate an "I' m  $X\%$  confident" statement. If a model could reliably say "I only have  $60\%$  confidence in this answer," that would be a game- changer for trust and usage. Users could then decide to verify such answers. Right now, a lot of effort is going into techniques like Monte Carlo dropout or ensembling for LMs to get uncertainty, or using large models to evaluate smaller models.

Minimizing the Alignment Tax: One concern is that techniques to reduce hallucination (especially heavy- handed ones like making the model very cautious or always retrieve) might impair other aspects of performance. This is sometimes called the alignment tax: making a model more truthful and safe could make it less creative or less fluent. For instance, a model that refuses to answer anything uncertain might become too terse or not attempt useful answers. There is a balancing act between helpfulness and honesty. Future research is looking at ways to mitigate hallucinations without overly compromising the model' s utility. For example, can we have the model mostly be creative but flip to a grounded mode when it detects a factual query? This dynamic adjustment is complex. The ideal is an Al that "knows when to improvise and when to stick to facts." Humans do this (we tell fanciful stories but switch to precise recall when giving someone directions to a location, ideally). Getting models to that level of situational awareness is a future goal.

Interactive and Explanatory Mitigations: Another direction is making the Al explain or justify its answers in a way that a user or another system can verify. If the model says: "The Battle of X took place in  $1605^{\circ}$  , it could add: "(Source: Encyclopedia Britannica)". Even if it didn' t literally retrieve Britannica, it could be made to produce such attributions, which can then be checked. Some research calls this self- referential transparency - the model should show its work, not just final answers 53 . This ties into chain- of- thought that is visible to the user. One risk: if the chainof- thought is also not grounded, it could just be a verbose hallucination. But perhaps requiring references in the chain- of- thought would keep it honest.

- Hybrid Systems: The future likely lies in hybrid AI systems – combining LLMs with symbolic reasoning, knowledge graphs, and logic engines. A hybrid system might, for example, use an LLM

to parse a question and propose an answer, but then use a symbolic module to verify the answer against a knowledge base or to ensure it doesn’t violate known constraints (like physical laws or arithmetic). If it does, the system can correct or refuse. These hybrids could catch hallucinations that purely neural methods might miss. On the flip side, making them work seamlessly is an engineering challenge.

- **Continual Learning and Correction:** Hallucinations can be addressed by teaching the model after the fact. If users consistently report a certain hallucination (e.g., the model always says some wrong info about a person), one could update the model (through fine-tuning or model editing techniques) to correct that. Continual learning systems could over time patch many frequent hallucinations. The challenge is not to break other knowledge while doing so (the plasticity-stability dilemma). There is interest in knowledge editing algorithms where you can directly correct a model’s belief on one fact without retraining from scratch. If those mature, they could be used to systematically eliminate certain common hallucinations (like all the fake citations it used to give can be replaced with a procedure to find real ones).

- **Evaluation Paradigms:** Going forward, how we evaluate LLM quality will heavily weight factuality. Benchmarks like HELM propose multi-metric evaluation, and “hallucination rate” or “factuality score” is a key metric. We might see standardized tests that any LLM must pass (e.g., less than  $x\%$  hallucinated content on a battery of tasks) before it’s considered deployable for certain uses. The community is likely to develop better and more challenging truthfulness benchmarks – e.g., combining knowledge and reasoning (questions that require reasoning and factual recall, where hallucination can happen in the reasoning step by assuming a false intermediate fact).

- **User Training and UI Design:** Another angle: train the users to work with AI in a way that mitigates hallucination impact. For instance, interfaces might highlight model-generated content that is not verbatim from a source, so the user knows what to double-check. Some chat interfaces already cite sources for factual statements. Maybe future UIs will have a “verify” button next to each sentence the AI writes – clicking it triggers a quick search to confirm or refute that sentence. Educating users that “AI can be wrong” is ongoing, but making it concrete in the UI is helpful. If a user sees a warning icon or different color text for potentially unverified info, they can treat it appropriately.

In conclusion, while hallucination in LLMs is a hard problem, the community is actively addressing it from many angles: data curation, model training tricks, retrieval augmentation, better prompting, robust evaluation, and user- centered design. The broad consensus is that hallucinations cannot be entirely eliminated (short of endowing models with human- like understanding and knowledge coverage, which is a distant goal), but they can be managed to a level where LLMs are still immensely useful. By combining multiple mitigation strategies – much like layers of defense – we can dramatically reduce the frequency of hallucinated outputs and build trust in LLM- driven systems. The journey will involve ongoing monitoring and refinement, especially as models become more complex or are given more autonomy. Nonetheless, the progress from the free- wheeling GPT- 3 of 2020 (which would casually make up almost anything) to the much more factual GPT- 4 of 2023 shows that we are on a promising trajectory towards LLMs that “know what they know, and know what they don’t know.”

## 8 Conclusion

Hallucination in large language models stands as a primary obstacle to their reliable deployment in many domains. We have surveyed the landscape of this issue – from its definitions and origins to the array of techniques being developed to detect and mitigate it. Hallucinations, defined as outputs that are

superficially plausible yet unsupported or incorrect, arise from the very nature of how LLMs are trained and operate. They reflect the misalignment between statistical language modeling and our expectations of factuality and faithfulness in information- providing AI.

The community's response to this challenge has been vigorous and creative. Formal taxonomies now categorize hallucinations, enabling more targeted analysis. We've seen that no single approach suffices: progress comes from hybrid strategies that improve training data, adjust modeling objectives, incorporate external knowledge, and leverage clever prompting and validation at inference time. Retrieval- augmented generation, in particular, has emerged as a powerful method to ground model outputs in reality, effectively reining in the model's free- form improvisation by tethering it to real documents.

Our historical overview highlighted that awareness of hallucination as an AI pathology has grown substantially, especially with the public deployment of LLM chatbots. Each generation of models - from early sequence- to- sequence models to today's GPT- 4 and beyond - has confronted the trade- off between linguistic fluency and truthful accuracy. Over time, we are learning to narrow that gap. Models today are far more factual than their predecessors, yet they remain imperfect. Importantly, they exhibit a kind of cognitive stubbornness - a tendency to confidently assert information regardless of veracity - which requires external correction mechanisms.

Looking forward, the quest for trustworthy AI will likely define the next era of language model research. Open challenges include developing nuanced self- awareness in models (so they can internalize uncertainty and decline to answer when likely to hallucinate), balancing creativity with accuracy (so that alignment with truth does not unduly hamper the generative capabilities that make LLMs so useful), and scaling evaluation to continually audit models as they learn and evolve. The integration of symbolic knowledge and reasoning with neural networks offers a promising avenue to achieve systems that have the best of both worlds: the flexibility of learning from data and the rigor of logical consistency and factual grounding.

In critical applications - from summarizing scientific research to providing medical or legal advice - even a single hallucinated statement can be harmful. Thus, researchers are increasingly adopting a safety- first approach, where an LLM's outputs are double- checked, sources are cited, and uncertain answers are treated with caution. This paradigm may slow down the interaction slightly or add complexity, but it is essential for responsibility.

In closing, hallucination in LLMs is not merely a quirk to be noted - it is a fundamental limitation to be addressed with the full weight of our technical ingenuity. The progress surveyed in this article demonstrates that while LLMs are not infallible, we have the tools to mitigate their failings. By doing so, we unlock the immense positive potential of these models - as assistants, educators, and creativity amplifiers - while minimizing the risks of misinformation. The path forward will require multidisciplinary effort, combining insights from machine learning, cognitive science, human- computer interaction, and more. But if successful, it will lead to AI systems that we can engage with confidence - systems that inform and augment human knowledge without persistently leading us astray with figments of their silicon imagination.

References: (Cited inline throughout the text with reference markers [] . Key references include Maynez et al. (2020) on summarization hallucinations 9 25, OpenAl' s analysis of why models guess when uncertain 41, Ji et al. (2023) survey defining intrinsic/extrinsic hallucinations 24, and Farquhar et al. (2024) on detecting confabulations via entropy 3 , among others.)

## References

1 3 38 42 46 68 76 Detecting hallucinations in large language models using semantic entropy | Nature https://www.nature.com/articles/s41586- 024- 07421- 0?error=cookies_not_supported&code=e61f1f73- c7ea- 4f9- 8a7e- 40a9320585d4

2 6 10 21 27 39 Loki' s Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models https://arxiv.org/html/2507.02870v1

4 5 20 LLM Hallucination—Types, Causes, and Solutions | Nexla https://nexla.com/ai- infrastructure/llm- hallucination/

7 8 30 78 Survey of Hallucination in Natural Language Generation https://arxiv.org/html/2202.03625v6

9 34 On Faithfulness and Factuality in Abstractive Summarization - ACL Anthology https://aclanthology.org/2020. acl- main.173/

11 12 13 14 24 31 55 56 [2401.11817] Hallucination is Inevitable: An Innate Limitation of Large Language Models

Language Models

https://ar5iv.labs.arxiv.org/html/2401.11817v2

15 16 50 51 58 LLMs Will Always Hallucinate, and We Need to Live With This https://arxiv.org/html/2409.05746v1

17 18 [2508.01781] A comprehensive taxonomy of hallucinations in Large Language Models https://arxiv.org/abs/2508.01781

19 22 23 25 26 28 29 35 48 49 57 69 74 On Faithfulness and Factuality in Abstractive Summarization https://aclanthology.org/2020. acl- main.173. pdf

32 33 [PDF] The Curious Case of Hallucinations in Neural Machine Translation https://aclanthology.org/2021. naacl- main.92. pdf

36 Asking and Answering Questions to Evaluate the Factual ... https://aclanthology.org/2020. acl- main.450/

37 [PDF] TruthfulQA: Measuring How Models Mimic Human Falsehoods https://aclanthology.org/2022. acl- long.229. pdf

40 53 70 71 72 73 75 aclanthology.org https://aclanthology.org/2024. findings- emnlp.685. pdf

41 43 44 45 54 cdn.openai.com https://cdn.openai.com/pdf/d04913be- 3f6f- 4d2b- b283- ff432ef4aaa5/why- language- models- hallucinate.pdf

47 Understanding and Detecting Hallucinations in Neural Machine ... https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00563/116414/Understanding- and- Detecting- Hallucinations- in

52 You believe your LLM is not delusional? Think again! a study of LLM ... https://link.springer.com/article/10.1007/s44248- 025- 00041- 7

59 60 61 62 65 66 67 What is Retrieval- Augmented Generation (RAG)? A Practical Guide https://www.k2view.com/what- is- retrieval- augmented- generation

63 64 Retrieval Augmented Generation

https://www.ibm.com/architectures/patterns/genai- rag

Medical Hallucination in Foundation Models and Their Impact on ... https://www.medrxiv.org/content/10.1101/2025.02.28.25323115v1. full- text