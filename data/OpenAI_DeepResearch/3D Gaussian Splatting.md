# 3D Gaussian Splatting: A Comprehensive Survey

## 1 Introduction

Novel view synthesis has seen a revolution with Neural Radiance Fields (NeRF) since 2020, enabling photorealistic 3D scene rendering from images. However, classic NeRFs rely on deep neural networks and dense volumetric sampling, resulting in slow training (hours to days) and rendering (often  $< 1$  fps) 1. Recent advances have sought faster, more efficient representations without sacrificing quality. 3D Gaussian Splatting (3DGS) emerged in 2023 as a breakthrough explicit representation that achieves real- time rendering  $(30+$  fps) at high resolution while maintaining high visual fidelity 2 . This survey provides a comprehensive overview of 3D Gaussian Splatting: we introduce key concepts and formalisms, trace its historical development and milestones, review foundational methods and recent improvements, discuss open challenges, and highlight applications across graphics, vision, and beyond. We aim to make the survey accessible to newcomers and useful for experts, balancing technical depth with clarity in a structured format.

## 2 Background and Key Concepts

Radiance Fields and Novel View Synthesis: A radiance field is a function that outputs color and density for any 3D point and viewing direction, effectively encoding a scene' s appearance from all angles. NeRF represents this as a continuous 5D function learned by a neural network, taking a 3D location  $\mathbb{S}(\mathbf{x},\mathbf{y},\mathbf{z})\mathbb{S}$  and viewing direction  $\mathbb{S}(\backslash \mathfrak{t}\mathfrak{t}\mathfrak{a}\mathfrak{t}\mathfrak{a},\mathfrak{t}\mathfrak{p}\mathfrak{h})\mathbb{S}$  to predict volumetric density and emitted radiance. Volume rendering techniques (integration of density and color along camera rays) are then used to synthesize new views. NeRFs can capture fine geometric detail and view- dependent effects (e.g. specular highlights), but they are implicit and require heavy computation for both training and ray- marching- based rendering. Subsequent work like Instant Neural Graphics Primitives (Instant- NGP) and Plenoxels replaced the neural MLP with explicit grids of features or voxels to accelerate training and inference, but these still sample many points in empty space and can be memory- intensive 1 .

Point- Based Representations and Splatting: An alternative to volumetric grids is to use point- based representations. In computer graphics, splatting refers to rendering points as discs or kernels on the image plane, "splattering" their influence onto nearby pixels instead of explicitly rasterizing triangles. This idea dates back to early volume rendering work by Westover in 1990, who introduced feed- forward splatting using Gaussian filter kernels 3 . Each voxel (or point sample) is treated like a tiny "snowball" thrown at the image plane, depositing a footprint (a Gaussian blob) onto the pixels it covers 3 . In 2001, Zwicker et al. developed surface splatting with elliptical weighted average (EwA) filters, where each point on a surface is rendered as an oriented elliptical Gaussian to achieve high- quality anti- aliased results 4 5 . These classic splatting techniques laid the groundwork by showing that collections of Gaussian kernels can represent surfaces or volumes smoothly. However, point- based rendering historically struggled with aliasing, visibility ordering, and lack of connectivity information, limiting its adoption versus mesh- based rendering in the past 6 7 .

From NeRF to Gaussian Splatting: 3D Gaussian Splatting marries the benefits of radiance fields with explicit point- based representations. Instead of encoding the scene in a neural network, the scene is represented as a set of 3D Gaussian primitives in space. Each Gaussian has continuous spatial extent (a "blob" in 3D) defined by parameters: a mean position  $\mathbb{S}\backslash \mathfrak{mu}\backslash \mathfrak{in}\backslash \mathfrak{m}$  athbb{R}^3 $\mathbb{S}$ , an oriented covariance

matrix  $\S \backslash \text{Sigma} \backslash \text{in} \backslash \text{mathbb{R}} \backslash \text{R} \backslash \text{3} \backslash \text{times3} \backslash \text{S}$  (which defines its size and anisotropic shape), and radiance properties like color and opacity. Intuitively, a single 3D Gaussian can be thought of as a fuzzy splat that covers a local region of space, contributing a smooth amount of density and color. By optimizing a sufficient number of such Gaussians, one can approximate the continuous radiance field of the scene. The rendering of a novel view from these Gaussians is done by projecting each 3D Gaussian onto the 2D image plane (resulting in an elliptical 2D footprint) and alpha- blending their contributions, a process often called rasterization- based splatting. This contrasts with NeRF's ray marching: instead of integrating samples along many rays, 3DGS "throws" Gaussians at the image. Because only Gaussians that project into a given pixel will contribute, computation is focused on relevant regions, avoiding wasted work in empty space  $^{8}$ . Importantly, 3DGS still retains volume rendering principles (alpha compositing, densities) to handle transparency and view- dependent color, preserving photorealism and effects like specular highlights but with an explicit model. In the original 3DGS, each Gaussian carries a color that can vary with view direction using learned spherical harmonic coefficients (analogous to how Plenoxels represented view- dependent appearance)  $^{9}$ . This allows 3DGS to model reflective or shiny surfaces by modulating splat colors based on camera angle.

Key Properties of 3D Gaussians: A 3D Gaussian  $\S \backslash \text{mathcal{G}G} \backslash \text{G} \backslash \text{S}$  in the scene is defined by a density function  $\S_{\text{g\_i}}(\backslash \text{mathbf{f}} [x]) = \backslash \exp \{- \backslash \text{mathbf{f}} [x] - \backslash \text{mu\_i}\} \backslash \text{T} \backslash \text{Sigma\_i} \backslash \{- 1\} (\backslash \text{mathbf{f}} [x] - \backslash \text{mu\_i}\})$ . (possibly with anisotropic covariance  $\S \backslash \text{Sigma\_i}$ ). Its projection onto the image is an ellipse (the covariance projected into 2D). The size (scale) of the Gaussian and its orientation (anisotropy axes) are critical parameters - these are optimized so that the collection of Gaussians, when splatted, reproduces the input images from all training views. Anisotropic Gaussians enable each splat to align with surfaces (e.g. an elongated Gaussian oriented along a wall or tree branch) to better approximate geometry. Moreover, Gaussians can freely overlap and cover continuous space, which gives a smooth interpolation of color and density - this helps avoid harsh artifacts that a point cloud might exhibit. The 3DGS pipeline typically begins with a sparse 3D point cloud (e.g. from Structure- from- Motion or depth estimations) to initialize Gaussian positions. Then it alternates between optimizing the Gaussian parameters (position, covariance, color, etc.) and pruning or adding Gaussians to best fit the data  $^{11}$ . This interleaved optimization and density control ensures the final set of Gaussians is compact yet expressive, concentrating Gaussians where detail is needed and removing them from empty or redundant regions  $^{12}$ .

Rendering Algorithm: A core innovation of 3DGS is a fast visibility- aware splatting algorithm. Naively splatting millions of Gaussians into an image could be slow, but Kerbl et al. propose a tiled rasterization approach on the GPU. Gaussians are binned into screen- space tiles, and only Gaussians overlapping a tile are considered for those pixels, significantly culling the workload. Anisotropic splats are approximated with an oriented elliptical footprint per Gaussian, and splats are alpha- composited in the correct back- to- front order (or using depth buffering) to handle occlusions. Thanks to this, 3DGS can render at 1080p in real- time (30- 60 fps) on modern GPUs - a dramatic improvement over the  $< 1$  fps typical of NeRF without sacrificing much quality. The combination of fast differentiable splatting and efficient optimization means training a scene with 3DGS can take only a few minutes (for a small scene) to an hour (for a large scene)  $[40 + \text{lock} 00]$ , compared to many hours for NeRF. The end result is a model that directly supports real- time view changes, making radiance fields practical for interactive applications for the first time.

## 3 Historical Timeline of Milestones

To put 3D Gaussian Splatting in context, we summarize major milestones in related research and technology:

- 1990: Westover introduces volume splatting 3 
- a forward rendering of volume data using Gaussian footprint kernels. Establishes the term "splatting."- 2000-2001: Pfister et al. propose Surfels (surface elements) for point-based rendering of objects. Zwicker et al. develop EWA Surface Splatting, using anisotropic Gaussian kernels to render point-sampled surfaces with high quality (SIGGRAPH 2001) 4 
- 5. These works show the viability of point-based representations for high-quality rendering, albeit mostly for static models.- 2020: Mildenhall et al. publish NeRF, igniting the neural rendering field. NeRF demonstrates unprecedented view synthesis quality, but with slow raymarching and heavy neural networks.- 2021-2022: Rapid progress in radiance fields. Notable advances include NeRF++/NeRF360 for unbounded scenes, PlenOctrees and Plenoxels for faster rendering via explicit voxel grids, Instant NGP (Neural Graphics Primitives) for fast training using multi-resolution hash grids, and Point-NeRF (CVPR 2022) which represents radiance fields with point clouds + neural features 13. These set the stage by highlighting trade-offs between implicit vs. explicit methods. Point-NeRF in particular suggests that distributing learnable features on points can accelerate rendering, but it still required a neural network for color estimation 13.- 2023 (April): 3D Gaussian Splatting (Kerbl et al.) is released. By July 2023 it appears in ACM TOG 42(4) and is presented at SIGGRAPH 2023, where it wins the Best Paper Award. This marks a turning point: 3DGS achieves real-time  $(30+$  fps) novel view synthesis at 1080p with visual quality on par with or exceeding NeRF. The authors open-source their code 14, sparking a wave of rapid adoption and follow-up research. By late 2023, companies and tools (Nerfstudio, Luma AI, Polycam, etc.) integrate 3DGS into their pipelines.- 2023 (Late): Early follow-ups tackle 3DGS limitations. CompGS (Compact Gaussian Splatting) introduces vector quantization to greatly compress the storage size of 3DGS models (reducing memory  $40 - 50\times$ ) while slightly speeding up rendering 15 
16. LightGaussian (NeurIPS 2024) uses pruning and encoding to achieve  $\sim 15\times$  compression and  $200+$  fps rendering 17 
18. These works address the fact that original 3DGS models can contain millions of Gaussians, far larger than a NeRF network in memory.- 2024 (early): Emphasis on geometric precision and detail. 2D Gaussian Splatting (2DGS) by Huang et al. (SIGGRAPH 2024) replaces 3D volumetric Gaussians with 2D oriented Gaussian disks (surfels) that lie directly on scene surfaces 19. This yields much more accurate surface geometry and eliminates "floaters" or fuzzy artifacts by enforcing multi-view consistent surfels 20. Another approach, TRIPS: Trilinear Point Splatting (Eurographics 2024) hybridizes splatting with a neural upsampling: it resturizes points into a multi-resolution pyramid to preserve fine details, then uses a lightweight network to fill in sub-splat details 21 
22. TRIPS specifically addresses 3DGS' s tendency to blur very thin structures by achieving crisper results (at 60 fps) 23 
24. Meanwhile, SuGaRt (CVPR 2024) proposes a pipeline to extract high-quality meshes from Gaussian Splatting in minutes 25, reflecting a push to convert radiance fields to standard geometric representations for use in content creation. On the view-dependence front, researchers note that using low-order spherical harmonics for Gaussian colors can struggle with complex specular highlights; e.g. Spec-Gaussian (NeurIPS 2024) introduces anisotropic lobes to better model view-dependent appearance 26 
27.- 2024 (late): Scaling and integration. Gaussian Opacity Fields (GOF) (SIGGRAPH Asia 2024) present a method to extract surfaces directly from the 3D Gaussian representation by identifying an "opacity level set" - effectively bridging the gap between volumetric Gaussians and an explicit surface 28 
29. Mip-Splatting (2024) introduces a mipmap-like scheme for Gaussians to

handle aliasing in zoom- outs  $^{30}$ , similar to how Mip- NeRF integrated multi- scale. Applications diversify: 3DGS is applied to urban- scale scenes (e.g. integrating with SLAM systems  $^{31}$ ), and to humans/avatars. Human Gaussian Splats (HuGS) (CVPR 2024 by Apple) represent dynamic human performances as Gaussians, enabling real- time free- viewpoint video of people  $^{32}$ . Generative AI also enters: e.g. GSGEN (ICCV 2023) and subsequent works generate Gaussian splat representations from text prompts, combining 2D diffusion models with 3DGS to produce 3D assets from text  $^{34}$ .  $^{35}$ . This indicates the community's interest in using 3DGS as a 3D backbone for generative models due to its explicit, fast- rendering nature.

- 2025: Ongoing research addresses remaining challenges. PUP 3D-GS (Principled Uncertainty Pruning) (CVPR 2025) provides a formal information-theoretic approach to prune  $90\%$  of Gaussians with minimal quality loss, yielding  $3.5 \times$  faster rendering  $^{36}$ .  $^{37}$ . SpeedySplat (CVPR 2025) redesigns the splatting rasterizer to precisely localize each Gaussian's footprint (avoiding overdraw) and integrates pruning during training, achieving  $\sim 6.7 \times$  speedups and  $10 \times$  fewer Gaussians  $^{38}$ . On the theory side, Mo et al. (2025) formulate 3DGS optimization as solving a partial differential equation, adding a viscous term to stabilize training and prevent overfitting to noisy "floaters"  $^{11}$ .  $^{40}$ . Dynamic scene reconstruction with Gaussians also sees progress: ATGS (2024) introduces adaptive per-frame Gaussian surfel optimization for multi-view videos, tackling temporal consistency so that Gaussians for moving objects don't jitter between frames  $^{41}$ .  $^{42}$ . By 2025, 3D Gaussian Splatting has firmly established itself as a foundational technology across computer graphics and vision, with a meteoric rise evidenced by dozens of follow-up papers and widespread usage in industry.

## 4 3D Gaussian Splatting Fundamentals and Methods

### 4.1 Original 3D Gaussian Splatting (Kerbl et al., 2023)

The seminal work by Kerbl et al. introduced the core ideas of 3D Gaussian Splatting (3DGS) for radiance fields. Figure 1 below (from their paper's results) compares 3DGS to prior explicit radiance field methods and ground truth:

Comparison of novel view synthesis results by prior methods vs. 3D Gaussian Splatting. From left: InstantNGP  $[40 + 100\times 0]$  and Plenoxels  $[40 + 100\times 0]$  (explicit grid- based methods), Mip- NeRF360  $[40 + 100\times 0]$  (NeRF- based, very slow), 3DGS ("Ours") with short training  $[40 + 100\times 0]$  and longer training  $[40 + 100\times 0]$  , and the ground- truth image. Frame rates (fps), training times, and PSNR quality are reported for each. 3DGS achieves state- of- the- art quality (PSNR \~25) while running in realtime (93- 135 fps), far outperforming NeRFs in speed 1 43 .

Representation: The scene is modeled as a set of \(\) N\\(3D\) Gaussians \(\) \{mathcal{G}(\mathbf{G})\} \} \(\) (sum of Gaussian densities), and radiance (color) can be similarly accumulated taking into account view direction (via \(\) \mathsf{c}_{- }i\mathsf{S}\(and any view - dependent component). One can think of this as a\)N\\(Each Gaussian\)\S\(\mathsf{mathcal{G}}\)\_i\(\) has parameters \(\) \mathsf{c}_{- }i\mathsf{N}\mathsf{u}_{- }i,\mathsf{N}\mathsf{S}\mathsf{g}\mathsf{m}_{- }i,\mathsf{c}_{- }i,\mathsf{N}\mathsf{g}\mathsf{m}_{- }i\)\(where\)\S\(\)i\(\)S\(is its center position,\)\S\(\)Sigma_iS\(is a\)3\times 3\(covariance matrix (defining an ellipsoid shape and orientation),\)S\mathsf{c}_{- }i\mathsf{S}\(is an RGB color (or a small set of coefficients for spherical harmonic shading), and\)\S\(\)i\(\)S\(is an opacity or density scalar. The collection of Gaussians forms a continuous volumetric field: the density at a point\)\S\(\)mathbf{f}(x)\(\) can be approximated by \(\) \mathsf{c}_{- }i\mathsf{N}\mathsf{u}_{- }i\(\)sum_i\(\)i\(\)mathbf{f}(x)\(Gaussian Mixture Model (GMM) of the scene's radiance field. Initially,\)S\(\) might be on the order of only a few thousand: the authors start from the sparse point cloud obtained by SfM (structure- from- motion) during camera pose estimation. These points (with normals, if available) initialize the Gaussian means, and covariances can start isotropic and small. During training, \(\) \mathsf{c}_{- }i\mathsf{N}\mathsf{u}_{- }i\(\) can increase as needed by splitting Gaussians (adding new ones) in detailed areas, or decrease by merging/removing Gaussians in flat regions - this is the density control aspect \(^{12}\)

Optimization: 3DGS is trained by minimizing the error between rendered images and the ground truth input images from all training views, using gradient descent to adjust Gaussian parameters 44. The rendering process is differentiable, so gradients w.r.t. each Gaussian' s parameters can be computed. A notable innovation is optimizing not just color and position, but also the anisotropic covariance of each Gaussian. Optimizing  $\S$  \Sigma_i\S effectively lets each Gaussian stretch or shrink to better fit the scene structure: for example, a Gaussian covering a portion of a flat wall can elongate along the wall' s plane, increasing its footprint to cover that area with fewer primitives. This anisotropy optimization is interleaved with an overall density regularization where Gaussians that carry little weight (opacity) are pruned and others may be split - preventing an excessive number of Gaussians and avoiding overfitting to noise 12 . As training proceeds, the set of Gaussians evolves to give a multiscale approximation of the scene: large broad Gaussians for smooth regions, and multiple small, oriented Gaussians for high- detail areas and edges 45 . Kerbl et al. reported that their interleaved strategy (optimize then prune/split, repeatedly) was crucial for both speed and quality 46 - it avoids spending computation on millions of Gaussians from the start, while still allowing the capacity to grow where needed.

Visibility and Rendering: The rendering algorithm is tailored for speed. Each Gaussian \(\S\) \mathbf{\lambda}\mathbf{cal{G}}\mathbf{\lambda}\mathbf{\xi}\) projects to an ellipse in the image. The color contribution of that Gaussian to a pixel \(\S_{P}\S\) at image position \(\S (\mathsf{u},\mathsf{v})\S\) can be written as \(\S\) \alpha_{i}(p)\), \(\mathsf{c}_{- }\mathsf{c}(\mathsf{p})\S\), where \(\S\) \alpha_{i}\) is the opaqueness (between 0 and 1) contributed by \(\S\) \mathbf{\lambda}\mathbf{cal{G}}\mathbf{\lambda}\mathbf{\xi}\) at that pixel, and \(\S_{C - i}(p)\S\) is the Gaussian' s color (potentially view- dependent) at that pixel. These are combined in back- to- front order: the final color \(\S_{C(u,v)}\S\) is \(\S\) \sum(1- \alpha_{i}(p))\S\), which is equivalent to the standard volume compositing formula. In practice, 3DGS sorts Gaussians roughly by depth and then splats them onto the image with blending. A \} \alpha_{i}(p) \(\mathsf{c}_{- }\mathsf{i}(\mathsf{p})\) \prod \text{p} \{j\} \text{t} \text{t} \{behind\} \} \text{t} \text{t} \text{rasterization is used: the screen is divided into tiles (e.g. } 8\times 8 \text{pixels), and for each Gaussian a tight bounding box of its projected ellipse is computed to determine which tiles/pixels need processing 47 48. This was later improved by the SpeedySplat method to be exact 49 50, but even the original method significantly reduced overdraw. The renderer also performs early depth testing - if a Gaussian is entirely occluded by others already splatted, it can be skipped. These tricks yield high performance, enabling the authors to train on full 1080p images directly and still render at \(\sim 135\) fps for simple scenes \([40 + \text{look} 00]\) or \(\sim 90\) fps for complex ones \([40 + \text{look} 00]\), where prior NeRF methods could hardly achieve 1 fps at much lower resolution \([40 + \text{look} 00]\). Notably, 3DGS' s rasterization approach means render time is largely independent of scene depth/complexity (it depends on number of Gaussians and footprint sizes, but not on traversing long rays or hierarchies). This made radiance fields scalable to large, unbounded scenes (like outdoor environments) that NeRF360 tackled with difficulty using octrees or mipmaps 51 43. In 3DGS, an outdoor scene with sky simply has no Gaussians in the empty sky regions, so rendering is still fast, and the camera can roam freely without hitting far- plane issues.

Results: Kerbl et al. demonstrated 3DGS on standard benchmarks (Tanks & Temples, Mip- NeRF360 dataset, etc.), achieving PSNR quality on par with or slightly better than NeRF- based SOTA methods, while being orders of magnitude faster  $[40 + \text{look} 00]$ . For example, on the T&T "Bike" scene (Figure 1), 3DGS achieved  $\sim 25.2$  PSNR with 93 fps rendering  $[40 + \text{look} 00]$ , versus Plenoxels  $\sim 21.9$  PSNR at 8 fps and Mip- NeRF360  $\sim 24.3$  PSNR at 0.07 fps  $[40 + \text{look} 00]$ . This combination of speed and quality was unprecedented 1 2. Additionally, 3DGS naturally supports continuous level- of- detail: since each Gaussian has continuous extent, zooming out of a scene just causes many splats to overlap and produce smooth averaged colors (much like a mipmap), avoiding blocky aliasing. This property was later formalized by follow- up work (MipSplat, etc.) but is inherent to the Gaussian representation. One limitation noted was that 3DGS might produce slight over- blurring of very fine details (like wires or text) because the Gaussian kernels act like a low- pass filter if not sufficiently small 52. But this is a trade- off that can be mitigated by using more Gaussians or specialized methods (discussed below).

Overall, the original 3DGS paper established a new explicit, hybrid rendering paradigm: optimize a point- based scene representation (Gaussians) to behave like a radiance field, and render via fast rasterization instead of slow ray marching. It preserved the "best of both worlds" - the flexibility of radiance fields (learning view- dependent colors, capturing soft volumetric effects) and the efficiency of point rendering (processing only visible samples, leveraging GPU rasterization). This foundation unlocked many new research directions, as we cover next.

### 4.2 Enhancements in Efficiency and Compression

While 3DGS enables fast rendering, the number of Gaussians and storage size of the model became a concern. A NeRF model might be  $\sim 5\mathrm{MB}$  (a set of network weights), but a 3DGS model could be hundreds of MBs because it explicitly stores thousands or millions of Gaussians and their parameters (position, covariance, color, etc.). Several works have tackled this:

- Compact Vector-Quantized GS (CompGS): Navaneet et al. introduced a vector quantization approach for 3DGS 15 16. They observed that many Gaussians have similar attributes. By clustering Gaussian parameters using K-means during training, they create a small codebook of prototype Gaussians. Each actual Gaussian then just stores an index to a prototype plus maybe small residual adjustments. This reduced storage by \(40 
- 50 \times\) with minimal quality loss 53 16. In experiments, scenes compressible to  $\sim 1 / 50$  of original size still retained image quality within  $\sim 1$  dB PSNR of the uncompressed model 54. An additional benefit is slightly faster rendering \((2 
- 3 \times)\) due to fewer unique states to process 55. This method highlights redundancy in the representation 
- e.g. many Gaussians on a flat wall might share the same shape and color.

- LightGaussian (15  $\times$  compression + 200 FPS): Fan et al. (NeurIPS 2024) proposed LightGaussian 17 56, a pipeline that prunes and quantizes Gaussians for unbounded scenes. They combine three techniques: (1) Prune Gaussians with low "global significance" (a heuristic similar to measuring each Gaussian's impact on overall reconstruction error) 57. (2) Distill spherical harmonic colors down to fewer coefficients or even single colors if possible (since view-dependence might not be needed for all points). (3) Vector Tree Quantization, an advanced hierarchical quantization of positions and attributes. With these, they report on average  $15 \times$  storage reduction, and they also boost rendering from 144 fps to 215 fps on their test scenes 17 58. LightGaussian emphasizes unbounded/outdoor scenes, showing that even large-scale scenes (which can produce millions of Gaussians) can be handled efficiently.

- Pruning via Sensitivity (PUP 3D-GS): Hanson et al. (CVPR 2025) presented a principled pruning algorithm using a second-order sensitivity analysis 36 59. They derive a Hessian-based importance score for each Gaussian (essentially how much the training error would increase if that Gaussian were removed) 36 59. This is reminiscent of classic network pruning using second-order derivatives (LeCun's Optimal Brain Damage), but applied to these explicit primitives. With an iterative prune-and-fixture scheme, they manage to remove 90% of Gaussians on average, achieving a  $3.56 \times$  speed increase and even better image quality in some cases (because pruning tends to remove redundant or possibly noisy Gaussians) 60 37. Notably, PUP 3D-GS outperformed simpler heuristics and maintained more detail in foreground regions 37. This research provides a strong theoretical grounding to the idea of Gaussian importance.

- Rendering Acceleration (SpeedySplat): While pruning reduces total Gaussians, SpeedySplat (CVPR 2025) targeted the per-frame cost by optimizing how splats are drawn 38 61. The original 3DGS used a conservative method to assign Gaussians to tiles, which sometimes considered tiles where the Gaussian's footprint barely touched. SpeedySplat introduced SnugBox and AccuTile

algorithms to tightly compute each Gaussian's exact image footprint 62 50 . By analytically finding the ellipse' s extrema and scanning only the needed pixels, they eliminated wasted computations on empty tile areas. Additionally, SpeedySplat integrated an on- the- fly pruning during training: as gradients are computed, they use an efficient score (inspired by PUP but simplified) to drop Gaussians that aren' t pulling their weight 61 64 . The outcome was an impressive  $- 6.7x$  render speedup and  $- 10.6x$  fewer Gaussians, with virtually no quality drop 65 66 . Scenes that rendered at 30 fps with original 3DGS could exceed 200 fps with SpeedySplat' s improvements, making real- time visualization even on weaker devices feasible.

- Minimal Gaussians (OMG): Lee et al. (arXiv 2025) took the approach of directly limiting the number of Gaussians. Their OMG: Optimized Minimal Gaussian representation attempts to represent a scene with as few Gaussians as possible, rather than simply compressing attributes 67 68 . They found that if each Gaussian can have high-precision parameters (color, etc.), you may not need millions of them 
- perhaps tens of thousands could suffice if placed optimally. OMG algorithm first merges nearby Gaussians that represent redundant spatial coverage 68, then uses a compact encoding with sub-vector quantization for the attributes 69 . They reported nearly 50% storage reduction beyond prior compression, and achieved an astounding 600+ fps rendering on some scenes 70 (albeit with some loss in very fine detail). The idea here is pushing to the extreme: how lightweight can a 3DGS model get while still looking good? Their results suggest scenes like indoor environments can be captured with on the order of $10^4$ Gaussians without major quality loss 71 68 . This is promising for mobile or AR applications where memory and compute are at a premium.

Collectively, these efficiency- focused works ensure that 3DGS is not just fast in rendering, but also economical in storage and scalable to deploy. By pruning and compressing, one could imagine downloading a radiance field model over a network to a VR headset or phone, and it running at interactive rates. This contrasts with original NeRF models which were too heavy to run outside of powerful GPUs. An open challenge remains to standardize a format for these compressed Gaussian scenes (the industry has started adopting formats like .gs or .spx - Niantic' s Lightship, for example, uses an .SPZ format for splitting that was quickly adopted by others). As of 2025, one can load 3DGS scenes in engines like Unreal or Three.js via plugins, demonstrating practical deployment.

### 4.3 Improving Geometric Accuracy and Surface Extraction

One critique of the original 3DGS is that it represents volumetric radiance (like NeRF), which can lead to slight blurriness or "floater" artifacts: semi- transparent blobs that don't correspond to real surfaces, causing a cloudy look in some regions. This is partly due to the implicit nature of volume rendering - it does not enforce a single, hard surface, so the optimization may settle for a few translucent Gaussians approximating a complex shape (e.g., tree leaves) which from some angles looks fuzzy 52 11 . To address this, researchers pivoted toward making the Gaussian representation more surface- aligned:

- 2D Gaussian Splatting (Surfels): The SIGGRAPH 2024 paper by Huang et al. reframes the problem as modeling surfaces directly with Gaussians 19 72 . In 2DGS, each primitive is a planar Gaussian disk oriented in 3D (often called a surfel). This disk lies on the estimated surface and only emits radiance in the half-space of the surface normal (so it's like a little patch of a surface). Because a 2D Gaussian has no thickness, it avoids multi-view inconsistency: it always appears at the correct depth from any angle, unlike a spherical 3D Gaussian which could be incorrectly intersected by rays from different views 20 . The authors start with an initial mesh or depth map to place these surfels. They then develop a perspective-correct splatting rasterizer 73 : instead of naive screen-space size, they compute each surfel's exact size based on ray intersection (accounting for

foreshortening) 73. This produces correct silhouettes and avoids warping. They also add regularizers for depth distortion and normal consistency 74 73 so that neighboring surfels align into a coherent surface. The result is significantly improved geometry: 2DGS can reconstruct thin structures (wires, edges of objects) sharply where 3DGS gave a blur 20 75. They even demonstrate extracting high- quality meshes from the 2DGS model via Poisson surface reconstruction on the surfels 76 77 . The trade- off is that 2DGS, by strictly modeling surfaces, might struggle with truly volumetric effects (like semi- transparent fluids or foliage interiors), but most real scenes are largely opaque surfaces so this is a good assumption. In terms of rendering speed and quality, 2DGS achieved comparable appearance to 3DGS (sometimes slightly better PSNR because it nails the geometry) 19 75 and also runs in real- time. It essentially turns 3DGS into a surface rendering method while retaining differentiability and speed.

- Trilinear Point Splatting (TRIPS): Linus Franke et al.'s TRIPS approach (EG 2024) aims for higher detail without sacrificing splat-based speed. They highlight that 3DGS's splats act like low-pass filters, so very high-frequency details (fine text, lattice structures) get blurred 21. TRIPS addresses this by a multi-scale rendering: each point is splatted into a Gaussian pyramid (mipmap of the image) at an appropriate level based on its size 78. Large splats affect low-res layers; small splats affect high-res layers. Then, a small neural network fuses the pyramid into a final image, effectively super-resolving details beyond the splat resolution 78 79. This network can hallucinate detail that a single-scale splat missed. Importantly, the whole pipeline remains end-to-end differentiable, so they can train the points and the network together 80. TRIPS managed to produce much sharper images than 3DGS on scenes with text and thin geometry 52 24, while still rendering at 60 fps. The downside is the addition of a learned network breaks the fully-explicit nature and could reintroduce some neural render artifacts or instability (they note temporal instability in ADOP, a prior neural point method, which they try to overcome) 81. TRIPS essentially combines explicit and implicit: explicit splats for coarse content, neural refinement for detail. It shows a path to get the "best of NeRF and GS" 
- quality of neural methods with speed of splatting 
- at the cost of some added complexity.

- Gaussian Opacity Fields (GOF): Instead of switching to surfels, GOF (SigAsia 2024 by Yu et al.) retains full 3D Gaussians but adds a mechanism to extract surfaces. GOF defines a volumetric opacity field  $\S \backslash \mathrm{tau}(\backslash \mathrm{mathbf{f}}\mathbf{x})\S$  from the Gaussians and then finds its level-set (e.g.  $\S$ $\backslash \mathrm{tau}(\backslash \mathrm{mathbf{f}}\mathbf{x}) = 0.59$ ) as the implicit surface 28 82. They derive this from ray tracing: essentially, they treat the collection of Gaussians as a continuous density and compute where along each camera ray the accumulated opacity is 50%. By doing this from many directions, they accumulate points on the surface. They also introduce regularizations to encourage Gaussians to align to a common surface rather than float in front of each other 83. GOF's advantage is that it doesn't fundamentally alter the 3DGS representation or renderer - it just adds an extra computation to derive a surface. They achieve high-quality mesh reconstructions from 3DGS that were previously hard to get 28 84. In comparisons, GOF surfaces are more complete and accurate than naive density thresholding or other heuristics. It's a step toward using 3DGS for applications that require explicit geometry (like physics simulation or CAD models).

- Surface-Aligned Reconstruction (SuGaR): This CVPR 2024 method (Eisemann et al.) is aptly named 
- it aligns Gaussians to surfaces for mesh extraction 25 85. SuGaR takes a trained 3DGS model and performs a fast post-processing: first, a short additional optimization where each Gaussian's covariance is squeezed onto the local surface (using cross-view consistency tests), effectively converting volumetric blobs into flat elliptical disks (surfers). Then, it uses a specialized screened Poisson surface reconstruction that treats each Gaussian as a spherical kernel added to a 3D grid to produce a mesh 25. The result is an editable, watertight mesh that matches the

radiance field' s surfaces, obtained in minutes on a GPU 86 85. While GOF finds surfaces implicitly, SuGaR explicitly reshapes the Gaussians to lie on one. The authors report that SuGaR' s meshes are on par with or better than those extracted from NeRF via techniques like NeuS (which require lengthy additional training), all in a fraction of the time 86 85 . This is highly useful for content creation - enabling one to go from a set of photos to not just a splat- based rendering, but a standard 3D mesh  $^+$  texture that can be edited in DCC (digital content creation) tools.

The above developments show a trend: enforcing or extracting geometric correctness from the Gaussian representation. With 3DGS alone, one might get beautiful renderings, but if you looked at the point cloud of Gaussians it might not align perfectly to surfaces - some Gaussians could be floating slightly off. Methods like 2DGS and GOF ensure the model truly represents a coherent 3D scene structure, not just a radiance field. This is important for integrating these models into pipelines like games or AR, where you might need collision geometry or want to insert new objects. It also improves visual quality for free- viewpoint motion: with correct geometry, you won't see "shimmering" or misregistration between left- eye/right- eye views in VR, for instance.

Open challenges remain in geometry for 3DGS. Extremely fine geometry (hair, foliage) is still hard - either one accepts some blur or one needs extremely many tiny Gaussians which then impact performance. Also, dynamic surfaces (see next section) add complexity in maintaining geometry over time. Nonetheless, with surfel- based methods and advanced extraction, 3DGS is closing the gap with traditional 3D models while retaining its original rendering prowess.

### 4.4 Dynamic Scenes and Temporal Extension

Capturing dynamic scenes (where the scene content moves or changes over time) is an area where radiance fields and now Gaussian splats are actively being extended. NeRF- based approaches have dynamic variants (D- NeRF, Nerfies, etc.) that often incorporate time as another dimension or model deformation fields. How does Gaussian Splatting handle dynamic content?

- Per-Frame Optimization vs. Global Model: A straightforward approach to dynamic scenes is to treat each time frame independently 
- essentially reconstruct each frame' s radiance field or Gaussian model, then establish correspondences. Early attempts applied 3DGS per frame of a multi-view video, but this can lead to temporal inconsistencies (points flickering) 87 88 . One solution, demonstrated by AT-GS (Adaptive Temporally-Consistent GS), is an incremental approach: optimize frame 1 as usual (getting a Gaussian set for frame 1). Then for frame 2, start from frame 1' s Gaussians as an initial guess, rigidly transformed by an estimated motion (they estimate an SE(3) transform for the whole set or parts) 89 . Then optimize that to fit frame 2' s images, adding or removing Gaussians as needed for new or vanished objects 87 90 . They also enforce a temporal consistency loss using curvature of surfaces to keep subsequent frames' geometry aligned 91 97. This yielded much smoother results for dynamic videos, with greatly reduced jitter and more coherent motion of Gaussians over time 92 88 . Essentially, AT-GS treats the Gaussian set like particles that flow over time with the scene, instead of independently rediscovering the scene each frame.

- Human Performance Capture: Dynamic humans (people moving, dancing, etc.) are a special case of dynamic scenes with high interest (for VR avatars, volumetric video, etc.). Apple' s HUGS: Human Gaussian Splats 32 33 introduced a pipeline for capturing humans in motion with 3DGS. They represent both the human and the background scene as Gaussians, but critically allow the human' s Gaussians to deform based on an underlying skeleton or mesh. They combine

parametric human models (like SMPL) with Gaussian splats, so that one can animate the human by moving the underlying skeleton and warping the Gaussians accordingly  $^{93}$  33. HUGS achieved state- of- the- art quality on human view synthesis and could render at  $20+$  fps  $^{94}$ . The animatable nature means one could not just capture a specific sequence, but reuse the model for new poses - bridging Gaussian splats with more traditional character animation. Another approach, by Perez- Bellitero et al. (likely the HumanGaussian reference), used 3DGS to capture humans and reported  $\sim 1.5$  dB PSNR improvement over prior methods on a human dataset while still being real- time  $^{94}$ . These works often segment the scene into dynamic foreground (the person) and static background, modeling each with Gaussians, and handle them somewhat separately.

Handling Topology Changes: Dynamic scenes can have objects appear/disappear or topology changes (e.g., a person picking up an object - now they are one connected component instead of two). Neural field methods struggle with that because a single network must model all time or use a deformation field that usually assumes a fixed topology. Gaussian splats, being an explicit set, can more easily handle such events by simply spawning or removing Gaussians. For instance, ATGS explicitly mentions handling emerging/disappearing objects by its densification step per frame  $^{95}$  42. This flexibility is a strength of point- based representations in dynamic contexts. However, ensuring that new Gaussians get appropriate correspondences or do not jitter is challenging. Some methods combine optical flow or feature tracking across frames to guide the Gaussians.

Temporal Regularization: A common theme is adding regularizers so that Gaussians do not wildly reposition between frames (which causes flicker). AT- GS uses curvature consistency  $^{87}$  91, others might use temporal smoothness on each Gaussian's parameters. An interesting idea is to incorporate a motion model: e.g., assume each Gaussian moves with some velocity and include that in the optimization (similar to dynamic point cloud tracking). This could reduce needing to re- optimize from scratch each frame.

Dynamic 3DGS is still a young area, but the initial results are promising. They show that 3D Gaussian Splatting can be extended beyond static scenes to capture moving worlds - potentially enabling "NeRF in motion" but with the speed to render them live. A futuristic application is live 3D telepresence: multiple cameras capture a dynamic scene (like a performance), reconstruct it as Gaussians in near real- time, and you stream those to a user who can then view the scene from any angle in VR. The pieces (fast per- frame 3DGS, temporal coherence, compression) are falling into place to make this feasible. Open challenges include: dealing with very fast motion (where correspondence between frames is harder), distributed capture (maybe different cameras every frame), and incorporating physical constraints (e.g., ensuring that a human's Gaussian splats respect joint limits, etc.).

### 4.5 View-Dependent Effects and Appearance Enhancement

Rendering quality is not just about geometry; capturing complex material appearance and lighting is equally important. The original 3DGS allowed view- dependent color via spherical harmonics (SH), but this has limits. Recent work addresses more challenging appearance features:

Specular and Anisotropic Reflection: Spherical harmonics are low- frequency and isotropic basis functions, so highly glossy or mirror- like reflections are hard to represent (they' d require many SH bands). The Spec- Gaussian method (NeurIPS 2024, also called Anisotropic View- Dependent Appearance for 3DGS) tackles this by augmenting each Gaussian with a small neural network conditioned on view direction 26 27. Alternatively, they propose using a mixture of lobes per Gaussian to model specular highlights. This is akin to giving each Gaussian a tiny BRDF model. The term "anisotropic" here refers to the reflectance, not just the shape: they allow e.g. a

Gaussian to represent a shiny surface that reflects light differently at different angles (like brushed metal which has direction- dependent reflections). Their results show crisper highlights and better reproduction of shiny materials that  $3\mathrm{DGS} + \mathrm{SH}$  blurred out  $^{10}$ $^{27}$ . There is a performance cost, but since Gaussians are few, a small MLPer Gaussian is still manageable in real- time.

- **Image-Based Rendering Hybrid:** An approach called IBR-GS (not a formal name, but referring to image-based view-dependent appearance) was explored by some (seen in a 2024 journal)  $^{96}$ . The idea is to use the input images themselves to assist in view-dependent effects. For example, one could store for each Gaussian not just a single color, but a few key views textures and then do a view-dependent interpolation. This is similar to deformable sprite ideas in other contexts. The paper mentioned (possibly by Li et al.) used an image-based proxy to get reflections correct – essentially if a Gaussian is on a shiny surface, use the environment map or neighboring view info to modulate its color. While not as principled as physically-based shading, it can hack in complex reflections cheaply.

- **Illumination and Shadows:** Radiance fields typically bake in all lighting (including shadows) into the representation. But as these techniques progress, one might want the ability to re-light scenes or handle high dynamic range. A work called 3iGS: Factorized Illumination for 3DGS (ECCV 2024) looked at separating illumination from the representation  $^{97}$ . They factor the color of each Gaussian into a product of an “illumination” term (which could be view-dependent, like using environment maps or SH light probes) and an albedo term. This moves towards an inverse rendering capability – not just modeling what the scene looks like, but understanding how light interacts with it. While still early, such efforts might allow Gaussian splitting models to be relit under new lighting conditions or to cast correct shadows if light moves. One challenge is Gaussian splats don’t have a notion of occluders for light (they are translucent by nature), so casting hard shadows is tricky. Methods might use the extracted surfaces (from GOF or 2DGS) as shadow-casting proxies for more physically-based rendering.

- **Uncertainty and Denoising:** A subtle aspect is that some parts of a scene may be ambiguous or under-sampled (e.g., glossy surfaces with sparse views). A 2023 paper on View-Dependent Uncertainty Estimation for 3DGS addressed quantifying where the model is confident vs uncertain  $^{98}$ . By predicting a per-Gaussian uncertainty (like a variance in color), one can identify regions that might benefit from more training data or from regularization. This can also improve denoising: if a Gaussian is uncertain, one might avoid overfitting its color and instead make it broader or more diffuse to avoid high-frequency artifacts. In NeRF literature, uncertainty was used in depth estimation and active learning; similarly, in 3DGS it could guide adding new Gaussians adaptively only where needed.

In summary, while 3DGS initially focused on matching NeRF’s image output, ongoing research is expanding its capabilities to more richly model material properties and lighting. This will be crucial for using these models in mixed reality – e.g., if you bring a 3DGS- captured object into a new environment, you’d want it to reflect the new environment’s lighting. Some very recent works even combine neural radiance fields and Gaussian splats (RadSplat 2024) to leverage neural field knowledge for robustness  $^{99}$ .

The appearance enhancements often come at the cost of extra computation (per- Gaussian networks, multiple textures, etc.), so maintaining real- time performance is a balancing act. But given baseline 3DGS can hit hundreds of fps, there is some budget to spend for better visuals. One open area is high dynamic range (HDR) – radiance fields can capture only what’s in the photos, but capturing full HDR (bright lights, etc.) might need special treatment (one radiancefields.com article mentions an HDR extension to

GS that captures vibrant colors and high contrast 100 101 ). Another area is semantic attributes: some recent efforts (e.g., SpectralGaussians) integrate semantics into each Gaussian, effectively doing both geometric and semantic reconstruction 102 . That could enable applications in scene understanding (the HUGS for Holistic Urban Scene Understanding is along these lines - mapping an entire street with Gaussians labeled by semantic category) 103 .

### 4.6 Applications Across Sulffields

The rapid maturation of 3D Gaussian Splatting has led to its adoption in various domains:

- Interactive Graphics and VR/AR: Because 3DGS can render at  $30+$  fps, it's naturally suited to VR/AR where high frame rate and low latency are needed. By late 2023, plugins appeared to integrate GS scenes into Unity and Unreal Engine. This allows using captured real scenes as environments in games or VR experiences. Mozilla's Hubs (WebVR platform) even saw experiments replacing static photogrammetry with Gaussian splats for more realistic avatars and scenes, delivered over the web. On mobile AR, companies like Luma AI and Polycam (which previously used NeRF) switched to Gaussian Splatting to let users capture spaces with a phone and then view them in AR with smooth performance. Nvidia's research on streamable radiance fields also embraced GS, with formats like .NRD / .SPZ meant for compact transmission of scenes. AR applications benefit from GS's explicit nature 
- e.g., one can easily decimate the Gaussians for Level-of-Detail, or perform collision detection by approximating each Gaussian as a sphere for physics.

- Cultural Heritage and Mapping: Reality capture for museums, archaeological sites, or large-scale outdoor mapping can leverage GS for creating digital twins. Photogrammetry has long been used, but is slow to render large models. GS offers an efficient alternative: scan a site with images or a drone video, reconstruct via 3DGS, and you get an exploitable model with real-time rendering. The Holistic Urban 3D Scene Understanding via GS (HUGS, CVPR 2024) actually combines mapping an urban scene with semantic segmentation 103. They use GS to reconstruct a street and simultaneously predict semantics (car, building, road) for each Gaussian. This shows the crossover of computer vision tasks (3D understanding) and graphics in one framework. City-scale models might still be heavy, but pruning methods and multi-resolution tiling (imagine splitting a city into a grid of Gaussian clusters) could handle it.

- Film and Media Production: There is burgeoning interest in using radiance field tech in filmmaking and VFX. An article on Radiance Fields site described using Gaussian Splatting in a New Yorker magazine production 104 and even in art installations 105. For example, instead of green-screening, one could capture a live set as Gaussians and then later insert CG elements with correct lighting. Since GS retains photorealism, it can be a powerful backdrop. The Superman reference 106 hints that Gaussian splats were used to create a "Kryptonian" volumetric effect 
- perhaps leveraging the volumetric nature to portray a hologram or ghost-like projection in a movie, blending real textures with ethereal presence. Because GS can blend seamlessly with real footage (it is reconstructed from real footage), it's a compelling tool for mixed reality storytelling.

- Image/Video Editing and Re-Editing: A fascinating application is taking a single image or video and turning it into a 3D scene that can be edited (like changing viewpoint or modifying objects). Some late-2024 works (e.g., one from ADSABS reference) used Gaussian Splatting to achieve one-shot reconstruction from a single image 107 108. They leverage strong priors (like diffusion models or known object shapes) to hallucinate what the back of the object looks like, then optimize Gaussians to fit the image. The result is a 3D model from one image that you can rotate somewhat. While not perfect, this "3D from 2D" approach benefits from GS's flexibility and the fact that it doesn't need a heavy network run at inference 
- it optimizes a lightweight explicit

model that can be stored and manipulated. Adobe and others are keen on such tools for creatives: imagine taking a photograph and getting a 3D scene you can explore or animate.

- **Generative AI for 3D:** Text-to-3D has exploded in interest. Early works (DreamFusion, Magic3D) produced NeRFs or meshes from text, but they were slow and often low-quality. GSGEN  $^{34}$  and related methods incorporate Gaussian Splatting as the 3D representation for generative models. They typically use a Score Distillation (like DreamFusion) where a diffusion model (text-to-image) guides the optimization of Gaussians to match imagined images from various angles  $^{34}$ .  $^{35}$ . The benefit of GS here is twofold: optimization is faster (explicit model, fewer parameters than a neural grid perhaps) and the final output is readily viewable in real-time. GSGEN demonstrated generating simple objects (e.g., a 3D splat model of a mythical creature from a text prompt) that can be rendered interactively  $^{34}$ . GALA3D goes further by adding compositional generation – arranging multiple Gaussian-splat objects into a scene via text instructions  $^{109}$ . Generating whole scenes with consistent layout and lighting is a frontier, but Gaussian splats provide a nice canvas due to their additivity (just union sets of Gaussians for composition).

- **Scientific Visualization and Medical Imaging:** Volume rendering with splatting originated in scientific vis (think CT or MRI data). There is potential cross-pollination back: 3DGS could be used to render volumetric scans faster or even to compress them. If one fits Gaussians to a volumetric dataset, it might approximate it with far fewer primitives (similar to how one might fit Gaussians to a galaxy distribution in astrophysics, etc.). The differentiability and optimization aspects of GS also mean one could do inverse visualization – e.g., tune Gaussians to highlight certain structures in data. While not a focus of current literature, the connection is clear from the historical "splatting" term.

In industry and open- source, the adoption of 3DGS within a year of its introduction has been remarkable. Nerfstudio (a popular open framework for radiance fields) added support for Gaussian Splatting soon after the paper. Libraries and engines are being built around it (for example, Luma's API for capturing with an iPhone now uses GS under the hood). This broad adoption underscores that 3DGS is not just an academic novelty but a practically useful technology.

## 5 Open Challenges and Future Directions

Despite the impressive progress, there remain several open challenges and avenues for future research in 3D Gaussian Splatting:

- **Fine Detail vs. Performance:** There is a constant tension between capturing extremely fine details and keeping the model lightweight. Approaches like TRIPS and 2DGS improved detail, but at some cost (neural refinement or more complex rasterizers). Future work may explore multi-resolution adaptive Gaussian splats – e.g., dynamically instantiating micro-Gaussians only when the camera is close to an object (similar to how terrain LOD works). Also, combining GS with super-resolution techniques (render low res then upsample with a learned filter) could allow a base GS model to focus on geometry while a separate module adds texture detail. Ensuring temporal stability of these details in dynamic scenes also remains challenging (to avoid flicker when detail comes and goes).

- **Material and Lighting Consistency:** As we introduce more advanced view-dependent models, a challenge is how to maintain physical consistency. If each Gaussian has its own little BRDF or SH lighting, ensuring these collectively correspond to a plausible global illumination solution is nontrivial. One possible future direction is integrating explicit light sources into the optimization –

e.g., if the sun is in the scene, have a Gaussian or directional light representing it, and let Gaussians actually cast shadows or have occlusion attributes. This might require mixing rasterization with ray- tracing (for shadows) or moving towards a unified shader model for Gaussians. The benefit would be radiance field models that one can relight or that respond correctly to environment changes. Some initial work in NeRF relighting could be translated to GS.

- Dynamic Scenes: Efficiency and Generality: While methods like AT-GS make strides, dynamic Gaussian Splatting is computationally heavy if done naively per frame. How to efficiently update a Gaussian scene model in real-time as new frames come in is an open question. One could imagine a live system where an initial GS model is built and then refined on the fly with incoming video (akin to SLAM). Techniques from real-time SLAM (pose graph optimization, keyframe selection) might be combined with the differentiable splats. Also, generalizing to non-rigid deformations (cloth, fluid) is very open – one might need to explicitly model deformation fields for Gaussians (like each Gaussian carries a velocity or flow). The animatable human case is one instance of non-rigid (using a skeleton), but beyond that, perhaps combining GS with grid-based deformation (embedding Gaussians in a deforming grid) could help.

- Scalability to Very Large Scenes: City-scale or even world-scale capture with Gaussians raises issues of memory and Level of Detail. One promising direction is a tiled or streamed Gaussian Splatting, where you break the world into regions with their own Gaussian sets. The challenge is smooth transition at boundaries and managing many millions of Gaussians without rendering cost blowing up. Some ideas include hierarchical GS (clusters of Gaussians represented by higher-order Gaussians at distance) – essentially an octree of Gaussians. There has been work on MultiScale 3DGS (noted in semantic scholar list) which might address this. Cloud gaming or map services might leverage such techniques to stream only the Gaussians near the user’s view.

- Standardization and Tooling: As adoption grows, having standard file formats (much like glTF for 3D models) will help interoperability. Currently we see .gs or .spz used in specific tools 110. An open format that can store Gaussians, their SH coefficients, etc., along with metadata (scale, possibly multiple sequences for dynamic scenes) would catalyze broader use. Tools for artists to edit Gaussian-splat scenes (e.g., paint on them, delete unwanted Gaussians, attach animations) are also in infancy. Since each Gaussian is like a point primitive, editing could leverage point cloud editing methods.

- Comparative Evaluation and Theory: With so many variants emerging, a rigorous comparison of quality vs. speed vs. memory across methods is needed. An academic challenge or benchmark for radiance field rendering speed/quality could be established. Also, theoretical understanding of Gaussian splatting optimization is still limited – e.g., does the loss landscape have nice properties? The PDE interpretation by Mo et al. 111 is a start at linking it to well-studied mathematical models (they added viscosity, which draws parallels to diffusive processes ensuring stability). More theory could guide optimal learning rates, schedules for splitting/pruning, and so on. Additionally, exploring alternative kernels beyond Gaussians is an interesting idea – Gaussians are nice due to mathematical convenience, but perhaps other radially bounded kernels or even learned shapes could approximate scenes with fewer primitives (think of using oriented discs for surfaces – which 2DGS did – or perhaps elongated cylinders for cables, etc.). Some recent works allow a mix of primitives, e.g., combining Gaussian splats with explicit meshes for some objects (the Bridging 3D Gaussian and Mesh paper hint from huggingface list) 112. This could yield a hybrid scene representation where big planar surfaces use meshes and fine detail uses splats.

- Privacy and Ethics: With easy capture comes concerns – a world where anyone can scan any environment or person quickly raises privacy questions. NeRF already posed this, but faster tech

accelerates it. Future work might consider privacy- preserving 3DGS, such as methods to remove individuals or scramble identifying details in a capture (but still keep the scene structure). Also, deepfakes in 3D become possible: one could capture a person and then change their appearance or motion, which while a technical achievement, has ethical implications.

In conclusion, 3D Gaussian Splatting has rapidly progressed from a single paper into a rich subfield bridging vision and graphics. It offers a new perspective on scene representation that is explicit, efficient, and flexible. In just over two years, we've seen solutions to many initial limitations and an expansion into dynamic, semantic, and generative realms. The journey from NeRF's first wow- factor to real- time radiance fields has been astonishingly quick, and 3DGS stands at the forefront of this journey as both an enabler and a beneficiary of the community's collective efforts. As research continues, we expect 3DGS and its descendants to become a staple tool - perhaps the way we capture and interact with 3D content in 5 years will be dominated by these ideas, in the same way rasterized triangles dominated the past decades. The story of 3D Gaussian Splatting is still being written, with each new development bringing us closer to effortlessly capturing our world in lifelike 3D.

## References

- Mildenhall et al. "NeRF: Representing Scenes as Neural Radiance Fields" 
- SIGGRAPH 2020- Kerbl et al. "3D Gaussian Splatting for Real-Time Radiance Field Rendering" 
- SIGGRAPH 2023 (Best Paper)- Huang et al. "2D Gaussian Splatting for Geometrically Accurate Radiance Fields" 
- SIGGRAPH 2024 19 12- Franke et al. "TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering" 
- EG 2024 21 22- Yu et al. "GOF: Gaussian Opacity Fields for Surface Reconstruction" 
- SIGGRAPH Asia 2024 28 29- Eisemann et al. "SuGaR: Surface-Aligned Gaussian Splatting for 3D Mesh Reconstruction" 
- CVPR 2024 25 85- Navaneet et al. "CompGS: Vector Quantization for Gaussian Splatting" 
- arXiv 2023 15 16- Fan et al. "LightGaussian:  $15x$  Compression for 3DGS" 
- NeurlPS 2024 17 56- Hanson et al. "PUP 3D-GS: Uncertainty Pruning for 3D Gaussian Splatting" 
- CVPR 2025 36 60- Hanson et al. "SpeedySplat: Fast 3D Gaussian Splatting" 
- CVPR 2025 38 62- Mo et al. "PDE Optimization for 3DGS" 
- arXiv 2025 11 40- Apple ML Team: "HUGS: Human Gaussian Splats" 
- CVPR 2024 32 33- Zhou et al. "Holistic Urban 3D Scene via Gaussian Splatting (HUGS)" 
- CVPR 2024 103- Meng et al. "GSGEN: Text-to-3D using Gaussian Splatting" 
- ICCV 2023 34 35

[PDF] Object Space EWA Surface Splatting: A Hardware Accelerated ... https://www.cs.umd.edu/~zwicker/publications/ObjectSpaceEwASplatting- CGF02. pdf

[PDF] Surface Splatting - UMD Department of Computer Science https://www.cs.umd.edu/~zwicker/publications/SurfaceSplatting- SIG01. pdf

3D Gaussian Splatting - Paper Explained, Training NeRFStudio https://learnopencv.com/3d- gaussian-splatting/

10 26 Anisotropic View- Dependent Appearance for 3D Gaussian Splatting https://openreview.net/forum? id=qpH- SwX5L&tref=9%5Dthe%20profile%20sih%20xiAG5GAN%20Qi70SB%(221- profile%3Fh1%3D- xiA03JAN_Q12)

11 12 40 45 46 111 [2509.13938] Plug- and- Play PDE Optimization for 3D Gaussian Splatting: Toward High- Quality Rendering and Reconstruction https://arxiv.org/abs/2509.13938

13 [PDF] Point- NeRF: Point- Based Neural Radiance Fields - CVF Open Access https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Point- NeRF_PointBased_Neural_Radiance_Fields_CVPR_2022_paper.pdf

15 16 53 54 55 [2311.18159] CompGS: Smaller and Faster Gaussian Splatting with Vector Quantization https://arxiv.org/abs/2311.18159

17 LightGaussian: Unbounded 3D Gaussian Compression with 15x ... https://arxiv.org/abs/2311.17245

18 56 LightGaussian: Unbounded 3D Gaussian Compression with 15x ... https://arxiv.org/html/2311.17245v4

19 20 72 74 75 2D Gaussian Splatting https://surf splatting.github.io/

21 22 23 24 52 78 79 80 81 [2401.06003] TRIPS: Trilinear Point Splatting for Real- Time Radiance Field Rendering https://arxiv.org/abs/2401.06003

25 86 [PDF] SuGaR: Surface- Aligned Gaussian Splatting for Efficient 3D Mesh ... https://openaccess.thecvf.com/content/CVPR2024/papers/Guedon_SuGaR_Surface- Aligned_Gaussian_Splatting_for_Efficient_3D_Mesh_Reconstruction_and_CVPR_2024_paper.pdf

27 Anisotropic View- Dependent Appearance for 3D Gaussian Splatting https://neurips.cc/virtual/2024/poster/93509

28 Gaussian Opacity Fields: Efficient Adaptive Surface Reconstruction ... https://arxiv.org/abs/2404.10772

29 82 Gaussian Opacity Fields: Efficient and Compact Surface ... arXiv https://arxiv.org/html/2404.10772v1

32 93 HUGS: Human Gaussian Splats - Apple Machine Learning Research https://machinelearning.apple.com/research/hugs

33 [PDF] HUGS: Human Gaussian Splats - CVF Open Access https://openaccess.thecvf.com/content/CVPR2024/papers/Kocabas_HUGS_Human_Gaussian_Splats_CVPR_2024_paper.pdf

34 [2309.16585] Text- to- 3D using Gaussian Splatting - arXiv https://arxiv.org/abs/2309.16585

35 GALA3D: Towards Text- to- 3D Complex Scene Generation ... - arXiv https://arxiv.org/abs/2402.07207

36 37 59 60 PUP 3D- GS: Principled Uncertainty Pruning for 3D Gaussian Splatting https://pup3dgs.github.io/

38 39 47 48 49 50 61 62 63 64 65 66 Speedy- Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse Primitives https://speedysplat.github.io/

41 42 87 88 89 90 91 92 95 Adaptive and Temporally Consistent Gaussian Surfels for Multi- view Dynamic Reconstruction https://arxiv.org/html/2411.06602v1

57 LightGaussian: unbounded 3D Gaussian compression with 15x ... https://dl.acm.org/doi/10.5555/3737916.3742363

58 LightGaussian: Unbounded 3D Gaussian Compression with 15x ... https://lightgaussian.github.io/

67 68 69 70 71 [2503.16924] Optimized Minimal 3D Gaussian Splatting https://arxiv.org/abs/2503.16924

73 112 Paper page - 2D Gaussian Splatting for Geometrically Accurate Radiance Fields https://huggingface.co/papers/2403.17888

76 77 GitHub - hbb1/2d- gaussian- splatting: [SIGGRAPH'24] 2D Gaussian Splatting for Geometrically Accurate Radiance Fields https://github.com/hbb1/2d- gaussian- splatting

83 [PDF] Geometry Field Splatting with Gaussian Surfels https://cseweb.ucsd.edu/\~ravir/kaiwensurfel_cvpr.pdf

84 [PDF] Gaussian Opacity Fields: Efficient Adaptive Surface Reconstruction ... https://www.cvlibs.net/publications/Yu2024SIGGRAPHASIA.pdf

85 SuGaR: Surface- Aligned Gaussian Splatting for Efficient 3D Mesh ... https://imagine.enpc.fr/\~guedona/sugar/

94 Human Gaussian Splatting: Real- time Rendering of Animatable ... https://perezpellitero.github.io/projects/hugs/index.html

96 Image- based view- dependent appearance for 3D gaussian splatting https://www.sciencedirect.com/science/article/abs/pii/S0141938225002227

97 3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting https://eccc.ecva.net/virtual/2024/poster/2035

98 View- Dependent Uncertainty Estimation of 3D Gaussian Splatting https://arxiv.org/abs/2504.07370

99 RadSplat: Radiance Field- Informed Gaussian Splatting for Robust ... https://m- niemeyer.github.io/radsplat/

100 101 Full HDR Gaussian Splatting Commercial - Radiance Fields https://radiancefields.com/full- hdr- gaussian- splatting- commercial

102 SpectralGaussians: Semantic, spectral 3D Gaussian splatting for ... https://www.sciencedirect.com/science/article/pii/S0924271625002345

103 Holistic Urban 3D Scene Understanding via Gaussian Splatting https://xdimlab.github.io/hugs_website/

104 Gaussian Splatting at the New Yorker - Radiance Fields https://radiancefields.com/gaussian- splatting- at- the- new- yorker

105 Gaussian Splatting Brings Art Exhibitions Online with Yulei https://radiancefields.com/gaussian- splatting- brings- art- exhibitions- online- with- yulei

106 Gaussian Splatting in Superman - Radiance Fields https://radiancefields.com/gaussian- splatting- in- superman

107 Titanic Digital Twin + Wild Gaussian Splatting + Smart Wind Farms + ... https://www.youtube.com/watch?v=pRvdxCQJ8qA

108 One Image to Re- editable Dynamic 3D Model and Video Generation https://ui.adsabs.harvard.edu/abs/2024arXiv240506547L/abstract

109 Spotlight: Unconstrained Text- to- 3D Scene Generation with ... https://www.ee.ucla.edu/spotlight- unconstrained- text- to- 3d- scene- generation- with- panoramic- gaussian- splatting/

110 Efficient Perspective- Correct 3D Gaussian Splatting Using Hybrid ... https://radiancefields.com/efficient- perspective- correct- 3d- gaussian- splatting- using- hybrid- transparency