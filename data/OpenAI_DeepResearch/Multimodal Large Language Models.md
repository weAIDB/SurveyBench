# Multimodal Large Language Models (MLLMs)

## 1 Introduction

Multimodal Large Language Models (MLLMs) are advanced AI systems that can understand and generate content across multiple forms of data (modalities), such as natural language, images, audio, and even video 1 2 . In essence, an MLLM extends a large language model (LLM) - a model trained on vast text corpora for language understanding/generation - to incorporate non- text modalities 3 2 . These models mark a significant step toward more human- like Al, since human cognition is inherently multimodal: we seamlessly combine vision, hearing, and language in reasoning about the world 4 . By enabling machine learning models to fuse visual, auditory, and textual information, MLLMs can tackle tasks that were previously out of reach for text- only models, offering richer context understanding and more versatile capabilities 5 6 . For example, a multimodal LLM could analyze a social media post not only by reading its text but also by interpreting an attached image and perhaps audio in a video, leading to a much deeper and nuanced response than a unimodal system 6 .

Key Motivation: Traditional LLMs are confined to textual input- output and struggle with tasks like describing an image, understanding a diagram, or following spoken instructions. MLLMs address this limitation by bridging language with other modalities, allowing Al to directly interpret and produce rich data like pictures, speech, and videos 1 7 . This multimodal ability opens up a breadth of applications - from more intuitive AI assistants that can "see" and "hear," to systems that integrate medical images with patient texts for better diagnostics, to creative tools that can generate images from a textual description. Recent models such as OpenAl' s GPT- 4 and Google DeepMind' s Gemini exemplify this trend: GPT- 4 accepts both text and image inputs (it can answer questions about an image) 8 , while Google' s Gemini (second- generation) not only processes text, images, audio, and video but can generate images and audio as outputs 9 . These breakthroughs have been hailed as major milestones, with GPT- 4' s multimodal capability described as a "holy grail" of Al progression in 2023 10 .

Scope of Survey: In this survey, we provide a comprehensive overview of Multimodal LLMs. We begin by defining key concepts and tracing the historical evolution of multimodal models, highlighting milestone systems. We then discuss the foundational architectures and methods that enable multimodal integration (such as how transformers are adapted for images and other data), and we summarize representative MLLM models and their design patterns. Following that, we explore current applications across various subfields - spanning vision- language understanding, speech- language processing, robotics, and more - to illustrate the broad impact of MLLMs. We also examine the open challenges that remain (for example, limitations in reasoning, data, and alignment) and outline potential future directions. Throughout, we aim to balance technical depth with clarity: formal definitions and frameworks are introduced, but we also use intuitive explanations and examples to make the survey accessible to newcomers. Figures and tables are included to illustrate architectures, timelines, and key comparisons for easy reference. By the end, readers should have a clear understanding of what multimodal LLMs are, how they work, what they can do today, and what challenges and opportunities lie ahead in this rapidly evolving area of AI.

## 2 Background and Key Concepts

Background and Key ConceptsModalities and Multimodal Learning: In AI parlance, a modality refers to a type of data or sensory input/output - for example, text, images (vision), audio (speech or sound), video (vision + temporal dimension), or even other signals like numerical tables or sensor readings  $\bigoplus$ . Multimodal learning is the field of study concerned with algorithms that can process and relate information from multiple such modalities. An everyday example of multimodal understanding is a person watching a movie with subtitles: the brain jointly interprets the visual scenes, the audio (dialogue, music), and the written text. Similarly, an MLLM might take an image and a question about that image as input, requiring it to integrate visual understanding with language comprehension. Formally, multimodality means having multiple modalities present in either the inputs or outputs (or both) of a model  $\bigoplus$ . MLLMs can thus be seen as multimodal extensions of language models: they possess the ability to perceive various data types (vision, audio, etc.) and often to generate in multiple modalities as well (e.g., generate a description of an image, or produce an image from a text prompt).

Large Language Models (LLMs): A brief definition of LLMs will help set the stage. An LLM is a language model (typically based on deep neural networks and specifically the Transformer architecture) trained with self- supervised learning on massive text datasets, and it is designed to perform natural language processing tasks, especially generation of text  $\bigoplus$ . "Large" refers to the scale - these models often have hundreds of millions to billions (or even trillions) of parameters, and are trained on billions of words, which endows them with surprisingly broad linguistic and world knowledge. Examples include GPT- 3, GPT- 4, Google's PaLM, Meta's LLaMA, etc. They work by predicting the next word in text, but when scaled up and appropriately trained, they demonstrate abilities like question- answering, dialogue, summarization, coding, and more via prompt conditioning. Importantly, recent top- performing LLMs (GPT- 3, GPT- 4, etc.) are foundation models, meaning they serve as general- purpose models that can be adapted or prompted for many tasks without task- specific training.

What Makes an LLM Multimodal: A Multimodal LLM inherits the language understanding and generation capability of an LLM, and augments it with the capacity to handle other modalities. In practical terms, this usually means modifying the model architecture and training process so that, for example, images can be fed into the model as part of its input sequence (or as an auxiliary input), or so that the model can emit outputs beyond text tokens. There are two broad directions: (1) Perception- augmented LLMs, where the model takes in non- text inputs (like an image) along with text and produces text. GPT- 4 (vision- enabled) is an example - it accepts image+text inputs and outputs text  $\bigoplus$ . (2) Generative multimodal models, where the model may output other modalities in addition to or instead of text. An example is text- to- image generation (like DALL  $\cdot$  E 2), or more recently Google's Gemini 2.0 which can generate images and speech as outputs natively  $\bigoplus$ . Some models are bidirectionally multimodal, meaning they both understand and produce multiple modalities (e.g., an image captioning model that can also perform text- to- image generation, though such fully symmetric capabilities in one model are just emerging). In summary, a multimodal LLM processes a combination of modalities as input and/or output, whereas a standard LLM is limited to text in/out. By learning joint representations of text with other data like pixels or audio waveforms, MLLMs can perform tasks like describing images, answering visual questions, transcribing and interpreting audio, controlling robots through language and vision, etc., all within one unified framework  $\bigoplus$ . This unification is seen as a path toward more general AI systems that move beyond narrow single- modality expertise  $\bigoplus$ .

Illustrative Tasks: To ground the concept, it's useful to list a few core tasks enabled by multimodal LLMs:

Image Captioning: Given an image, produce a descriptive caption in natural language. This was one of the earliest vision- language tasks tackled with neural networks around 2014- 2015  $\bigoplus$ . An

MLLM can generate fluent captions for images, e.g., "A brown dog chasing a ball on a grassy lawn."

- Visual Question Answering (VQA): Answer a natural language question about an image. For example, input an image and the question "What is the person in the image holding?" 
- the model must combine vision and language understanding to answer. This requires fine-grained scene understanding and grounding of language in the image (e.g., identify objects and relate them to the question).

- Visual Dialogue: Engage in a multi-turn conversation about an image. The model may be asked a series of questions about an image, maintaining context. This tests the model's ability to reference the image and dialogue history together.

- Text-to-Image Generation: Create an image from a text description. E.g., "Generate an image of a cat riding a bicycle in the style of a watercolor painting." Models like DALL  $\cdot$  E 2 and Stable Diffusion are not LLMs per se, but they illustrate the generative side of multimodality. Newer multimodal models strive to combine this with language reasoning (e.g., a chat model that can also output images).

- Speech and Audio Understanding: E.g., transcribe audio to text (speech recognition), or answer questions about an audio clip. Some multimodal models incorporate audio processing, using speech encoders along with text 14. For instance, an MLLM might take an audio recording of a question and respond with text (combining speech recognition with language answer generation).

- Video Understanding: Analyze video content (sequence of image frames + sound) and produce a summary or answer queries. This is an extension of image + audio processing with a temporal dimension. 
- Embodied/Robotic Tasks: Use multimodal perception for control or planning. For example, a robot with an MLLM "brain" might take visual observations (camera input) and an instruction in text, and output a plan or action commands. In such cases, modalities include real-world sensor data (vision, depth, etc.) combined with language instructions 11.

These tasks span multiple subfields - computer vision, NLP, speech, robotics - underscoring that MLLMs sit at the intersection of these traditionally separate domains. Next, we will review how this field evolved historically, leading up to the sophisticated multimodal models of today.

## 3 Evolution and Milestones of Multimodal Models

The journey to modern MLLMs can be traced through a series of research milestones over the past decade (see Table 1 for a summary timeline). We highlight key developments, roughly in chronological order, that contributed to the emergence of powerful multimodal LLMs:

- 2014-2015: Neural Image Captioning and Early Vision-Language Models. In the mid-2010s, researchers achieved the first convincing results in automatically describing images with sentences. A notable model was Show & Tell by Vinyals et al. (2015), which used a convolutional neural network (CNN) to encode an image and a recurrent neural network (RNN) (an LSTM) to decode a caption 13. This model treated image captioning like machine translation - "translating" an image to a sentence. Around the same time, Visual Question Answering (VQA) was introduced as a challenge task (Antot et al., 2015), spurring models that combined CNN image features with language models to answer questions about images. These early models were relatively small by today's standards and task-specific, but they proved that joint vision-language modeling was feasible. Techniques like spatial attention were soon applied (Show, Attend and Tell, 2015) to let language models focus on relevant parts of an image when generating each word 15. This era established the paradigm of dual networks - one for vision, one for text - that communicate (e.g. via feeding image features into an RNN) to produce a multimodal output.

- 2016-2018: Fusion of Deep Learning and Representation Learning Advances. Following the initial successes, multiple works improved vision-language models using better visual features

and training methods. Image feature extractors evolved from early CNNs to more powerful models (ResNets, etc.), and language representations moved from basic word embeddings to richer models. However, a transformative milestone in 2017 indirectly set the stage: the invention of the Transformer architecture (Vaswani et al. 2017) 16. Transformers, with their self- attention mechanism, soon became the state- of- the- art architecture for language models and began to influence multimodal research. By 2018, the first large language models like BERT and GPT were introduced, though these were text- only. Researchers started asking how to leverage transformers for multimodal tasks. Late 2018 saw a model called ViLBERT (Lu et al. 2019) which extended the BERT concept to vision+language by using two transformer streams (one for image regions, one for text) that interact through co- attention. This was quickly joined by others like VisualBERT, LXMERT, and UNITER (all 2019), which all used transformer- based architectures to jointly encode images and text 17. These models were typically pretrained on image- caption datasets to learn aligned representations and then fine- tuned for tasks like VQA. They established the practice of multimodal pre- training analogous to language model pre- training, often using objectives like image- text matching and masked language modeling with visual context.

- 2019–2020: Emergence of Multimodal Pre-training and Larger Models. The year 2019 marked the convergence of transformer LMs with vision: models like LXMERT and UNITER were on the order of hundreds of millions of parameters, already “large” for their time, and trained on millions of image-text pairs. They significantly outperformed earlier non-transformer models on VQA and captioning benchmarks, demonstrating the power of large-scale joint representation learning. In 2020, advances continued with models such as VL-BERT, Oscar (which incorporated object tags), and ClipCap (for captioning). However, a different approach to multimodality also appeared: dual encoder models for cross-modal retrieval and alignment. Notably, CLIP (Contrastive Language-Image Pretraining) was developed by OpenAI and announced in early 2021 18 (research performed likely in 2020). CLIP took a contrastive learning approach: a large image encoder and text encoder were trained to produce embeddings such that an image is similar to its paired caption and dissimilar to other captions 18. With 400 million image-text pairs from the web for training, CLIP learned a broad semantic space connecting text and images, enabling zero-shot image recognition (it could classify images by choosing the text label whose embedding best matches the image embedding) 19 20. CLIP was a milestone because it showed a model could learn visual concepts from natural language supervision at scale, and its release in 2021 catalyzed a wave of multimodal research leveraging its pre-trained embeddings. Around the same time, DALL  $\cdot$  E (2021) by OpenAI demonstrated the generative converse: given text, generate images – effectively introducing a powerful multimodal output modality (images) via transformer- based generative modeling. While DALL  $\cdot$  E and its successor DALL  $\cdot$  E 2 (2022) are often categorized under image generation, their architecture (transformers and diffusion models conditioned on text) and success underscored the feasibility of jointly modeling text and pixel data for generation.

- 2021: Multimodal Models at Greater Scale and New Modalities. By 2021, multimodal “foundation models” started to appear. Google Research introduced SimVLM (Simple Visual Language Model) in 2021, a model that treated image patches as pseudo-words (tokens) prepended to text and trained an end-to-end transformer on massive data 21 22. This was an example of early fusion at the input level – images converted to a sequence of embeddings fed into a transformer along with text tokens. Meanwhile, Baidu’s ERNIE-ViL and Microsoft’s Florence explored larger-scale vision-language pretraining. Importantly, multilingual and non-English multimodal datasets also grew, expanding the cultural scope of these models. By late 2021, we see the trend of any-to-any mappings emerging: Alibaba’s M6 was a gigantic 10-trillion-parameter sparse multimodal Transformer capable of “multi-modality-to-multimodality” generation (it could take in text and output images or vice versa, among other combinations) 23. Though not as widely known as Western models, M6 demonstrated that

industry labs were pushing parameter counts in multimodal models to enormous scales even then.

- 2022: Few-Shot and Unified Multimodal Models (Flamingo, PaLI, Gato). A pivotal development came from DeepMind in 2022 with Flamingo  $^{24}$ . Flamingo introduced a clever architecture to combine a pre-trained LLM (in this case, a 70B Chinchilla model) with a visual encoder: it inserted special cross-attention layers that allow the text model to attend to image features  $^{17}$ . This design let Flamingo accept interleaved images and text as input and produce text, all in a flexible few-shot setting 
- one could prime it with a couple of image-caption examples and then it could describe a new image. Flamingo was notable for strong performance on tasks like image captioning and VQA with just a few examples provided, showing the power of marrying a large language model with visual inputs via learned adapters. Around the same time, Google scaled up a model called PaLI (Pathways Language and Image model) which was pretrained on an enormous corpus of image-text pairs (10+ billion) and could perform multilingual image captioning and VQA at state-of-the-art levels. Another interesting 2022 result was Gato from DeepMind  $^{25}$  
- a single generalist agent that could process images, robotic sensor data, and text, and output text or control signals. Although Gato (with  $\sim 1B$  parameters) was far smaller than contemporary LLMs, it was an early attempt at embodied multimodal learning (covering vision, language, and reinforcement learning data). Collectively, 2022 solidified the paradigm of using LLMs as the backbone for multimodal AI: rather than training a separate vision+language model from scratch, researchers began to attach visual interfaces to powerful pre-trained LLMs. This approach foreshadowed what was to come next.

- 2023: GPT-4 and the Era of Multimodal Foundation Models. The year 2023 was a watershed moment for multimodal AI in the public eye. OpenAI's GPT-4 was announced in March 2023 as a large-scale, multimodal model accepting image and text inputs (and producing text)  $^{26}$ . GPT-4's ability to parse images 
- for example, explaining memes, analyzing charts, or solving visual puzzles 
- alongside text represented a striking leap in capability available via a single model. Although the details of GPT-4's architecture were not publicly disclosed, it sparked huge interest and many efforts to replicate its functionality. Only weeks later, Microsoft researchers introduced Kosmos-1, describing it explicitly as a "Multimodal Large Language Model" that can perceive various modalities and follow instructions  $^{12}$ . Kosmos-1 was trained from scratch on web-scale data of mixed modality (images interleaved with text) and showed strong performance on tasks like multimodal QA and even non-verbal reasoning tests  $^{26}$ . Also in 2023, Google unveiled PaLM-E, an embodied multimodal version of their PaLM LLM, designed to integrate vision and language for robotics. PaLM-E feeds visual observations into a textual LLM (PaLM-540B) by encoding images and inserting the embeddings as pseudo-tokens; it achieved impressive results in guiding robots with language instructions grounded in visual context  $^{21}$ .  $^{22}$ . The open-source community also accelerated in 2023: models like LLaVA (Large Language and Vision Assistant) and MiniGPT-4 combined open LLMs (like LLaMA/Vicuna) with pre-trained vision encoders by learning small projection modules (often called Q-Formers or adapters) to align the modalities  $^{27}$ .  $^{28}$ . Within a few months, there was an explosion of such MLLM fine-tunes 
- e.g., Otter, InstructBLIP, BLIP-2, and others 
- extending various base LLMs with visual capability using relatively modest data and compute. By late 2023, multimodality became a standard feature in the next generation of AI models. Notably, Google DeepMind's Gemini (announced at end of 2023) is a family of multimodal models with reported abilities to accept images, audio, and video as input; by version 2.0 in 2024, Gemini can also generate images and speak audio outputs  $^{9}$ . This represents a full duplex multimodality, integrating what had been separate threads (vision-language understanding and text-to-image generation) into one model.

- 2024 and Beyond: The trend in 2024 has been towards larger context lengths and more "agentic" behavior in MLLMs. Models are being equipped with the ability to handle longer multimodal inputs (e.g., entire documents with images, long videos) and to use tools (like search or vision APIs) when their own capacity is insufficient 29 30. There is also movement toward incorporating new modalities: e.g., Meta AI' s work on ImageBind suggests future models that bind together image, audio, text, depth, thermal, and even sensorial modalities into one representation space. Another development is in evaluation and robustness testing: as models like GPT-4V (the vision-extension of GPT-4 available through ChatGPT) become widely used, researchers are rigorously testing their limits - finding, for instance, that they can describe many images well but may falter on complex diagrams or detailed spatial reasoning tasks 31 . The field is grappling with how to measure "understanding" across modalities and ensure reliability (more on challenges in a later section). Nonetheless, the arc from 2015' s caption generators to 2025' s multimodal agents clearly shows an acceleration: the community is moving from handling pairwise modality tasks (e.g., image+text) to aiming for any- to- any multimodal AI, where a single model could flexibly take any combination of inputs (say, an image  $^+$  a spoken question  $^+$  additional textual context) and produce any combination of outputs needed (a spoken answer with a generated diagram, for example). Achieving this generality is an ongoing ambition.

Table 1. Timeline of notable multimodal models and milestones.  

<table><tr><td>Year</td><td>Milestone/Model</td><td>Description and Significance</td></tr><tr><td>2014-15</td><td>Show &amp;amp; Tell (Vinyals et al.) 
13 &amp;lt;br&amp;gt;VQA dataset (Antol et al.)</td><td>First neural image captioning (CNN+LSTM) – generated sentences from images. Demonstrated vision↔language mapping. VQA introduced as a task combining image understanding with question answering.</td></tr><tr><td>2016</td><td>Show Attend &amp;amp; Tell (Xu et al.)</td><td>Applied attention mechanism to image captioning, improving fine-grained alignment between words and image regions (e.g. attend to “dog” region when generating “dog”).</td></tr><tr><td>2017</td><td>Transformer architecture (Vaswani et al.) 16</td><td>Introduced self-attention and transformer design. Not multimodal itself, but became the backbone for subsequent large text and multimodal models.</td></tr><tr><td>2018</td><td>BERT (Devlin et al.) &amp;lt;br&amp;gt;GPT-2 (OpenAI)</td><td>Large pre-trained language models (unimodal text) released. Showed power of scaling; inspired adaptations for vision-language. GPT-2 demonstrated zero-shot language tasks, foreshadowing zero-shot multimodal abilities.</td></tr><tr><td>2019</td><td>LXMERT (H. Tan et al.) &amp;lt;br&amp;gt;VisualBERT, ViLBERT &amp;lt;br&amp;gt;UNITER (Chen et al.)</td><td>Transformer-based vision-language models pretrained on image-text data. Used joint learned embeddings or co-attention between object region features and text. Greatly improved VQA, captioning scores. Kickstarted multimodal pre-training trend.</td></tr></table>

<table><tr><td>Year</td><td>Milestone/Model</td><td>Description and Significance</td></tr><tr><td>2020</td><td>UNITER, VL-BERT 
(Continued)&lt;br&gt;CLIP 
(trained in 2020, OpenAI 2021)</td><td>Refinements in V+L BERT-like models. CLIP 19 (released Jan 2021) trained on 400M image-text pairs using contrastive learning; learned a unified image/text embedding space enabling robust zero-shot image recognition. Signaled effectiveness of web-scale weakly-supervised training.</td></tr><tr><td>2021</td><td>DALL·E 
(OpenAI)&lt;br&gt;OpenVLM (Wang et al.)&lt;br&gt;Flamingo (DeepMind) 24&lt;br&gt;PaLI (Google)&lt;br&gt;Gato (DeepMind)&lt;br&gt;Stable Diffusion (CompVis)</td><td>DALL·E: transformer that generates images from text prompts - showed generative multimodality (text→image) at high quality. SimVLM 21 22: single-stream transformer treating image patches as tokens, achieving strong results on captioning/VQA. ALIGN: similar to CLIP (contrastive pretraining) but even larger dataset (1.8B pairs). Multimodal models reaching billions of parameters, multi-lingual, etc.</td></tr><tr><td>2022</td><td>Flamingo (DeepMind) 24&lt;br&gt;PaLI (Google)&lt;br&gt;Gato (DeepMind)&lt;br&gt;Stable Diffusion (CompVis)</td><td>Flamingo: few-shot VLM using LLM+visual cross-attention, set new state-of-art in few-shot captioning and VQA. PaLI: 17B-parameter model, achieved SOTA on many vision-language tasks, using multitask and multilingual data. Gato: one model for vision, text, and control (illustrated potential for one generalist agent). Stable Diffusion: open-source text-to-image diffusion model; not an LLM, but its success pushed multimodal generation forward (many MLLMs now incorporate image generation).</td></tr><tr><td>2023</td><td>GPT-4 (OpenAI) 8&lt;br&gt;br&gt;Kosmos-1 (Microsoft) 12&lt;br&gt;PaLM-E (Google)&lt;br&gt;BLIP-2 / MiniGPT-4 / LLaVA (open)</td><td>GPT-4: 1&lt;sup&gt;sup&amp;gt;st&lt;/sup&gt; widely used MLLM (accepts images + text). Achieved human-level performance on some exams 8; showcased powerful multimodal reasoning (e.g., explaining humor in images). Kosmos-1: introduced term MLLM; showed multimodal in-context learning and zero-shot abilities 26. PaLM-E: LLM with vision for robotics, demonstrated benefits of injecting visual tokens into LLM for action planning. BLIP-2, etc.: multiple open-source efforts connecting vision encoders with smaller LLMs, often via a learned projection module (Q-Former) 27, making GPT-4-like image understanding more accessible.</td></tr></table>


</sup></sup></sup>

<table><tr><td>Year</td><td>Milestone/Model</td><td>Description and Significance</td></tr><tr><td>2024</td><td>GPT-4V (Vision) via ChatGPT&lt;br&gt;Gemini 1.0 &amp;amp; 2.0 (Google) &lt;br&gt;ImageBind (Meta)</td><td>GPT-4&#x27;s vision feature made available to users (allowing image uploads in ChatGPT) – mass deployment of an MLLM. Gemini 2.0: heralded as “our most capable model yet” – natively multimodal (image, video, audio input; image/audio output) and tool-use integrated 9. Represents state-of-art integration of modalities (e.g., describe an image and then generate a new image based on that dialog). Meta&#x27;s ImageBind: not a single model outputting language, but a framework aligning 6 modalities in one embedding space; points toward future MLLMs that might seamlessly connect text, vision, audio, 3D, and more.</td></tr></table>


</br>

Figure 1 illustrates this evolution by highlighting a few key milestones on a timeline, from early image captioning to today's multimodal agents. The field has moved from isolated, task- specific multimodal models to large, general- purpose MLLMs that are often pretrained on web- scale multimodal data and then adapted to myriad tasks. Particularly after 2023, there is a shift toward unification: rather than separate models for each modality or task, a single neural architecture (typically based on a transformer LLM backbone) is being pushed to handle many modalities with minimal task- specific customization 32. This reflects a broader trend in AI towards foundation models that learn a wide range of capabilities and can be steered with prompts or fine- tuning for specific uses.

[No image was embedded for the timeline due to environment limitations, but one can imagine a timeline figure here marking the above events.]

## 4 Architecture and Foundations of Multimodal LLMs

In this section, we delve into how multimodal LLMs are designed under the hood. We first discuss the Transformer architecture – the workhorse behind modern LLMs – and how it extends to multimodal inputs. Then we outline common strategies for multimodal fusion (combining information from different modalities), including early, intermediate, and late fusion approaches. We also describe typical model components: modality- specific encoders for processing raw inputs, connectors or adapters that align encodings from different modalities, and the central LLM backbone that performs unified reasoning 33 34. Finally, we touch on training objectives and techniques that underpin multimodal learning (contrastive learning, image- text matching, masked modeling, etc.), as well as how instruction tuning is done for MLLMs.

### 4.1 Transformer Foundation for Multimodal Models

Most state- of- the- art MLLMs are built upon the Transformer architecture or its variants 35 36. The Transformer's key innovation, the self- attention mechanism, allows it to flexibly attend to different parts of the input sequence. This is crucial for multimodality: for example, a transformer can learn to attend to an image region token when processing a related word in the text 35. Transformers are naturally modality- agnostic in terms of sequence processing – they just handle sequences of vectors (embeddings) with position indices. This means we can potentially feed in embeddings from images or audio alongside word embeddings. However, raw images or audio are not sequential symbols like text, so we need front- end modules to convert those into sequences of vectors (this is where modality encoders come in, next subsection). Once all inputs are in a compatible vector/token form, a transformer can integrate them with minimal architectural change. For instance, if we treat image patch features as

"visual tokens," we can prepend or intersperse them with text tokens in a Transformer's input 21 22 . This approach was taken by models like SimVLM and certain GPT- 4V implementations - effectively an early fusion where one big transformer sees both modalities from the start.

Transformers can also support cross- attention mechanisms, which are often used in encoder- decoder architectures and multimodal fusion. Cross- attention means one sequence (say, text) attending to another sequence's representations (say, image features). Flamingo's design is a good example: a pretrained language transformer is augmented with cross- attention layers that allow its intermediate layers to attend to image encoder outputs 17 . This injects visual information into the language model' s processing stream without disrupting the language model' s weights. We will discuss this more in fusion approaches.

In summary, the Transformer's modularity and attention make it ideally suited for building MLLMs: one can either use a single unified transformer for all modalities or a combination of transformers (each handling one modality) that communicate via attention layers 37 38 . Virtually all large multimodal models today - GPT- 4, Kosmos- 1, PaLM- E, OpenFlamingo, etc. - leverage transformers in some form, whether for encoding images or for the central language reasoning component. Even diffusion models for image generation often use transformer text encoders or cross- attention to condition on text. Thus, understanding MLLM architecture largely means understanding how to plug modalities into a transformer.

### 4.2 Modality Encoders and Feature Extraction

Before different modalities can be fused, each modality is typically processed by a specialized encoder network to extract high- level features 27 . These encoders play the role of "sensory organs" for the model:

For text, the encoder might simply be the initial token embedding lookup followed by some transformer layers (in an encoder- decoder, the language model itself acts as the text encoder/ decoder). In decoder- only LLMs, the "encoder" is just the embedding layer  $^+$  positional encoding feeding into the transformer. Text is tokenized into subword tokens (using BPE or similar) to turn sentences into sequences of discrete inputs for the model.

For images, common encoders include convolutional neural networks (CNNs) or Vision Transformers (ViT) that turn an image into a set of feature vectors 27 . A CNN (like ResNet) might output a feature map which can be flattened into a sequence, or region- based features (e.g., Faster R- CNN detecting objects) can produce a set of object features. A Vision Transformer splits an image into patches and processes them as tokens, yielding patch feature vectors. For example, OpenAI' s CLIP used either a ResNet or ViT as its image tower; many later MLLMs use pre- trained ViTs. The output of an image encoder could be, say, a 768- dimensional vector for each  $16\times 16$  patch of the image if using ViT.  $\mathsf{B} / 16$

For audio, encoders like wav2vec 2.0 or HuBERT (for speech) or spectrogram- based CNNs convert raw audio waveforms into a sequence of acoustic feature vectors 24 . These can be thought of analogous to how text is tokenized: e.g., one could split audio into frames or segments and each becomes a "token embedding" representing that snippet of sound. Some multimodal models also utilize pre- trained speech recognition models as encoders, effectively turning spoken words into text (though end- to- end audio- text models aim to learn directly).

- For video, encoders often combine spatial and temporal processing: e.g., a 3D ConvNet or a TimeSformer (time-aware ViT) that yields a sequence of frame or clip features. Video can be treated as a sequence of image frames (each with its own patch features) or using a specialized video model that outputs a sequence of features capturing movement.

- For other modalities (e.g. sensor data, embeddings from other systems), one can design or use modality-specific networks. For instance, robot proprioception (angles of joints, etc.) might just be a vector of numbers that a small MLP encodes into some embedding.

Crucially, these modality encoders often are pretrained or taken off- the- shelf. A common strategy is to use a frozen encoder (e.g., a ResNet trained on ImageNet, or a ViT trained on image classification) to get image features, then learn the rest of the multimodal model around it 39 40 . This saves training cost and leverages existing visual knowledge. Recent models like BLIP- 2 go a step further: they use a frozen image encoder and a frozen language model, and train only an intermediate connector (more on that shortly) 41 . By keeping the big towers fixed, training becomes more efficient. However, some models (like Kosmos- 1, PaLI, or others) train everything from scratch multimodally, which can fully optimize performance at the cost of requiring huge data/compute.

The output of a modality encoder is typically a set of feature vectors  $\{(\mathsf{mathbf{b}}\mathsf{f}\{\mathsf{e}\} \_ 1,\mathsf{\backslash mathbf{b}}\mathsf{f}\{\mathsf{e}\} \_ 2,\ldots ,$ $\mathsf{\backslash mathbf{b}}\mathsf{f}\{\mathsf{e}\} \_ \mathsf{m}\} \mathsf{\S}$  for an input (e.g.,  $\S \mathfrak{m}\S$  might be the number of image patches or detected objects). These need to be transformed into a form usable by the LLM. If the LLM expects sequences of, say, 1024- dimensional token embeddings, we must project the image features to 1024- d and somehow position or align them in the sequence. That brings us to the connector stage.

### 4.3 Alignment and Fusion: Connectors and Fusion Strategies

Data/Feature Projection (Connector): Because each modality' s encoder might produce features in a different vector space (and of different dimensionality), MLLMs include a stop to align or project features into a common representation space 42 . A simple case is a learned linear layer (matrix) that takes an image feature vector and outputs a vector in the same dimension as the text embeddings of the LLM 43 . This way, an image feature can be "inserted" as a pseudo- token embedding in the text sequence 43 . More sophisticated connectors include:

- Multilayer Perceptrons (MLPs): small neural nets that transform modality-specific features to a common size 44.- Learned Query Transformers (Q-Formers): used in BLIP-2, for example. A Q-Former is a lightweight transformer that takes image encoder outputs and a set of learnable query vectors, and produces a fixed number of latent "query embeddings" that summarize the image 45. These query embeddings then serve as the interface to the language model (fed as tokens). This can be more expressive than a single linear layer and can learn to pick up relevant aspects of the image.- Adapter modules: e.g., LLaMA-Adapter adds small adapters that project image features and insert them into an LLM without altering the LLM weights much 45.- Tokenizers for modalities: Some approaches treat non-text data as if it were language by "tokenizing" them. E.g., VQ-VAE can tokenize images into a sequence of discrete codes; one could then train a language model that just treats those codes as additional vocabulary. This was explored in some multi-modal generative models and in the "Type-10" architectures identified by Wadekar et al. (2024) 38 46. It's effectively another way to align modalities – at the discrete token level instead of continuous embeddings.

The goal of the connector is to ensure that when, say, an image of a cat is input, the resulting vector(s) that go into the LLM are in a space that the LLM can meaningfully integrate with its text representations of "cat". Projection into a common embedding space allows fusion to occur.

Fusion Strategies: How and where to combine multimodal information in the model is a critical architectural choice. Generally, there are three broad strategies (often termed early, intermediate, and late fusion) 47 48 :

- Early Fusion (Feature-level fusion): Combine modality data as inputs to a single model from the very beginning 47. For example, represent an image as a sequence of patch embeddings and just concatenate it with the text token sequence, then feed the whole thing into a transformer that processes it jointly 21 22. The model learns from scratch the relationships between modalities in its lowest layers. Early fusion has the advantage of maximal interaction – every layer can attend across modalities – but it requires the modalities to be made commensurate early (which might be hard if they have very different information content or lengths), and a single model must handle potentially very different data types. Kosmos-1 and some "Type-D" models 49 46 use a form of early fusion (they tokenize images and feed them into a unified Transformer alongside text from the start). Early fusion can be efficient for tasks where modalities are tightly coupled, but it may also learn spurious correlations if not carefully trained due to data complexity.

- Intermediate (Mid) Fusion: Process each modality independently up to a certain layer, then join them in some middle layers for interaction 50. For instance, you might have a visual encoder producing features, a linguistic encoder for text, and then have a subsequent transformer or cross-modal layers that fuse these representations 51. Flamingo's design can be seen as a mid-fusion: the language model's lower layers handle text, and visual features are injected via cross-attention at intermediate layers 17. This allows the model to first understand each modality in its own "language" and then learn relationships at a higher level. Mid-fusion often yields a more nuanced understanding because it lets modality-specific processing happen before mixing, possibly capturing modality-specific patterns better 50. Many vision-language transformers of 2019 were effectively mid-fusion: image regions went through some layers, text through some, then co-attention combined them in later layers.

- Late Fusion (Decision-level fusion): Combine modality outputs only at the final decision stage 48. For example, have separate models that produce predictions or scores from image and text, then simply merge those predictions (e.g., via averaging or a shallow network). Late fusion treats each modality's processing almost entirely separate – the combination happens after each modality has been processed in isolation 48. This was common in early multimedia applications (like having a speech recognizer and a visual classifier whose outputs are merged). It's easier to implement but misses rich cross-modal interactions. In deep learning era, pure late fusion is less common for integrated understanding tasks, but it appears in scenarios like ensemble models or when modalities are weakly related. One example might be a system that transcribes audio to text with one model and analyzes an image with another, then simply concatenates the results – technically multimodal, but the learning of each part is disjoint. In academic literature, late fusion is often a baseline, while early or mid fusion models tend to perform better on tasks requiring joint reasoning.

In practice, many MLLMs use a hybrid of these. BLIP- 2, for instance, does early fusion of vision+text at the level of the Q- Former (creating a joint representation), and then essentially late- fuses by feeding that representation into a frozen LLM (where the only connection is through prompt embeddings). Others might do multi- stage fusion: e.g., do some cross- attention early, then again later. The design pattern

identified by the 2024 architecture survey 38 52 classifies Type- A (deep fusion via cross- attention throughout), Type- B (deep fusion via custom layers), Type- C (modality- specific encoders then fuse - mid fusion), Type- D (tokenize everything - early fusion). They noted that Type- C and D (mid or early fusion with modality- specific encoders vs a fully tokenized approach) are favored for building "any- to- any" models, and Type- C (which does not fully tokenize non- text modalities) is emerging as a viable alternative that may simplify adding new modalities 46 53.

Cross- Modal Attention: Regardless of fusion timing, one mechanism appears repeatedly: cross- modal attention. This is where the model learns to attend from one modality' s features to another' s, aligning elements (e.g., words to image regions) dynamically 54 55 . Cross- attention modules often take the form: Query  $=$  features from modality A, Key/Value  $=$  features from modality B. For instance, in LXMERT or LLaVA, you might have the text as query attending to visual keys/values to incorporate information about referenced objects 56 . Cross- attention is powerful because it doesn' t force a static alignment (like fixed position concatenation) but allows the model to flexibly find relevant connections (e.g., the token "cat" will attend to the image region with a cat). Flamingo' s gated cross- attention layers allowed each language layer to pull in visual info as needed 17 . This technique is key in tasks like VQA where the answer might depend on relating a specific word to a specific part of the image. It' s also used in videotext models (attend text to video frames, etc.) 57 . Notably, cross- attention is also how text- to- image models condition on text: the generative model (diffusion or autoregressive image model) will use crossattn with text encodings to guide image generation. In an MLLM context, if the model is to generate an image from text, it could likewise use the text embedding as keys/values to a decoder that generates image tokens. While current LLM- centric MLLMs mostly generate text, research is ongoing on how best to architect multi- output models (e.g., one approach is a multi- decoder system, where the model has a text decoder and an image decoder that share some layers).

The architecture design thus involves choosing how to encode each modality, how to project those features, and at what points to allow the model to attend across modalities. Figure 2 below shows a generic architecture template for a multimodal LLM, highlighting these components (encoders, connector, fusion mechanism, LLM core, output decoders).

Figure 2: Generic architecture of a Multimodal LLM. Each input modality (image, audio, video, text) is processed by a modality- specific Encoder (e.g., CNN or ViT for images, spectrogram CNN for audio, or just token embedding for text) to produce feature embeddings. A Connector (also called aligner/projector) then transforms these embeddings to a common space (for example, via linear layers or a Q- Former) so that they are compatible with the LLM' s token representations 27 28 . The transformed features are integrated through a Fusion Mechanism - which could be early (concatenation of token sequences), late (independent processing then combine outputs), or intermediate via cross- attention or multiple interaction layers 47 48 . A central LLM Backbone (often a Transformer decoder) attends to and processes the fused multimodal information to produce a unified representation and perform reasoning 34 . Finally, an Output Decoder/Head produces the result in the desired format: typically an autoregressive text decoder (for answers, captions, etc.), but potentially also other outputs like classification labels or even generated images/audio if the model supports it. In many implementations, the LLM backbone itself serves as the decoder for text output, while other outputs might require additional heads.

(Note: Figure drawn conceptually; not all MLLMs have dedicated "output decoders" beyond the LLM, except in cases of non- text generation.)

### 4.4 Training Paradigms for MLLMs

Training Paradigms for MLLMsBuilding a multimodal model is not just about architecture – how it is trained is equally important. Early multimodal models often used supervised learning on specific tasks (e.g., train end- to- end on a VQA dataset to answer questions). However, the modern trend, akin to language models, is pretrain then fine- tune (or prompt). So we pretrain an MLLM on large- scale data with self- supervised objectives, then fine- tune or prompt it for downstream tasks. Key training methods include:

- Contrastive Learning: As exemplified by CLIP, this objective teaches the model to align modalities by pairing positive image-text examples against negatives 19 58. The model (usually dual-encoder style) tries to produce embeddings where each image is closest to its true caption's embedding and far from others. This yields a joint space and zero-shot capabilities, but the model itself (as trained) is not generative. It's often used to initialize encoders that can then be used in a generative model or as feature extractors. Many vision-language models in 2021-2022 employed a contrastive loss alongside other losses.

- Masked Language Modeling with Vision: Similar to BERT's training but provide images as context. For example, randomly mask some text tokens in a caption and have the model predict them, while giving the model the associated image. The image helps predict masked words (e.g., if the caption is "A [MASK] sitting on a bench" and image shows a dog, the model should infer "dog"). This was used in models like VisualBERT, UNITER, etc. It teaches the model cross-modal grounding (image features provide cues for language). There is also masked image modeling (predict masked patches or attributes) sometimes, though predicting raw pixels is hard – some works predict image feature tokens or cluster IDs instead when doing this.

- Image-Text Matching / Alignment Loss: A simpler objective – have the model output a score (through a classifier head) whether a given image and text belong together or not. This was often combined with masked LM in the 2019-era models. It's like a shallow form of contrastive learning.

- Image Captioning (Language Modeling): Using a next-word prediction loss on captions with the image as input. This effectively trains a model to be a captioner – given the image (encoded) and start-of-sentence token, predict the caption text autoregressively. Models like SimVLM used a prefix LM approach: attach a special token and image patches at the start of the sequence, then the caption text, and train the transformer to predict the text 21 22. In doing so, the model learns to condition its text generation on the visual content. This is a generative pretraining, which can later generalize to other text generation conditioned on images (like VQA if you format the question as prompt and answer as continuation).

- Multimodal Instruction Tuning: In the era of ChatGPT and instruction-tuned LLMs, a parallel effort happens for MLLMs. After base multimodal pretraining (which might not produce a dialogue-capable model yet), researchers fine-tune the model on instruction-following data that include multimodal prompts. For example, an instruction tuning dataset for a vision-chat model might include exchanges like: "User: (image) What is in this image? Assistant: There is a dog sitting on a bench...". Early 2023 saw datasets generated by GPT-4 (text-only) to help teach smaller models; e.g., LLaVA used GPT-4 to generate thousands of image-question-answer triples by describing images, and used those to fine-tune a Vicura+CLIP model to produce conversational answers about images 59 60. This process aligns the model with user-style instructions and desired responses, much like ChatGPT's RLHF process but for multimodal input. The result is models that can engage in interactive multimodal dialogues, not just one-off captioning. Instruction-tuned MLLMs (like InstructBLIP, LLaVA) tend to be more robust and user-

friendly, at the cost of being somewhat specialist (they're tuned for the Q&A or conversational format).

- Chain-of-Thought and Reasoning Training: A recent idea is to train or prompt models to produce rationale steps when answering multimodal questions (Multimodal CoT) 61. For example, for a complex visual question, the model first describes relevant parts of the image ("I see X and Y...") then reasons ("X implies answer is Z"). Some datasets and methods encourage the model to output such reasoning chains, which has been shown to help with complex reasoning. Kosmos-1 and others have tested "multimodal few-shot prompting" where you give an example of thought process involving an image, and the model follows that pattern 62. This is still an emerging area but ties into improving cross-modal reasoning abilities, identified as a challenge 63.

- Reinforcement Learning / Alignment: So far, vision-language models haven't used RL as prominently as text (no equivalent of ChatGPT's RLHF widely known for images yet), but there are some works using human feedback to rate image descriptions or answers for truthfulness. As multimodal AI enters open-ended generation (especially image creation with possible misuse), alignment and guardrails become a training consideration here too. For instance, ensuring an MLLM does not produce prohibited imagery or descriptions might involve curated training on ethical guidelines. OpenAI likely applied some internal alignment training for GPT-4's vision outputs to avoid problematic behavior.

During training, one also has to handle data mixing: MLLMs may be trained on a blend of pure text data and image- text data (and possibly others). For example, Kosmos- 1 was trained on "interleaved web data" which had text and images in the same documents 26. This can help the model not forget its pure language skills while learning multimodal ones, and also leverage the vast text corpora out there in addition to more limited image- caption corpora. The model might occasionally see an image during training with alt- text, and other times just text. Curriculum learning (first train on easier tasks like single modality, then add multimodal tasks) is also sometimes used.

### 4.5 Output Decoders and Multi-Modal Output

Most current MLLMs have a single- mode output: text. They generate text responses that may describe or reason about input images, etc. However, as noted, some emerging models can generate other modalities. How is that achieved? There are a few approaches:

- Text-to-image as a sub-module: For example, Visual ChatGPT is a system that routes requests to different models (an LLM plus an image generator). If an MLLM wants to produce an image, it actually delegates to a diffusion model by outputting a description or a special code. This is more of a pipeline than a single model, but it's an approach to get multimodal output without training one model to do both 64, 65.

- One model, multiple heads: A single transformer could have multiple output heads 
- one that generates text tokens, another that generates image pixels or image tokens. The model would need a way to know which head to use for a given query. Google's Gemini reportedly can output images and audio natively 9, which suggests it might have an image decoder module integrated (possibly similar to how Parti or Imagen 
- text-to-image models 
- are built). One speculative design is a unified encoder-decoder where the encoder is multimodal and then you have different decoder modules for text vs. image. Or a single decoder that can output both token types (text tokens and image tokens). For instance, a unified vocabulary could include special tokens that

stand for image pixels or an image latent code, which the model could output. This is experimental, and one challenge is evaluation and training signal for generated images – it’s easier to train text outputs with cross- entropy than images.

- Audio output: It is relatively simpler to have audio output by generating text (the transcript or content) and then using a separate TTS (text-to-speech) system. Native audio generation (like producing a waveform for music or speech with specific intonation) is a complex generative modeling problem on its own (see e.g. MusicLM, VALL-E). A truly end-to-end MLLM that outputs waveforms might internally incorporate a neural vocoder or similar. Current systems usually keep it modular for practical reasons.

In our scope, most discussion focuses on models that output text, since that covers the majority of MLLMs currently. But we highlight that generating other modalities is a frontier: e.g., multimodal dialogue agents that not only speak but also draw. The research community is actively exploring this, and tools like Microsoft’s VALL-E (for speech) or OpenAI’s image models could be plugged in. Gemini’s mention of “multimodal output with image generation” suggests a tight coupling not just a plug- in. This will likely be detailed in future technical reports.

### 4.6 Example: How an MLLM Processes an Image+Text Query

To make the architecture concrete, consider an example: User gives a photo and asks: “What is happening in this picture?” An MLLM like GPT- 4V or LLaVA would:

1. Image Encoding: The photo is fed through a vision encoder (e.g., a VIT). Suppose it outputs a set of 50 patch feature vectors (each 768-d).

2. Projection: A linear layer (learned during fine-tuning) maps each 768-d vector to 4096-d, which is the dimension of the LLM’s embedding space (assuming the LLM uses 4096-d hidden states). Now we have 50 “image token” embeddings. If the model uses positional encodings, these might be assigned pseudo positions or a separate position scheme. Some implementations prepend a special vision prefix token as well.

3. Text Processing: The question “What is happening in this picture?” is tokenized into, say, 7 tokens including a question mark and maybe an end-of-sequence. These are embedded into 4096-d each using the LLM’s embedding matrix.

4. Fusion in LLM: Now the model has a sequence of (image_tokens..., question_tokens). If early fusion, they might all go through the same stack of transformer layers together. If Flamingo-style, the question goes through language layers but at certain layers, cross-attention pulls information from the image tokens (which are stored separately, or as part of context). Regardless, in the course of processing, the self-attention in the LLM can attend across text and image representations. For instance, when the LLM is encoding the word “picture”, cross-attention may tie it to salient image regions. The multimodal layers enable the model to form an internal representation that combines the visual scene with the linguistic query.

5. Answer Decoding: Finally, the model generates text output token by token (since it’s a decoder). At each generation step, it attends to the image-informed representations. For example, to start the answer, it might output “It looks like...”. As it generates “a person is riding a bike”, it has attended to the image tokens corresponding to a person and a bike in forming each word (though not in a one-to-one manner, rather as context). The result is a coherent sentence describing the scene.

6. If this were an instruction-tuned model in a chat format, it might include polite prefixes like “Assistant:” and ensure it follows any style guidelines it was tuned on. GPT-4, for instance, has system message instructions that also apply (e.g., not revealing certain info, being detailed, etc.).

This process relies on the learned alignment between visual features and language in the transformer's weights, achieved through the training methods above.

### 4.7 Model Size and Compute Considerations

Large multimodal models can be extremely demanding in terms of memory and compute. An image is a high- bandwidth input (e.g., a 224x224 image has 50k pixels, often encoded to, say, 197 patches in ViT- B/16). That means adding hundreds of tokens worth of information to an LLM's context. Video or long documents with images are even larger. Therefore, many MLLMs face context length and efficiency challenges. Some recent research looks at efficient attention or memory- augmented architectures to handle longer multimodal inputs  $\oplus$ . Also, training on image data is heavier than text (images are bigger and augmentation is needed). Some works use mixed- modal batches (some pure text, some image- text) to not overburden training. There's also a trend to use parameter- efficient fine- tuning (like LoRA adapters) to add multimodality to a large frozen language model, instead of full training, which drastically cuts compute.

### 4.8 Summary of Foundations

To summarize this section: MLLMs build on transformer LLMs by adding modality- specific front- ends and clever fusion layers. Key architectural components are modality encoders (CNNs, ViTs, etc.), projection layers to align feature dimensions, and attention- based fusion in either a unified or multi- stream transformer. Training involves self- supervised objectives that align and integrate modalities, often followed by instruction- tuning for usability. This combination enables the model to develop a joint representation space where, for instance, the concept of "a cat" spoken, written, or shown in an image all map to related representations that the model can reason with  $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ $\oplus$ . In the next section, we'll look at some specific models and methods in more detail, and how they instantiate these architectural principles.

## 5 Representative MLLMs and Methods

Having covered the general architecture and training strategies, we now survey a range of notable multimodal LLM models, emphasizing both foundational models and recent state- of- the- art methods. We group them by their conceptual approach or lineage:

- Contrastive Vision-Language Models (Dual Encoders): e.g., CLIP and its successors.- Unified Transformer Models: single-stream models like SimVLM, OFA (One-for-All), and early "Visual BERT" style models.- LLM with Cross-Modal Adapters: e.g., Flamingo, BLIP-2, LLaVA, MiniGPT-4 – which use pre-trained LLMs augmented with visual modules.- Multimodal Generative Models: e.g., DALL  $\cdot$  E 2, Stable Diffusion (text-to-image) and newer models that integrate those capabilities with language (like GPT-4V's image output via tools, Gemini's native generation).- Specialized Domains and Extensions: e.g., models bridging to robotics (PaLM-E, RT-1), document understanding (DocLMs that handle PDFs with text and layout), or audio-visual understanding (like VideoGPT variants).

For each, we highlight what the model introduced and how it fits into the evolution. (Where possible, we cite the original papers or authoritative sources for factual details).

CLIP (2021, OpenAI)  $\oplus$  : CLIP stands for Contrastive Language- Image Pretraining. It is a dual- encoder model: an image encoder (ResNet or ViT) and a text encoder (Transformer) are trained to produce

embeddings such that real image- caption pairs have high cosine similarity, and mismatched pairs have low similarity 19 . The training data was 400 million image- text pairs from the internet. After training, CLIP could perform zero- shot image classification by embedding an image and a set of candidate text labels (e.g., "a photo of a cat", "a photo of a dog", etc.) and choosing the label with highest similarity 18 . This zero- shot ability - treating a task as a text matching problem - was analogous to GPT- 3' s zeroshot learning but in vision, CLIP' s impact was huge: it provided a way to turn semantic concepts expressed in language into a form that a vision model can recognize in images, without explicit supervision for that particular category 58 . Additionally, CLIP' s image embeddings turned out to be very robust and general, benefitting many applications (from powering the image understanding in DALL  $\cdot \mathsf{E}2$  to being used in robotics and medical imaging tasks as a feature extractor). CLIP is not an LLM that generates language output by itself; it' s more like a foundation stone for multimodal systems. However, it exemplifies aligning representations across modalities. Later models such as ALIGN (from Google) and Florence (from Microsoft) followed similar approaches with even more data. Also, LiT (Locked- image Text tuning) was a Google variant where they started from a pre- trained image classifier and just learned a text model to align to it. These contrastive models typically have hundreds of millions of params in each encoder and train on billions of pairs nowadays.

VisualBERT, UNITER, OSCAR (2019- 2020): These were single- stream transformer models designed for image- text inputs. They often took in a sequence composed of image region embeddings + text token embeddings, plus segment embeddings to distinguish modalities. They were pretrained on tasks like masked language modeling with images and image- text alignment. For example, UNITER (Chen et al. ICCV 2019) achieved strong performance on VQA, captioning, etc., by unified encoding. OSCAR (Li et al. 2020) introduced the idea of using object tags (predicted object labels) as anchor points to help align text and image (essentially adding keywords detected in the image into the text stream). These models had ~100- 300M parameters and set the stage that larger vision- language models can be built akin to BERT. They however were primarily understanding models (classification, QA) rather than free- flowing generation.

SimVLM (2021, Google) 21: This model was important as a precursor to GPT- 4 style multimodality. SimVLM stands for Simple Visual Language Model. It essentially treated an image as a "prefix" token sequence to a text transformer. An image encoder (ResNet) produced a single vector which was transformed into a sequence of prefix tokens (somehow repeating or using multiple vectors) - or in a later version, used patch embeddings directly like a sequence. The model then was trained on a prefix language modeling task: predict the caption/text following the image. They used a massive dataset (1.8B image- text pairs). The result was a model that could generate captions impressively and also do VQA by prompt (e.g., prefix an image and question, and have it generate the answer). SimVLM was one of the first to show that pure generative pretraining at scale on image+text can yield very general multimodal capabilities, not needing separate objectives like contrastive. Its architecture is basically a unified transformer - early fusion style - which was simpler than dual encoders or two- tower models. This simplicity ( "simple" in the name) was a selling point, though training it obviously wasn't simple in terms of compute.

OfA - One for All (2022, Microsoft): OfA was a multitask unified model handling vision, text and even speech (in some versions) by casting everything as a sequence- to- sequence problem. It used an encoder- decoder transformer and was trained on a mix of tasks like image captioning, VQA, text- only tasks, etc. OfA could take an image and generate a caption, or take a question and generate an answer, etc., depending on the prompt format. It was an attempt at a unified multitask multimodal model. While it showed decent performance across tasks, it was not as large or general as some others (I believe it had around 180M or 370M parameters in different versions). But it indicated a direction to train simultaneously on many tasks to get a jack- of- all- trades.

Flamingo (2022, DeepMind) 24: As discussed, Flamingo combined a fixed language model (70B Chinchilla) with visual features from a ViT+ResNet hybrid encoder (Perceiver). The novel part was inserting gated cross- attention layers into the LLM. During training, they froze the LLM's original weights and only learned the added layers and the vision encoder. The model was trained on a mixture of multimodal data (image/video+text) including some narrated videos. This gave it the ability to handle sequences of interleaved images and text inputs, making it versatile for image discussion and video analysis (it can take a sequence of video frames as individual image inputs in a dialog). Flamingo's performance in few- shot settings for captioning and VQA was near SOTA with no fine- tuning - a big deal for generality 24. It showed that leveraging a strong LLM (with all its world knowledge and language fluency) is extremely useful in multimodal tasks, as long as you can effectively feed it the visual information. Flamingo's method of doing so (cross- attend at multiple layers) became influential. OpenFlamingo is an open- source reimplementation using LLaMA that has been explored in the community.

BLIP & BLIP- 2 (2022- 2023, Salesforce): BLIP (Bootstrapping Language- Image Pretraining) was a 2022 model that introduced a two- stage pretraining (first learn a captioning model, then use it to boot- strap further learning including filtering noise). It had an encoder- decoder design for VQA and captioning. BLIP- 2 (2023) took a different approach aligned with the LLM- augmented trend: it used a frozen ViT image encoder and a frozen LLM (they experimented with GPT- NeoX or OPT models) and learned an intermediate Q- Former (12 layer transformer) that connects the two 41. The Q- Former queries the image encoder and produces a small number of visual tokens (like 32 tokens) that the LLM can accept as a prefix. By not finetuning the big models, BLIP- 2 was very efficient to train. It achieved strong results on captioning and zero- shot VQA by virtue of the LLM's power plus visual grounding. BLIP- 2 basically set a template for many "mini- GPT4" projects - the idea that you can take a pre- trained ViT and a pre- trained language model and just learn a bridge between them. This is appealing because one can plug in increasingly powerful open LLMs as they become available (like today, one could try BLIP- 2 style with LLaMA- 2 70B).

MiniGPT- 4, LLaVA, etc (2023, Academia/Open- Source): After GPT- 4's announcement, many groups attempted to create smaller- scale imitators. LLaVA (Large Language and Vision Assistant) from Berkeley/UCSB used CLIP's ViT- L/14 image encoder and LLaMA- 13B as the base. They generated ~158K training examples by asking GPT- 4 to describe images and ask/answer questions about them (using the COCO and Laion datasets for images). Then they fine- tuned the LLaMA (with visual inputs via a one- layer projection) on this synthetic Q&A data. The result was a model that can do basic GPT- 4V- like tasks (describe image, identify objects, read some text in images, etc.) albeit at much lower accuracy than GPT- 4. MiniGPT- 4 similarly used Vicuna- 13B (which is a dialog- tuned LLaMA) and paired it with a ViT (through a single projection layer that maps image features to the embedding space). They first pre- trained that projection on 4 million image- text pairs with a simple alignment loss, then fine- tuned on a smaller set of curated image- instruction data. The success of these projects showed that even 13B- parameter LLMs, if given decent visual alignment, can exhibit useful multimodal abilities - though they still lag behind the likes of GPT- 4, especially in complex reasoning or OCR- heavy tasks. There have been numerous follow- ons: PaLM- 2 Image Extension (Google's internal, aligning vision with PaLM- 2), Qwen- VL (Alibaba's open 7B model with vision, which topped some benchmarks in mid- 2023), Otter (by an academic team, doing multi- turn dialogues by improving on LLaVA), KOSMOS- 2 (Microsoft's next iteration possibly), etc. The proliferation of these indicates how accessible multimodal modeling became once the recipe was out: you need a vision encoder, a strong language model, and some paired data or an intermediary model like BLIP- 2 or GPT- 4 to generate training data.

GPT- 4 (Multimodal) 8: The technical details of GPT- 4's vision input are not fully public, but user and partner experiments have revealed its capabilities. GPT- 4's vision system can handle non- trivial images: it can read small text in images (like labels or exam problems), interpret humorous or meme images by

combining visual cues with world knowledge, and explain charts or screenshots. It still has some limitations (for instance, as of late 2023 it sometimes struggled with very dense documents or complex math diagrams), but it represented a qualitative leap. One notable ability was solving tasks like drawing on a napkin and asking GPT- 4 to produce code (it could read the hand- drawn mockup and output HTML/ CSS). Under the hood, it is likely GPT- 4 takes image patches via a vision transformer module and then uses the same transformer decoder for text generation 8 . OpenAI hinted that they predict performance by scaling laws using smaller models 67, implying they trained smaller multimodal models before the full one. GPT- 4' s success rests on combining a massive (probably  $\sim 1$  trillion parameter- equivalent) model with a broad training diet (possibly including web images with all text, video transcripts with frames, OCR data, etc.). It also underwent alignment (RLHF) to make its outputs follow instructions well and avoid unsafe content - presumably extended to the vision domain. E.g., GPT- 4 was tuned not to identify human faces in images or do surveillance, as part of its safety. The GPT- 4 Technical Report described it as "accepting image and text inputs, and emitting text outputs" 8 . Notably, by design, it does not output images; it' s a text- only responder. In that sense, it' s focused on understanding visual inputs and responding linguistically.

Kosmos- 1 (2023, Microsoft) 12 : Kosmos- 1 is interesting because it' s an example of training a multimodal model from scratch (as opposed to adding vision to an existing LLM). They took a 1.6Bparameter Transformer and trained it on a mix of data: web text, image- text pairs (with the image converted to patches and embedded), and some synthetic multimodal data (like rendered text in images to force it to learn some OCR). The result was a model that could do few- shot learning on both language and multimodal tasks - hence "multimodal few- shot learning" in their claim 68 . Its scale was not huge, but it achieved reasonable performance on tasks like VQA and even passed some nonverbal IQ tests better than random. This work suggests even moderate- sized models can learn multimodal skills if given varied training signals. Microsoft hinted at "Kosmos- 2" with larger scale and more modalities (perhaps including audio, as the name "Language Is Not All You Need" implies more senses).

PaLM- E (2023, Google): PaLM- E combined the giant PaLM 540B language model with vision for robotics. The "E" stands for embodied. They fed images (and robot sensor data) into PaLM by first encoding the image with ViT, projecting to token dimension, and inserting those embeddings into PaLM' s input sequence 69 22 . They found the resulting model retained the language skills of PaLM (like it could still do math or logical reasoning) while gaining new skills like describing an image or guiding a robot arm. One intriguing finding was positive transfer: training the model on some vision- language tasks actually made it better at pure language tasks than baseline PaLM (perhaps due to regularization or new context handling skills). This hints that multimodal training can improve general models by grounding them. PaLM- E' s success in robotics tasks (like following instructions to pick up objects seen in a camera feed) is a step toward real- world deployment of LLMs, using multimodality as the interface to physical environments 62 .

Image- to- text vs Text- to- image divide: It' s worth noting a dichotomy: some models specialize in image understanding (input images, output text), others in image generation (input text, output image). Historically, these two were separate communities (vision- language vs generative models). Now, MLLMs aim to bridge them. For instance, IDEFICS (2023) is a model that extended the text- to- image Stable Diffusion model by adding a language backbone, making it capable of both understanding and generating images within a conversation (it can take an image, talk about it, and also create new images). OpenAI' s internal system likely combos GPT- 4 with DALL- E for their Vision+GenAI demos (e.g., ChatGPT can generate a webpage mockup from a sketch by actually producing an image of the design). Google Gemini apparently will natively produce images without calling an external model 9 , meaning the image decoder is integrated. This convergence might yield very powerful creative assistants - e.g., ask the model to design something and it outputs a design image, then you ask questions about the design and

the same model can discuss it, iterating in a loop. That level of integration is cutting- edge as of 2024- 2025.

Other Modalities - Audio and Video: While images have been the focus, audio and video are coming up fast. Whisper (OpenAI 2022) is a large speech recognition model (not an LLM for generation, but very capable transcriber). Combining Whisper with GPT- style reasoning can allow voice- dialogue agents (OpenAI's recent system allows spoken conversation by doing speech- to- text, feeding to GPT- 4, then text- to- speech). Some models like ImageBind and VideoCoCa (Google's video version of CoCa) incorporate audio: e.g., VideoCoCa can take video frames plus audio waveform and output a textual description 57. SpeechGPT- like efforts are finetuning LLMs to consume audio tokens (like via AudioLM or encoded units). It's likely that future MLLMs will include audio understanding natively (GPT- 5 rumors, etc., often mention multi- modality including audio). Additionally, 3D modalities (point clouds, depth maps) are relevant for specialized tasks like navigation or medical (e.g., MRI images, which are 3D). Those are still niche research areas but some multi- modal models for 3D data have been developed (e.g., PointBERT, and combining text with 3D scenes for captioning).

Given the breadth of models, Table 2 provides a high- level comparison of a few representative MLLMs from different categories:

Table 2. Selected Multimodal Model Comparison.  

<table><tr><td>Model</td><td>Year</td><td>Base Parameters</td><td>Modalities</td><td>Approach</td><td>Notable Capabilities</td></tr><tr><td>CLIP (OpenAI) 19</td><td>2021</td><td>2×{~400M encoders}</td><td>Image ↔ Text</td><td>Dual encoder, contrastive pretrain</td><td>Robust zero-shot image classification; image-text retrieval; forms basis of many later models.</td></tr><tr><td>Flamingo (DeepMind) 24</td><td>2022</td><td>80B (70B LLM + adapters)</td><td>Image + Text (video via frames)</td><td>LLM (Chinchilla) with inserted cross-attention layers to visual encoder</td><td>Few-shot learning for captioning &amp;amp; VQA; handles interleaved image-text dialogue; high performance with minimal finetuning.</td></tr><tr><td>BLIP-2 (Salesforce) 41</td><td>2023</td><td>~1B (ViT + Q-Former + LLM)</td><td>Image + Text</td><td>Frozen ViT and LLM, learn small Q-Former connector</td><td>Efficient alignment of vision with language; strong zero-shot results; enabled many open-source multimodal LMs by reusing this strategy.</td></tr><tr><td>GPT-4 (OpenAI) 8</td><td>2023</td><td>Not disclosed (~1T est.)</td><td>Image + Text (inputs)</td><td>Unified transformer (likely) with vision encoder front-end; RLHF-tuned</td><td>Accepts complex images (OCR, charts, memes) and gives detailed text answers; near human-level on many tasks; integrated into ChatGPT.</td></tr></table>

<table><tr><td>Model</td><td>Year</td><td>Base Parameters</td><td>Modalities</td><td>Approach</td><td>Notable Capabilities</td></tr><tr><td>Kosmos-1 (Microsoft) 12</td><td>2023</td><td>1.6B</td><td>Image + Text</td><td>Transformer encoder-decoder trained from scratch on multimodal corpora</td><td>Multimodal few-shot learning; performs OCR-free document QA (image input) and visual reasoning puzzles; smaller scale demonstration of multimodal training.</td></tr><tr><td>PaLM-E (Google) 21</td><td>2023</td><td>562B (540B LLM + vision)</td><td>Image + Text (+robot state)</td><td>PaLM LLM with image tokens inserted (via ViT); fine-tuned on robot tasks</td><td>Language-model-level reasoning with real-world perception; can output robot actions from textual instructions + image input; showed multimodal training can improve pure text performance too.</td></tr><tr><td>LLaVA (Open Source)</td><td>2023</td><td>13B (LLaMA) + ViT</td><td>Image + Text</td><td>Frozen CLIP ViT-L, LLM fine-tuned on GPT-4 generated QA data</td><td>Good at basic image description and simple queries; cheap to reproduce (-1-2 days on 8 GPUs); opened research on multimodal tuning data quality.</td></tr><tr><td>Gemini (Google) 9</td><td>2024</td><td>Not fully public</td><td>Text, Image, Audio, Video (in/out)</td><td>Likely mixture-of-experts + multimodal encoders; tool-use integration</td><td>Early reports: can generate images and speech from prompts; powers Bard&#x27;s improvements; seen as next-gen foundation model with agentic (tool-using) capabilities.</td></tr></table>

(Table 2 notes: Parameter counts where known; GPT- 4' s size is not confirmed. Gemini details are based on Google' s brief descriptions 9 . Models above 2023 often build on earlier ones - e.g., many 2023 open models use CLIP or ViTs from 2021. "Tool- use integration" refers to models having the ability to call external APIs (like image generation tools) as part of their pipeline, as mentioned for Gemini 2.0 9 .)

## 6 Applications Across Domains and Modalities

One of the exciting aspects of MLLMs is their applicability to a wide range of tasks spanning multiple fields. We now survey how multimodal LLMs are being applied (or have the potential to be applied) in various domains. This also implicitly highlights some subfields related to MLLMs: computer vision (when combined with NLP), speech and audio processing, robotics and embodied AI, education and assistive technology, healthcare, entertainment and creative arts, and more. We will cover the major

categories and provide examples of each, noting where current models are making an impact and where future opportunities lie.

### 6.1 Vision-Language Understanding (VQA, Captioning, Visual Reasoning)

Perhaps the most direct application of MLLMs is in tasks that involve understanding an image (or video) and producing a textual response. This includes:

- Image Captioning: Generating a natural language description for an image. MLLMs that take images as input can serve as general captioners. This is useful for creating alt-text for accessibility (helping visually impaired users by describing images on websites), or for organizing and searching photo collections. Modern captioning models (like OSCAR, BLIP, or GPT-4V) produce quite fluent and detailed captions. For example, GPT-4 can look at a photo and not just list objects but also convey high-level context (e.g., "A family posing in front of a statue on a sunny day"). This is qualitatively better than earlier caption systems that might say just "A group of people standing next to each other".

- Visual Question Answering (VQA): Answering questions about images. This can be seen in applications like querying a catalog image ("What color is this shirt?"), analyzing diagrams ("Which part labeled in this figure corresponds to X?"), or general curiosity ("How many people are in this photo and what are they doing?"). Systems like the Visual ChatGPT (which combined an LLM with vision models) or Google's Bard with image upload, allow users to ask questions about images directly (61). This has practical use in e-commerce (ask about product images), documents (ask about the content of a chart), and surveillance or security (ask "do you see any vehicles in this camera feed?"). Research benchmarks for VQA include ones with complex reasoning (e.g., CLEVR that tests logic with synthetic scenes). MLLMs with chain-of-thought prompting have made strides in those - by generating a step-by-step solution, they can solve things like counting objects or comparing attributes better (61).

- Visual Reasoning and Common Sense: Going beyond explicit questions, MLLMs can tackle tasks like explaining a joke in a meme (requires cultural knowledge + visual perception), or predicting what might happen next in an image (basic physical reasoning from a scene), or identifying if an image is doctored/fake. This blends into cognitive AI territory - understanding causality or intent from images. Some datasets like ScienceQA have diagrams with questions, and models like Kosmos and GPT-4 have been used to parse diagrams for reasoning (62).

- Document Image Understanding: Many documents (slides, forms, textbooks) contain both text and graphics. MLLMs can be applied to tasks like form understanding (extracting information from a scanned form), chart QA (answer questions about a chart's data), or slide summarization (reading a presentation slide image and summarizing it). Models such as Microsoft's layoutLM (predecessor using text+layout info) are being augmented by multimodal LLMs that take the actual rendered image of the document. Kosmos-1 was tested on "OCR-free NLP" 
- giving it an image of a document and asking questions without explicitly extracting text 
- and it did reasonably well (62), meaning it learned some vision-based text reading internally. The advantage is a single model can combine layout, text, and visuals to answer, which is useful in e.g. reading an infographics or answering "According to this brochure, what is the price of XYZ product?"

- Cross-Modal Retrieval: While not generation, another application is using MLLMs for retrieving relevant data across modalities. For instance, given a text query "a beach at sunset with palm trees", an MLLM's embeddings (like CLIP's space) can find the best matching image from a

database 19 . Vice versa, using an image to find relevant text documents or tags. Multimodal models enable an en- bedding- based search that is more semantic. This can be deployed in stock photo search, video scene search (find me scenes in security footage where two people shake hands), or audio retrieval (find me sounds similar to this one).

### 6.2 Embodied AI and Robotics

In robotics and embodied AI, MLLMs offer a way to interface high- level reasoning with perception and action. Some applications:

- Robotic Instruction Following: Natural language is a convenient way for humans to instruct robots (e.g., "Pick up the red apple from the table and place it in the blue bowl"). A robot to do this must parse the language, and use vision to locate objects, and then plan motor actions. MLLMs like PaLM-E address exactly this: they take visual observations and a command, and generate an action plan or direct motor control signals 21 22. This has been demonstrated on mobile robots and manipulators (robot arms). The MLLM can reason about obstacles ("If the path is blocked, go around") using general knowledge while also grounding the instructions in the camera view. This is a big improvement over traditional robotics pipelines which required manual calibration between language and vision (like hand-coded object detection pipelines + separate planners).

- Multimodal Navigation: Consider an agent that navigates based on both vision and language 
- e.g., following route instructions ("Go down the hallway, turn left at the painting, enter the second door"). This requires aligning the language ("painting") to a visual landmark. Prior systems did this with separate modules, but an MLLM could potentially read the instruction and current images and output navigation decisions (like "move forward"). Work in this area includes instruction-following in simulators and real environments (the VLN task). An MLLM could be fine-tuned for such tasks and possibly leverage general knowledge to handle ambiguities.

- Tool Use and API Calling: A form of "action" that is not physical but virtual 
- MLLMs can decide to invoke tools (as has been done in text-only LLM agents). For example, the model might choose to click a camera or retrieve a webpage if needed, by producing an action token. In an embodied setting, an MLLM could decide to switch between modalities or call specialized functions. Microsoft's Prometheus model (Bing Chat) can use image creation or search tools. Google's Gemini is said to have native tool use abilities (like calling a calculator or code interpreter) 9. For robotics, a tool might be "turn on depth sensor" or "switch camera angle" 
- an MLLM agent could reason about when to do that. So, multimodality also encompasses actions as outputs, not just content.

- Reasoning about Physical Environments: By combining vision (current state) and memory (dialog or past states), MLLMs can do things like assist humans in Augmented Reality: e.g., see what the user sees through AR glasses and answer questions ("The screw I'm looking for, is it the one on the left?", etc.). Or help in monitoring scenarios: an AI analyzing security camera feeds and alerting if someone falls (it has to understand the visual and communicate an alert). In such cases, reliability and real-time performance are challenges (LLMs are often slow). But smaller distilled versions could be used.

One interesting example is using ChatGPT with plugins to control a drone: the user could say "Scan this building's facade and report any cracks," and the system would plan drone waypoints (language-

>code- >action) and perhaps identify cracks in images (via an object detection plugin), then summarize. While not a single end- to- end MLLM, it's an orchestration that leverages multimodal understanding.

In summary, for robotics, MLLMs bring "common sense" and flexible instruction handling, which were lacking in classical robotics solutions. The open challenges are model reliability, real- time operation, and integration with low- level control (which often still uses separate controllers for motion execution for safety and precision).

### 6.3 Multimodal Content Creation and Entertainment

Another burgeoning area is using MLLMs for creative applications:

Image Generation from Description: Models like DALL  $\cdot$  E 2, Stable Diffusion, Midjourney have already revolutionized graphic design by allowing anyone to create art with a prompt. MLLMs are now being employed to enhance this process - e.g., by having a dialogue with the AI to refine an image. For instance, you could say: "Create an image of a medieval town." Model generates one (via an integrated generator or calling an API). Then you say, "Make it dusk with lanterns glowing." The system updates the image. This iterative loop is facilitated by an LLM understanding instructions and either modifying an internal representation or re- prompting the image model. Some research (Visual ChatGPT, etc.) has shown that linking ChatGPT with image editing tools (like removing an object, applying a style transfer) allows complex image manipulations via conversation 64 65. We expect future creative software to have multimodal assistants - Adobe is already prototyping such as the "Firefly" AI where you can type commands to edit images ( "replace the background with a forest," etc.).

Video Generation or Editing: Extending the above to video - imagine being able to ask an AI to generate a short animation or to edit a video ( "cut this clip at 1:30 and add a fade" ). Early text- to- video models exist (Meta's Make- a- Video, Google's Phenaki), though they are not yet as advanced as image generation. However, with LLMs coordinating, one could do storyboarding: the user writes a script, the AI creates scenes and even voices (text- to- speech for dialogue). Runway's Gen2 is a text- to- video tool, and others are integrating it into pipelines. MLLMs could help by ensuring consistency (the character looks same across scenes) through memory, something image- only models lack.

Interactive storytelling and games: MLLMs can serve as game engines that respond to player inputs not just in text but also by generating images or controlling NPC behavior. For example, in a text adventure game with images, the model can show you a scene image as well as describe it. If integrated with 3D engines, it might even generate or select appropriate 3D assets on the fly. Although realtime performance is a barrier, research is exploring using distilled smaller models for such interactive use.

Music and Audio generation: While less directly tied to language, some models like MusicLM take text descriptions and produce music. LLMs might assist by structuring compositions or writing lyrics that then feed into other models. A multimodal model might take an image or story and create sound effects or background music accordingly (imagine dynamic soundtracks for videos generated by understanding the mood of each scene).

Humor and Meme generation: Since MLLMs can understand jokes and context, they could also help create memes or witty captions for images. A trivial but fun application: give the model an image and ask it to generate a meme caption. Already, GPT- 4 excels at explaining memes 10;

generating them is an extension. Some social media companies are looking into AI to assist content creators (like suggesting image edits or taglines).

From an industry perspective, these creative applications of MLLMs are compelling because they open up new user experiences in entertainment, marketing, and design. A key challenge is controlling the output quality and style to meet user intent – often requiring fine- tuning or RLHF with human preferences specifically for aesthetic tasks. There's also the question of bias and appropriateness in generated content, which is a subset of the broader AI safety issue.

### 6.4 Education and Accessibility

MLLMs can also contribute to educational technology and making information accessible:

- Visual Tutors: An AI that can see what a student is working on (via a camera) and provide guidance. For example, a student solving a math problem on paper – the AI watches and offers hints if they get stuck, or checks their work. This requires recognizing handwriting (vision) and understanding the problem (language + math). GPT-4 has been shown to solve handwritten problems from images 70, so it's feasible. For science classes, a student could point their phone at a physics diagram and ask the AI to explain it. Multimodal models could provide richer help than text-only (which can't see the diagram).

- Language Learning with Visual Context: For language learners, an MLLM could use images to teach vocabulary ("This is a cat" [shows image], "Describe what the cat is doing in the picture" etc.). Or for practicing conversation, it might simulate a scenario with images ("You are at a restaurant" and shows a menu image, then engages in dialogue). The combination of visual and textual context can make learning more immersive.

- Accessibility for Visually Impaired Users: This is a major real-world impact of image-to-text capabilities. Apps like SeeingAI and Be My Eyes (which integrated GPT-4) allow blind users to take a picture and ask questions like "What's in this photo?" or "Read me the text on this label" or "Does this outfit match?". GPT-4's strong image understanding and contextual reasoning made it a powerful assistant for these users, going beyond previous tools that might only do OCR or basic labeling. It can answer follow-up questions about an image, providing a conversational experience 6. For hearing-impaired or other disabilities, multimodal models can also help – e.g., automatically generating sign language (though that's more generation), or converting complex info into simpler modalities.

- Multimodal Search and Study: Students or researchers often have to deal with textbooks that have images, charts, etc. A multimodal AI could be asked, "Explain Figure 3.2 and how it relates to the text on this page." It can parse both and give an integrated explanation. Or one could search a large video lecture library for where a certain diagram appears and what was said about it – needing combined text (transcript) and image matching.

- Augmentative Communication: People with communication impairments (for example, cannot speak but can type or use gaze) could use an MLLM to generate images that express what they mean, not just text. Or interpret their gestures via vision and translate to speech. This is speculative but aligns with the idea of AI as an intermediary to assist communication in any modality that suits the user.

In education and accessibility, the goal is often not just raw capability, but trustworthiness and adaptiveness. For instance, an educational AI should provide correct and pedagogically sound explanations, not just any explanation. So beyond the core MLLM, layers of verification or alignment with curriculum might be needed. However, the ability of a single model to analyze and discuss complex multimodal materials is a promising tool for personalized learning.

### 6.5 Healthcare and Scientific Analysis

Multimodal models are beginning to be applied in specialized domains like medicine, where data can be multimodal (medical images, sensor readings, patient reports):

- Medical Imaging + Text: Radiologists often compare textual reports with X-rays, MRIs, etc. An MLLM could potentially take an MRI image and the radiologist's notes and either check consistency or answer questions. There's research on models that generate radiology report impressions from scans, or vice versa highlight regions on an image given a report. For example, Microsoft's research on BioGPT-Vision or some extensions of BLIP in healthcare domain. The IBM mention of a model CONCH for histopathology images is one such case – it aligns captions/ descriptions with pathology slide images 71 72. This can help retrieve similar cases or help doctors identify features by describing them in familiar language.

- Electronic Health Records (EHR): Patient data includes structured data (lab results), unstructured text (doctor's notes), and sometimes images (like dermatology photos) or signals (EKG waveforms). A multimodal model could summarize a patient's condition by looking at both numbers and notes, or flag concerns by correlating modalities (like a note says "possible fracture", check if an X-ray image is indeed present and what it shows). While directly feeding an image and text to an LLM is feasible, regulated domains need correctness so these models might assist rather than finalize decisions.

- Scientific Research: Consider a model that reads a research paper which has charts and diagrams. It can answer questions about the paper by analyzing both the text and figures (this is a multimodal QA scenario). Or in chemistry, a model could take a diagram of a molecule and a description of a reaction and reason about the outcome (some work in progress on connecting chemical structure images with text). In astronomy, maybe combine sky images with research literature to identify objects. These are nascent but plausible as MLLMs get more specialized knowledge. In Table 2's footnotes, reference [22] mentions a survey of MLLMs in healthcare 73 and another in biological sciences (GITMol for molecular data) 74, indicating the interest in domain-specific MLLMs.

- CAD and Engineering: A domain like computer-aided design (CAD) involves diagrams, specifications, and sometimes natural language descriptions. A multimodal assistant might help an engineer by analyzing a schematic diagram alongside requirements and catching inconsistencies. In architecture, it could look at a blueprint and answer questions ("How many windows on the north side?"). Some specialized research (the science- direct link [19 + L8-L16] hints at parametric CAD with LLMs) is exploring that 75.

- Geospatial and Remote Sensing: Satellite images plus text reports (or maps plus instructions). A model could read a disaster report and satellite image to estimate damage described, or answer "Show me areas of deforestation between these dates" by comparing images and generating a map or text. MLLMs could help by using both description and visual evidence.

Each scientific domain likely needs fine- tuning of the MLLM on domain- specific data and jargon, as well as checks to ensure factual accuracy (perhaps via retrieval of trusted knowledge). The general MLLMs we have are a foundation, but for critical fields like healthcare, a fine- tuned specialized variant (with additional safety layers) is expected.

### 6.6 Social Media and Surveillance

MM LLMs can also be used to analyze multimedia content on the internet and elsewhere:

- Content Moderation: Platforms with images, video, and text comments can employ MLLMs to detect policy violations that are multimodal 
- e.g., an image that's benign on its own might become problematic with a certain caption (or vice versa). A multimodal model can consider both together and understand context (like a meme with hateful text embedded in the image). Already, some content moderation tools use image classifiers plus separate text classifiers; an integrated model could reduce false negatives/positives by jointly interpreting, say, sarcasm or context that spans image and text.

- Misinformation Detection: An example 
- a fake "news" image circulates with a misleading description. A model could flag inconsistency (the text says "this politician at a riot" but the image is actually from a different event). Also deepfakes 
- a multimodal model might look at a video and the claimed context and detect subtle mismatches (like mouth movements not matching audio text, etc.). While this is very challenging, combining modalities is the only way to catch some kinds of misinformation. However, ironically the same tech can generate misinformation (like AI-generated images with plausible text). It's an arms race where multimodal analysis is key on defense.

- Surveillance and Security: On a benign side, analyzing CCTV footage with an AI that can describe what's happening ("Person in red shirt left a package and exited") is useful for security monitoring. Add language input, one could query "Find clips where a person in red shirt left something behind". That's retrieval again, but a smart system can combine description and search. These tasks are being tackled with image and video captioning plus NLP queries (some startups working on "video intelligence"). Privacy and bias are big concerns here, of course, as automated surveillance can be misused or err.

- Multimodal Sentiment and Behavior Analysis: For social media posts that include text and images (think Instagram captions or Twitter images), an AI could gauge sentiment or perform analysis like "are people happy or angry in this video and what are they saying?" . Businesses may want to analyze customer sentiment from review videos or multimodal posts. Similarly, in politics, analyze propaganda that might use both imagery and slogan text together. The AI needs to catch subtle cues that emerge only when modalities are combined.

All these applications highlight the versatility of multimodal models. They also emphasize that the deployment of MLLMs in the wild must deal with reliability (no hallucinating answers about an image that aren't true), bias (images might trigger certain biases in descriptions, like describing people differently based on demographics - this has been documented in some captioning models), and privacy (especially if analyzing user- provided images or videos). We will talk more about these issues next.

## 7 Open Challenges and Future Directions

Open Challenges and Future DirectionsDespite the rapid progress in multimodal large language models, several open challenges remain. Addressing these is crucial for advancing the field and safely deploying MLLMs in real- world scenarios. We outline key challenges and research questions below, spanning technical limitations, data issues, and ethical/societal considerations, along with pointers to possible future directions for each.

### 7.1 Complex Reasoning and Consistency Across Modalities

While MLLMs can handle straightforward tasks, they often struggle with complex multi- step reasoning that involves multiple modalities. For example, solving a puzzle that requires reading a diagram and doing math, or answering a question that requires correlating text in an image with external knowledge. Some limitations observed:

- Multi-hop reasoning: If an answer requires combining clues from different parts of an image or multiple images and text, models may miss a step or make logical errors. They might describe each modality correctly but fail to draw the correct conclusion (a kind of "reasoning gap").- Spatial understanding: Models can fail at precise spatial relations (e.g., counting objects that overlap, understanding left vs right in an image) because their vision encoders might not capture spatial arrangement in detail, or the LLM might not inherently have spatial concepts. Even GPT-4V was reported to make mistakes on counting or describing relative positions in some cases.- Temporal reasoning in videos: Understanding the sequence of events in a video and answering "before/after" questions or summarizing a complex interaction over time is very challenging. Current models often treat frames as independent images and have limited temporal memory. Integrating a sense of time (maybe via specialized video architectures or by training on video-text data with temporal queries) is an open research area.- Consistency and Coherence: When generating outputs like long descriptions or narratives grounded in images, models sometimes flip details or get inconsistent (e.g., say a shirt is blue in one sentence and later say it's red). Ensuring cross-modal coreference (that "the man" in text consistently refers to the same person in the image throughout) is non-trivial. Some works on aligned representations and referring expression comprehension try to tackle this.

One future direction is enhancing multimodal chain- of- thought (CoT) capabilities 61. For example, a model might explicitly generate intermediate reasoning steps that mention the image ("Step 1: In the image I see a cat on a mat. Step 2: The question asks X. Step 3: Therefore Y."). Some initial studies show CoT helps in complex VQA. Another direction is to integrate symbolic or structured reasoning modules - e.g., if a question involves counting or geometry, the model might internally formulate a small program or query. Retrieval- based augmentation could also help if a model can query external knowledge relevant to an image (like seeing a landmark and querying a knowledge base for its info). These hybrid approaches, however, complicate training and inference.

### 7.2 Data and Multimodal Knowledge Gaps

High- quality multimodal datasets are not as abundant as text data. Models like GPT- 3 were trained on trillions of words, whereas even the largest image- text datasets (LAION, etc.) are on the order of billions of pairs, many of which are noisy. Challenges include:

Limited paired data: Some modality combinations are scarce. For example, there' s relatively little video+text data publicly available with detailed descriptions. Ontext- audio pairs beyond speech transcripts (like "audio of a dog barking" paired with "dog bark" label - those exist,

but more complex audio scene descriptions are few). Also certain domains (medical, scientific) have limited multimodal data due to privacy or complexity.

- Noisy alignment: Web-scrapped image-text pairs might not truly describe each other (a caption might be misleading or irrelevant). This can confuse training – the model might learn to describe images in an overly general way or latch onto spurious correlations (like always seeing beach images with the word “vacation” and then hallucinating “vacation” when it’s not actually indicated). There’s active research in data filtering and curriculum to improve signal in multimodal training 56 58.

- Multi-turn multimodality: Datasets for dialogue that include images are relatively new (e.g., the VisDial dataset was one, and more recently people create chat datasets with images in context). We need more examples of interactive multimodal exchange to fine-tune dialog models properly. Otherwise, you see many current models treat each image query independently and don’t “remember” or refer to it later in the conversation unless engineered to.

- Knowledge cutoff: Similar to text LLMs, MLLMs have a knowledge cutoff based on their training data. But additionally, their visual understanding might be frozen to the imagery they saw. For instance, an MLLM might not recognize a new model of car released after its training, or a new meme format. Updating multimodal models is harder than text because you need new image-text pairs. There’s interest in modular learning where you could update a vision encoder separately. Or use an LLM with image search so that if it doesn’t recognize something, it can query a plugin (like “search the web for images similar to X”).

To alleviate data issues, techniques like multimodal pre- training on generated data have been proposed – e.g., use an image generator to create synthetic image- caption pairs beyond what’s available. Or use models like GPT- 4 to label images (as LLaVA did) 59. However, these bring biases from the generating model. Another approach is weak supervision: using alt- text, surrounding text, even audio transcripts from videos can serve as weak labels to scale up. Self- supervised objectives like contrastive learning don’t need labeled captions, but they need diversity in data which web crawl provides to some extent.

In summary, while text data is virtually unlimited (just scrape more of the internet), multimodal data often requires deliberate collection and annotation (e.g., collecting medical images with reports, or specialized video data). Bridging that gap is both an engineering effort (data curation) and a research one (learning better from less or noisier data).

### 7.3 Long Context and Multimodal Context Windows

Many multimodal tasks involve long contexts – e.g., a multi- page PDF with diagrams, a 2- hour video, or a series of images (like a photo album or a surveillance feed). Current model architectures (transformers) have limited context windows (maybe 1- 2k tokens for older ones, up to 8k or more for some new ones). But an image can equate to hundreds of tokens (if treated patchwise). A video might be thousands of frames. Some challenges:

- Memory & computation: Handling a 100-page document with images in one shot is infeasible for a typical transformer (would be tens of thousands of tokens). Ditto for high-res images or long videos. Methods like chunking (processing piece by piece) lose global context. Sparse attention or hierarchical models (first encode locally, then summarize) are active areas to address this, as are state-space models or RNN variants that can, in principle, handle longer sequences 77 78. Referencing across a long context: In a conversation, a user might show an image, talk about it, then later refer back like “in that image I showed earlier...” The model needs to remember visual context over multiple turns. This is partly a memory problem and partly representation –

how to efficiently keep an image "in mind". Some solutions might be to have a persistent vector summary of the image that the model carries in the conversation state.

- Streaming data: For applications like real-time video analysis or continuous egocentric vision (AR glasses), the model should ideally handle a stream without needing to reset context every few seconds. Stream processing with transformers is hard, but some research on transformer variants that can do streaming or pseudo-continuous processing is emerging.

- Tool use to augment context: If an MLLM can't ingest everything at once, perhaps it can intelligently choose what to look at. For instance, if given a long book with images, the model could decide to focus on one chapter at a time ("open to page 50 and read figure caption"), i.e., it might have a mechanism to request relevant segments (like how retrieval-augmented models fetch text passages). For images, retrieval could mean cropping or selecting regions of interest (as some VQA models do – focusing on the region where the question points to).

The IBM challenge list explicitly mentions difficulty with long contexts missing text and images 79 80. They anticipate improved architectures beyond the "attention- heavy bottlenecks" of current transformers to allow longer multimodal sequences 29. Some possibilities are combining transformer with perceiver style latent arrays (so length of latent is fixed, used in Perceiver IO, etc.), or using memory units that can accumulate information.

### 7.4 Modality Integration and Expansion

Integrating new modalities beyond the initial set is not always seamless. For example: adding audio to a model that was trained on image- text might require retraining or at least careful alignment of audio features to the existing representation space. As we seek "any- to- any" models (text, vision, audio, 3D, speech, etc.), some challenges are:

- Calibration between modalities: Different modalities have different information density and noise characteristics. An image might provide detailed background context, whereas a short audio clip provides transient info. Models sometimes get overwhelmed by one modality or ignore another (known as modality collapse or dominance). Balancing attention such that each modality contributes appropriately is an area of study. Gating mechanisms (like in Flamingo they had gated attn) help regulate flow. But as we add more modalities, the interplay can get complex.- Unified representation vs modality-specific modules: Some argue for a uni-modal encoder + single transformer approach (Type D from earlier, i.e., tokenize everything), while others prefer separate encoders feeding a joint module (Type C) 46. There's also the question of how many modalities can we encode before the model's capacity is stretched. Any-to-any generation also implies the model's output space becomes more complicated (need to decide output modality).- Modality-specific skills: Certain tasks need specialized processing – e.g., an audio spectrogram might benefit from convolution for local patterns, or a document image might need OCR capabilities. If we rely purely on a generic transformer, we might sacrifice some efficiency or accuracy. On the other hand, too much specialization makes it hard for the model to learn cross-modal relations. So researchers explore modular architectures (maybe one transformer block specialized for audio, another for text, with cross-attention connecting them). There's an element of neuroscience analogy here: brain has specialized areas but integrates them – how to mimic that in AI is open.- Tool integration: The expansion can also mean linking non-traditional "modalities" like databases, calculators, code execution – making the model an agent that uses tools. Some frameworks (Huggingface's Transformers Agents, OpenAI plugins) give a glimpse of this. The challenge is training the model to know when to output a tool command vs solve itself. This may be addressed by more advanced finetuning or meta-learning. Gemini's mention of "native use

of tools" suggests they might train the model with an API calling ability built- in  $^{9}$ , which could be via including API call examples in the training data or a dedicated jinetuning stage.

Expanding modalities also introduces evaluation challenges: how do we systematically measure a model that e.g. takes audio, video, text input and can generate any of those? New benchmarks combining modalities would be needed. Some early multi- modal benchmarks (like AudioVisual QA, text- video retrieval tasks) exist, but a general AGI- like test is not defined. Possibly something like an AI driving test: feed it various inputs, it must perform a suite of tasks across modalities, could be conceived.

### 7.5 Evaluation and Benchmarking

Evaluating MLLMs is harder than evaluating single- modal models, due to the richness of output and lack of single ground truth for generative tasks. Challenges include:

- Subjectivity in generation: For captioning or descriptions, there could be many correct answers. Automatic metrics like BLEU or CIDEr often don't correlate well with human judgment of quality, especially for complex descriptions or explanations. There's a need for better eval metrics – possibly learned evaluators (some use CLIP to see if image and caption match, or use GPT-4 as a judge to score an answer).- Multifaceted success criteria: In tasks like VQA, an answer might be partially correct or correct but not well-justified. Some benchmarks now look at rationale + answer. For video, one might want factual accuracy, temporal coherence, etc. Traditional single-number metrics may not capture all aspects.- Bias and fairness: We have to evaluate how the model performs across different subgroups or contents (does it caption people of different ethnicities with equal respect and accuracy? Does it work for images from non-Western cultures?). There have been instances of vision-language models showing bias (e.g., misidentifying objects more often in certain contexts, or making gendered assumptions in captions). We need benchmarks that include diverse inputs to measure this. For example, REVISE is a framework that examined bias in vision models, similar are needed for MLLMs.- Robustness and adversarial tests: Multimodal models can be fooled by adversarial inputs – an image with a tiny sticker might cause a misclassification, or a prompt that mixes modalities weirdly might confuse it. Evaluating adversarial robustness means testing on slightly perturbed inputs (noisy images, paraphrased questions, etc.). Some works have looked at how adding unrelated images might distract a model, for instance.- Domain generalization: Does an MLLM trained on web images and captions work for, say, medical images and reports without fine-tuning? Often not directly; evaluating zero-shot transfer to new domains is important to understand limitations.- Efficiency metrics: If deploying on device (like AR glasses), we care about speed and memory. So beyond accuracy, evaluation might include throughput (frames per second processed), memory footprint, etc., particularly as models become large. There's work on distilling and quantizing models for efficiency, but measuring the trade-off is part of evaluation.

New benchmarks are emerging, for example: MME (MultimodalEvaluation) which tests instruction- following in vision- language models, LLaVA- Bench for comparing outputs on a set of questions, and old ones like VQAv2, VizWiz (for images taken by blind people, to test robustness), TextVQA (requires reading text in images), etc. Each provides insight, but no single benchmark covers everything. Likely the community will adopt a battery of tests. The NSR survey cited in IBM's footnote 81 82 emphasizes the range of tasks to test (they mention "across different tasks" which implies broad eval).

### 7.6 Ethical and Societal Issues

Multimodal models inherit and multiply the ethical concerns of single- modality models:

Privacy: Visual data can be very sensitive. A text model might inadvertently reveal private info seen during training, but a vision model might even show someone's face or environment. When integrated into AR glasses or home assistants, an MLLM effectively "see" a lot of personal context. Ensuring that data is handled with consent, and that models don't leak visual details (like describing a photo that was in its training set of a private moment) is critical. OpenAl said GPT- 4V was tested to not reveal identities of people in images 21 83 , aligning with our policy that it shouldn' t identify real individuals from images. Solutions include not training on private images (hard to guarantee if scraping web), or implementing filters at inference (e.g., blurring faces or refusing to identify them, which GPT- 4 does).

Misuse and Deepfakes: The generative side enables creation of very realistic fake images or videos. Combined with an LLM' s ability to create narratives, this could supercharge disinformation. There' s an arms race to develop detection methods, including potentially having models watermark their outputs or having discriminators. But as models improve, detection gets harder. This is a societal issue: how to trust media when Al can fabricate it convincingly? Policies and perhaps cryptographic signing of authentic media might be needed. MLLM developers have a responsibility to consider how easily their model could be repurposed for harm (e.g., an open multimodal model might be fine- tuned by bad actors to generate harmful content). Some voluntary limits: e.g. image generators often block generating certain faces or violent content. Possibly multimodal LLM APIs will do similar (OpenAl, for example, disallows using GPT- 4 vision to identify real persons or do surveillance).

Bias and Fairness: Visual biases combined with textual biases can result in problematic outputs. For example, a model might caption a picture of a woman in kitchen as "housewife" (a stereotype) or a group of Black teens as "gang members" (which happened with some vision models historically). MLLMs need careful evaluation for such biases 81 84 . This might require new data augmentation (show diverse contexts) and possibly fine- tuning with bias correction. However, bias in multimodal space is less studied than in text; it's an active area. Also, text in images (like memes) can be extremely toxic or coded language; the model might learn bad language from that.

Transparency: Explaining why a model said something about an image is often harder than text because the "features" it saw are not human- interpretable easily. Some work on attention visualization or saliency maps for VQA attempts to highlight which image regions influenced the answer. This can sometimes catch mistakes (like if the model looked at the wrong thing).

Providing users with an indication of confidence or source (e.g., "I inferred this because I noticed X in the image" ) would increase trust. But current MLLMs are not great at self- explanation; they might just rationalize after the fact.

Regulation and Policy: There might be domain- specific regulations (e.g., in healthcare, you cannot deploy a medical image analysis model without meeting certain standards). If an MLLM is used there, it falls under those. Similarly, face recognition is banned in some contexts; an MLLM inadvertently doing that could cause compliance issues. The integration of modalities might slip in capabilities that cross regulatory lines (like combining public info with a private photo to identify someone). There' s likely going to be legal scrutiny on how these models handle such cases. For instance, the EU Al Act may classify certain multimodal uses as high- risk.

Future directions to address ethical issues include developing safety filters and moderators specifically for multimodal content. OpenAl and others use image classifiers to filter what images can be fed to GPT- 4 (they block violent or sexual images to prevent certain outputs). Similarly, the output can be moderated (don' t describe certain sensitive attributes). These are being built but require constant

tuning. Another direction is value alignment in a cross- modal way: making sure the model's "values" (like not being racist or not disclosing private data) hold regardless of input form. If it sees a person's face, it should know not to identify them or comment on sensitive traits; if it reads a piece of text in an image that has slurs, it should handle that like text input. Essentially, all the content moderation challenges of text now apply to images too – plus new ones like graphic violence in images.

### 7.7 Lifelong Learning and Adaptability

IBM's list mentioned lifelong learning as an open challenge 85 86. Indeed, current MLLMs are static after training, barring fine- tuning. Future systems might need to continuously learn from new multimodal data without retraining from scratch (for instance, an AI assistant that gets better as it sees more user images over time, while respecting privacy). This is tough because of catastrophic forgetting and the sheer computational cost of updating large models frequently. Research in parameter- efficient finetuning, memory- based learning (like non- parametric memories that can store new examples), and modular expansions (adding new expert components for new data) is ongoing to allow models to grow and adapt.

For example, one could imagine an MLLM that has a plug- in "visual memory" – when it encounters a new object and the user labels it ("This gadget is a x200 sensor"), it stores an embedding with that label so later it can recognize it. Few systems do this now, but it's conceptually similar to how some vision systems can do one- shot learning with embedding models (e.g., CLIP can classify new classes via embeddings). Extending that to full MLLM integration is a challenge.

### 7.8 Multimodal Output and Feedback Loops

Another challenge is effectively incorporating multimodal feedback: if an AI generates an image or action, how does it evaluate the result and refine it? Humans use visual feedback naturally (draw a sketch, step back and see it's wrong, then adjust). An MLLM could do something similar: e.g., it generates an image via a diffusion model and then "looks" at that image with its vision capability and notices mistakes compared to the instruction, then fixes the prompt and regenerates. This kind of self- refinement loop would improve output quality. Some works call this "See- As- You- Say" or "Paint- With- Words" etc. It's an open area requiring bridging generation and understanding tightly. In a prototype, one might literally call the model on its own output image to get a description and compare to target. This is computationally heavy but could be optimized.

### 7.9 Concluding Challenges

In summary, while MLLMs have made remarkable progress, achieving truly human- level multimodal intelligence remains a work in progress. Challenges span from core technical hurdles (long context, reasoning, data) to critical ethical guardrails (bias, privacy). The field is actively pursuing solutions: new model architectures (perhaps transformer alternatives or improvements) promise better efficiency 29, smarter training paradigms (like multimodal RLHF, where humans might rank not just text answers but e.g. the quality of an image description or a graph drawn by the model), and interdisciplinary approaches combining vision, NLP, and other fields. Many of these challenges mirror those in text LLMs but multiplied by the complexity of additional modalities.

## 8 Conclusion

Multimodal Large Language Models stand at the frontier of AI research, integrating the once- disparate skills of understanding language, vision, audio, and more into unified systems. In this survey, we have

traced the evolution of MLLMs from early image captioning models to contemporary giants like GPT- 4 and Gemini that fluidly blend modalities. We provided formal definitions of key concepts (modality, multimodality, LLMs) and discussed how transformers and attention mechanisms form the backbone of current architectures, enabling models to fuse information from text, images, and other sources 87 51. A historical overview highlighted the milestones and innovations that have brought us to this point - notably the advent of transformer- based multimodal encoders in 2019, the contrastive learning breakthroughs in 2021 that aligned visual and textual representations 19 , and the integration of powerful language models with vision in 2022- 2023 that delivered unprecedented capabilities 24 8 .

We have seen that foundational models for multimodality are not just scaling up in size, but also in scope: moving from handling images+text to video, audio, and beyond. In reviewing architectures, we described typical MLLM components - from modality- specific encoders (CNNs, ViTs, wav2vec, etc.) to fusion layers and promptable LLM backbones - and contrasted early, late, and mid- fusion strategies 47 48. We presented an example architecture (Fig. 2) to illustrate how these pieces connect. Training paradigms were also discussed, emphasizing that today' s MLLMs often rely on a combination of selfsupervised learning (e.g., predicting masked text with image context, contrastive alignment) and instruction tuning to align with human preferences 59 88 . This dual approach endows them with both broad knowledge and user- friendly behavior.

Our survey covered a broad range of recent methods and models to exemplify the state- of- the- art. From CLIP' s two- tower model that taught models to see with language 18 , to Flamingo' s elegant merging of a pre- trained LLM with vision through gated attention 17 , to open- source efforts like BLIP- 2 and LLaVA that make multimodal AI more accessible, each represents a stepping stone toward more general AI. Notably, GPT- 4' s multimodal launch demonstrated to the world what such models can do - parsing exam papers and humor in images with notable skill 8 - while Google' s Gemini is poised to further blur the line between understanding and generation by handling multi- sensory input and output (text, images, audio) in one model 9 . The field is moving fast: what was frontier last year (like perhaps Flamingo or PaLi) has been surpassed by newer models with greater scale and integration. Yet the earlier models contributed ideas that live on in current systems (e.g., CLIP' s embeddings are still widely used, Flamingo' s cross- attention idea is now common).

Applications of MLLMs are proliferating across subfields. We discussed how they enhance vision- and- language tasks (from captioning and VQA to aiding the visually impaired with image descriptions), contribute to robotics (enabling language- driven perception and action 21), and unlock new possibilities in education, creativity, and science (like multimodal tutoring or analyzing scientific imagery). They also raise the bar in content generation, allowing for interactive creation of richly illustrated or audio- supported narratives. It's clear that any domain dealing with rich data can potentially benefit from multimodal AI: doctors might get AI assistance reading scans and notes together, architects might converse with an AI about blueprint diagrams, and everyday users gain AI that can see and hear, making interactions much more natural.

Despite the impressive achievements, we underscored several open challenges. Technically, models need improvement in deep reasoning, long- term memory, data efficiency, and incorporation of new modalities - challenges acknowledged by researchers 80 31 . Equally importantly, issues of bias, privacy, and ethical deployment loom large: an AI that can analyze images and video can be incredibly useful, but also prone to misuse or harmful errors if not properly governed. Future research is focusing not only on making models more capable, but also more robust, interpretable, and fair. For instance, incorporating user feedback and continuous learning in a safe way is an ongoing area (how can a deployed model improve over time without retraining from scratch, and without drifting into undesirable behaviors?). The community is also exploring hybrid systems that combine neural and symbolic

components for better factual grounding and reasoning – perhaps the pendulum may swing slightly back to structured representations for certain aspects (especially in high- stakes domains like medicine or law).

Looking ahead, one can envision that in a few years, we will have highly versatile multimodal assistants widely available. These assistants might seamlessly take in voice instructions, look at the world through our device cameras, consult databases, and respond with helpful answers or creative outputs that include text, visuals, or sounds as needed. Achieving this will require solving many of the research problems discussed, but the rapid progress of the past two years gives reason for optimism. The convergence of different AI subfields – NLP, computer vision, speech, etc. – around the transformer/LLM paradigm is accelerating cross- pollination of ideas. Each breakthrough in one modality (like a better vision transformer, or a better text generation technique) can now quickly benefit the others when integrated into an MLLM.

In concluding, Multimodal LLMs represent a significant step toward more general AI, reducing the siloing of perception and language capabilities. They bring us closer to AI systems that understand context more like humans do – in a rich, multi- sensory way. However, with great capability comes greater responsibility to ensure these models are used for the benefit of society. Ongoing collaboration between researchers, ethicists, and regulators will be key in charting a course where multimodal AI can flourish safely and equitably. The coming years will no doubt be exciting as we witness MLLMs tackling increasingly complex and diverse tasks, potentially even unlocking forms of problem- solving that neither vision nor language models could achieve alone. The survey presented here aimed to provide a broad yet detailed foundation for understanding this fast- evolving landscape, balancing technical depth with clarity. As this field advances, continual surveys and interdisciplinary research will be needed – but certainly, the age of multimodal AI has arrived, and it holds immense promise for transforming how we interact with technology and the world.

Acknowledgments: This survey compiled knowledge from numerous sources and recent research. We preserved citations to key works throughout (indicated by bracketed numbers) to credit original ideas and findings. (The references list below includes these sources, providing further reading for interested readers.)

## References

- Huang et al., Language Is Not All You Need: Aligning Perception with Language Models, arXiv preprint 2302.14045, 2023. (Introduced Kosmos-1 multimodal LLM; discussed multimodal chain-of-thought and cross-modal transfer) 12 26.  
- OpenAI, GPT-4 Technical Report, arXiv preprint 2303.08774, 2023. (Describes GPT-4 as a large-scale multimodal model accepting image and text inputs, with performance details) 8.  
- Wikipedia, Large language model – Multimodality section, retrieved 2025. (Provides definitions of modality and examples of multimodal LLMs like PaLM-E, LLaMA adaptations, GPT-4) 21 22.  
- GeeksforGeeks, Multimodal Large Language Models, 2025. (Overview of MLLM components and architecture patterns, e.g., encoders, connectors, fusion types, with examples) 27 34.  
- OpenAI Blog, CLIP: Connecting Text and Images, 2021. (Introduced CLIP model; explains contrastive learning setup and zero-shot capabilities) 18 58.  
- DeepMind Blog, Flamingo, 2022. (Article + paper on Flamingo model; few-shot learning with images and text via cross-attention in LLM) 24.  
- IBM, What is a Multimodal LLM?, 2023. (Technical blog covering MLLM pipeline: encoding, projection, fusion, decoding, with examples like LLaVA, MiniGPT-4, and listing challenges) 89 59.

- Yin et al., A Survey on Multimodal Large Language Models, National Science Review 11(12), 2024. (Comprehensive survey referenced in IBM footnotes; covers performance and challenges across tasks) 73 82.- Li et al., BLIP-2: Bootstrapping Language-Image Pre-training, arXiv preprint 2301.12597, 2023. (BLIP-2 method of frozen encoders + learned Q-Former to bridge modalities; state-of-art vision-language results) 41.- Alayrac et al., Flamingo + Visual Language Model for Few-Shot Learning, arXiv preprint 2204.14198, 2022. (Flamingo model details: interleaved sequence handling, performance on captioning/VQA in few-shot) 51.- Radford et al., Learning Transferable Visual Models From Natural Language Supervision, ICML 2021 (CLIP paper). (Demonstrated large-scale image-text contrastive training closing robustness gap; basis for many later models) 19 58.- Tsimpoukelli et al., Multimodal Few-Shot Learning with Frozen LLMs (OpenFlamingo), NeurIPS 2021. (Preliminary work leading to Flamingo; used frozen GPT-3 with learned perception module) 
- [Not directly cited above but relevant].- Li et al., Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks, ECCV 2020. (Introduced using object tags as anchors in vision-language pretraining; improved VQA/captioning) 
- [Not cited but influential].- Vinyals et al., Show and Tell: A Neural Image Caption Generator, CVPR 2015. (Early image captioning with CNN+LSTM; first to achieve high scores on COCO) 13.- Alan Thompson, Timeline of AI and Language Models, lifearchitect.ai (2025). (Chronology of major AI model releases including multimodal ones; used for timeline facts) 90 32.- Google AI Blog, Introducing Gemini 2.0, 2024. (Describes Gemini 2.0' s multimodal input/output and tool use features) 9.- etc... (Further references can include any other sources cited inline above, numbered accordingly.)

1 4 5 6 35 47 48 50 What are Multimodal Large Language Models? - Innodata https://innodata.com/what- are- multimodal- large- language- models/

2 3 7 10 17 21 22 36 43 51 69 77 78 83 87 Large language model - Wikipedia https://en.wikipedia.org/wiki/Large_language_model

8 67 70 [2303.08774] GPT- 4 Technical Report https://arxiv.org/abs/2303.08774

9 Gemini 2.0: Our latest, most capable AI model yet - Google Blog https://blog.google/products/gemini/google- gemini- ai- collection- 2024/

11 12 26 62 68 [2302.14045] Language Is Not All You Need: Aligning Perception with Language Models https://arxiv.org/abs/2302.14045

13 [1411.4555] Show and Tell: A Neural Image Caption Generator https://arxiv.org/abs/1411.4555

14 29 30 31 39 40 45 54 55 56 57 59 60 61 63 64 65 66 71 72 73 74 79 80 81 82 84 85 86 88 89 What is a Multimodal LLM (MLLM)? | IBM https://www.ibm.com/think/topics/multimodal- llm

15 Show, Attend and Tell: Neural Image Caption Generation with Visual ... https://proceedings.mlr.press/v37/xuc15. html

16 23 24 25 Timeline of AI and language models - Dr Alan D. Thompson - LifeArchitect.ai https://lifearchitect.ai/timeline/

18 19 20 58 76 CLIP: Connecting text and images | OpenAl https://openai.com/index/clip/

27 28 33 34 42 44 Multimodal Large Language Models - GeeksforGeeks https://www.geeksforgeeks.org/artificial- intelligence/multimodal- large- language- models/

32 90 The evolution of key multimodal AI models from 2018 to 2024. Notable... | Download Scientific Diagram

https://www.researchgate.net/figure/The- evolution- of- key- multimodal- AI- models- from- 2018- to- 2024- Notable- milestones- include_fig1_393617268

37 38 46 49 52 53 [2405.17927] The Evolution of Multimodal Model Architectures https://arxiv.org/abs/2405.17927

41 BLIP- 2 - Hugging Face https://huggingface.co/docs/transformers/en/model_doc/blip- 2

75 The status, evolution, and future challenges of multimodal large ... https://www.sciencedirect.com/science/article/pii/S095741742501142X