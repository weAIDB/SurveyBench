# Vision Transformers: From Attention Mechanisms to Visual Understanding

## 1 Introduction

IntroductionTransformers have revolutionized natural language processing since the introduction of the self- attention based Transformer model in 2017 1. In the years since, researchers have successfully adapted Transformers to computer vision tasks, giving rise to Vision Transformers (ViTs). A Vision Transformer applies the Transformer's attention mechanism to image data, offering an alternative to convolutional neural networks (CNNs) for visual recognition problems 2 3. The ViT model was first demonstrated in 2020 to achieve state- of- the- art image classification results using a pure Transformer architecture applied to image patches 4. This survey provides a comprehensive overview of Vision Transformers: we begin with the foundations of the Transformer architecture and how it is adapted for images, then trace a historical timeline of key developments, discuss major architectural variants and recent methods, examine their applications across different vision subfields, and finally outline open challenges and future directions. The goal is to balance technical depth with clarity, making the survey accessible to newcomers while providing insights for advanced researchers.

## 2 Background: Transformers and Vision Transformer Architecture

Background: Transformers and Vision Transformer ArchitectureTransformers in a Nutshell: A Transformer is a deep learning architecture originally designed for sequence- to- sequence tasks in NLP 1. Its core innovation is the self- attention mechanism, which allows the model to weigh the importance of different parts of the input sequence when encoding information 5. Instead of processing inputs sequentially, self- attention computes interactions between every pair of input tokens in parallel. Each input token is first mapped to query, key, and value vectors, and attention weights are obtained by computing similarity (dot- products) between queries and keys, followed by a softmax normalization 5. These weights then scale the value vectors, enabling each token to attend to others and aggregate information from the whole sequence. Transformers typically use multi- head self- attention, where multiple attention heads operate in parallel on different learned projections of the inputs, capturing diverse aspects of relationships (e.g. short- range vs long- range dependencies) 6. The outputs of all heads are concatenated and passed through a feed- forward network, with skip connections and layer normalization applied to stabilize training 7 8. Formally, given a sequence of  $\) n\(input tokens, the self - attention's computational cost scales on the order of$ \ $0(n^2)$  due to comparing all token pairs 9. This quadratic complexity posed a challenge for directly applying Transformers to images, since an image naively represented as a sequence of pixels would result in extremely large  $\) n\(s (e.g. a$ \ $256$  (times256) image has 65,336 pixels) 10 11. Early research circumvented this by using local attention windows or sparse attention patterns for images 12, but the Vision Transformer introduced a simpler solution: treat small image patches as tokens.

Vision Transformer Architecture: In a Vision Transformer (ViT), an input image is first split into a grid of fixed- size patches (for example,  $\) 16\(times16$  pixels each) 13 14. Each patch is then flattened into a 1D vector and projected (via a learned linear embedding) to the model's hidden dimension 15. This yields a sequence of patch embeddings representing the image. A learnable classification token ([CLS]) may be prepended to this sequence; the Transformer can learn to use the [CLS] token's output embedding as an aggregate representation for classification tasks 16. Since Transformers have no inherent notion of

spatial location, a positional encoding vector is added to each patch embedding to retain information about patch positions in the original image grid  $^{17}$ . The resulting sequence of vectors (length  $\S N S =$  number of patches  $+1$ ) is fed into a Transformer encoder stack just as if they were word embeddings in an NLP model  $^{2}$ .  $^{17}$ . The Transformer encoder in ViT is typically an adaptation of the encoder from the original Transformer: it consists of multiple layers of multi- head self- attention and feed- forward sublayers, with residual connections and normalization  $^{7}$ . The output is a sequence of image patch representations enriched by global context – effectively, each output token (especially the [CLS] token) attends to information from across the entire image  $^{18}$ .  $^{19}$ .

![](images/0daac8f11b5001ed096ab64d626e5b6e2a276c4523a992ca640a37f1087542e9.jpg)  
Figure: Vision Transformer architecture. An input image is divided into patches, each of which is flattened and linearly embedded. A special classification token ([CLS], purple) is prepended. Learned positional encodings (not shown) are added to indicate patch positions. The sequence of embedded patches is then processed by a stack of Transformer encoder blocks, producing a final [CLS] output that is fed to an MLP head for classification  $^{16}$ .

Comparison to CNNs: Unlike CNNs, which inductively encode translational locality and invariance via convolutional filters and pooling, Vision Transformers learn to interpret images with minimal built- in assumptions  $^{20}$ . CNNs have strong inductive biases – e.g. a convolution kernel only covers a small receptive field and the same weights are applied across the image (translation equivariance). These biases help CNNs generalize from fewer examples but can also limit their capacity. ViTs, by contrast, rely on global self- attention to integrate information and have no restriction on receptive field – a patch can attend to any other patch regardless of distance. This flexibility comes at the cost of requiring more data for training, since ViTs must learn spatial relationships and low- level feature detectors from scratch rather than assuming locality  $^{20}$ . Indeed, early studies found that Vision Transformers underperform CNNs when trained on limited data, but excel given sufficient scale  $^{22}$ . Dosovitskiy et al. showed that with large- scale pre- training (on datasets of 14 million to 300 million images), a vanilla ViT can match or surpass CNN performance on image classification benchmarks  $^{24}$ . For example, a ViT model was reported to outperform a ResNet of comparable size on ImageNet while using  $4 \times$  fewer computational resources during pre- training  $^{25}$ . In summary, CNNs have efficiency advantages on smaller data regimes due to built- in structure, whereas Transformers offer higher capacity and capture long- range dependencies naturally, making them formidable when ample data or pre- training is available  $^{21}$ .

## 3 Timeline of Key Developments in Vision Transformers

The concept of applying Transformers to visual data has rapidly evolved from initial ideas to a plethora of architectures. Below is a chronological overview of major milestones and developments in Vision Transformers:

- 2017 
- "Attention Is All You Need": Vaswani et al. introduce the Transformer for machine translation, built entirely on self-attention layers 1. This work establishes the transformer architecture that will later inspire Vision Transformers, though at this point it is purely an NLP innovation.

- 2018-2019 
- Early Attention in Vision: Researchers begin exploring self-attention within CNN frameworks. For instance, Ramachandran et al. (2019) replaced convolutions with local self-attention in a ResNet-like architecture and achieved competitive image recognition performance 28. These efforts still relied on CNN-style structures or limited attention patterns, foreshadowing fully transformer-based vision models.

- Mid-2020 
- Transformers Meet Vision: Several landmark works apply Transformers to vision tasks:

- Detection Transformer (DETR) by Carion et al. (May 2020) formulates object detection as a direct set prediction problem and uses a transformer encoder-decoder to predict bounding boxes in an end-to-end fashion 29. DETR eliminated hand-crafted components like proposal generation and non-maximum suppression, demonstrating the elegance of Transformer-based design for object detection.

- Image GPT (iGPT) by Chen et al. (mid-2020) uses a GPT-like transformer to auto-regressively generate image pixels, treating images as sequences of pixels. This illustrated that Transformers can model image distributions and be used for unsupervised pre-training on images 30.

- Vision Transformer (ViT) by Dosovitskiy et al. (Oct 2020) is the first demonstration that a pure Transformer applied to image patches can excel at image classification given large-scale training 4. The ViT paper introduced the patch embedding technique and showed ViT attaining state-of-the-art accuracy on ImageNet when pre-trained on a huge private dataset (JFT-300M) 24.

- CLIP by Radford et al. (Dec 2020) trains a vision-language transformer model on 400 million image-text pairs, pairing a ViT image encoder with a text transformer. CLIP learns rich visual representations aligned with language, enabling zero-shot image classification (recognizing objects from textual descriptions without explicit training on those classes) 31. This work highlighted Transformers' capability in multi-modal understanding.

- 2021 
- Explosion of ViT Variants: Following ViT's success, numerous variants and improvements emerge:

- DeiT (Data-Efficient Image Transformer) by Touvron et al. (Jan 2021) shows that ViTs can be trained on smaller datasets (ImageNet-1k) by incorporating strong data augmentation, regularization, and knowledge distillation from a CNN teacher 32. DeiT achieves high accuracy without external data, greatly lowering the entry barrier for ViTs.

- Swin Transformer by Liu et al. (Mar 2021) introduces a hierarchical Transformer backbone using shifted window attention 33. Swin Transformer restricts self-attention to non-overlapping local windows (patch regions) to reduce complexity, and shifts the windows between layers to allow cross-region connections 34. This design gives linear scalability to image size and produces a

multi- scale feature hierarchy (much like CNNs), making Swin highly effective for dense tasks like object detection and segmentation. Swin outperformed previous backbones on COCO detection and ADE20K segmentation benchmarks by a significant margin  $^{35}$ , establishing Transformers at the forefront of these tasks.

- Other 2021 Architectures: Pyramid Vision Transformer (PVT) introduced a CNN-like pyramid of progressively reduced resolution using transformers, TNT (Transformer-in-Transformer) embedded a local transformer inside each patch to model finer details, CrossViT combined dual-scale patch tokens for multi-scale feature fusion, and numerous others (Twins, CvT, CMT, ViT-C, etc.) experimented with hybridizing convolutions and transformers. Researchers also explored lightweight Transformers for mobile/edge (e.g. MobileViT) and pure MLP alternatives (MLP-Mixer, which interestingly showed that even without attention, patch-based architectures can perform well). By late 2021, vision transformers had firmly penetrated all major vision benchmarks, often surpassing CNNs in accuracy.

- 2022 – Unsupervised and Scalable Transformers: Two notable trends mark this period:

- Masked Image Modeling: He et al. (2022) propose Masked Autoencoders (MAE), extending the success of BERT's masked token prediction to images  $^{36}$ . A ViT is used to encode a partially masked image (with a high percentage of patches hidden), and a lightweight decoder reconstructs the missing pixels. This self-supervised pre-training significantly improved data efficiency, allowing ViTs to learn powerful representations from unlabeled images and boosting performance on downstream tasks. Masked image modeling quickly became a standard pre-training approach for ViTs, alongside related efforts like iBOT and BEiT.

- Scaling to Extreme Sizes: Zhai et al. (2022) and later researchers at Google scale Vision Transformers to unprecedented sizes. ViT-22B (a model with 22 billion parameters) was introduced as the largest dense vision model to date  $^{37}$ . Techniques were developed to train such models efficiently (e.g. gradient checkpointing, sharded data pipelines), and these giant ViTs demonstrated strong performance across many tasks, pushing the limits of what purely attention-based vision models can do. The scaling experiments suggested that ViT performance continues to improve with model size and data, echoing trends observed in NLP  $^{38}$ .  $^{39}$ .

- 2023 – Foundation Models and Vision Transformers: Vision Transformers became central components of emerging foundation models in vision:

- Segment Anything Model (SAM) by Kirillov et al. (2023) is a promptable segmentation model using a ViT backbone trained on billions of segmentation masks. SAM can produce accurate object masks from simple prompts (points or boxes), demonstrating the utility of ViT's generalization in zero-shot settings.

- Multi-Modal Models: Aligning with the trend of joint vision-language models, frameworks like Florence (Microsoft) and Basic/VL (Google) used ViT-based image encoders trained with billion-scale image-text data, enabling capabilities like image captioning, retrieval, and question answering with high accuracy.

- Continued Model Innovation: New architectures such as MaxViT (combining convolutional and axial-attention patterns) and refinements like SwinV2, ConvNeXt (a CNN inspired by Transformer design choices  $^{40}$ ), and ViT-Adapter (adding task-specific modules to a frozen ViT) were introduced. The line between CNNs and Transformers blurred further as each class of model adopted ideas from the other (e.g., CNNs borrowing attention layers, Transformers borrowing convolutional token embeddings)  $^{41}$ .

This timeline illustrates a rapid evolution: in just a few years, Vision Transformers went from a novel idea to a dominant paradigm across numerous vision tasks. Next, we delve deeper into the key methods and architectures that have emerged, and how they differ and improve upon the original ViT.

## 4 Vision Transformer Variants and Architectural Innovations

Original ViT (2020): The baseline Vision Transformer by Dosovitskiy et al. is a direct adaptation of a Transformer encoder for image classification 42. It uses a constant patch size (e.g.  $16 \times 16$ ), a fixed number of encoder layers (e.g. 12 layers for ViT- Base), and outputs classification via the [CLS] token with a simple feed- forward head. The original ViT models (Base, Large, Huge) achieved excellent results when pre- trained on very large datasets (ImageNet- 21k or JFT- 300M) and fine- tuned 24. However, training them from scratch on a medium- scale dataset like ImageNet- 1k yielded subpar results without extensive regularization 22. The authors noted two main challenges: data inefficiency (due to lack of inductive bias) and sometimes instability in training for very deep/large Transformers. These observations catalyzed multiple research directions to improve ViTs.

Data Efficiency Improvements: One of the first questions was how to enable ViTs to reach high accuracy without enormous training data. DeiT (Data- Efficient Image Transformer) answered this by incorporating training strategies that make better use of limited data 32. Specifically, DeiT introduced heavy data augmentation and a form of knowledge distillation wherein a pretrained CNN "teacher" guides the ViT during training 43. With these techniques, DeiT achieved  $81.8\%$  top- 1 accuracy on ImageNet using only the 1.2M images of ImageNet (no extra data), a result on pwr with the best CNNs of similar size at the time. Notably, the DeiT model architecture is identical to ViT; the contribution was showing that with the right training recipe, ViTs can succeed on normal- scale datasets 32. Subsequent works proposed other efficiency boosts: for example, using CNN- like early layers to extract low- level features (hybrid models) 44, or improved initialization and optimization techniques to stabilize ViT training. By mid- 2021, training a ViT from scratch on ImageNet became relatively common, helped by open source libraries and pretrained weights.

Hierarchical and Multi- Scale Vision Transformers: Another line of innovation focused on architectural changes to better capture the multi- scale nature of images. CNNs naturally form a feature hierarchy (gradually reducing spatial resolution and increasing semantic abstraction), which vanilla ViT lacked. The Swin Transformer addressed this by dividing the image into patches and then applying self- attention within local windows of patches 34. By shifting the window positions in alternating layers, Swin allows information to flow between windows, achieving a good balance of local modeling and global communication 33. Moreover, after a few layers, Swin reduces the number of tokens (merging patches) to form a pyramid of stages (minicking how a CNN pools feature maps) 45. This hierarchical structure gave Swin a significant advantage on dense prediction tasks (object detection, segmentation) because it can provide multi- scale feature maps to the detector/segmenter heads - a role traditionally filled by CNN backbones 35. Other pyramid Transformers like PVT and SegFormer similarly introduced multi- stage designs. These models demonstrated that inductive biases for locality and scale can be reintroduced to Transformers to improve data efficiency and task performance without sacrificing the long- range dependency modeling. For example, PVT (2021) achieved strong detection results by gradually shrinking the token sequence, and SegFormer (2021) combined a lightweight hierarchical encoder with a simple segmentation head to achieve state- of- the- art semantic segmentation.

Hybrid CNN- Transformer Models: Given the complementary strengths of CNNs and Transformers, researchers also experimented with hybrid architectures. One approach is to use a CNN front- end: e.g., a small convolutional stem to extract low- level features or even entire convolutional stages before feeding into a Transformer. This was explored in works like CvT (Convolutional Vision Transformer) which used

convolutional token embedding layers, or CMT (Coarse- to- fine Mini- Fuse Transformer) which interleaved conv blocks and self- attention. These hybrids often showed improved robustness and sometimes faster convergence 46 47 . The idea is that convolutions excel at capturing local texture and edges (which require fewer parameters and augmentations to learn), so a front- end CNN can handle those, after which a Transformer processes the higher- level interactions. Empirically, even the ViT authors later adopted a convolutional stem in follow- up works to ViT, noting it provided better spatial inductive bias without hindering performance 46 . Conversely, CNN models began to incorporate ideas from Transformers: the ConvNeXt architecture (2022) is a pure CNN that was redesigned with inspiration from ViT (e.g., using fewer activation layers, a transformer- like normalization and activation placement, etc.), and it achieved CNN state- of- the- art results 42 48 . These cross- fertilization efforts underscore that the line between "CNN" and "Transformer" is blurring, as the community searches for the best of both worlds.

Efficient Attention and Scaling: Vanilla self- attention has  $\{0(n^2)\}$  complexity in the number of tokens, which can be a bottleneck for high- resolution images or video frames. Beyond restricting attention to local windows (as in Swin), many efficient attention mechanisms were proposed. Some employ sparse attention (attending only to some tokens), low- rank approximations, or kernel- based methods to reduce complexity (e.g., Linformer, Performer, Nyströmformer, etc.). In vision, one notable variant is BigBird and Longformer adaptations for images, or the use of axial attention (treating 2D image attention as two 1D attentions along height and width). While a detailed survey of efficient attention is beyond our scope, it's worth noting that such techniques have been integrated into some vision models to handle larger images. For instance, CSWin Transformer (2022) introduced a cross- shaped window attention which alternates horizontal and vertical strips of attention to mix local and global information efficiently 49 50 . MaxViT (2022) combined block local attention and grid global attention in a repeating pattern, achieving excellent accuracy- speed tradeoffs. These innovations allow Vision Transformers to be deployed on higher- resolution inputs and even resource- constrained devices by lowering memory and compute usage, which is crucial for tasks like medical imaging or ultra HD image processing.

Self- Supervised Vision Transformers: Following the success of masked autoencoding (MAE) mentioned earlier, vision transformers have become a backbone of choice in self- supervised learning. Besides MAE, the DINO framework (2021) used a ViT in a student- teacher setup to learn image features without labels, yielding representations that perform well on classification and even show emergent properties like unsupervised segmentation. Other approaches like BEiT (2022) combined ViT with a discrete image tokenizer for BERT- like pretraining. The general finding is that Transformer- based vision models benefit greatly from unsupervised pretraining, often more so than CNNs, closing the gap in data efficiency 51 52 . Moreover, some self- supervised ViTs demonstrate remarkable transfer learning ability and robustness to distribution shifts 53 . As a result, by 2023 many state- of- the- art results on standard vision benchmarks are obtained by ViTs that were first pretrained on large unlabeled corpora (with techniques like MAE, contrastive learning, or multimodal alignment) and then fine- tuned - mirroring the pretrain- finetune paradigm long used in NLP 54 38 .

In summary, since the original ViT, we have witnessed extensive architectural innovation: from improving training protocols (DeiT), adding hierarchies (Swin, PVT), hybridizing with CNNs, to crafting specialized attention mechanisms and scaling to extreme model sizes. These developments have continually pushed the performance and applicability of Vision Transformers. Next, we will see how ViTs have been applied across various computer vision domains.

## 5 Applications Across Vision and Beyond

Applications Across Vision and BeyondVision Transformers were first proven on image classification, but their impact has quickly spread to virtually all areas of computer vision, as well as to multi- modal tasks. Here we survey how ViTs are applied in several key subfields:

- Image Classification: This is the domain where ViTs made their debut, directly challenging CNNs. On ImageNet and other classification benchmarks, ViT models have achieved top-tier accuracy  $^{24}$ . One striking observation is that when properly trained, ViTs can surpass CNN accuracy while being more computationally efficient to train  $^{25,26}$ . For instance, a ViT trained on sufficient data matched ResNet accuracy with fewer FLOPs due to faster convergence  $^{25}$ . Vision Transformers also exhibit interesting robustness properties: research indicates ViTs are less biased towards texture and can be more robust to some input perturbations than CNNs  $^{53}$ . Moreover, ViTs are effective in transfer learning; a large ViT pre-trained on a big dataset can be fine-tuned to many classification tasks, often outperforming CNNs that were pre-trained on the same data. Variants like CrossViT have been proposed that use multiple patch sizes (e.g. splitting an image into a mix of large and small patches) to capture fine and coarse features, further boosting classification performance  $^{55}$ . Privacy and security researchers have also explored ViTs for tasks like privacy-preserving classification and adversarial robustness, finding that ViTs may offer advantages in resisting certain adversarial attacks  $^{56,57}$ .

- Object Detection: Transformers have redefined how we approach object detection. The pioneering DETR model proved that a transformer with object query tokens can directly output a set of bounding boxes and class labels, simplifying the detection pipeline  $^{29,58}$ . DETR's encoder processes image features (often from a CNN backbone) and the decoder uses learnable queries that attend to those features to predict objects. While DETR achieved competitive results, it had slower training convergence. Subsequent works like Deformable DETR introduced sparser attention (each query attends to a small set of key spatial locations) to speed up learning  $^{59,60}$ . Meanwhile, with ViT backbones becoming powerful, some detectors like ViTDet (from Facebook AI) have used a plain ViT as the feature extractor for a Faster R-CNN style detector and achieved state-of-the-art results. Swin Transformer based detectors (e.g. Swin + Mask R-CNN) significantly advanced the state of the art in 2021  $^{35}$ , thanks to the strong multi-scale features from Swin. Transformers have also enabled end-to-end object detection free of non-maximum suppression and other post-processing, by formulating detection as a set prediction problem  $^{29}$ . As a result, modern object detection architectures often either incorporate transformer modules or are built entirely upon transformers. We are also seeing Transformers in specialized detection problems (e.g. tracking objects across video frames using attention to link detections over time, and 3D object detection in point clouds by transformer-based feature fusion).

- Image Segmentation: Vision Transformers have been applied to semantic segmentation, instance segmentation, and panoptic segmentation with great success. One approach is to use a ViT as the encoder in a segmentation model (replacing a CNN encoder in architectures like UNet or encoder-decoder models). For example, SETR (2021) used a ViT encoder to generate a full-resolution feature map which is then upsampled for segmentation, showing that pure transformers can perform segmentation without convolution  $^{61}$ . More recent approaches combine ideas from DETR: models like MaskFormer/Mask2Former use a set of learnable queries in a transformer decoder to predict masks (treating segmentation as a mask set prediction problem, analogous to DETR's box prediction). These achieve a unified view of segmentation where semantic and instance segmentation can be done in one framework. Indeed, transformer-based segmentation models have reached new state-of-the-art accuracies; for example,

Mask2Former (2022) significantly outperforms prior CNN- based mask predictors. Research suggests that a transformer encoder- decoder with learnable query embeddings can serve as a universal solver for segmentation tasks  $^{62}$ $^{63}$ . Additionally, Swin Transformer's strong performance on segmentation (e.g. in the SwIN- Unet variant for medical image segmentation) underlines the benefit of ViT's global context - the self- attention mechanism can inherently group distant parts of an image that belong to the same object or category, which is very useful in segmentation.

- Video Understanding: Video analysis extends image recognition to the time dimension, and Vision Transformers have been adapted to handle sequences of video frames. The main challenge is the greatly increased input size 
- a video is a sequence of images. TimeSformer (Facebook AI, 2021) showed one solution by factorizing attention into spatial and temporal components  $^{64}$ . It treats a video as a sequence of frame patches and applies self-attention separately over the spatial patches within each frame and across the temporal axis (between corresponding patches in different frames)  $^{64}$ . This factorization reduces complexity from quadratic in  $\sin \xi$  (number of patches times number of frames) to linear in frames for each attention operation. TimeSformer demonstrated that a pure transformer approach can achieve state-of-the-art on video action recognition tasks  $^{65}$ . Another approach, ViViT (Video ViT) by Google, processes a video by first embedding patches per frame and then either flattening all patches of all frames (treating it like a long sequence) or doing two-step attention (first spatial, then temporal). Variants of ViViT achieved strong accuracy on Kinetics-400 and other benchmarks. Streaming video (continuous frame input) has also been tackled with transformers that have memory mechanisms to handle long sequences. Overall, transformer models are attractive for video because they can capture long-range temporal dependencies (e.g. events happening many frames apart) more naturally than CNN/LSTM hybrids. However, computational cost remains a concern - thus recent work explores sparser temporal attention or efficient video tokens to keep video transformers tractable.

- Multi-Modal Vision-Language Applications: Perhaps one of the most impactful uses of vision transformers is as part of multi-modal models that connect vision and language. The CLIP model mentioned earlier uses a ViT to encode images into an embedding space, and a text transformer to encode text; they are trained together to align image and text embeddings  $^{31}$ . The result is a model that can perform zero-shot image classification (by choosing the text label embedding most similar to the image embedding) and has learned a broad semantic understanding of visual concepts and their names. Following CLIP, many Vision-Language Transformers have emerged: for example, ViLBERT and LXMERT actually predate CLIP (2019) and use two-stream architectures (a CNN for images + transformer for text, with cross-attention to fuse them) for tasks like visual question answering. More recent systems like FLAVA and BEiT-3 use unified transformer backbones that jointly attend to image patches and text tokens. Vision Transformers are crucial here because the flexibility of the attention mechanism allows mixing modalities in a single model. Also, the strong representations from ViTs serve as a foundation for tasks such as image captioning (transformer decoder generates text from ViT image features), text-to-image retrieval, and even generative models. Generative AI for images has also benefited: the first DALL-E (2021) model used a transformer to autoregressively generate image pixels (or rather discrete image tokens) given a text prompt. While modern image generators use diffusion models (often with CNN U-Nets), even those use cross-attention (a form of transformer) to integrate text embeddings from a CLIP text encoder. In summary, Vision Transformers have become the default choice for the vision component of multi-modal AI systems, which are driving a lot of recent breakthroughs in AI.

- Other Domains: Beyond conventional images and video, transformer models are being used in 3D vision (point clouds, LiDAR data) and medical imaging. For 3D point clouds, there are Point Transformers that attend to coordinates and features of points, achieving state-of-the-art on

classification and segmentation of 3D data by capturing long- range geometric relationships. In medical imaging, ViTs (and hybrids like Swin- Unet) have shown promise on tasks like tumor segmentation, organ detection in CT scans, etc., often surpassing CNNs when sufficient training data is available. Their ability to model global context is valuable for medical images where subtle global structures can matter. However, medical datasets are often small, so self- supervised pretraining and hybrid models are commonly employed to get the best of ViTs in this field.

![](images/d2c4eb94b628ffbc8b37f62b580e541e129684ca20d619c4afd9b6c80e67098d.jpg)  
Figure: Examples of images (left column) and the corresponding learned attention maps (right column) from a Vision Transformer model. The heatmaps highlight which regions of the image the ViT considered important for classification. Attention maps show that ViTs often focus on semantically meaningful parts of the object (e.g. the tennis ball or the animal) rather than background, suggesting an interpretable pooling of global information 66.

One interesting aspect across applications is interpretability. The attention weights in Vision Transformers can be visualized to show which patches attend to which, offering a window into the model's reasoning. As illustrated in the figure above, one can often see the model "highlighting" object regions. While attention may not always equate to explanation, these visualizations provide qualitative insight and have been used to debug and analyze model behavior. Researchers have even utilized attention maps for unsupervised object discovery - essentially letting the ViT's attention reveal objects in an image without explicit supervision.

## 6 Open Challenges and Future Directions

Despite the remarkable progress of Vision Transformers, several challenges and research questions remain:

- Data Efficiency and Inductive Bias: Large-scale data has been a key factor in ViTs' success 4. When data is limited, ViTs still tend to underperform unless supplemented with strong augmentations or pretraining 22. One challenge is introducing the right inductive biases to improve data efficiency without sacrificing the flexibility of the transformer. Hybrid models (CNN-Transformer combinations) are one answer, but is there a more principled way to encode inductive bias (like locality) into transformers? Some recent works try to learn positional embeddings or incorporate convolutional layers for low-level feature extraction 67 68, while others explore training techniques (e.g. better initialization, regularization) to make ViTs generalize from fewer examples. Bridging this gap will be important for domains where massive

data is not available – for example, medical imaging or scientific imagery. Self- supervised learning is a promising avenue here: models like MAE enable ViTs to learn from unlabeled data, effectively increasing the data pool without manual annotations.

- Computational Cost and Model Efficiency: Self-attention's quadratic complexity means scaling to higher resolutions or longer sequences is expensive 69 10. While patch embeddings already reduce sequence length compared to pixels, applications like high-resolution satellite imagery or video with many frames still pose computational challenges. There is active research on efficient transformers – techniques such as sparse attention, low-rank approximations, token pruning, and others can greatly reduce computation. For instance, employing a local attention (as in Swin Transformer) limits computation and can even make complexity linear in image size 34. Another approach is dynamically pruning tokens: a ViT could learn to ignore unimportant patches (background, for example) at later layers, effectively reducing sequence length as depth increases. This is analogous to spatial downsampling in CNNs, but learned and content-adaptive. Such ideas are just beginning to be explored. Furthermore, model compression (pruning, quantization) for ViTs is an emerging area, aimed at deploying ViTs on edge devices. Early studies indicate that ViTs have some redundancy that can be pruned, but specialized techniques are needed since transformers behave differently than CNNs under compression 70 71. Solving efficiency will broaden the applicability of Vision Transformers, allowing their use in real-time systems and low-power devices.

- Understanding and Interpretability: As models get larger (billions of parameters) and more complex, understanding how a Vision Transformer makes decisions is both important and challenging. Attention visualizations offer one approach, but they are coarse. There is a need for better tools to interpret ViTs – for example, identifying which neurons or heads correspond to human-interpretable concepts, or why certain failures occur (when they do). Some research has found that Vision Transformers tend to exhibit “token uniformity” in deeper layers (i.e., patch representations become similar) which relates to the model focusing on global image-level features 72. How this emergent behavior connects to model decisions is still being studied. Additionally, as ViTs are used in critical applications (e.g. medical diagnosis), interpretability will be key for building trust and meeting regulatory requirements. Future work might involve designing transparent attention mechanisms or hybrid models that are easier to explain.

- Towards Unified Models: One of the most exciting prospects is that transformers provide a universal modeling framework that could unify modalities and tasks 52 73. Already, we see models like Perceiver and unified transformer architectures handling images, text, and even audio with minimal changes. Vision Transformers might evolve into components of more generalist systems – for example, a single transformer that can ingest an image and a prompt and output a detailed analysis or a modified image. To achieve this, questions of scaling (to handle multiple huge inputs), modularity, and efficient training will need to be addressed. Moreover, while Transformers have dominated NLP and are now doing so in vision, integrating other sensory data (like point clouds for 3D, video with audio, etc.) is an open field. A future goal is a multimodal transformer that can reason jointly about a complex scene with visual, textual, and perhaps other inputs (sound, depth, etc.), bringing us closer to general AI.

- Continued CNN-Transformer Synergy: Rather than one replacing the other, we can expect CNNs and Transformers to continue influencing each other. CNNs might incorporate more attention mechanisms for long-range interactions, whereas Vision Transformers might adopt CNN-like operations for efficiency. The result could be hybrid architectures that we have yet to imagine – perhaps something that uses convolution for early layers, transformer blocks for mid-range, and again convolution or other specialized layers for certain structured outputs. The recently

proposed ConvMixer and HybridViT models already hint at this mixing. Additionally, exploring different nonlinearities or activation functions within transformers (beyond the standard GELU) and different normalization schemes could yield architectural improvements.

- Domain-Specific Challenges: Each subfield that Vision Transformers enter brings unique challenges. In medical imaging, for example, segmenting high-resolution pathology slides with a transformer requires handling gigapixel images – methods like recursive attention or region cropping might be needed. In video, capturing very long temporal relationships (minutes of video) without blowing up compute will require hierarchical temporal transformers or memory-augmented models. For autonomous driving, real-time processing is critical, so latency-optimized ViTs or token sparsification are relevant. As ViTs venture into these areas, task-specific innovations will likely emerge.

In conclusion, Vision Transformers have ushered in a new era in computer vision, demonstrating that attention mechanisms can serve as a powerful alternative to convolutions for visual feature learning 74 21 . They have already achieved or surpassed state- of- the- art performance in image classification, detection, segmentation, and more, all within a few years of their introduction. The research community has responded with an outpouring of creativity - from model designs to training strategies - to address the initial limitations of ViTs. Many of those challenges (such as the need for huge data) are being steadily overcome, and Vision Transformers continue to grow more capable, efficient, and versatile.

As we look ahead, the convergence of transformers across vision, language, and other modalities is a tantalizing direction 52 . It suggests a future in which a single unified model might understand and reason about diverse inputs seamlessly. There are open questions on the road to such general models, but the progress so far with Vision Transformers provides a strong foundation. The journey of Vision Transformers - from a novel idea inspired by NLP to a cornerstone of modern visual intelligence - exemplifies how fundamental advances in machine learning can transform entire fields, and it is likely far from over. We anticipate continued advances in this area, driven by both theoretical insights and practical needs, ultimately leading to more powerful and general vision systems that leverage the full potential of the transformer architecture.

## References

- A. Vaswani et al., "Attention Is All You Need," NeurIPS, 2017.  
- A. Dosovitskiy et al., "An Image is Worth  $16 \times 16$  Words: Transformers for Image Recognition at Scale," ICLR, 2021 4 22.  
- H. Touvron et al., "Training data-efficient image transformers & distillation through attention (DeiT)," ICML, 2021 32 43.  
- Z. Liu et al., "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows," ICCV, 2021 33 35.  
- N. Caron et al., "End-to-End Object Detection with Transformers (DETR)," ECCV, 2020 29 38.  
- X. He et al., "Masked Autoencoders Are Scalable Vision Learners," CVPR, 2022 36.  
- M. Radford et al., "Learning Transferable Visual Models From Natural Language Supervision (CLIP)," ICML, 2021 35.  
- Meta AI, "Segment Anything: Segmenting objects by prompting," 2023.  
- A. Zhai et al., "Scaling Vision Transformers to 22 Billion Parameters," arXiv preprint, 2022 37.  
- B. Singh et al., "Revisiting Weaknesses of Vision Transformers and CNNs," arXiv preprint, 2022 21 25. (analysis of inductive biases and robustness)

1 5 6 10 11 27 69 A Brief History of Vision Transformers: Revisiting Two Years of Vision Research | by Merantix Momentum | Merantix Momentum Insights | Medium https://medium.com/merantix- momentum- insights/a- brief- history- of- vision- transformers- revisiting- two- years- of- vision- research- 26a6bd3251f3

2 3 9 15 16 17 36 37 40 41 42 44 48 Vision transformer - Wikipedia https://en.wikipedia.org/wiki/Vision_transformer

4 12 13 14 20 22 23 24 28 38 39 54 [2010.11929] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale https://ar5iv.labs.arxiv.org/html/2010.11929

7 8 18 19 21 25 26 30 31 49 50 53 55 56 57 61 66 74 75 Vision Transformer: A New Era in Image Recognition https://viso.ai/deep- learning/vision- transformer- vit/

29 58 [2005.12872] End- to- End Object Detection with Transformers https://arxiv.org/abs/2005.12872

32 43 71 Training data- efficient image transformers & distillation through attention | Request PDF https://www.researchgate.net/publication/347797071_Training_data- efficient_image_transformers_distillation_through_attention

33 34 35 45 [2103.14030] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows https://arxiv.org/abs/2103.14030

46 47 51 52 59 60 62 63 67 68 72 73 arxiv.org https://arxiv.org/pdf/2111.06091

64 Is Space- Time Attention All You Need for Video Understanding? https://arxiv.org/abs/2102.05095

65 TimeSformer: A new architecture for video understanding - Meta AI https://ai.meta.com/blog/timesformer- a- new- architecture- for- video- understanding/

70 Recent Advances in Vision Transformer: A Survey and Outlook of ... https://arxiv.org/abs/2203.01536