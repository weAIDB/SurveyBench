# Large Language Models for Recommendation

## 1 Introduction

Recommender systems are software tools that suggest relevant items to users based on their interests and past interactions 1 . Classic examples include Amazon' s product recommendations or Netflix' s movie suggestions 1 . Traditionally, such systems have relied on models like collaborative filtering (finding patterns in user- item interaction data) and content- based filtering (using item features such as text descriptions) to compute affinity between users and items. In recent years, Large Language Models (LLMs) - extremely large neural networks trained on vast text corpora - have emerged as a powerful paradigm in Al, capable of understanding and generating human- like language 2 . These LLMs (often based on Transformer architectures) can be adapted to a wide range of tasks through fine- tuning or prompting 2 3 . This survey explores the intersection of these two fields: how LLMs are transforming recommender systems. We provide formal definitions of key concepts, trace a historical timeline of developments, and review state- of- the- art methods, open challenges, and new applications enabled by LLMs in recommendation. The goal is to offer a broad yet in- depth understanding accessible to newcomers and useful to experts alike.

Large Language Model (LLM) - Definition: An LLM is a language model (often with hundreds of millions or billions of parameters) trained via self- supervised learning on massive text datasets, designed to perform natural language processing tasks, especially text generation 2 . Contemporary LLMs, such as GPT- style models, acquire strong capabilities in syntax and semantics from their pre- training and can be fine- tuned or steered with prompts for specific tasks 4 . Many of the most capable LLMs are based on the Transformer architecture and exhibit emergent abilities due to their scale 5 .

Recommender System - Definition: A recommender system aims to generate meaningful item suggestions to users that align with their preferences 1 . Formally, given a set of users \(\) 15\(and items\)\S15\(along with historical interactions (e.g. ratings, clicks, or purchases), a recommender seeks to estimate a utility score\)\S\backslash\mathsf{h}\mathsf{a}\mathsf{r}(\mathsf{r}(\mathsf{u},\mathsf{l})\S\(for any user - item pair and recommend top items that maximize user utility\)^1\(. The design can utilize various data: past user - item interactions, user attributes (age, location, etc.), item content (descriptions, reviews), or external knowledge. Major solution approaches include: (1) Collaborative Filtering (CF), which learns from patterns of user behavior (e.g. matrix factorization to learn latent user and item vectors from interaction data); (2) Content - Based Filtering, which uses item features (textual or otherwise) to recommend similar items to those a user liked; and (3) Hybrid methods, combining multiple signals\)^{6}\(7. Quality is measured by how well the recommendations match user interests (often via metrics like precision, recall, NDCG) and by user satisfaction in practice.

Why LLMs for Recommendation? LLMs offer significant advantages for recommender systems due to their ability to understand and generate natural language 8 . Many recommendation tasks can be formulated as language problems - for example: predicting a rating or next item can be expressed as a fill- in- the- blank or question- answering task, generating a list of recommendations can be a text generation task, and explaining why an item is recommended is itself a language generation task. By unifying these tasks into text prompts and responses, a single LLM can serve as a universal recommendation engine handling various subtasks 8 . Additionally, LLMs come pre- loaded with broad world knowledge and linguistic context from their training corpora 9 . This means an LLM- based system can comprehend user preferences expressed in rich natural language, interpret item descriptions or

reviews, and incorporate contextual knowledge (e.g. inferring that “science fiction time- travel show” refers to certain movies) when making recommendations 9. This stands in contrast to traditional recommenders that often rely on numeric IDs or sparse vectors, and struggle to utilize unstructured data. In short, LLMs bring a semantic understanding and flexibility that can potentially improve recommendation relevance and enable new capabilities like conversational interaction and explanation generation. At the same time, employing LLMs in recommendation introduces challenges – how to integrate them with collaborative filtering signals, how to scale to huge item catalogs, how to avoid generating non- existent items (hallucinations), and how to maintain fairness and efficiency. We will delve into these issues after establishing the foundations and evolution of this emerging area.

## 2 Foundations and Background

Evolution of Recommender Models: Early recommendation algorithms (1990s- 2000s) were relatively “shallow” models – for example, neighborhood- based collaborative filtering computed similarities between users or items, and matrix factorization learned low- dimensional ID embeddings by minimizing rating prediction error 6. In the 2010s, the field embraced deep learning: neural networks incorporated into recommenders to model non- linear feature interactions and sequential user behavior. For instance, convolutional or recurrent neural networks were used to learn representations from item texts and from user activity sequences, improving over linear models especially in leveraging content features and complex patterns. By the late 2010s, the Transformer architecture from NLP started to influence recommender systems. The self- attention mechanism enabled models to capture long- range dependencies in user sequences more effectively than RNNs 10. A notable example is SAsRec (Self- Attentive Sequential Recommendation, 2018), which applied a transformer- based self- attention model to users’ item histories. SAsRec demonstrated that attention could identify which past items are most relevant for predicting a user’s next item, capturing long- term preferences with higher efficiency than RNNs 10. Shortly after, researchers introduced BERT4Rec (2019), which adapts the bidirectional Transformer (BERT) to sequential recommendation by using a Cloze (mask- and- predict) training objective 11. BERT4Rec predicts masked items in a user’s history by looking at both left and right context, thereby learning a bidirectional representation of the sequence 11. This yielded stronger performance than unidirectional models, establishing Transformers as state- of- the- art for sequence- based recommendation.

Parallel to sequence modeling advances, another thread in the late 2010s was incorporating textual content via pretrained language models. E- commerce and media recommenders began leveraging textual descriptions, reviews, and other natural language data to improve recommendations, especially for new items with little interaction history (the cold- start problem). Early approaches used static word embeddings (e.g. Word2Vec, GloVe) to represent text in recommendation models 12. However, these were context- independent and had limitations. The advent of large pretrained models like BERT offered a richer solution: by 2020, research showed that using BERT to encode news articles or product descriptions significantly boosts recommendation of new content 13 14. For example, UNBERT (UserNews BERT, 2021) introduced a BERT- based model for news recommendation that leverages BERT’s language understanding to match news articles with user interests 13. It uses a pretrained BERT to encode both user’s browsed news and candidate news, capturing multi- grained matching signals (keywords, topics, etc.) between them 13. By transferring linguistic knowledge from large- scale text pretraining, UNBERT and similar models improved generalization to newly published news and mitigated cold- start issues (since the model understands text content without extensive new- user training) 15 14.

Large Language Models: The term “Large Language Model” generally refers to Transformer- based models with hundreds of millions or more parameters trained on broad text corpora. Key milestones in the development of LLMs include OpenAI’s GPT series (GPT- 2 in 2019 with 1.5B parameters, GPT- 3 in

2020 with 175B), Google's BERT (2018) and later bidirectional models like RoBERTa and T5 (2019), and conversationally fine- tuned models like ChatGPT (2022) and GPT- 4 (2023). By virtue of their scale and training, LLMs exhibit surprising abilities to perform tasks they were not explicitly trained for - a phenomenon known as emergent behavior 16. They can be adapted to new tasks with minimal examples (few- shot learning) or none at all (zero- shot inference) by properly phrasing the task as a textual prompt. This flexibility has led researchers to view LLMs as foundation models: general- purpose models that serve as a base for many downstream applications 17 3 . In recommender systems, the rise of LLMs opens up the possibility of unifying many subtasks (rating prediction, item ranking, explanation, etc.) under one umbrella by "talking to" a model that understands language. Moreover, LLMs bring some level of world knowledge - for example, a large model may know relationships between books, movies, or music genres from having read about them, which a traditional recommender would have to learn purely from user- item data. These appealing properties have spurred a new wave of research into LLM- based recommenders, which we review in the sequel.

Before diving deeper, Table 1 (below) summarizes a timeline of major milestones leading to today's LLM- powered recommender systems:

- 1992-2000: Foundations of recommender systems laid with collaborative filtering (e.g. the GroupLens project) and content-based filtering. Algorithms like user neighborhood models and early matrix factorization introduced 18. Text was used in rudimentary ways (e.g. keyword matching for content-based methods) but no deep language understanding yet.

- 2006: Netflix Prize competition popularizes modern collaborative filtering (matrix factorization). Emphasis on latent factor models for ratings.

- 2012-2016: Emergence of deep learning for recommendation. Models incorporate neural networks and word embeddings. E.g., DeepCoNN (2017) uses two parallel CNN networks on user reviews and item reviews to predict ratings, one of the first to use deep NLP in recommendation. YouTube's deep candidate generation (2016) and others show industrial success of deep models. However, these methods typically train on recommendation data end-to-end; they do not leverage pretrained language models yet.

- 2017: Google's Transformer architecture is introduced 19. This quickly influences sequential recommender models. Attention-based methods like SASRec (2018) come out, using self-attention to model item sequences and outperforming RNN approaches 10.

- 2019: BERT4Rec (CIKM 2019) adapts the BERT transformer for recommendations, using a masked-item prediction task to learn bidirectional representations of user histories 11. It achieves state-of-the-art sequential recommendation accuracy, proving the value of large bidirectional language models on implicit feedback data. This year also sees increasing use of BERT for content understanding in recommenders (e.g. encoding news articles, product descriptions).

- 2020: OpenAI's GPT-3 is released (175B parameters), marking the era of extremely large language models. Though not a recommender model, its capabilities (zero-shot question answering, etc.) inspire researchers to consider using such models for generating recommendations with minimal training.

- 2021: The term "foundation model" is coined by Bommasani et al. to describe models like GPT-3 that can be adapted to many tasks 17 3. Research on multimodal foundation models

(e.g. CLIP for vision- language) suggests future recommenders might also leverage multiple modalities. Early work begins on fine- tuning large transformers for recommendation- specific tasks (e.g. a 2021 paper uses T5 to generate review summaries and recommendations jointly).

- 2022: P5 (Prompt- &-Predict Paradigm) is introduced as a unified framework for multi-task recommendation 20 21. P5 uses a pretrained T5 (an LLM for text generation) and formulates various recommendation tasks as natural language prompts, allowing one model to handle rating prediction, item ranking, sequential recommendation, explanation generation, and more 22 21. Accepted at RecSys 2022, P5 demonstrates that a single large language model can be fine-tuned to perform dozens of recommendation tasks via prompting, and do so in a zero-shot manner for new users or tasks by virtue of language understanding 23 21. This represents a significant step toward universal recommender systems.

- 2023: Explosive growth of LLM-for-rec research. The public release of ChatGPT (Dec 2022) showcases the power of LLMs in dialogue; soon people begin using it for ad-hoc recommendations (e.g. asking ChatGPT for travel or movie suggestions). Researchers formally evaluate LLMs as recommenders: e.g., Hou et al. show that GPT-4 and similar LLMs can serve as zero-shot ranking models for recommendation when given a proper prompt format 24 25. They find LLMs can generate fairly good item rankings without any task-specific training, though they have biases (like favoring popular items) that require prompt engineering to mitigate 25. Another study by Sanner et al. finds that LLMs are competitive in cold-start settings, where a user provides a natural-language description of their preferences instead of historical item IDs 26. They report that for users with no prior interaction data, an LLM using the user's language input can achieve accuracy comparable to traditional collaborative filtering that has a few interaction data points 26. Meanwhile, new architectures emerge that combine LLMs with traditional recommender components. For example, RecMind (NAACL 2024) introduces an LLM-powered recommendation agent that can plan multi-step recommendation dialogues and use external tools/databases, achieving performance on par with fully-supervised models in zero-shot settings 27 28. On the systems side, companies like Microsoft and Google integrate LLMs into search and recommendation interfaces (Bing Chat, Google Bard), blurring lines between search, Q&A, and recommendation. By late 2023, the research community has organized multiple workshops and tutorials (e.g. the RecSys 2023 tutorial on LLMs for Rec 29) and surveys on this topic, reflecting its maturation.

- 2024: Focus shifts to addressing limitations and scaling up deployment. One direction is bridging ID-based collaborative filtering with LLMs 
- e.g. the CLLM4Rec framework (WWW 2024) introduces a hybrid model that augments a pretrained language model with learnable ID tokens for every user and item, thus combining the collaborative signal of ID embeddings with the semantic power of text modeling 30 31. CLLM4Rec uses a novel prompting and training strategy to ensure the LLM learns both types of information (text and IDs) and can generate multiple item recommendations without deviating off-track (reducing "hallucinated" items) 32 33. Another direction is improving prompting strategies for better recommendations. AdaptRec (2025), for instance, proposes a self-adaptive prompting method where the LLM itself is used to pick which similar users' histories to include as examples in a prompt for sequential recommendation 34 35. This dynamic use of collaborative filtering inside the LLM's prompting loop led to improved accuracy, highlighting an interesting trend of letting LLMs learn how to use CF data on the fly. We also see deeper exploration of trustworthy AI aspects: studies on controlling LLMs to avoid unsafe or unfair recommendations, dealing with biases, and explaining recommendations in a faithful way.

This timeline shows a rapid convergence of NLP and recommender system research. In the next sections, we categorize and discuss the major methods and paradigms that have emerged, from using LLMs as feature extractors to using them as end- to- end recommenders or as parts of larger recommendation pipelines.

## 3 LLM-Based Recommendation Paradigms

Large Language Models can be integrated into recommendation systems in multiple ways. We organize the discussion into several paradigms: (1) LLM as feature encoder within a conventional recommender, (2) LLM as a generative recommender producing recommendations in natural language, (3) LLM for zero- shot or few- shot recommendation via prompting, and (4) Hybrid approaches that combine LLMs with traditional recommendation components (e.g. collaborative filtering or external knowledge). These paradigms often overlap, but separating them helps clarify the range of techniques.

### 3.1 LLMs as Content and Feature Encoders

One straightforward way to leverage LLMs is to use their text understanding capabilities to enrich item or user representations in a standard recommendation model. In this approach, the LLM is not directly generating recommendations; instead, it provides features that a downstream model can use for ranking. For example, a pretrained language model like BERT or GPT can encode an item's description, reviews, or related text into a dense vector. This vector can serve as the item's content representation, which is then fed into a collaborative filtering model or another ranking model. Similarly, an LLM could encode user- related text (user profile information, search queries, social media posts) into user features. Using LLM- derived features can dramatically improve recommendations in cases where pure ID- based approaches struggle, such as cold- start items or new domains 15 14.

A concrete instance is in news recommendation: each news article is essentially a piece of text (headline + body). Models like UNBERT (mentioned earlier) use BERT to encode articles and also to capture fine- grained interactions between words in user- clicked articles and words in candidate articles 13 36. By initializing with a model pretrained on general text, UNBERT gains general language knowledge (e.g. understanding that "election" and "president" are related) which improves matching between users and news beyond what an in- domain model could do 37. Results on the public MIND dataset showed that this approach outperformed earlier methods that learned embeddings from scratch or used simpler content representations 36 38. This principle has been applied in other domains too: e- commerce recommenders encode product descriptions using language models to better recommend new products; book or anime recommendation models encode plot summaries or reviews to find thematically similar items for users. In summary, LLMs used as encoders serve as advanced feature extractors, translating unstructured text into features that a recommender system can incorporate 39. This can be done either via direct use of a frozen pretrained model or via fine- tuning the LLM on the recommendation domain texts.

Advantages: Using LLMs in this way is comparatively low- risk - it doesn't change the core recommendation algorithm, but augments it with richer data. It typically improves content understanding and can make recommendations more explainable (since we can later use the text features to explain why an item was recommended). It is also modular: one can swap in improved language models as they become available.

Challenges: A challenge is efficiency - encoding every item's text via a large model can be computationally expensive (but this can often be done offline and stored). Another consideration is domain adaptation: a general LLM might not capture domain- specific terminology (say, biomedical

articles or video game jargon) perfectly, so some additional fine- tuning of domain- specific pretraining may be needed to get optimal representations  $15 \quad 14$ . Nonetheless, the use of LLMs as feature encoders has quickly become a standard practice in modern recommenders, forming a foundation for more ambitious integrations described next.

### 3.2 Generative Recommendation and Prompting Paradigm

One of the most intriguing paradigms is to use an LLM itself as the recommender - that is, to generate recommendations in natural language by prompting the model, possibly with some conditioning on user data. This represents a shift from the traditional recommender pipeline of "retrieve candidates, then rank them" to a more holistic generation of recommendations. The idea is that an LLM, when given a suitable textual prompt describing the recommendation context, can directly output a list of item names or a descriptive recommendation. For example, we might prompt a model like: "User profile: likes science fiction movies with time travel. Recommend 5 movies they might enjoy." and have the model produce a list of movie titles with explanations. This approach leverages the LLM's internal knowledge and reasoning. Crucially, it can potentially consider the entire item space (the "full spectrum of options") in one step, rather than relying on the multi- stage filtering of traditional systems  $40$ . If successful, this could streamline recommendation generation and even improve relevance by removing the hard boundary between retrieval and ranking  $40$ .

![](images/1b3c87445b1ec86e4d491816c5941cb879d18a82dafdb502d72b8e8aa9215ed1.jpg)  
Figure 1: Comparison of a traditional multi-stage recommendation pipeline (left) versus a generative LLM-based approach (right). In traditional recommendation, a large candidate set is narrowed down through stages (recall, pre-rank, rank, etc.) based on ID embeddings or simple filters. In a generative LLM recommender, the model can take into account an entire item corpus (through tokenized representations or an index) and directly produce recommended item identifiers or names in one step, guided by learned knowledge and prompting.

The generative paradigm became especially prominent with the advent of models like GPT- 3 and ChatGPT that can produce fluent lists and justifications. Researchers have explored how to structure the prompts to coax high- quality recommendations. One approach is to supply the LLM with a list of candidate items (perhaps retrieved by a simpler model) and ask it to rank or pick the best ones. Another approach is truly open- ended generation, where the LLM must recall or infer items from its training knowledge. The latter is risky because the model might hallucinate items that sound plausible but don't actually exist or are not in the target database. To mitigate this, current research suggests constraining the output space. For example, one can prompt the model to output item IDs (numerical or alphanumeric identifiers) instead of free- form names, and impose a decoding constraint that the ID must belong to the known catalog  $41 \quad 42$ . As an illustration, if each item ID is treated as a special token or a sequence of tokens, one can modify the LLM's decoding algorithm to only allow sequences that

correspond to real items - effectively preventing nonsensical IDs from being generated 42. Another strategy is Retrieval- Augmented Generation: before asking the LLM for recommendations, perform a broad retrieval of candidate items (e.g. using an efficient vector search), then provide those as context (in a prompt or through an external tool) so that the LLM chooses among real items.

In terms of output format, an LLM could generate a natural language recommendation ("You might enjoy Movie X because...", ), but for system integration it is often preferable that it outputs a structured result (like a list of item IDs/names). Some research has focused on ensuring the model's output is parseable and tied to the item corpus. For example, the P5 framework we discussed uses constrained decoding for certain prompt templates to output item identifiers in a bracketed format that can be easily parsed and matched to database entries 22 21. This way, the generative model's recommendations can plug back into a production system (to display the items or track clicks).

Results so far indicate that purely prompting a large model without any fine- tuning on recommendation data yields mixed outcomes. Quantitatively, zero- shot LLM recommendations often underperform specialized models on traditional metrics like recall or NDCG 43 44. For instance, evaluations of ChatGPT/GPT- 4 on standard recommendation benchmarks found its recall was noticeably lower than that of models trained on those datasets (as the LLM may not know specific user- item interactions) 43. However, interestingly, when evaluated by human users, LLM- generated recommendations can be quite competitive or even preferred 44 45. This discrepancy is partly attributed to LLMs' ability to produce plausible and well- justified recommendations, enhanced by alignment techniques like Reinforcement Learning from Human Feedback (RLHF) which optimize the model for human satisfaction 44. In other words, an LLM might recommend slightly less personalized items (by strict ranking measures) but present them with convincing reasons, leading users to judge the recommendations favorably. This raises an open question on evaluation: should we value the classical accuracy metrics more, or the holistic user experience? It's possible that RLHF- tuned LLMs (like ChatGPT) are optimizing for the latter.

A notable generative system is the aforementioned P5 (RecSys 2022) 20 21. P5 treated all recommendation tasks as "language processing." During training, it fed the model prompts like "User: [userID] liked [ItemA] ... What is a good next item?" or "User: ... Rate [ItemX]" along with the ground truth answer or rating as text. The T5- based model thereby learned to output appropriate item names or rating values in text form. The same model could also be prompted for an explanation (e.g. "Why would User U like Item I?") and it would generate a plausible explanation sentence. P5 was essentially multitask learning via prompting. It achieved strong results across tasks like rating prediction, sequential item prediction, and explanation generation in a single unified model 22 21. This demonstrated the feasibility of generative recommendation: rather than predicting a score for each item with a fixed architecture, the model generates an answer in the form we want, using natural language as the interface. The benefit is flexibility - we can craft new prompts for new tasks and the model can handle them (to some extent) without needing new output heads or architecture changes. It also easily incorporates multi- modal data by describing it in text (for example, including a mention of a movie genre or a tag as part of the prompt).

In summary, the generative paradigm treats recommendation as an NLP problem and capitalizes on the generalization of LLMs. It is a radical departure from the rigid pipelines of the past. However, it brings challenges: controlling output validity, ensuring the model "knows" the item catalog (often requiring feeding in item names or IDs), and scaling to very large catalogs (which might involve very long prompts or external memory). Ongoing research is actively addressing these, which leads us to hybrid methods.

### 3.3 Hybrid and Integrated Approaches

Hybrid and Integrated ApproachesHybrid approaches seek to combine the strengths of LLMs with the proven techniques of traditional recommenders, aiming for the best of both worlds. Two key motivations for hybrid designs are: (1) Collaborative Signal Integration: Traditional CF excels at capturing patterns in user behavior (the "wisdom of the crowd"), whereas an LLM on its own might not know a niche item is popular among similar users. (2) Efficiency and Constraints: Traditional systems are optimized for large- scale filtering (millions of items), while LLMs have token and memory limitations. So hybrids use classical methods to pre- filter or post- filter items around an LLM.

One line of work introduces LLMs as a component in the multi- stage pipeline. For example, using an LLM- based re- ranker: after generating a candidate list with a fast retrieval model, an LLM could re- rank those candidates by analyzing richer contextual signals (descriptions, current session context expressed in text, etc.). In this role, the LLM acts as a sophisticated scoring function 46. Alternatively, an LLM could serve as a conversational agent on top of a standard recommender - effectively handling the dialogue with the user, but calling a database or collaborative model to fetch candidate items. This is the architecture behind some production systems where ChatGPT is connected to a live database: the LLM handles natural language understanding and generation, while ensuring factuality by querying an external recommender.

A prominent example of hybrid architecture is RecMind 27 28. RecMind is described as an LLM- powered autonomous recommender agent. It doesn't rely solely on the internal knowledge of an LLM. Instead, it equips the LLM with tools: it can issue a search query to an external knowledge base or database of items, it can obtain additional information, and then formulate a recommendation. RecMind introduces a planning algorithm called "Self- Inspiration" where at each step of a multi- turn recommendation session, the LLM reflects on all previously taken actions and user feedback to plan the next query or suggestion 47 48. This reduces the chance the model forgets earlier interactions (a common problem in dialogue). In experiments, RecMind was able to handle diverse recommendation tasks (movie recommendation, music, etc.) in a zero- shot manner, outperforming baseline LLMs that didn't use tool augmentation and even matching a fully trained traditional model on some metrics 49. Essentially, this approach uses the LLM's reasoning ability plus the reliability of an external database. It addresses the knowledge cutoff issue: a standalone LLM might not know about items introduced after its training data, but an augmented LLM can query up- to- date data.

Another hybrid approach focuses on injecting collaborative filtering information directly into the LLM's modeling. The earlier- mentioned CLLM4Rec (Collaborative LLM for Rec) is one such method 32 50. It extends a pretrained LLM by adding special tokens representing user IDs and item IDs to the model's vocabulary. The LLM is then further pre- trained on large corpora of recommendation data (e.g. interaction sequences formatted as sentences of tokens, mixing words and ID tokens) 50 51. By doing so, the LLM learns to treat user and item IDs as part of the "language" it understands. The model can then predict the next item token in a sequence, effectively performing recommendation. CLLM4Rec also introduces training tricks to address the semantic gap between language modeling and recommendation: a soft+hard prompting mechanism to combine user/item tokens with textual context, and a mutual regularization loss to keep the model from overfitting spurious correlations in the data 51 52. Finally, at fine- tuning time it adds a simple prediction head to directly output the next item ID (or a set of item IDs) given a user's interaction history 33. An advantage of this integrated approach is that the model can output recommendations efficiently without hallucination, because it has effectively been trained to generate only valid item tokens as outputs 33. Initial results show that CLLM4Rec can outperform both pure ID- based models and pure language- based models on benchmark datasets, validating the idea that coupling a collaborative filtering backbone with an LLM's knowledge is beneficial.

Similarly, to integrate collaborative signals without altering the LLM's architecture, prompting strategies have been developed. In the AdaptRec framework 34 35, the LLM is prompted with example interaction sequences from not just the target user but also from other similar users. The innovation is that the LLM is involved in selecting those similar users. AdaptRec does a two- stage prompting: first, it gives the LLM a set of candidate user histories (found via conventional similarity search on embeddings) and asks the LLM to rank which ones are most relevant to the target user 33 35. The LLM can leverage its understanding of item semantics and sequence patterns to decide which user's behavior is a good analogy. Then, in the second stage, the LLM is given the target user's history plus a few top similar- user histories (as demonstrations) and prompted to recommend the next item 54 55. This "LLM- in- the- loop" approach to collaborative filtering showed improved accuracy over static prompt baselines that fed in neighbors chosen by simple similarity measures 56 34. Essentially, AdaptRec makes the LLM a participant in collaborative filtering, not just a passive consumer of its results. The authors reported 7- 18% improvement in hit rate over prior LLM- based recommenders in few- shot scenarios by doing this 57. This addresses a core hurdle: LLMs don't inherently understand user IDs or collaborative latent factors, but through clever prompting, we can translate those signals into a form the LLM can reason about (e.g. showing actual sequences of item names).

To sum up, hybrid approaches acknowledge that while LLMs are powerful, they thrive even more with support from recommendation- specific techniques. By combining data from user interaction graphs, using external tools/knowledge bases, or blending ID embeddings, these methods aim to create a next- generation recommender that is both knowledgeable (leveraging global content and context via the LLM) and collaborative (leveraging community preferences via traditional methods). This reflects a broader trend: rather than viewing LLMs as a wholesale replacement for collaborative filtering, researchers increasingly see them as a component in neurosymbolic systems or AI agents that can interact with other systems.

## 4 Applications and Related Subfields

The integration of large language models into recommender systems has opened up new applications and blurred boundaries between related fields. We highlight a few key areas where LLM- based recommendation is making an impact:

### 4.1 Conversational and Interactive Recommendation

One of the most visible applications is conversational recommender systems (CRs). Traditional CRSs involved pipeline architectures with separate natural language understanding modules, dialogue managers, and recommendation algorithms. Now, an LLM can serve as a unified conversational agent that interprets user queries, maintains context, and generates recommendations in dialogue form. For example, ChatGPT (an LLM) can take a conversation like: User: "I enjoyed the movie Inception. Can you suggest similar movies?" and directly respond with recommendations and explanations in a conversational style. Researchers have begun formally evaluating how well ChatGPT performs as a movie recommender. One user study with ~190 participants found that ChatGPT's suggestions were often reasonable, and users appreciated the natural language interaction, but it sometimes missed specific context or had a bias toward very popular items 58. Another study noted that ChatGPT's initial recommendations could be improved by iterative reprompting with user feedback - essentially, if the user says "No, I've seen those" or provides more detail, feeding that back into ChatGPT yields more refined results, and this process can mitigate popularity bias in the recommendations 59. Such capabilities are hard to achieve with static recommendation algorithms but come naturally to an interactive LLM.

The ability to ask clarifying questions is another advantage. An LLM- based CRS can proactively ask the user for preferences (e.g., "What aspect of Inception did you like? The mind- bending plot or the visual effects?") and tailor its next suggestion accordingly. This aligns with the concept of critiquing in conversational recommendation, now handled via free- form dialogue. Early research shows that with minimal fine- tuning, LLMs can adhere to dialogue formats and incorporate recommendation- specific goals (like balancing the dialogue length vs. being concise).

However, conversational use of LLMs also brings challenges: the model might generate overly verbose answers or deviate from the recommendation task into chit- chat. Careful prompt design or fine- tuning (perhaps with reinforcement learning to stay on task) is needed. There's also the issue of truthfulness - - an LLM may fabricate an explanation for a recommendation that sounds plausible but isn't based on real item features. Ensuring that explanations and item details mentioned are correct likely requires integrating a knowledge check (retrieval) step.

Despite these challenges, LLMs are arguably pushing conversational recommenders from research labs into real- world use. Microsoft's Bing Chat and Google's Bard, for instance, both combine search/ recommendation with dialogue. A user might ask Bard for "a good Italian restaurant around here", effectively a recommendation query, and the LLM responds conversationally with a few suggestions and reasons. This hybrid of search, Q&A, and recommendation via conversation is a direct outcome of LLM technology.

### 4.2 Personalized Explanation and Justification

LLMs excel at natural language generation, which makes them ideal for producing explanations for recommendations. In traditional systems, explanations were often templated ("Because you watched X, we recommend "or "Users similar to you liked Z"). With LLMs, we can generate richer, more personalized explanations. For example, given a user's profile and an item, an LLM can be prompted to output a sentence like: "We suggest The Prestige - it's another mind- bending Christopher Nolan film with a complex narrative structure, which matches what you enjoyed in Inception." . This not only tells the user what is recommended, but why it suits them, in a fluent manner.

The P5 model we mentioned had the capability to generate such explanations as one of its multitasks. Likewise, other recent works fine- tune language models on datasets of user reviews or reasonings so that the model can justify a recommendation by highlighting relevant item attributes (learned from reviews) or linking to the user's stated preferences. One interesting direction is using LLMs to summarize a set of user reviews for an item into a concise explanation. For instance, if recommending a restaurant, the model might summarize: "This restaurant is frequently praised for its authentic Neapolitan pizza and cozy atmosphere - exactly what you're looking for." This merges recommendation with summarization.

Explanations help with transparency and user trust, but a caution is needed: an LLM might present an explanation that sounds convincing even if the true reason the system recommended something is different. There's ongoing research into aligning LLM- generated explanations with the actual decision process (so- called "faithful explanations"). Some techniques include constraining the LLM to use only certain data (like the user's known likes or the item's features) when forming the explanation, or even training it jointly with the recommendation task so that its hidden state reflects the recommendation rationale.

### 4.3 Cross-Domain and Cold-Start Recommendation

Cross- Domain and Cold- Start RecommendationLLMs' broad training knowledge allows them to make connections across domains that traditional systems normally treat separately. For example, a language model might know from text that a particular book was adapted into a movie, or that people who enjoy a certain video game often like a related novel series. This opens up the possibility of cross- domain recommendations (e.g., recommend a book based on a movie) which are challenging for standard recommenders due to lack of overlapping user data. Because an LLM has absorbed knowledge from encyclopedias, fan forums, or Wikipedia, it might have the semantic links needed to bridge domains. Researchers are experimenting with prompting LLMs in scenarios like: "This user loved the Harry Potter movies; suggest a video game they might like" - leveraging the model's internal knowledge of fantasy- themed games or narrative styles that align with Harry Potter. Early evidence suggests LLMs can indeed make reasonable cross- domain recommendations by relying on learned associations between entities mentioned in text (movies, games, books, etc.), something a collaborative filter would struggle with without explicit cross- domain data.

Cold- start recommendation (for new users or new items) is another area improved by LLMs. For a new item, since the LLM has probably seen a description or related info during pre- training, it can immediately place that item in a semantic space of related items. It can recommend the new item to users if prompted appropriately ("We have a new movie about XYZ, would this user like it?") by drawing analogies to existing items the user likes, all through natural language reasoning. For a new user, instead of forcing them to rate some items to bootstrap a profile, we could allow them to describe their tastes in words. An LLM can take that description and map it to item recommendations, as was done in the study by Sanner et al. where users' language inputs led to competitive recommendations without any interaction history 26. This is far more user- friendly than cold- start strategies of the past (which often asked users to manually follow topics or rate items). It turns onboarding into a chat: "Tell me about some movies or books you enjoyed recently," and the LLM handles the rest.

### 4.4 Multi-modal Recommendation

Many recommendation scenarios involve multiple modalities - not just text, but images, audio, etc. Large language models are specialized for text, but the recent trend is toward multi- modal foundation models. There are already extensions of LLMs that can accept image inputs (e.g. BLIP, GPT- 4 with vision) or that can generate text based on visual understanding. In a recommendation context, one might want to leverage images (cover art, product photos) or audio (a clip of a song) alongside text.

One approach is to use separate encoders for each modality and then interface them with an LLM. For instance, an image encoder can produce a text description or embedding that is fed into the LLM's context. The RTBHouse tech blog notes that it's feasible to equip LLMs with modality- specific encoders so that all data is translated into a "common token space," allowing the LLM to reason over it 60. For example, an outfit recommendation system might use an image encoder to describe clothing items ("red floral dress") and then the LLM can say "User likes [image_token1] and [image_token2], which are red dresses; recommend other dresses or matching accessories." Preliminary experiments show multi- modal training can indeed improve recommendation performance, as models capture aspects like visual style or audio genre that are not evident from text alone 60. This is still an emerging area - essentially combining computer vision or audio processing with LLM- driven reasoning for recommendation. It aligns with how humans make recommendations (we consider how something looks or sounds, not just its description). In the near future, we may see foundation models that unify language, vision, and perhaps graph data to provide a truly holistic recommendation model.

### 4.5 Pipeline Control and Meta-Learning

Pipeline Control and Meta- LearningBeyond direct recommendations, LLMs can also help in meta- tasks around recommendation. One idea is using an LLM as a pipeline controller  $^{46}$ . In a complex system with multiple algorithms (for different contexts or user segments), an LLM could be prompted with scenario descriptions and decide which algorithm's result to trust more. For instance, "If the user asks for something very specific, use the content- based recommender; if they ask broadly, use the collaborative model." The LLM essentially does strategy selection or parameter tuning on the fly by interpreting the problem. Since LLMs are good at following high- level instructions, one can imagine giving it a prompt like: "You are a recommender system. If the user explicitly names something, do X, otherwise do Y" and let it orchestrate accordingly. This moves towards AutoML or self- configuring recommenders powered by LLM reasoning.

Another meta- application is in A/B testing and optimization: LLMs can quickly simulate user responses to recommended items by generating plausible feedback. While not a substitute for real experiments, this use of generative models can help narrow down candidate algorithms or fine- tune prompts before live testing, saving time.

In summary, LLMs in recommendation are not limited to the core act of choosing items - they are influencing how users interact with recommender systems (through conversation), how results are delivered (with natural language explanations), and even how the systems are designed (with more unified multi- task behavior and dynamic control). This cross- pollination enriches related fields: conversational AI, explainable AI, and multimodal AI all intersect with recommender systems now more than ever, thanks to the common language interface provided by LLMs.

## 5 Open Challenges and Future Directions

While the progress in using LLMs for recommendation is exciting, there remain significant challenges and open research questions. We outline some of the key issues and future directions:

1. Scalability and Efficiency: Real-world recommender systems may need to handle millions of users and items with sub-second latency. Large language models, especially when used naively (e.g. prompting GPT-4 for every user query), can be computationally expensive and slow. One challenge is how to serve LLM-based recommenders efficiently. Potential directions include distilling large models into smaller specialized models for recommendation, or using caching and retrieval to limit the scope of what the LLM has to consider. Hybrid solutions might use LLMs only for certain high-value scenarios (like cold-start or explanation generation) and default to cheaper models for routine recommendations. There is active research on compressing LLMs or quantizing them to run on edge devices, which could in the future allow on-device personalized LLM recommenders (improving privacy and speed).

2. Integration of Collaborative Signals: As discussed, LLMs don't inherently understand the collaborative filtering concept of "people like you also liked X". Feeding them ID-based interaction data is non-trivial. Techniques like CLLM4Rec and AdaptRec are first steps  $^{30}$ ,  $^{35}$ , but a general solution is still open. One future direction is to train foundation models that jointly learn from text and interaction graphs (some early works call these "Graph-augmented Language Models"). Another approach is to use the LLM to generate synthetic data: for instance, have it imagine a set of user profiles and interactions consistent with certain patterns, which can then augment collaborative data for a traditional model. Conversely, using collaborative models to guide LLM generation (constrain with learned user embeddings, etc.) is being explored. Achieving a seamless blend of content understanding and collaborative filtering remains a holy grail.

3. Controlling Hallucination and Ensuring Validity: A notorious issue with LLMs is hallucination - generating outputs that are fictional or not grounded in the data. In recommendation, hallucination could mean recommending an item that doesn't exist or is unavailable, or attributing a wrong reason for a recommendation ("This camera has 50MP" when it does not). This is problematic for user trust. Future systems must incorporate verification steps. For example, after an LLM generates a recommendation or explanation, a secondary process could verify item details against a database (and either correct the LLM or refrain from showing that explanation). Retrieval augmentation is another safeguard: always provide the LLM with factual context (item attributes, real user reviews) so that it doesn't rely on potentially incorrect memory. Fine-tuning the LLM on dialogues or data that emphasize truthfulness can also help, though it's hard to eliminate hallucinations entirely. As a related challenge, consistency is important - if a user asks twice for a recommendation, the system shouldn't give wildly different answers without a change in input (unless randomness is desired). LLMs can be somewhat stochastic, so controlling for consistency (perhaps by anchoring generation with certain fixed retrieved points) is an ongoing research area.

4. Bias, Fairness, and Filter Bubbles: Recommender systems have long struggled with biases (e.g., popularity bias where only the most popular items are recommended, or biases against certain demographic groups' content). LLMs introduce new layers to this. They have their own pre-training biases from language data, and when generating recommendations, they might over-emphasize popular or well-known items (as observed in some zero-shot studies 25). There's a risk that an LLM-based system could reinforce a "rich get richer" cycle if not carefully calibrated. Moreover, fairness concerns arise if the model's training data had underrepresentation of certain groups or cultures, which could reflect in the recommendations (for instance, always recommending mainstream Western movies and not surfacing niche foreign films). Addressing this requires techniques like bias detection in LLM outputs, and possibly fine-tuning or re-ranking steps that ensure a diverse and fair set of recommendations. The tutorial literature suggests investigating trustworthy LLMRec including fairness and transparency is a priority 62. One interesting approach is to explicitly prompt the LLM to be mindful of diversity ("Recommend a mix of popular and lesser-known items") - something not possible to "ask" of a matrix factorization model, but natural when dealing with a language model.

5. Evaluation Metrics and Strategies: With LLM-based systems, standard recommendation metrics might not tell the full story. As noted, an LLM might score lower on NDCG but provide better user experience. Thus, we need evaluation frameworks that capture aspects like user satisfaction, engagement, novelty, explanation helpfulness, conversational fluency, etc. Human studies become more important - e.g., comparing a traditional recommender vs. an LLM-based one through A/B tests or user surveys. There's also the question of multi-turn interaction: evaluating a conversational recommender requires measuring the quality of the dialogue, not just the final recommendation. New metrics or adaptations of dialogue metrics (like coherence, informativeness) could be introduced to the RecSys field. Additionally, evaluating the reasoning of an LLM in making recommendations is interesting - some work uses chain-of-thought prompts where the LLM explains its intermediate reasoning. Ensuring that reasoning is correct (or at least sensible) might be part of future evaluation.

6. Personalization vs. Privacy: LLMs can personalize recommendations if given access to personal user data (history, profile text, etc.), but that raises privacy concerns. Naively prompting a cloud-based LLM with a user's entire history or sensitive data could violate privacy policies. Techniques like federated learning or on-device LLM inference might be needed to keep personal data local. Another angle is to use pseudo personalization - for example, feed only high-level abstractions of a user's preferences to the LLM (like "user likes genre X") rather than raw logs. There's also the risk of LLMs leaking training data: if an LLM was fine-tuned on real user interactions, it might inadvertently regurgitate some of those in its output. Careful data governance and possibly differential privacy mechanisms will be required as we integrate private recommendation datasets with large models.

7. **Continual Learning and Adaptation:** Item catalogs and user interests evolve rapidly (new songs come out, trending topics changed). LLMs as static models could become stale – e.g., a model trained in 2022 might not know a popular 2025 video game. Traditional recommender systems handle this by retraining frequently or updating embeddings online. For LLMs, full retraining is expensive, so alternatives are explored: prompting with up-to-date context (so the model doesn’t need retraining to know new items), fine-tuning small portions of the model (like LoRA adapters) with new data, or using a modular architecture where new knowledge can be plugged in (via a knowledge graph or memory network that the LLM attends to). Ensuring LLM-based recommenders remain current is an important practical concern. Techniques that enable rapid learning of new items (one-shot learning of a new entity) will be valuable – for example, if a new movie is released, quickly teach the model about it by injecting a sentence of description into its context or weights.

8. **Model Ethics and Safety:** As recommenders become more conversational and knowledge-driven, they raise ethical questions. An LLM-based recommender could be asked for sensitive recommendations (health-related, financial advice, etc.). How should it respond? There is a risk of it producing inappropriate or harmful suggestions if not properly constrained. OpenAI and others include content filters in their chat models; similar safety layers will be needed in recommender contexts. Also, the persuasive tone of a well-articulated LLM recommendation might impact user autonomy – if the AI writes a very convincing pitch for a product, is that crossing into unwanted advertising or manipulation? The community will need to establish guidelines for ethical use of LLM in influencing user choices, ensuring transparency (maybe the system should disclose “I am an AI, here’s why I recommended this”).

9. **Combining Strengths of Multiple Models:** Future systems might not rely on a single giant model, but an ensemble or society of models. For instance, a vision-focused model could identify visually appealing items for the user, a graph-based model could ensure novelty by exploring the user-item graph, and an LLM could sit on top as the “brain” that composes the final recommendation reasoning. Developing frameworks for such collaboration (where an LLM can call other specialized models as tools) is an exciting frontier. It ties into the concept of AI agents that can perform complex tasks by breaking them down – we already see LLMs that use tool APIs (like browsing or database query). Recommendation could be approached similarly: the LLM-agent decides when to query a CF model vs. when to rely on its own knowledge.

In conclusion, **Large Language Models for recommendation** present both opportunities and challenges. They push us toward more human- centric recommendation systems – ones that can understand nuanced user inputs and provide richer outputs (recommendations that come with explanations or dialogue). They also encourage a rethinking of architectures: moving from static monolithic pipelines to fluid, knowledge- infused, interactive systems. Many challenges remain to be solved before LLM- based recommenders can fully surpass traditional methods in all aspects (accuracy, efficiency, trustworthiness). However, the progress in just the last couple of years has been remarkable. As LLMs continue to improve (and possibly become more accessible via open- source efforts), we can expect **next- generation recommender systems** that are far more versatile, conversational, and knowledgeable than those of the past. The convergence of NLP and recommender systems is likely to yield not just better recommendations, but also a more natural and engaging user experience – bringing us closer to AI assistants that truly understand our preferences and help us discover new content in an intuitive way. The journey has just begun, and it is a fascinating time for researchers and practitioners at this intersection.

## 6 Conclusion

Large Language Models are catalyzing a paradigm shift in recommender systems research and practice. We surveyed how LLMs, with their powerful language understanding and generation capabilities, can enhance recommendation across numerous dimensions - from encoding content and context, to serving as end- to- end generative recommenders, to facilitating conversational interactions and explanations. We traced the historical evolution from early shallow methods through deep learning approaches to the current state where foundation models play a central role. Recent approaches demonstrate that LLMs can act as zero- shot recommenders, leverage external knowledge sources, integrate collaborative signals, and unify diverse recommendation tasks within one framework.

At the same time, deploying LLMs for recommendation introduces challenges in scalability, output controllability, bias, and evaluation. Addressing these will require interdisciplinary efforts, combining insights from information retrieval, natural language processing, human- computer interaction, and beyond. The open problems discussed - such as balancing LLMs' semantic prowess with traditional collaborative filtering, ensuring factual and fair recommendations, and maintaining efficiency - define a rich agenda for future research.

In closing, LLM- based recommendation stands at the confluence of personalization and understanding. It aims to retain the rigorous preference learning of classic recommender systems while infusing it with the flexibility and knowledge of language models. This fusion has already enabled systems that can talk to users, reason about their needs, and justify their suggestions in ways not previously possible. As the technology matures, we anticipate that many online platforms will evolve to have recommendation engines that feel less like algorithmic black boxes and more like knowledgeable assistants or companions helping users navigate choices. The survey presented here provides a foundation for understanding this fast- emerging area. By building on the strengths of both recommender systems and large language models - and by squarely facing the challenges - the community can work towards recommender systems that are not only more accurate, but also more transparent, interactive, and aligned with human values.

Acknowledgments: This document was synthesized based on a wide range of sources in the public literature and does not directly reuse text from any single prior survey. We thank the authors of the cited works for laying the groundwork in their respective areas. All errors or interpretations remain our own.

## References

References: (Included in- line as per the citation format)

1 6 7 18 Recommender Systems | SpringerLink https://link.springer.com/rwe/10.1007/978- 0- 387- 30164- 8_705?error=cookies_not_supported&code=673e88b6- b8b9- 41a8- 8ded- 2de1df3b5cca

2 4 5 19 Large language model - Wikipedia https://en.wikipedia.org/wiki/Large_language_model

3 16 17 Foundation Models | Business & Information Systems Engineering https://link.springer.com/article/10.1007/s12599- 024- 00851- 0

8 9 29 62 RecSys 2023 Tutorial: Large Language Models for Recommendation https://timrecsys.github.io/

10 [1808.09781] Self- Attentive Sequential Recommendation https://arxiv.org/abs/1808.09781

11 [1904.06690] BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer https://arxiv.org/abs/1904.06690

12 13 14 15 36 37 38 UNBERT: User- News Matching BERT for News Recommendation https://www.ijcai.org/proceedings/2021/0462. pdf

20 21 22 23 [2203.13366] Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5) https://arxiv.org/abs/2203.13366

24 25 [2305.08845] Large Language Models are Zero- Shot Rankers for Recommender Systems https://arxiv.org/abs/2305.08845

26 [2307.14225] Large Language Models are Competitive Near Cold- start Recommenders for Language- and Item- based Preferences https://arxiv.org/abs/2307.14225

27 28 47 48 49 [2308.14296] RecMind: Large Language Model Powered Agent For Recommendation https://arxiv.org/abs/2308.14296

30 31 32 33 50 51 52 Collaborative Large Language Model for Recommender Systems https://arxiv.org/html/2311.01343v4

34 35 53 54 55 56 57 Bringing Collaborative Filtering to LLMs with Adaptec | Shaped Blog https://www.shaped.ai/blog/bringing- collaborative- filtering- to- llms- with- adaptec

39 40 41 42 43 44 45 46 60 61 Large language models in recommendation systems - Techblog https://techblog.rtthouse.com/large- language- models- in- recommendation- systems/

58 ChatGPT as a Conversational Recommender System

https://dl.acm.org/doi:10.1145/3627043.3659574

59 Refining Recommendations by Reprompting with Feedback - arXiv https://arxiv.org/html/2401.03605v1