# Agentic Reinforcement Learning

## 1 Introduction

Agentic Reinforcement LearningAgentic reinforcement learning (RL) refers to treating a (typically large) language model or AI as an autonomous agent that interacts over multiple turns with an environment, using RL to optimize behavior across extended episodes. Unlike conventional LLM training (e.g. standard fine- tuning or RL from human feedback) which typically treats each prompt- response as a single- step decision, agentic RL models tasks as extended partially- observable Markov decision processes (POMDPs). In this setting the agent selects actions (which may include text outputs, API calls, or tool invocations), receives observations (text, images, environment states) and scalar rewards, and updates its policy to maximize cumulative return  $1 \times 2$ . In other words, the agent's policy  $\pi$  maps a history or belief- state to actions, aiming to maximize  $\mathbb{S}\mathbb{E}[\backslash \mathsf{sum}_{- }\{\mathsf{t}\} \backslash \mathsf{gamma}^{\wedge}\mathsf{t}\backslash \mathsf{r}_{- }\mathsf{t}]\mathbb{S}$ . Formally, an MDP is defined by states  $\mathbb{S}\mathbb{S}$ , actions  $\mathbb{S}\mathbb{A}$ , transition probabilities  $\mathbb{S}\mathbb{T}\mathbb{S}^{\dagger}\backslash \mathsf{mid}\mathbb{S},\mathsf{a})\mathbb{S}$ , and rewards  $\mathbb{S}\mathbb{R}(\mathbb{s},\mathbb{a})\mathbb{S}^{3}$ . A POMDP extends this with observations  $\mathbb{S}\backslash \mathsf{Omega}\mathbb{S}$  and an observation function  $\mathbb{S}\mathbb{O}(\mathbb{o}\backslash \mathsf{mid}\mathbb{s},\mathbb{a})\mathbb{S}$ , so that the agent must infer hidden state from sensory inputs  $2$ . Agentic RL thus shifts from a "degenerate" one- step MDP (each prompt as a stateless interaction) to a full episodic POMDP where chain- of- thought and intermediate steps are part of the decision process  $1$ .

Key Concepts: In agentic RL, each episode consists of a sequence of states, actions, and rewards. The agent's objective  $\mathbb{S}\mathbb{J}(\theta) = \backslash \mathsf{mathbb{H}b}\mathbb{b}\{\mathbb{E}\} \_ \{\mathsf{tau}\backslash \mathsf{sim}\pi_{- }\theta \} [\mathbb{R}(\mathsf{t})]\mathbb{S}$  (e.g. expected sum of rewards) is optimized via policy- gradient or value- based methods. Modern approaches often use on- policy algorithms like PPO and its variants (for example, Group Relative Proximal Policy Optimization or GRPO)  $4 \times 5$ , which estimate baselines from groups of trajectories to improve stability. Offline techniques like Direct Preference Optimization (DPO) may also be used. Crucially, agentic RL systems integrate external tools and environments into the action space. For instance, an agent action might be "execute code snippet", "issue web search query", or "draw on canvas"; the environment then returns the result (output, memory state, images, etc.) as new observations  $6 \times 7$ . Reward design is more complex than standard RL: in addition to final- task rewards, many methods use process- level rewards (for intermediate correctness or efficiency) to alleviate sparsity  $8 \times 9$ .

Agentic RL thus combines language planning, reasoning, tool use, and memory in a learning framework. Core capabilities include:

- Planning and Multistep Reasoning: The agent decomposes complex tasks into sequences of subgoals and actions. This often involves explicit chain-of-thought or hierarchical planning, enabling long-horizon reasoning across an episode  $10 \times 1$ .- Tool Use and Interaction: The agent learns when and how to invoke external tools (code interpreters, APIs, search engines, vision models, etc.) as part of its policy. Unlike fixed prompt templates, policies are learned to decide which tool to call and when, based on environmental feedback  $6 \times 11$ .- Memory and Context Persistence: Agentic policies can store and recall information over time, effectively giving the model working memory or an episodic memory for past observations. This allows multi-turn tasks with context retention (e.g. keeping track of a conversation or cumulative knowledge across steps).- Meta-Reasoning and Self-Improvement: Agents can reflect on their performance, adapt on-policy, or incorporate feedback loops for self-correction. For example, policies may include self-verification steps or online fine-tuning as sub-actions  $11 \times 12$ .

Multi- Modal Perception: Agentic RL can handle multi- modal inputs and outputs. The agent's environment might include visual scenes, structured databases (SQL), or web interfaces. Perception models or image tools feed back into the agent's state 13.

Applications of agentic RL span many domains: from mathematical and scientific reasoning to autonomous software engineering and interactive data processing. For example, VerlTool trains a unified agent across six diverse tasks - mathematical problem- solving, knowledge question answering, SQL generation, visual reasoning, web search, and software engineering - using a shared RL framework 13. The ARTIST system applies agentic RL to multi- turn function- calling benchmarks (Q&A tasks that require correct use of functions or tools), and demonstrates large accuracy gains on math and planning problems 14. In one study, a TB- parameter LLM agent trained with RL on just nine machine- learning tasks outperformed a 671B baseline agent on autonomous ML engineering 15 16. Similarly, in retrieval- augmented generation (RAG) tasks, process- level RL (e.g. the ReasonRAG framework) significantly beats vanilla retrieval or single- turn methods by optimizing the search- and- answer sequence 8 17. These examples illustrate that agentic RL can unlock capabilities beyond static prompting, enabling LLMs to operate as persistent, adaptive agents solving complex, open- ended tasks 18 10.

## 2 Historical Timeline

Historical Timeline- 1980s- 2010s: Foundations of RL laid (Bellman, Watkins, etc.); classical RL (Q- learning, policy gradient) is applied to sequential decision tasks 3 5. Deep reinforcement learning (DQN, AlphaGo) scales RL to high- dimensional inputs 5. - 2020: Large- scale language models (BERT, GPT) achieve strong performance on NLP tasks, and early work integrates RL in NLP (e.g. dialogue systems, knowledge retrieval). However, most LLM training remains single- turn (with RL mainly for alignment/RLHF). - 2021- 2022: Chain- of- thought prompting (Wei et al., 2022) shows that encouraging LLMs to generate intermediate reasoning improves accuracy on math and logic tasks. This hints at the power of multi- step reasoning in language models. - 2023: The paradigm of LLM agents emerges. The ReAct framework (Yao et al., 2023) demonstrates that LLMs can interleave Reasoning and Action steps (e.g. thinking vs. calling a tool) to improve performance on interactive tasks 19. Simultaneously, works like Toolformer (Schick et al., 2023) use supervised fine- tuning to teach LLMs to insert API calls, showing that LLMs can learn to use external tools on the fly 20. These advances highlight the value of autonomous multi- step problem- solving beyond one- shot responses. - Early 2024: Commercial LLMs gain new capabilities for "function calling" (e.g. GPT- 4' s API), making it easier to chain tool executions. Research on RL in LLM agents accelerates. For instance, DeepSeek- R1 (Guo et al., 2025) applies RL to enable an LLM to autonomously solve math problems with near- human performance. New methods like CodeAct and self- refinement train LLMs as interactive solvers. - 2025: The term Agentic RL is formalized and surveyed in the literature 21. Several unified frameworks appear: VerlTool (Jiang et al.) provides infrastructure for multi- domain RL with tools 22; ARTIST (Wu et al.) learns LLM policies for function- calling tasks 14; ReasonRAG (Zhang et al.) and Deep- DxSearch train retrieval- and- reasoning pipelines with process rewards 8. Autonomous agents are developed for complex domains: e.g. ML- Agent (Liu et al., 2025) trains an LLM to design and run ML experiments, outperforming much larger models 15. The field's vocabulary now includes "ARLT" (Agentic RL with Tool use) and "RLVR" (Reinforcement Learning with Verifiable Rewards) as subcategories 22.

This timeline shows how agentic RL builds on decades of RL and recent LLM innovations. The core shift is treating LLMs as interactive agents: moving from static input- output mappings to learning policies over multi- step, partially- observed tasks 1 19.

## 3 Foundations of Agentic RL

Agentic RL draws on both traditional RL theory and modern language- model techniques:

Markov Decision Processes (MDPs): The backbone of RL is the MDP framework 3 . An agent observes a state  $\) 5\(in$ 55 $and chooses action$ \S a\backslash $in$ \mathbb{A}\mathbb{S} $according to a policy$ \S 1 $pi(a\mid\operatorname{mid}s)$  . The environment transitions to a new state  $\) 5^{\prime}s\(with probability$ \mathbb{S}\top(\mathbb{S}^{\prime}!\backslash\operatorname{mid}\mathbb{s},\mathbb{a})\mathbb{S} $, and the agent receives reward$ \S r=R(s,a)\S $. The goal is to find a policy maximizing expected return$ \S\mathsf{E}[ $sum_t$ \backslash $\)\mathtt{gamma}^{\mathtt{A}}\mathtt{t}\(r_t)$ . In fully observable settings (MDP), policies map states to actions; in partially observable settings (POMDP), the agent must infer state from observations 2.

Policy Optimization: Learning typically uses policy- gradient or actor- critic methods (e.g. REINFORCE, PPO 4). For LLM agents, large action spaces and sparse rewards motivate recent innovations. One example is Group Relative Policy Optimization (GRPO), which estimates baselines from groups of sampled responses to avoid using a value network 4 23 . These methods allow training LLM policies with only reward signals (no intermediate supervision).

Trajectory- Centric RL: Classical supervised learning on LLMs treats each output independently. In agentic RL, the entire dialogue or reasoning trace is treated as one trajectory  $\) 1\(tau$ , with final (or intermediate) rewards. Optimization is done over sequences, e.g. maximizing  $\) 2\(- [t - \pi)[R(t)]$ \S 24 $. This requires rolling out the model in the environment, collecting$ \mathbb{S}(s,a,\tau)\mathbb{S}$ steps until termination. Such episodic training adapts any existing RL algorithm to language settings.

Chain- of- Thought and Self- Supervised Hints: Techniques like chain- of- thought prompting remain useful: they provide LLMs with an inductive bias for multi- step reasoning, which RL can then refine. In practice, many systems combine initial CoT prompts with RL fine- tuning. But RL alone can discover reasoning strategies; e.g., training rewards can encourage longer reasoning chains when beneficial 25 .

Reward Engineering: Designing rewards is critical. Agentic RL often uses verifiable or outcome- based rewards (did the final answer or action achieve the goal?) as well as process- level rewards (penalize unnecessary steps, encourage helpful intermediate actions). For instance, in retrieval tasks, the ReasonRAG framework uses Monte Carlo tree search to assign rewards to substeps, encouraging shorter correct reasoning paths 8 . In math or code domains, rewards may be correctness of solution or pass rates. Techniques like Direct Preference Optimization (DPO) can also translate human preferences into scalar rewards for the agent.

Exploration Strategies: Agentic tasks often have combinatorially large action spaces (e.g. what tool to use next). To improve exploration, some systems trigger additional rollouts when uncertainty spikes (e.g. entropy- adaptive rollout branching 26 ), or use Monte Carlo Tree Search (MCTS) to explore decision trees 8 .

Overall, the foundational view is: an agentic RL problem  $=$  (P)OMDP  $^+$  LLM policy  $^+$  external tools, where standard RL machinery (policy gradients, replay buffers, etc.) is adapted to handle language actions, episodic memory, and tool interactions 1 6 .

## 4 System Architectures

System ArchitecturesModern agentic RL systems use modular, decoupled architectures to handle the high complexity of agents acting in diverse environments. A typical design introduces an intermediate "agent layer" between the LLM trainer (the "brain") and the execution sandboxes (the "body")  $^{27}$ . Figure 1 illustrates this concept:

Figure 1: Modular architecture of an agentic RL system. The RL framework (left) contains the LLM policy and trainer, while diverse execution environments (right) host the agent's "body" and tools. An intermediate agent layer (center) schedules rollout tasks, dispatches them to environments, and collects the resulting state- action trajectories for training  $^{27}$ .

Modern agentic RL deployments often decouple the language model training from the agent execution. The agent layer acts as a scheduler: it dispatches parallel rollout tasks to separate worker environments (each with necessary tools, compute resources, or sandboxes) and collects back the agent's observations, actions, and rewards. Meanwhile, the LLM trainer consumes these trajectories to update the policy. This design has several advantages  $^{27}$ : the LLM inference pipeline can be optimized for model throughput, while the environment simulators (code execution, browsers, robots) run independently and potentially distributed. It also means that different agent implementations (with different prompts or tools) can be plugged into the same training framework by simply tracing their inputs/outputs. By separating concerns in this way, large- scale agentic RL systems can scale to many parallel agents and manage the heavy resource demands of multi- tool tasks  $^{27}$ .

## 5 Methodological Advances

Recent research has introduced a range of methods tailored to agentic RL:

Tool Integration as First- Class Actions: Agentic RL frameworks treat tool calls (APIs, function calls, etc.) as discrete actions in the policy space  $^{6}$ $^{28}$ . During training, the LLM outputs a token signaling a tool invocation; the environment executes the tool and returns the result as the next observation. Crucially, only the language- generated decision is backpropagated (the tool execution is not differentiable), but its output influences future states. This interleaving allows agents to learn when to use which tool as part of the policy rather than relying on fixed prompt templates  $^{28}$ $^{6}$ .

Policy Optimization Innovations: To handle large action spaces and language- based policies, new algorithms are used. For example, Group Relative Policy Optimization (GRPO) extends PPO by estimating value baselines across multiple sampled responses (a "group"), removing the need for a learned critic network  $^{4}$ $^{23}$ . GRPO has been shown to yield more stable and self- corrective reasoning in LLMs  $^{28}$ . Other works explore off- policy or multi- agent techniques to improve sample efficiency. In ARTIST, for instance, the authors adapt GRPO for multi- turn tool use, alternating reasoning and tool actions in rollouts  $^{29}$ $^{23}$ .

Reward Structuring: Given the long horizons, many approaches break rewards into subcomponents. For example, in function- calling tasks, agents might get one reward for each correct tool usage and a final reward for task completion. In ReasonRAG, a shortest- path reward estimation (SPRE) is introduced: it simulates completions of each partial reasoning trajectory and assigns higher reward to shorter successful paths  $^{8}$ . This encourages efficiency in multi- step retrieval tasks. Such dense intermediate rewards make training much more sample- efficient than relying on a single binary reward at the end.

Training Pipelines: Commonly, training alternates between rollouts (collecting trajectories) and policy updates. Many studies use mini- batch rollouts of fixed length to accommodate long contexts. Some add randomized initializations or temperature sampling to encourage exploration. Others use fine- tuning + RL hybrid schemes: e.g. ML- Agent first fine- tunes the LLM on an "exploration- enriched" dataset to diversify actions, then switches to RL for policy refinement 12.

Memory Augmentation: To handle long- range dependencies, some works integrate external memory modules or retrieval buffers into the agent. For example, an agent may store past observations or reasoning steps in an external datastore and query them as needed. While details vary, the common theme is that agentic models can learn to read/write memory as part of the action space, extending their effective context beyond the LLM's static window.

Benchmarking and Evaluation: Researchers have created new evaluation suites for agentic behavior. For instance, WebArena is a browser- based environment testing multi- step search and reasoning. GAIA is a benchmark for multi- tool tasks with long horizons. These benchmarks stress- test agentic capabilities like planning and tool use. When possible, performance is measured by final task success rates (e.g. correct answer or completed goal); auxiliary metrics include efficiency (steps to solution) or number of correct tool calls (see ARTIST results 30 31). Such metrics help quantify how much agentic RL improves over static prompting baselines.

## 6 Applications

Agentic RL has been applied across many domains:

Complex Reasoning & QA: Multi- step math and logic problems benefit from agentic RL. The ARTIST system trains an LLM to solve math puzzles by decomposing them into steps and calling computational tools. It achieved large accuracy gains on complex benchmarks (e.g. improvements of ~9- 14 percentage points on hardest subsets) compared to a prompt- only baseline 14 10. Similarly, in open- domain question- answering with tools (e.g. web search or database queries), agentic policies manage the retrieval sequence. For example, ReasonRAG trains an LLM to plan multiple search queries and evidence synthesis, outperforming standard RAG and search- agents 8 17.

Code and Software Engineering: Agentic RL can guide code synthesis and debugging. VerlTool and related frameworks have tasks like SQL generation or code editing, where the agent must iteratively refine code with execution feedback. In system optimization, OS- R1 uses RL to tune operating system kernels by framing system configuration as an MDP 32. In all these, the agent issues "run code" or "modify code" actions and learns from test results. ML- Agent goes further: it treats designing and running machine learning experiments as a task. A 7B- agent trained on only 9 ML tasks was able to sequentially decide actions (data processing, model training, evaluation) to outperform a much larger (671B) baseline 15.

Retrieval and Knowledge Work: Document understanding and data extraction can be framed as agent tasks. For instance, an agentic RL system may navigate a document in steps, extracting tables or facts, with self- correction along the way. Multi- agent setups have been proposed where several subagents divide a document into pieces 33. In a retrieval- augmented setting, agentic RL coordinates search queries and answer generation; Deep- DxSearch shows such agents consistently beat simpler or one- shot retrieval systems 34.

- Vision and Robotics: Agentic RL extends to vision-language tasks. For example, a Vision-Language Model agent could issue "pan left", "zoom in" actions in an image-based environment, or select regions of an image to analyze. New works (e.g. Pixel-Reasoner) explore "pixel-space reasoning" where the LLM agent outputs actions that manipulate images, guided by RL rewards  $^{6}$ . In robotics, an LLM agent might output high-level commands (e.g. "pick up the red block"), with the environment returning sensor feedback. While still nascent, these applications illustrate the potential for agentic RL to handle multi-modal control.

- Gaming and Multi-Agent: Some research frames LLM collaboration as multi-agent RL. For example, a team of LLM-based agents may negotiate or coordinate in a simulated economy or game. Decentralized training and self-play enable agents to learn communication and coordination. Methods like Chain-of-Agents train multiple models jointly via distillation to capture multi-agent dynamics  $^{35}$ . Though distinct from single-agent RL, these efforts share the agentic philosophy of combining language reasoning with decision-making.

Across these applications, a common theme is that agentic RL consistently outperforms static prompt- based baselines on complex, multi- step tasks  $^{14}$ . By learning when to think, when to act, and how to integrate external tools, RL- trained agents achieve both higher success rates and more efficient solutions (fewer steps per task) than pure prompting or supervised methods  $^{36}$ .

## 7 Open Challenges

Despite rapid progress, many challenges remain:

- Long-Horizon Credit Assignment: In lengthy tasks, rewards are sparse and delayed, making it hard to assign credit to individual actions. Without dense intermediate supervision, agents may drift or learn suboptimal shortcuts. Approaches like process-level rewards and hierarchical policies help, but more research is needed on stabilizing learning for very long episodes  $^{8}$ .

- Data Efficiency and Stability: Training LLM agents can be extremely sample-inefficient. Collecting trajectories (especially when each step may involve running code or searching the web) is expensive. Distributed rollout systems (e.g. AWorld infrastructure) can generate thousands of episodes in parallel  $^{38}$ , but optimizing efficiency remains critical. Methods that reuse data (off-policy updates) or leverage imitation learning could help.

- Generalization: Agents often overfit to the specifics of their training tasks and prompt formats. A policy trained in one environment may break in another if the action space or feedback format changes. This demands research into robust representations and transfer learning for agentic policies. Some success is seen: ML-Agent generalized to new ML tasks not seen in training  $^{15}$ , but broader generalization (e.g. to new tools or modalities) is still an open problem.

- Safety and Alignment: Powerful autonomous agents raise safety concerns. An agentic RL system, if not properly constrained, might exploit loopholes in reward functions or pursue unintended behaviors when operating in the real world. For example, a web-browsing agent might repeatedly query a search API until rewards saturate, or a code-writing agent might produce harmful code if not carefully supervised. Ensuring alignment with human values requires formal oversight: incorporating constraints, human-in-the-loop checks, or formal verification for critical steps. As Schneider (2025) notes, agentic AI "cautions against risks that can emerge when exceeding human intelligence"  $^{39}$ . Early work on safe RL and constrained RL provides a starting point, but new frameworks are needed for the language-driven, open-ended actions of agents.

Evaluation Metrics: Measuring agent performance is harder than static accuracy. Interactive tasks are stochastic and open- ended. Standard metrics (final task success, accuracy) are necessary but not sufficient: one must also consider efficiency (steps taken), robustness (performance under perturbations), and the faithfulness of intermediate reasoning. Research is needed on benchmarks and evaluation protocols for agentic behavior, analogous to how SuperGLUE measures static language understanding.

Interpretability and Debugging: Agentic policies are complex and may fail in opaque ways. While having intermediate steps ("chain- of- thought") aids interpretability, verifying the correctness of those steps is nontrivial. Debugging a learned agent's policy or identifying why it makes a bad decision remains a challenge. Tools for visualizing policy decisions, summarizing decision trees, or logging tool interactions will be important for trustworthy deployment.

In summary, agentic RL opens vast possibilities for autonomous AI, but also introduces new difficulties in learning, evaluation, and safety. Progress requires integrating insights from RL, language understanding, systems engineering, and ethics 39 37.

References: This survey is based on recent foundational sources and research literature. Key references include formal definitions of MDPs/POMDPs 2 3; pioneering RL algorithms and LLM agent frameworks 5 19 6 4; and cutting- edge agentic RL studies (e.g. ARTIST 14, VerlTool 13, ML- Agent 12, ReasonRAG 8, among others) that illustrate the concepts and applications discussed. These sources collectively document the emergence of Agentic RL as a new paradigm at the intersection of reinforcement learning and large language models.

## References

1 18 21 [2509.02547] The Landscape of Agentic Reinforcement Learning for LLMs: A Survey https://arxiv.org/abs/2509.02547

2 3 7 Partially observable Markov decision process - Wikipedia https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process

4 10 14 23 25 29 30 31 36 Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning https://arxiv.org/html/2505.01441v1

5 19 From Correction to Mastery: Reinforced Distillation of Large Language Model Agents https://www.arxiv.org/pdf/2509.14257

6 20 VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use https://arxiv.org/html/2509.01055v1

8 Process vs. Outcome Reward: Which is Better for Agentic RAG Reinforcement Learning https://arxiv.org/html/2505.14069v1

9 11 16 17 24 26 28 32 33 34 35 37 38 Agentic RL: Autonomous Reinforcement Learning https://www.emergentmind.com/topics/agentic- reinforcement- learning- agentic- rl

12 15 [2505.23723] ML- Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering https://arxiv.org/abs/2505.23723

13 22 [2509.01055] VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use https://arxiv.org/abs/2509.01055

When LLMs Grow Hands and Feet, How to Design our Agentic RL Systems? https://amberljc.github.io/blog/2025- 09- 05- agentic- rl- systems.html?utm_source=hnblogs.substack.com [2504.18875] Generative to Agentic AI: Survey, Conceptualization, and Challenges https://arxiv.org/abs/2504.18875