# LLM-Based Multi-Agent Systems: A Comprehensive Survey

## 1 Introduction

Large Language Models (LLMs) have revolutionized AI with their human- level language understanding and reasoning abilities. In parallel, multi- agent systems (MAS)—where multiple autonomous agents interact or collaborate—have long been studied for solving complex tasks beyond the capabilities of any single agent. LLM- based multi- agent systems merge these strands by using LLMs as the "brains" of agents, enabling rich natural language communication and sophisticated reasoning within agent collectives. This survey provides a broad overview of this emerging field, covering fundamental concepts, historical milestones, core architectures, state- of- the- art methods, applications, and open challenges. We aim for a balance of technical depth and clarity, making the survey accessible to newcomers while informative for experts.

## 2 Background and Key Concepts

Large Language Models (LLMs): LLMs are deep neural networks (often Transformer- based) trained on massive text corpora to predict and generate text. Modern LLMs (e.g. GPT- 4, Claude, PaLM, etc.) contain billions of parameters and exhibit remarkable zero- shot and few- shot learning capabilities 1 . They serve as a "cognitive core" for agents, capable of understanding instructions, maintaining context, and performing complex reasoning in natural language 1 . Interaction with an LLM agent typically occurs through prompts, which steer its reasoning and outputs. LLMs can also integrate non- text modalities and tools, meaning an LLM agent' s perception isn' t limited to text, and its action space isn' t limited to textual replies 2 . For example, an LLM- driven agent might process images or execute API calls as part of its actions.

Multi- Agent Systems (MAS): A multi- agent system is a group of multiple interacting intelligent agents that collaborate or compete to achieve goals or solve problems that are beyond the capability of a single agent 3 . Agents in an MAs can share information, negotiate, coordinate actions, and form organizational structures. Classic MAS research (e.g. Wooldridge 2009) emphasizes properties like autonomy (agents operate without direct human intervention), social ability (agents communicate and cooperate), reactivity (agents perceive and respond to their environment), and pro- activeness (agents take initiative towards goals) 4 5 . Until recently, most agents in MAS were either simple rule- based programs or learned policies with limited communication protocols. Early research on emergent communication showed that even simple neural agents could invent symbolic protocols to cooperate on tasks 6 . However, those emergent languages were far from human language in expressivity or interpretability.

LLM- Based Agents: The advent of powerful LLMs introduced a paradigm shift: an agent' s "mind" can be an LLM that is already fluent in natural language and possesses broad knowledge. Formally, an LLMbased agent can be described as a tuple capturing its core components 7 8 :

- LLM Core (L): The large language model itself, which generates the agent's decisions or outputs. This is often a pre-trained model (like ChatGPT, Claude, PaLM, etc.) possibly specialized or fine-tuned for the domain 1.

- Objective (0): The goal or task the agent aims to accomplish 9. This drives the agent's planning and behavior (e.g. solve a puzzle, achieve a negotiation outcome, etc.).- Perception (P): The agent's ability to observe its environment or receive inputs 10. For an LLM agent, perception might include reading textual descriptions, parsing documents, or processing sensory data if linked to vision/speech models.- Memory (M): An internal repository of knowledge and context 11. Memory can store dialogue history, world state, or long-term knowledge. Some architectures extend the LLM with explicit memory modules, storing the agent's past experiences in natural language form 12.- Action Interface (A): The ways the agent can act on the environment 8. Actions may range from emitting messages (to communicate with other agents or humans), to invoking tools/code, to manipulating an external environment (e.g. moving in a game world or updating a database).- Reflection/Revisions (R): A feedback mechanism for the agent to evaluate and adjust its behavior after acting 13. This could involve self-critique, error correction, or incorporating feedback from others (akin to a "critic" that prompts the agent to rethink its approach).

Each LLM- based agent can be thought of as an intelligent node that interacts with other agents and the environment through these components. By combining multiple such agents, we obtain an LLM- based Multi- Agent System (LLM- MAS).

![](images/b37b915151fb03f85149dc133c13a44110b369cb716a138d068ddfaaf986e024.jpg)  
Figure 1: Conceptual architecture of an LLM-based multi-agent system. Each agent is powered by an LLM "brain" with internal modules (profile/goal, communication module, beliefs/memory, etc.). An orchestration layer manages agents' interactions (communication protocols, coordination strategies, etc.). Agents perceive the environment and other agents' messages, then take actions (including tool use or environment manipulation). Environments may be physical, virtual (e.g. games, simulations), and can involve human participants. This figure is adapted from an open-source survey 14 15 to illustrate key components.

LLM- Based Multi- Agent Systems: In an LLM- MAS, a collection of LLM- driven agents engage in collaborative or competitive interactions via natural language communication. By leveraging multiple agents with diverse specialties or viewpoints, an LLM- MAS can tackle problems that exceed the expertise of any single model 16 17. For example, one agent might specialize in generating code while another specializes in testing it, or agents might play different roles in a debate to explore a question from multiple angles. Recent studies show that such multi- agent discussions can encourage divergent thinking

18 and yield more factual, well- reasoned outcomes than a single model working alone 19. In essence, an LLM- based MAS harnesses a "society of minds" approach, where each agent contributes its knowledge and the group achieves a form of collective intelligence 20.

Natural Language as a Communication Medium: A defining feature of LLM- based agents is that they communicate in human language by default. This makes inter- agent communication interpretable (to developers or even end- users) and highly expressive. As a simple illustration, an LLM- driven agent can generate a text message that another LLM agent can understand, enabling complex dialogues to emerge 21. This stands in contrast to earlier multi- agent systems that used either no communication or task- specific symbolic messages. Natural language allows agents to negotiate plans, explain reasoning, teach each other, and even deceive or persuade, much like humans do. Of course, using language also introduces challenges: ensuring the dialogue remains coherent, truthful, and on- topic is non- trivial when multiple free- running generative models are chatting. We will discuss these challenges later.

## 3 Historical Timeline and Milestones

The convergence of LLMs and multi- agent research is very recent, with an explosion of work mostly since 2023. Below is a brief timeline of major milestones leading up to and through this emergence:

- 2016-2017: Early neural MAS research on emergent communication. For example, Lazaridou et al. (2017) demonstrated that two agents could invent a shared discrete code to play referential games 6. This showed the feasibility of agents learning to communicate, but the "language" was primitive and not human-readable.

- 2018-2019: Rise of multi-agent reinforcement learning (MARl) in complex games. Agents mastered purely adversarial games like Go and StarCraft, but those works largely avoided language. Notably, in 2019, OpenAI's Dota-5 and AlphaStar were multi-agent (team) systems but with no communication component. Separately, pre-trained language models were rapidly improving (Transformer models like BERT, GPT-2), setting the stage for future LLM agents.

- 2020: The debut of GPT-3 (175B parameters) marked a turning point. GPT-3's capacity for open-ended generation and few-shot task adaptation suggested that a single model could perform many agent-like functions given the right prompts. Though GPT-3 itself wasn't immediately used in a multi-agent context, it introduced the era of foundation models that could be prompted to carry out interactive roles. This year also saw interest in using language models for tasks like negotiation. For instance, Facebook AI's CaSiNo dataset (2020) involved a negotiation game in natural language, hinting at the possibilities of language-enabled agents.

- 2022: Two notable developments. First, the CICERO agent by Meta AI achieved human-level performance in the board game Diplomacy, which requires natural language negotiation among 7 players. CICERO integrated a large language model for dialogue with a strategic planning module, enabling it to cooperate and occasionally bluff in human conversational play 22 23. This was a breakthrough in competitive multi-agent communication. Second, late 2022 saw the public release of ChatGPT, which, while a single-agent conversational AI, familiarized millions with the concept of natural dialogue with AI. Almost immediately, users began experimenting with prompts that made ChatGPT role-play as multiple agents (e.g. simulating a debate or interviewer/interviewee scenario). The idea of "AI agents talking to each other" entered popular discourse.

- Early 2023: The field truly took off. Researchers introduced frameworks explicitly designed for multi-agent LLM interaction. For example, CAMEL (Communicative Agents for "Mind"

Exploration) proposed a role- playing framework in which two GPT- 3.5 agents (say, a "user" and a "assistant" agent) converse to solve tasks without human intervention  $^{24}$ . CAMEL employed "inception prompting" to seed each agent with a distinct persona and objectives, then let them autonomously continue the dialogue. Around the same time (Q1 2023), we saw the first instances of LLM agents being used for software development and complex problem- solving in multi- step workflows. The concept of an "AI Society" of agents began to appear, where multiple LLMs could simulate a team or community.

- Mid 2023: A proliferation of systems and papers:

- Generative Agents (Park et al., Apr 2023): Demonstrated a small simulated town populated by 25 LLM-based agents that live out day-to-day activities and socialize believably  $^{25}$ . This striking work (inspired by "The Sims") showed emergent social behavior: one agent decided to throw a party, and others organically spread invitations, formed new friendships, and arrived on time for the party – all through their own autonomous interactions  $^{25}$ . Generative Agents introduced architectural patterns (memory streams, reflection) to maintain long- term coherence in agent behavior.

- MetaGPT (Hong et al., Aug 2023): Proposed a multi-agent framework for collaborative software engineering. It organized LLM agents into roles like Architect, Coder, Tester, etc., analogous to an agile software team, and had them cooperate to build software.

- AutoGen (Wu et al., Aug 2023): Introduced an open-source framework enabling developers to compose multiple LLM agents that converse to accomplish tasks  $^{26}$ . AutoGen emphasized easy "conversation programming," allowing agents to communicate in natural language or code, and to incorporate human feedback or tool use as needed.

- AgentVerse, OpenAgents, etc.: Many open-source libraries emerged to support multi-agent simulations and applications (AgentVerse, OpenAgents, Honeycomb, and others). These provided infrastructures for defining agent personas and managing chat exchanges.

- Multi-Agent Debate techniques: Researchers revisited the idea of using multiple models to critique or verify each other's answers. For example, Liang et al. (2023) introduced a Multi-Agent Debate (MAD) framework to mitigate the "degeneration of thought" issue in single-model self-reflection, showing that two agents arguing can explore a problem more thoroughly  $^{27}$ . Similarly, Du et al. (2023) demonstrated that having models debate and then agree on an answer significantly improved factual accuracy and reasoning on tasks like math word problems  $^{28}$ .

- Late 2023: Continued growth and analysis:

- By the end of 2023, dozens of papers had been published on LLM-based multi-agent systems across various domains (coding assistants, scientific discovery, social science simulations, etc.). The community began to categorize these efforts and identify common challenges  $^{29}$ . For instance, Guo et al. (2024) released a comprehensive survey of LLM-based MAS, documenting progress in complex problem-solving and world simulation and compiling benchmarks  $^{29}$ . Initial benchmarks and evaluation frameworks were proposed. E.g., RoleLLM (2023) aimed to benchmark how well LLMs can adhere to given roles in multi-agent conversations, and Arena (2023) provided a platform for pitting LLM agents against each other in dialogue for evaluation. Specialized MAS for niche tasks appeared, such as collaborative math solving (where agents with different strategies or expertise work together on challenging math problems  $^{30}$ ) and tool-augmented agents (where each agent could invoke external APIs or code for subtasks  $^{31}$ ).

![](images/4001ffc92e248773def56fe59baa8f007792aed1d9dea492f3832ca422112b89.jpg)  
Figure 2: Timeline of the emergence of LLM-based multi-agent systems, primarily during 2023. Major branches of development include general frameworks (orange, e.g. CAMEL, MetaGPT, AutoGen), applications in problem-solving (green, e.g. coding agents), world simulation (pink, e.g. Generative Agents for society, game, psychology, etc.), methods for enhancing agent capabilities (blue, e.g. debate, learning, self-adaptation), and datasets/benchmarks (teal). The vertical axis is time (March to Dec 2023). Colored boxes indicate notable papers or systems, and numbers (if present) denote counts of related works. This illustrates the rapid growth and diversification of the field in 2023.

- 2024 and beyond: Research now is addressing scalability and robustness. For example, Wang et al. (2024) introduced MegaAgent, a framework for organizing large swarms of LLM agents to cooperate on many sub-tasks in parallel. Others explored dynamic team structures and self-organization in agent societies. A study in 2025 investigated the resilience of LLM-MAS when some agents are faulty or malicious, finding that hierarchical team structures and adding reviewer or challenger agents greatly improve robustness 32 33. Application-wise, very recent work has even applied LLM agents to economic simulations and public policy, using hundreds of agents to model consumers and voters (with different demographics) to test policy outcomes 34. We will discuss applications in a later section.

In summary, within just a couple of years, the field has progressed from speculative idea to initial frameworks, and now to rich simulations and real- world prototypes. Next, we delve into the core designs and methodologies enabling these systems.

## 4 Architectures and Frameworks for LLM-Based Multi-Agent Systems

Building an LLM- based MAS requires addressing two levels of design: (1) the architecture of each individual agent, and (2) the orchestration of multiple agents (i.e. how they communicate and coordinate). We discuss each in turn, and then survey representative frameworks that embody these designs.

### 4.1 Individual LLM Agent Architecture

Individual LLM Agent ArchitectureAn LLM agent needs additional structure around the raw language model to function autonomously in an environment. Key architectural augmentations include:

- **Profile & Role Specification:** Each agent is typically given a role or persona that conditions its behavior. This can be as simple as a one-sentence description (e.g. "You are a medical expert agent advising a patient") or as complex as a full prompt with identity, expertise, and goals. Role specification is crucial in multi-agent setups to create heterogeneity – i.e., agents with diverse knowledge or viewpoints. In research prototypes, roles are often implemented via initial system prompts (sometimes called "inception prompts" as in CAMEL 35). These prompts tell the agent who it is, what it knows, and what it's trying to do. For instance, one agent might be prompted to behave as a questioner and another as a solver, or one as a CEO and another as a software engineer, etc. Clearly defined roles prevent agents from collapsing into identical behaviors and allow a form of specialization within the team 17.

- **Goal and Belief State:** Beyond the high-level objective, an agent maintains an internal state of what it believes about the world and other agents. This is sometimes explicitly modeled as a belief module or knowledge base 36. For example, an agent might have a belief like "Agent B is probably unaware of X, so I should inform them." In practice, such beliefs can be stored in text form and prepended to the LLM's input (giving it a kind of theory of mind) or handled by additional neural modules. In Generative Agents, each agent stored factual memories of observations and then synthesized higher-level reflections (which act like beliefs or attitudes) 12.

- **Long-term Memory and Knowledge Storage:** Unlike static prompts, a memory module accumulates information over time. Some frameworks use vector databases or memory graphs to let agents remember important facts about past interactions or environment dynamics. For example, an agent in a simulation might remember "I promised Agent C to help with their task tomorrow" and retrieve that later. Memory can also include domain knowledge (documents, FAQs) that the agent can query as needed 37. Efficient memory retrieval is an active area of research, as naively giving the entire dialogue history to the LLM each time is costly and can exceed context windows.

- **Perception and Tool Interfaces:** Agents often need to perceive non-linguistic information. One approach is to describe the world state in text (natural language or structured JSON) and feed that to the agent. Another approach is to equip agents with **tool use abilities** – for example, an agent can be allowed to run code, query a database, or call an API (with the LLM generating the appropriate commands). This extends perception (the agent can "see" environment data via queries) and action (the agent can affect the environment by executing code or API calls). In LLM-MAS frameworks like AutoGen, agents can be designed to use tools and even request human input if needed 26. Such capabilities mitigate the LLM's limitations by offloading certain tasks (e.g. calculations, image recognition) to specialized tools. Importantly, tool usage by one agent can produce outputs that become perceptible to other agents, creating an additional channel for coordination (beyond just natural language chat).

- **Action Selection & Safety Filters:** When an LLM decides on an action (which could be a message to send or a tool to invoke), there is often a post-processing step. For messages, this might involve ensuring the content adheres to guidelines (to avoid agents going into toxic or irrelevant dialogues). For tool actions, it might involve validating the format (e.g. making sure code syntax is correct if the agent outputs code). Some frameworks include a supervisory layer that can veto or modify agent outputs for safety and coherence, acting like a guardrail around the LLM.

Reflection and Learning: Advanced agents incorporate feedback loops to improve performance over multiple turns or tasks. A simple form is self- reflection: after completing a task or making a mistake, the agent produces a summary or analysis of what went wrong, which is then stored or used to modify its strategy next time. For example, if two agents debate a question and get it wrong, a reflection phase might allow them to identify which assumptions were flawed and try a different approach. In multi- agent debate systems, researchers have implemented a "judge" or aggregator agent that reflects on the arguments and decides the final answer 27. In other cases, a Challenger agent may listen to others and point out errors, prompting a revision 38. This kind of meta- reasoning greatly helps in maintaining solution quality and truthfulness, as shown by Wu et al. (2023) where adding a verification agent improved math problem solving accuracy with GPT- 4 31.

Overall, an individual LLM- based agent is more than just a generative model - it' s an encapsulated autonomous unit with memory, a defined persona, and interfaces to communicate and act. Designing such agents requires prompt engineering, sometimes plugin or tool APIs, and careful consideration of what the agent should or shouldn' t know initially.

### 4.2 Orchestration and Communication in Multi-Agent Systems

When multiple LLM agents are deployed together, we need an orchestration mechanism to manage their interactions 39. Key aspects of orchestration include:

Coordination Paradigm: Are agents cooperating towards a shared goal, competing with each other, or a mix of both? The coordination model defines this dynamic 40. In a cooperative setting, agents share information freely and attempt to maximize a joint utility (e.g. a team of coding assistants dividing up a programming task). In a competitive setting, agents have individual goals that may conflict (e.g. in a market simulation or game theory scenario). Some systems implement Hierarchical coordination 41, where a lead agent (or a group of supervisors) directs the work of others. For instance, a "manager" agent might assign sub- tasks to "worker" agents. Hierarchies can combine cooperation at one level with competition at another. The choice of coordination paradigm influences how communication is structured (e.g. honest sharing vs. bluffing is context- dependent).

Communication Topology: How do agents communicate? Common patterns are:

Centralized: Agents send messages to a central coordinator which then disseminates relevant info to others. This could be a special agent (like a moderator or blackboard) that all others trust 42 . Centralized comms simplify design and can prevent uncontrolled chatter, but introduces a single point of failure/bias. Peer- to- Peer (Decentralized): Agents send messages directly to one another as needed 42 . This mirrors human communication in teams (anyone can talk to anyone). It allows rich interactions but can get chaotic or overwhelming as agent count grows. Ensuring every agent stays informed of relevant facts is non- trivial. Broadcast or Group Communication: An agent' s message could be visible to all, or there might be channels for subsets of agents. For example, in a simulation of a social network, an agent might post a public message (broadcast) versus whisper to one agent (direct).

Many systems default to a simple turn- taking roundtable: e.g., "Agent A speaks, then Agent B responds, and so on," effectively creating a dialogue thread involving all agents. Others use event- driven messaging where any agent can output a message when it has something to say. The design should

prevent infinite message loops and ensure timely convergence (or termination criteria for the conversation).

- **Turn Management and Scheduling:** In human conversations, we naturally manage turns to avoid everyone talking at once. Similarly, MAS frameworks often implement a turn scheduler. This could be as strict as a fixed sequence or as dynamic as selecting the next speaker based on context (e.g. the one addressed by the last message responds). Some frameworks allow interrupts or concurrent messages if needed (especially in simulations with many agents). Turn management also interacts with the environment update cycle in simulations: e.g., agents might communicate for a while and then all take a simultaneous action in the environment, and repeat.

- **Shared Memory or Blackboard:** A pattern from classic MAS is a blackboard system where agents post information to a shared repository accessible by all. In LLM-MAS, one could implement a shared document or table that agents can read/write in natural language. This can help with global state tracking (e.g. in a coding project, a shared document could hold the evolving code). However, merging changes and avoiding confusion on a shared board is challenging, especially with LLMs that might overwrite each other's contributions. Instead of a literal shared memory, some systems simply ensure each agent receives transcripts of all communications, which in effect gives them common ground knowledge.

- **Environment Mediation:** If agents act in a simulated world (like a game or a physical process), the environment itself can mediate interactions. For example, two agents might both try to move a piece on a game board – the environment's physics or rules determine the outcome and inform all agents. Thus, not all agent effects are communicated via messages; some are implicit via environment state changes. Designing the perception interface so that agents notice relevant changes is crucial (e.g. "Agent A moved to room 2" should be conveyed to Agent B if it matters for B's goals).

- **Protocols and Language Games:** To make multi-agent communication effective, developers sometimes define a protocol or format for messages. For instance, in a coding team scenario, one could enforce that messages start with a tag like "[Design]" or "[Review]" to indicate the purpose. Some works impose a semi-structured dialogue (like question-answer pairs, or proposal-acceptance patterns). These can be described in the system prompt given to all agents to follow. If done well, it can reduce misunderstandings and keep the conversation organized. On the other hand, one advantage of LLM agents is their flexibility – they can negotiate the conversation structure on the fly if not strictly constrained.

Orchestration Platform: Many research papers refer to an orchestration platform that handles the above concerns 39. In practice, this could be a piece of code (outside the agents) that routes messages, enforces order, and monitors the session. For example, the platform might implement: "If no new information is being exchanged (agents repeating themselves), terminate the dialogue." It may also allocate new agents dynamically, which leads to another advanced idea: dynamic agent creation.

Dynamic Agent Generation: A fascinating capability in some systems is the ability to spawn new agents on the fly. Suppose during a multi- agent conversation, the need for a particular expertise arises that none of the current agents possess. An existing agent (or the orchestration system) could create a new LLM agent with a role tailored to that need. Shen et al. (2024) demonstrate this with an approach called DRTAG (Dynamic Real- Time Agent Generation): an LLM agent can take a description of a needed role and generate a prompt to instantiate another agent to fulfill it 43 44. For instance, agents discussing a legal issue might spawn a "LegalExpert" agent (another LLM instance prompted with legal knowledge) to

join the discussion. Dynamic generation increases flexibility and scalability, as systems no longer have a fixed cast of agents limited to pre- defined roles. However, it raises issues of managing an expanding pool of agents and integrating the newcomer smoothly into the ongoing context. It also carries computation costs if overused. Early results show promise: automatically generated agents can successfully integrate and enrich the problem- solving process without human intervention 44 45.

Homogeneous vs. Heterogeneous Agents: Orchestration must consider whether all agents use the same underlying LLM model or different ones. Homogeneous setups (e.g. five copies of GPT- 4 with different prompts) are common and easier to implement. Heterogeneous setups might combine models of different sizes or from different providers, or even mix LLM- based agents with non- LLM agents (like a reinforcement learning agent). Heterogeneity can simulate diverse "personalities" or capabilities. A recent example mapped different LLMs to different roles based on their strengths: an economics simulation used GPT- 4 for some agents (representing highly educated actors) and smaller models for others (less informed actors) 34. The inherent differences in verbosity, creativity, or accuracy across models then become a feature (reflecting heterogeneity in the population). Orchestration in such cases needs to account for the varying reliability and knowledge of agents. On the flip side, heterogeneity can improve robustness: if one model has a blind spot, maybe another doesn't. Ensemble- like effects might emerge where agents validate or correct each other, as seen in debate scenarios where models with different initial opinions eventually agree on a correct answer 46.

Illustrative Frameworks: To make these concepts concrete, let's briefly describe a few influential frameworks and how they implement agent and orchestration design:

- ChatDev (2023): ChatDev is a system that simulates a software company with multiple LLM-based agents collaborating to develop software 47 48. It defines distinct roles like Chief Executive (CEO), Lead Developer, Coder, Tester, etc., each instantiated as a ChatGPT agent with a role-specific prompt. The orchestration is structured along phases of software development: design  $\Rightarrow$  coding  $\Rightarrow$  testing. In each phase, a sequence of conversations happens among the relevant agents. For example, in the design phase, the CEO and Lead Developer discuss requirements; in coding, the Lead Developer coordinates multiple Coder agents, etc. The system uses a chain-of-dialogues approach, where the outcome of one phase (e.g. a design specification document) is passed as input to the next 49. ChatDev also incorporates a "communicative dehallucination" mechanism: if a coder agent is unsure or the code seems inconsistent, it can proactively ask for clarifications or additional details before proceeding 50. This reduces the incidence of code hallucinations (incoherent or incorrect code) by ensuring agents don't assume false facts. Empirically, ChatDev was able to generate simple games (like Snake and Tetris) autonomously through agent collaboration, though the more complex Tetris game required several iterations and still missed some functionality 51. This highlighted that while multi-agent collaboration sped up development and handled modular tasks well, deeper reasoning (like game logic for clearing lines in Tetris) remained challenging for the agents, requiring improvements in their planning abilities 52.

- CAMEL (2023): In CAMEL's basic setup, two LLM agents are assigned roles such as "AI user" and "AI assistant" to work on a task (for instance, writing a piece of code) 24. The novelty was having zero human messages after the initial prompt. The two agents keep conversing until they solve the task or get stuck. CAMEL introduced specific prompting techniques: inception prompts gave each agent a detailed identity and initial agenda, and a role-playing paradigm was enforced (the user-agent always states requirements or queries, the assistant-agent provides solutions) 53. This structure ensured a productive dialogue pattern. CAMEL demonstrated that such agent pairs could generate useful outputs collaboratively and could also produce conversational data that

might be used to train or analyze LLMs' behavior in multi- agent modes. It effectively bootstrapped multi- agent cooperation without extra training, relying purely on clever prompting. This paved the way for more complex multi- agent self- chat systems.

- AutoGen (2023): AutoGen can be seen as an orchestration toolkit. It allows developers to define agent classes with certain abilities (LLM-powered or even a simple Python function), and then compose a system where these agents send messages to each other according to a specified pattern 26. Developers can script conversation flows using a mix of natural language instructions and Python code. For example, one might create an agent that specializes in web search and another that synthesizes results, then program a loop where the search agent fetches info and the synthesis agent compiles an answer, until a stopping criterion is met. AutoGen agents are "conversable, customizable, and can operate in various modes" (including involving humans in the loop) 26. This flexibility means AutoGen can implement many of the designs described earlier: central or decentralized comms, tool use, dynamic role assignment, etc., depending on how the developer configures it. The key contribution of AutoGen is treating multi-agent conversation almost like a programming paradigm—one can, "program by prompting" and orchestrate complex workflows, leveraging the natural language interface of the LLMs to integrate logic.

The above examples barely scratch the surface. Other frameworks like AgentVerse, LangChain Agents, Voyager, HuggingGPT, etc., each have their own nuances (e.g. Voyager uses an LLM agent to autonomously explore a game world by writing and improving its own code, an implicitly single- agent but self- collaborative approach). What unites all these is the core idea: by decomposing tasks into interactive subtasks and assigning them to multiple LLM- powered agents, we can achieve outcomes that are hard to attain with a single monolithic model. Next, we will explore what those outcomes are by surveying applications across different fields.

## 5 Applications and Case Studies across Domains

LLM- based multi- agent systems have been applied (or at least prototyped) in a range of domains. Here we cover several major areas, illustrating how the general principles are specialized for each context.

### 5.1 Complex Problem Solving and Planning

One natural use of multiple agents is to tackle problems that benefit from divide- and- conquer or from multiple perspectives. Examples include:

- Software Engineering Teams: As introduced with ChatDev and MetaGPT, multi-agent systems have shown promise in automating parts of the software development lifecycle. A complex project can be partitioned into roles like requirements analysis, coding, testing, and documentation. Each agent, armed with an LLM, focuses on one aspect but communicates with others to ensure consistency. For instance, a Requirements Analyst agent might clarify specs with a Client agent (simulated), then pass requirements to a Designer agent to outline a solution, which then guides a Coder agent. A Tester agent generates test cases, finds bugs, and asks the Coder to fix them. This resembles a human agile team. Early case studies show such LLM teams can indeed produce working software for well-specified tasks (like simple web apps or games) within minutes 51. The benefits observed include: autonomous problem decomposition (the agents ask each other the right questions to break down tasks) and parallelism (some subtasks can be done concurrently by different agents). However, limitations include the quality of generated code (still not at senior developer level for complex tasks) and the need for human

oversight for critical decisions (especially for security or architectural choices that LLMs might mishandle). Nonetheless, the concept of "Software Engineering 2.0" via LLM MAS is compelling – by augmenting each development phase with specialized AI agents, one can automate end- to- end pipeline from requirement to tested code  $^{54}$ .  $^{16}$ . Industry is actively experimenting with this (e.g. internal tools at companies that have multiple codex- like agents reviewing each other's output to reduce errors).

- Mathematical and Logical Reasoning: Multi-agent approaches have been used to improve performance on tasks like math word problems, logical puzzles, or coding challenges. The idea is to have agents cross-verify answers or propose different solution paths. Yilun Du and colleagues (2023) set up LLM agents to debate the answer to math problems and only output an answer if they reached a consensus  $^{28}$ . This significantly reduced errors by filtering out one-shot mistakes that a single model might make. Another work had one agent generate a solution and another agent critically evaluate it (a "solver" and a "checker"). If the checker found an issue, the solver would try again, and so on – akin to pair programming or proof validation. This approach boosted accuracy on benchmark problems like GSM8K (grade-school math) because the checker often caught obvious mistakes that the original solver overlooked. More generally, for any task where verification is easier than generation, one can set up a generator agent and a verifier agent. Examples include code generation (run test cases to verify)  $^{50}$ , puzzle solving (plug the solution back into check), translation (back-translate and compare meaning), etc. Multi-agent setups shine here by introducing redundancy and reflection – it's less likely for two independent agents to overlook the exact same flaw.

- Tool Use and Workflow Automation: In complex workflows (like data analysis pipelines, business processes), one agent can serve as a controller/orchestrator that delegates sub-tasks to either other agents or external tools. A real-world example from Microsoft's HuggingGPT system: one LLM agent reads a user request, breaks it into steps, and for each step either uses a specialized ML model (image classifier, speech recognizer, etc.) or consults another LLM agent better at that subtask  $^{55}$ .  $^{56}$ . The controller then gathers the results and composes an answer. While HuggingGPT's focus is on chaining AI models, the concept extends to multiple agents: each tool could be wrapped as an agent that "communicates" via a standardized interface. For instance, an agent with web-browsing capability might take a question, do a web search, then summarize the findings to another agent that compiles a final report. In software terms, this is akin to microservices architecture, but with each service being controlled by an LLM. The advantage is modularity – one can plug in new tools or agents as needed. The challenge is making sure all these pieces speak the same language and that the orchestrator agent knows the capabilities of each (which again might be encoded in its prompt as a list of available experts).

- Robotics and Embodied Planning: Although still preliminary, LLM-based multi-agent concepts are entering robotics. One scenario is a team of embodied agents (e.g. a group of robots or drones) that need to coordinate a task like search-and-rescue or assembly. LLMs can be used to reason about high-level strategies and communication between robots. For example, if each robot is controlled by an LLM agent, they could communicate in natural language to negotiate which robot covers which area, or to alert each other of discoveries ("I found the target, come assist"). Alternatively, a single LLM agent might coordinate multiple physical robots by instructing them. Some recent works have used LLMs for multi-step planning where the LLM outputs structured plans that assign actions to different agents (human or robot) in a team  $^{57}$ . One can imagine a future "AI foreman" agent that, given a construction task, produces a plan assigning subtasks to various robotic workers and then adjusts the plan on the fly through dialogue with those workers. There are significant challenges here (e.g. real-time constraints, grounding language to physical actions reliably, safety), but early studies (like instructing two robotic arms to collaborate via an

LLM intermediary) show that language can serve as a universal interface for multi- robot cooperation, translating abstract goals into coordinated action sequences.

### 5.2 Simulation of Social Behavior and Worlds

One of the most intriguing applications of LLM- based agents is simulating environments with many agents - essentially using AI to create synthetic societies, game worlds, or populations for research. Because LLM agents can exhibit surprisingly human- like behavior when prompted correctly, they can serve as stand- ins for people in certain simulations.

- Generative Social Simulations: The Generative Agents work by Park et al. (2023) demonstrated a miniature town where 25 agents (each an LLM with a backstory and memory) interacted continuously, producing emergent social dynamics 58 25. Agents woke up, went to work, formed opinions about each other, and even coordinated a party as mentioned. The significance is that no single script dictated these events – they emerged from the interplay of the agents' individual routines and communications. This kind of agent-based modeling has long been used in social science (with simpler agents) to study phenomena like the spread of information, group formation, or economic transactions. LLM agents take it to a new level of fidelity. They can carry out open-ended conversations that reflect psychological nuances (e.g. an agent feeling "lonely" might initiate more conversations). Researchers are using such simulations to explore hypotheses in psychology and sociology. For instance, how do rumors spread in a closed community? One can plant a piece of info with one agent and observe through the chat logs how it propagates and possibly mutates as agents talk to each other. Another example: simulating an online forum with multiple agent participants to see what moderation policies might be needed or how echo chambers form. Because the agents behave plausibly (using human language), analysts can qualitatively examine their interactions as if observing real humans, albeit it's important to remember these are still models following learned patterns.

- Games and Virtual Worlds: Multi-agent LLMs are also being used to populate NPC (non-player character) behaviors in games. Imagine an RPG game where every villager in a town is an LLM agent with memory of the game events – they can generate dialogue dynamically, form relationships with the player and each other, and adapt to the evolving game state. This would provide a far richer experience than pre-scripted NPC lines. A recent environment called LIGHT from Meta AI is a text-based fantasy world where multiple agents (either human or LLM) can interact, and LLM agents have been tested in roles like innkeepers or quest-givers, improvising dialogue and quests based on world context. Similarly, generative agents could be used to simulate economies in games (each merchant agent sets prices, each consumer agent decides what to buy based on needs) or political systems (agents vote or lobby based on their beliefs). The benefit in games is improved realism and replayability – no two playthroughs are the same if the NPCs are driven by stochastic generative models with emergent interactions. However, it requires careful alignment to avoid agents going off the rails (e.g. saying lore-inconsistent things or producing offensive content).

- Economic and Policy Simulations: As noted, researchers have started using LLM agents to model complex economic scenarios. For example, LLM Economist (2024) created hundreds of agent "citizens" with different income levels and preferences, and simulated the impact of a tax policy change 59. Each agent was an LLM prompted with a persona (e.g. a middle-class single parent) and asked to decide, say, how much to work and spend given a new tax rate. The aggregate of their decisions gave insights into macroeconomic effects, and their dialogues/explanations provided interpretable rationales for those decisions (something traditional economic simulations lack, since those agents are often just equations). Another example simulated Federal

Reserve meetings with multiple LLM agents taking the roles of central bank committee members discussing interest rates 60. The potential here is to have AI- enabled "what- if" simulations: policy- makers could ask, "What if we introduce this law or this market perturbation?" and the LLM agents representing different stakeholders would simulate the likely responses (e.g. businesses cut hiring, consumers shift spending, etc.). While promising, one must be cautious - LLM agents are not perfect economic reasoners and they might mirror biases in their training data. Nonetheless, this approach offers a way to stress- test policies in a safe sandbox with qualitatively rich outcomes (like being able to read the simulated statements of each stakeholder to understand their viewpoint 61). It combines quantitative modeling with narrative insight.

- Education and Training: Multi-agent simulations can also serve as training environments for humans. Consider language learning: a student could practice a foreign language by entering a virtual scenario (say, a restaurant) where multiple LLM agents (the waiter, other patrons) converse with them and with each other in that language. This provides immersive conversational practice. In professional training, one could simulate scenarios like a medical emergency with multiple roles (doctor, nurse, patient's family, etc. all played by agents) to let a trainee practice communication and decision-making. The key advantage is consistency and control – one can reset and replay scenarios, adjust difficulty (agents could intentionally behave in challenging ways), and no human confederates are needed to role-play. Such applications require agents to exhibit emotional and social understanding, which LLMs are somewhat capable of (they can express and react to sentiments in language). Early experiments show users often react to empathetic or adversarial agents as if they were real, indicating the potential for effective training simulations. Again, the challenge is ensuring reliability: an agent saying something truly inappropriate or incorrect in a training simulation could teach bad habits or misinformation. Careful prompt design and content filtering are crucial.

- Creative Collaboration (Storytelling): Another domain is creative arts – multi-agent systems have been used to co-write stories or scripts. For example, one agent might play the role of a protagonist and another the antagonist, and they improvise a story through dialogue. This can lead to imaginative, unexpected plot developments, essentially performing a theatrical scene with AI actors. Projects like "AI Dungeon" and other interactive fiction have started to incorporate multi-agent mechanics to avoid a single narrative voice becoming monotonous. By having characters driven by independent LLMs with their own goals, the narrative becomes more dynamic. Similarly in brainstorming or design, multiple agents can take on different perspectives (the optimist, the critic, the user, etc.) and discuss a concept, giving a human creator a range of ideas to consider. This is like an AI writers' room. Ensuring the conversation stays productive is the main issue – agents can potentially ramble without moving the plot forward unless guided by an overarching narrative goal or moderated by the user.

In summary, simulation applications of LLM- MAS show how qualitatively rich and quantitatively complex behavior can arise from relatively simple ingredients (LLMs + prompting + basic environment rules). These applications blur the line between analysis and generation: e.g., using agents for policy analysis relies on their generative modeling of human- like responses. While extremely promising, it's important to validate these simulations against real- world data whenever possible (for instance, do the economic agent behaviors align with real consumer survey data? Do generative social agents obey known social psychology theories like Durbar's number for friendships?). Initial reports are encouraging – generative agents in the Park et al. work exhibited believable patterns such as forming daily routines and new relationships that a human observer found plausible 25. This indicates that with the right architecture (especially long- term memory and reflection to avoid incoherence), LLM agents can indeed capture many aspects of human behavior.

### 5.3 Domain-Specific Collaborative Agents

Beyond general problem solving and simulation, researchers have crafted multi- agent setups for specific domains:

- Scientific Research and Discovery: There are visions of AI agents acting as a "research team" to autonomously make scientific progress. For example, one agent generates hypotheses, another designs experiments (in simulation), a third analyzes results, and a fourth writes up the findings. In 2023, a prototype named SciAgent had agents debate scientific questions (like finding the best material for a given application) by reviewing literature and arguing pros/cons of different candidates. This is a natural extension of the debate frameworks but applied to scientific reasoning: the multi-agent discussion ensures that multiple hypotheses are considered and evidence is cross-examined, potentially leading to a more robust conclusion. Although in early stages, such systems could accelerate literature review or even suggest novel experiments (with a human in the loop to actually carry them out in the lab). One can also imagine agents representing different disciplines (a biologist, a chemist, an engineer) collaborating on an interdisciplinary problem, each bringing knowledge from their domain. LLMs certainly possess the knowledge base (trained on scientific papers, for instance), but the challenge is factual accuracy and the ability to reason about experimental feasibility – areas where current LLMs can stumble without human oversight or additional tool support.

- Healthcare Multi-Agent Systems: Another emerging application is in telemedicine or patient support, where multiple agents with different roles help a patient. For example, an "AI nurse" agent could gather symptoms via chat, then consult an "AI doctor" agent for a diagnosis, which might in turn query an "AI pharmacist" about medication interactions. This compartmentalization could mirror how a real clinic team works. It could also serve to double-check critical decisions (the doctor agent proposes a diagnosis, but a second agent double-checks guidelines or suggests alternatives). The collaborative consultation approach might reduce errors compared to a single AI doctor model. However, high reliability and strict safety are paramount; these agents would need to be thoroughly validated and likely remain advisory, with human medical professionals making final decisions.

- Finance and Trading: In algorithmic trading, one could have multiple agents each with a different strategy or market focus (e.g. one monitors news sentiment, another focuses on technical indicators, a third gauges social media buzz). They could discuss whether to buy or sell an asset, essentially forming an ensemble decision. The natural language communication might not be strictly necessary (since one could just combine signals mathematically), but advocates suggest it provides transparency – the agents can "explain" their views to each other and ultimately to the human operators. For instance, the news agent might say "I see negative earnings reports coming out" and the technical agent might say "trend is still upward", and they debate which factor is more critical. This is hypothetical at this stage; financial institutions are exploring LLMs for analysis but likely not yet trusting multi-agent chat for live trades. Still, as a decision support tool, such dialogues could summarize complex, multi-factor situations in a way that a human can follow (much more so than a black-box ensemble model voting to buy/sell without explanation).

- Data Analysis and Analytics: Imagine a multi-agent system that performs a comprehensive data analysis task: one agent writes queries to retrieve data, another agent visualizes it, a third agent interprets the patterns, and a fourth writes a report. Some recent work indeed looked at LLMs as data analysts, where a data scientist agent in natural language would generate SQL queries, get results, and pass them to a visualizer agent, then collectively they describe insights 62. By splitting these roles, each subtask can be tackled with more focus (and even different tools; the

query agent might use a Python tool to run SQL, etc.). It also reflects real- world workflows, making it easier to integrate into human teams (e.g. the report- writing agent produces a draft that a human data scientist can refine). The challenges are ensuring correctness of analysis (LLMs might misinterpret statistical significance, etc.) and aligning the agents on the overall goal (they must work on the same analysis question and not diverge).

- Self-Driving and Smart infrastructure: In autonomous driving, a single vehicle's AI might be considered an agent, and having vehicles communicate can form a multi-agent network improving safety (vehicle-to-vehicle communication). LLMs are not likely to run the core control of a car (too slow and unpredictable), but they could be involved in coordination communication – for example, in traffic management, an LLM agent could act as an intersection manager negotiating right-of-way among approaching autonomous cars (each car could be "voiced" by an agent sending requests like "i'm an ambulance in emergency, please clear my path" and the manager agent coordinates). This is speculative but shows the breadth of where multi-agent dialogue could apply.

The pattern in domain- specific cases is to leverage specialization and cross- checking, much like in general cases but tailored to domain constraints. An encouraging sign from software (ChatDev, etc.) and debate (Du et al.) is that multi- agent approaches tend to improve reliability and performance through redundancy and diversity. They encourage agents to "think twice" or view a problem from multiple angles before finalizing an answer 19. In domains where errors are costly, this is a valuable property. Of course, it also doubles or triples the computational cost (multiple model calls instead of one), which is a practical trade- off. For critical domains, the cost is often justified if it catches mistakes.

## 6 Open Challenges

Despite the rapid progress, LLM- based multi- agent systems face numerous challenges and open research questions. We highlight some of the key issues:

1. Controlling Dialogue Quality and Coherence: When multiple LLMs talk to each other, there's a risk of conversations veering off-topic, becoming incoherent, or getting stuck in loops. Without a human user to reinforce relevancy, agents might start to hallucinate new goals or engage in role-unrelated chit-chat. Ensuring that agents stay on task is hard, especially in long conversations. Techniques like grounding agents with explicit knowledge (so they don't need to guess facts) and using orchestration rules (e.g. timeouts, focus reminders) can help. But fundamentally, open-ended language is unpredictable. An open challenge is to develop robust dialogue management: how can agents detect when the conversation is no longer productive and recover? How to prevent the "echo chamber" effect where agents amplify each other's incorrect statements? Current solutions include adding a moderator agent or periodically summarizing and refocusing the discussion, but these are not foolproof.

2. Truthfulness and Knowledge Verification: LLMs are known to hallucinate facts. In a multi-agent setting, this can be amplified: one agent might generate a false fact, and others could take it as truth and build on it. There is a danger of groupthink among agents. Approaches like multi-agent debate try to address this by encouraging disagreement and fact-checking 18 19. Indeed, Liang et al. found that having agents in a debate adopt opposing stances helped surface more ideas and reduced uncritical acceptance of an answer 27. Yet, if all agents share the same underlying knowledge cutoff (say, all were trained on data only up to 2021), they might collectively be ignorant of newer facts. Integrating external knowledge sources (tools, updated databases) is thus crucial for truthfulness. Another solution is to diversify the model architectures or training data of agents so they have slightly different knowledge and strengths; as mentioned, heterogeneous agent teams might catch each other's errors 63. Evaluating

truthfulness in multi- agent output remains an open problem – it’s not just about factual accuracy, but also whether the reasoning process the agents followed is sound (to avoid coincidentally correct answers for the wrong reasons).

3. Scalability and Efficiency: Coordinating a large number of LLM agents is computationally expensive. If you have 100 agents each powered by a 10-billion-parameter model, it’s likely impractical to run them all simultaneously in real time. There are research questions on scaling laws for multi-agent systems: e.g., how does performance improve or degrade as you increase the number of agents? Is there a point of diminishing returns (or even negative returns due to communication overhead and conflict)? Some work (Chen et al. 2024) studied compound inference and found that beyond a certain point, adding more calls (agents) doesn’t help and can introduce more confusion 64. Efficient orchestration might involve dynamically selecting a subset of agents who are most relevant at any given time (like how in a meeting, not everyone speaks on every topic). Caching and reusing partial results is another idea – if one agent has already summarized a document, others should reuse that instead of re-reading the document themselves. There’s also interest in model compression or distillation in this context: perhaps train a smaller model to imitate the behavior of a group of larger models, to get similar benefits at lower cost.

4. Learning and Adaptation: Most current LLM-MAS operate in a zero-shot or few-shot manner – the agents aren’t learning from their interactions (apart from within the limited context of a single session). They don’t improve their base weights based on multi-agent experience. An open question is whether multi-agent interactions can be used as a form of training data to further fine-tune LLMs. For example, if two agents debating reach a correct conclusion, can that dialogue be fed back into a training process so the model gets better at that task next time? If agents consistently make a certain mistake in coordination, can the system adjust (e.g. by refining prompts or learning a new policy)? There are initial forays into this: e.g., using self-play dialogues to fine-tune models for better cooperative behavior. One complication is that multi-agent outcomes can be noisy or hard to label – who says what in a debate might not straightforwardly indicate which parts were good or bad. Reinforcement learning techniques could be applied (defining a reward for successful multi-agent task completion), but credit assignment to individual agent contributions is tricky. A related challenge is memory lifespan – if an agent has a memory module that grows with experience, how to prevent it from becoming unwieldy or forgetting important things (catastrophic forgetting vs. information overload)? There may be a need for agents to learn to learn from each other, e.g. one agent could adjust its internal model of another agent over time to predict their behavior (like theory-of-mind modeling). This brings in game theory and adaptive strategy: agents might shift from purely cooperative to more competitive if they infer others are not trustworthy, and vice versa. Designing agents that can evolve their interaction strategies over long-term scenarios (days of simulated time, say) is an open frontier.

5. Robustness to Malfunctioning or Adversarial Agents: In any team, one weak or malicious member can spoil the outcome. Huang et al. (2025) studied MAS resilience and found that if one agent is faulty (making random errors or adversarially sabotaging), certain structures cope better 65. Specifically, a hierarchical setup where a “leader” can double-check others limited the damage to ~5% performance drop, whereas a peer-to-peer chain saw up to ~24% drop when one link went bad 66 67. They also introduced mechanisms like the Challenger and Inspector agents to improve resilience, which dramatically corrected errors by questioning and reviewing messages 38 67. This suggests that adding some redundancy (like an oversight agent) and structured critique can mitigate issues. Nonetheless, an adversarial agent might also learn to hide its sabotage or manipulate others (imagine a deceptive agent lying subtly so the others build a plan on false premises). There’s a need for trust and verification protocols: how do agents verify information provided by others? How to detect if an agent is not acting in good faith or is compromised (in a cybersecurity sense, if an external attacker injects a rogue agent into the system)? Human organizations use reputation systems and authentication; similar ideas could be applied (agents could have reputations based on past reliability, and critical info might need to be

confirmed by a second agent). This veers into multi- agent alignment: not only aligning agents with human values individually, but aligning them with each other to share a common goal and not prioritize winning over truth when it's supposed to be cooperative.

6. Evaluation and Benchmarking: It's inherently challenging to evaluate multi-agent systems. Traditional NLP metrics (like BLEU, accuracy, etc.) don't directly apply to a multi-turn multi-party interaction. What metric signifies that a multi-agent dialogue was successful? It could be task success (did they achieve the goal?), solution quality (for a design task, how good is the design?), or human preference (would a human judge find the conversation useful/coherent?). Additionally, emergent behaviors (like forming a social norm in a simulation) are hard to quantify but important to observe. The field is working on benchmarks - e.g. MultiAgent Dialogue tasks where a scenario and desired outcome are defined, and the agents' conversation is evaluated on completeness and correctness of the outcome 68 69. Some benchmarks involve humans in the evaluation loop (rating the helpfulness or truthfulness of dialogues). Another approach is behavioral metrics: measuring if agents' behaviors satisfy certain logic (like "no agent's message contradicted a fact from the environment" or "all requirements were discussed by the agents"). As LLM-MAS are applied to more concrete tasks (coding, math, games), task-specific metrics help (did the code run, was the math problem solved, did the agents win the game). But for open-ended systems (like simulating a society), evaluation might need social science methods (e.g. statistical properties of the society generated). Developing standardized benchmarks and performance metrics is critical for comparing approaches and tracking progress. The "Benchmark Self-Evolving" framework 70 is one example where a multi-agent system itself generates new test problems to evaluate another, highlighting a possible path of using multi-agent generation to create evaluation data, which is quite meta.

7. Ethical and Safety Concerns: Having multiple agents interact introduces new safety considerations. They might egg each other on into problematic content ("Yes, that conspiracy theory sounds plausible!"). They could jointly produce something no single agent would, by combining their knowledge (for instance, one agent knows half of the steps to build a weapon, another knows the rest; together they produce dangerous instructions that were not in any single training set). Coordinating multiple agents also means more complex failure modes - e.g. they might do things in the environment that cause harm if not properly constrained (imagine two agents controlling smart home devices; miscommunication could lead to the stove being left on and the door locked, etc.). Ensuring alignment in a multi-agent context means making sure each agent individually is aligned and that the emergent outcomes of their collaboration are also aligned with human values. There's also the matter of transparency: multi-agent systems can be harder to interpret because you have to follow a whole conversation, not just a single model's reasoning. However, one could argue they are more interpretable since you can see the thought process in their dialogue. From a governance standpoint, if an LLM-MAS makes a decision (say, denies a loan in a financial setting), who is responsible? Is it the collective or a particular agent? These questions don't have easy answers yet. Research into audit tools for multi-agent systems (like logging all communications and analyzing them for bias or errors post-hoc) will be important. Perhaps constraints can be built in, such as a rule that if agents are planning an action that affects a human, a special "ethics" agent must approve it. This is reminiscent of Asimov's laws, but implementing that in a robust way across an agent swarm is uncharted territory.

8. Domain Adaptation and Generality: Currently, many multi-agent systems are built for a specific domain or even a specific task. A big question: Will there emerge general multi-agent intelligence, or will it always be bespoke? Can we have a plug-and-play system where, if you throw in 10 LLM agents and define roles, they can handle arbitrary cooperative tasks out of the box? Or do we need to fine-tune agents for each new domain (e.g. fine-tune a model to be especially good as a coding reviewer to use in a coding team)? The answer likely depends on the domain complexity. Some roles might benefit from fine-tuning (like a medical advisor agent should probably be fine-tuned on medical knowledge and dialogue

style to be safe and accurate). Others might be general (any planning role might use similar chain- of- thought patterns). Achieving generalist multi- agent teams that can fluidly reconfigure to solve novel problems is a long- term vision. We might need meta- learning: agents that learn how to quickly adopt new roles or learn new coordination protocols. There is also the issue of prompt sensitivity – small differences in how you specify roles or goals can lead to very different outcomes, so making systems that are robust to prompt changes (or automatically find the best prompt for a given objective) is an ongoing challenge.

In summary, while LLM- based multi- agent systems have shown exciting capabilities, they also present a combinatorial increase in complexity compared to single- agent systems. Each challenge is an active research direction. The good news is that some challenges have analogues in human multi- agent contexts (e.g. managing a team, preventing groupthink, establishing trust), so researchers are drawing on decades of organizational science, economics, and multi- agent theory to inform solutions. The interplay of LLM technology with multi- agent theory is where much innovation is happening. For instance, bringing concepts like mechanism design (incentive structures for agents) or formal dialogue games (from philosophy) into LLM MAS design could systematically address some issues. Solving these challenges will likely involve both algorithmic advances (new training methods, architectures) and policy/tooling advances (better evaluation, guardrails, and domain- specific guidelines).

## 7 Conclusion and Outlook

LLM- based multi- agent systems represent a convergence of advances in natural language processing and multi- agent coordination. They offer a new paradigm for AI: systems that are not monolithic, but composed of multiple reasoning entities that communicate and collaborate much like humans do in groups. This survey has outlined the foundations of this paradigm, key developments in the brief period of its rise, and numerous applications from software development and mathematics to social simulation and beyond. We have also discussed the myriad challenges that must be addressed for such systems to be reliable, efficient, and aligned with human goals.

Why does this approach matter? One fundamental advantage is modularity: complex problems can be broken down, with different agents tackling pieces and then integrating the results. This mimics a well- proven strategy in human organizations and could lead to more scalable AI solutions. Another advantage is interpretability: reading a dialogue between agents can be more insightful than staring at the hidden states of a single neural net. In the dialogue, one can pinpoint where a mistake or decision was made, which agent made it, and possibly why (if the agents articulate reasoning). This could greatly aid debugging and trust in AI systems.

Moreover, LLM- based agent societies may be a stepping stone toward more general intelligence. Some cognitive scientists argue that social interaction is a key component of human intelligence – we learn through discussion, argument, teaching, and consensus- building. Perhaps AI agents, by similarly engaging each other, can reach understanding or solve problems that individual AI could not (by leveraging diverse knowledge and reciprocal feedback). There are early glimmers of this: multi- agent discussions have solved tricky problems that stumped single agents  $\widehat{\Xi}$ , and they have invented language- like codes to coordinate in novel environments (e.g., agents developing nicknames for locations in a game to coordinate better – a kind of emergent language within English).

Looking forward, we expect to see:

- Improved Frameworks: Ongoing development of libraries that make it easier to deploy multi-agent systems (with built-in support for common patterns like debate, voting, hierarchy, etc.).

These will likely integrate with popular ML pipelines and allow hybrid human- AI teams (with humans as just another agent in the loop).

- Emergence of Standards: Just as single-model NLP has benchmarks (GLUE, etc.), multi-agent AI will likely converge on some standardized tasks to measure progress: e.g., collaborative games, negotiation tasks, multi-agent reasoning benchmarks. Also, best practices for prompt engineering in multi-agent contexts will be shared (like how to phrase roles for maximal effectiveness, or how to design a good stopping criterion for agent chats).

- Specialized Agent Models: We might see the rise of models pre-trained or fine-tuned specifically for multi-agent interactions. For example, a model could be trained to follow cooperative dialogue rules (never ignore a question from another agent, always provide rationale, etc.), making it inherently better as a team-player in an AI society. Anthropic's work on Constitutional AI is somewhat related: training models to critique and revise outputs could be seen as preparing them for multi-agent self-critiquing scenarios.

- Multi-Agent Learning Algorithms: New algorithms that allow multi-agent systems to learn end-to-end to improve a collective objective. This could involve novel reinforcement learning techniques where the action space is the joint action of all agents (which is huge) or decentralized learning where each agent learns its policy but with shared reward. Solving credit assignment here (which agent's change led to improvement) will be key. Techniques from game theory (like regret minimization, Nash equilibria finding) might inform these algorithms.

- Cross-Disciplinary Insights: We anticipate more collaboration between NLP researchers and those in multi-agent systems (MAS) and human-computer interaction (HCI). From MAS, things like mechanism design, auction theory, and distributed optimization could improve how AI agents reach agreements or allocate tasks. From HCI and social science, understanding how humans coordinate in groups can inspire how to prompt AI agents to do so (for instance, giving agents "emotional" incentives to encourage cooperation, or modeling leadership behaviors).

- Applications Maturing into Products: In the near term, we might see enterprise products that use multi-agent backends: for example, an AI project management assistant that actually consists of a team of sub-agents (scheduling, risk analysis, documentation) working together. Or AI customer service that can handle a complex user request by internally delegating subtasks to different expert agents (billing, tech support, upselling), rather than a single bot trying to do it all. These products will test the real-world viability of LLM-MAS and likely drive requirements for reliability and efficiency.

In conclusion, LLM- based multi- agent systems are a young but rapidly growing field at the intersection of language, reasoning, and interaction. They hold promise for creating AI that is more collaborative, context- aware, and closer to human- like problem solving than ever before. By blending formal multi- agent algorithms with the flexibility of language models, we can build systems that not only think but also talk their way through the world's toughest challenges. Achieving this at scale and safely will require concerted effort, but the progress so far suggests a bright future where AI agents might very well be colleagues and participants in our social and economic systems, not just tools. The journey has only begun, and we hope this survey provides a solid foundation and inspiration for further explorations in this exciting frontier of AI.

## References

1. Guo, T. et al. (2024). Large Language Model based Multi-Agents: A Survey of Progress and Challenges. arXiv:2402.01680 29 172. He, J. et al. (2024). LLM-Based Multi-Agent Systems for Software Engineering: Literature Review, Vision and the Road Ahead. arXiv:2404.04834 16 713. Park, J.S. et al. (2023). Generative Agents: Interactive Simulacra of Human Behavior. arXiv: 2304.03442 12 254. Li, G. et al. (2023). CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society. NeurIPS 2023 355. Wu, Q. et al. (2023). AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation. arXiv:2308.08155 266. Huang, J. et al. (2025). On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents. ICML 2025 65 337. Liang, T. et al. (2024). Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate. EMNLP 2024 18 278. Du, Y. et al. (2023). Improving Factuality and Reasoning in Language Models through Multiagent Debate. arXiv:2305.14325 289. Qian, C. et al. (2023). ChatDev: Communicative Agents for Software Development. arXiv: 2307.07924 47 5010. Shen, S. et al. (2024). Auto-scaling LLM-based multi-agent systems through dynamic integration of agents. Frontiers in AI 43 4411. Lazaridou, A., Peysakhovich, A., & Baroni, M. (2017). Multi-Agent Cooperation and the Emergence of (Natural) Language. ICLR 2017 612. Bakhtin, A. et al. (2022). Human-level play in the game of Diplomacy by combining language models with strategic reasoning. Science, 378(6622) 22 2313. Wang, Z. et al. (2023). Self-collaboration Code Generation via ChatGPT. arXiv:2304.00067 5114. Gao, S. et al. (2023). Large language models empowered agent-based modeling and simulation: a survey and perspectives. Nat. Hum. Behav. (preprint) 21 7215. Hao, Y. et al. (2025). A Multi-LLM-Agent-Based Framework for Economic and Public Policy Analysis. arXiv:2502.16879 34

(Additional references from within the text have been omitted for brevity, but can be found in the inline citations above.)

1 3 7 8 9 10 11 13 14 15 16 18 19 30 31 39 40 41 42 51 52 53 57 70 71 LLM- Based Multi- Agent Systems for Software Engineering: Literature Review, Vision and the Road Ahead https://arxiv.org/html/2404.04834/4

2 4 5 21 37 72 Large language models empowered agent- based modeling and simulation: a survey and perspectives | Humanities and Social Sciences Communications https://www.nature.com/articles/s41599- 024- 03611- 3? error=cookies_not_supported&code=fl17393a- 2751- 49a0- 97fd- 58240747135a

6 [1612.07182] Multi- Agent Cooperation and the Emergence of (Natural) Language https://arxiv.org/abs/1612.07182

12 25 58 [2304.03442] Generative Agents: Interactive Simulacra of Human Behavior https://arxiv.org/abs/2304.03442

17 43 44 45 68 69 Frontiers | Auto- scaling LLM- based multi- agent systems through dynamic integration of agents https://www.frontiersin.org/journals/artificial- intelligence/articles/10.3389/frai.2025.16382/27/full

20 28 46 [2305.14325] Improving Factuality and Reasoning in Language Models through Multiagent Debate https://arxiv.org/abs/2305.14325

22 23 noambrown.github.io https://noambrown.github.io/papers/22- Science- Diplomacy- TR.pdf

24 35 53 [2303.17760] CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society https://arxiv.org/abs/2303.17760

26 [2308.08155] AutoGen: Enabling Next- Gen LLM Applications via Multi- Agent Conversation https://arxiv.org/abs/2308.08155

27 [2305.19118] Encouraging Divergent Thinking in Large Language Models through Multi- Agent Debate https://arxiv.org/abs/2305.19118

29 [2402.01680] Large Language Model based Multi- Agents: A Survey of Progress and Challenges https://arxiv.org/abs/2402.01680

32 33 38 65 66 67 On the Resilience of LLM- Based Multi- Agent Collaboration with Faulty Agents | OpenReview

https://openreview.net/forum?id=okiM54Qftz

34 61 63 [2502.16879] A Multi- LLM- Agent- Based Framework for Economic and Public Policy Analysis https://arxiv.org/abs/2502.16879

36 github.com https://github.com/taichengguo/LLM_MultiAgents_Survey_Papers/raw/main/LLM- MA.png

47 48 49 50 ChatDev: Communicative Agents for Software Development https://arxiv.org/html/2307.07924v5

55 HuggingGPT - Solving AI Tasks with ChatGPT - Learn Prompt https://www.learnprompt.pro/docs/llm- agents/hugginggpt/

56 [PDF] Solving AI Tasks with ChatGPT and its Friends in Hugging Face - arXiv https://arxiv.org/pdf/2303.17580

59 LLM Economist: Large Population Models and Mechanism Design ... https://arxiv.org/abs/2507.15815

60 [PDF] FOMC In Silico: A Multi- Agent System for Monetary Policy Decision ... https://www2. gwu.edu/\~forcpgm/2025- 005. pdf

62 64 GitHub - taichengguo/LLM_MultiAgents_Survey_Papers: Large Language Model based Multi- Agents: A Survey of Progress and Challenges https://github.com/taichengguo/LLM_MultiAgents_Survey_Papers