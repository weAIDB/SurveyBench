# Scientific Large Language Models

Abstract: Large Language Models (LLMs) have emerged as a transformative class of AI systems capable of understanding and generating human- like text. Scientific LLMs refer to large language models specialized or applied in scientific domains - from biology and medicine to physics and mathematics - with the goal of assisting in literature review, hypothesis generation, problem- solving, and other research tasks. This survey provides a comprehensive overview of scientific LLMs, balancing technical depth with clarity. We begin by formally defining key concepts such as LLMs and foundation models, and outlining the transformer- based architecture that underpins modern LLMs. A historical timeline of major milestones is presented, tracing the evolution from early language models and the 2017 Transformer breakthrough to the latest developments by 2025. We review the foundational methods of self- supervised pre- training and fine- tuning, as well as recent techniques (instruction tuning, reinforcement learning from human feedback, retrieval augmentation, etc.) that enhance LLM performance and alignment. We then examine scientific domain adaptations - models like SciBERT, BioGPT, Galactica, and others - and how they are trained on scientific literature to acquire domain- specific knowledge. Next, we survey a broad range of applications across scientific subfields: literature analysis and summarization, scientific question answering, mathematical reasoning, drug discovery and materials science, and research writing assistance, highlighting both successes and current limitations. Open challenges are discussed, including issues of factual accuracy (hallucinations), reasoning gaps, interpretability, data bias, and the need for incorporating external tools or domain knowledge. Throughout, we illustrate concepts with examples (including model outputs and diagrams) and emphasize recent developments up to 2025. We conclude with an outlook on future directions for scientific LLMs, envisioning their potential to accelerate scientific discovery while underscoring the importance of responsible and informed use.

## 1 Introduction

The rapid advancement of Large Language Models (LLMs) in recent years is reshaping how we interact with information and solve complex problems. LLMs are deep neural network models with extremely large numbers of parameters (often billions) trained on massive text corpora, enabling them to generate fluent text and perform a wide range of language tasks 1 2 . Crucially, these models are trained with self- supervised learning: learning from raw text by predicting masked or future tokens, rather than relying on manual labels 3 . This approach unlocks the use of essentially unlimited unlabeled data (such as the entire contents of the web or scientific literature) to imbue the model with broad knowledge and linguistic patterns.

Scientific Large Language Models refer to LLMs that are tailored for scientific domains or tasks. The motivation for developing scientific LLMs arises from the information overload in science: the volume of research publications and data is growing exponentially, making it challenging for human researchers to stay abreast 4 5 . LLMs offer a compelling new interface for organizing and querying scientific knowledge 6 . By training on scientific papers, textbooks, and technical content, an LLM can store, combine, and reason about scientific knowledge 7 . For example, an LLM might help summarize the state- of- the- art in a field, answer technical questions, draft research proposals or code, and even assist in discovering insights across papers. Early demonstrations like Galactica (a 120- billion- parameter model for science) showed the promise of a "knowledge interface" for science, outperforming general models on tasks like solving LaTeX equations and answering science questions 8 .

At the same time, applying LLMs in science raises unique challenges. Scientific writing often includes specialized jargon, formulas, and data that are rare or absent in general web text. Scientific questions may require precise reasoning, factual accuracy, and even performing calculations or logical derivations – areas where generic LLMs can struggle 9. Furthermore, while LLMs can generate text that sounds authoritative, they may produce incorrect statements or even fabricate plausible- looking references (a form of hallucination) 10 11. Such issues pose risks in a scientific context, where trustworthiness and verifiability of information are paramount. As a result, a major theme of this survey is how recent research has adapted LLMs to the scientific domain and attempted to address these concerns.

In the following, we provide background on the fundamental architecture and training regime of LLMs, then trace the historical development leading to today's powerful models. We discuss how foundation models (general- purpose pretrained LLMs) are adapted into scientific LLMs via domain- specific training or fine- tuning. We examine a variety of applications in science and the techniques enabling them – from summarizing literature and answering scientific questions to solving equations with step- by- step reasoning. We also highlight open challenges and recent efforts to overcome them, such as improving factual accuracy and integrating external tools. The aim is to give a broad yet detailed understanding of the state of scientific LLMs circa 2025, accessible to both newcomers and experts, and to point toward future directions in this rapidly evolving intersection of AI and scientific research.

## 2 Background: Large Language Models and Foundation Models

Definition and Key Concepts: A Large Language Model (LLM) is essentially a language model (a probabilistic model of sequences of words) distinguished by its scale in terms of model size and training data 1. There is no strict threshold for "large", but the term generally applies to models with hundreds of millions to hundreds of billions of parameters, trained on extremely large text corpora. For example, OpenAI's GPT- 3 model contains 175 billion parameters and was trained on hundreds of billions of words of Internet text, making it a quintessential LLM. The "large" in LLM refers to both the number of parameters in the neural network and the vast amount of training data consumed 12. The training process is typically self- supervised, meaning the model learns to predict missing pieces of text (such as the next word in a sentence) from unlabeled data 3. By doing so at scale, LLMs acquire statistical knowledge of language structure (syntax), word meanings and relations (semantics), and even factual or commonsense associations present in the data 13.

Most state- of- the- art LLMs are built on the Transformer architecture 14. The Transformer, introduced by Vaswani et al. in 2017, is a deep neural network architecture based on self- attention mechanisms, which enable the model to weigh the relevance of different words in a sequence to each other 15. This was a breakthrough in overcoming the limitations of earlier recurrent neural networks (RNNs) and Long Short- Term Memory (LSTM) networks, which struggled with long- range dependencies in text 16 17. Transformers process sentences (or even whole documents) in parallel and can capture relationships between words regardless of their distance apart. An LLM typically consists of a stack of transformer layers (either decoder- only for autoregressive models like GPT, or encoder- decoder for sequence- to- sequence models like T5 and BART). Each layer has multiple "heads" of self- attention that learn different aspects of language context, and feed- forward networks that transform representations. This architecture scales efficiently with data and model size, and has been the backbone of essentially all modern large language models.

Foundation Models: In recent AI literature, LLMs are often discussed in the context of foundation models – a term popularized by the Stanford Center for Research on Foundation Models (CRFM) 18. A foundation model is any model that is trained on broad data at scale (usually via self- supervised learning) and can be adapted to a wide range of downstream tasks 19. Large language models are a

prime example of foundation models for natural language. By first "pretraining" an LLM on general text (e.g., crawling the web, books, Wikipedia, etc.), it develops a broad foundation of linguistic and factual knowledge. This pretrained model can then be fine- tuned on specific tasks or domains with comparatively little additional data to achieve high performance 20. The fine- tuning may be supervised (using labeled examples for a task like question- answering or text summarization) or through other means like instruction tuning, where the model is trained to follow human- provided instructions.

A critical aspect of LLMs is their ability to generalize and perform tasks in a zero- shot or few- shot setting 21. Because an LLM's training data is so broad, users found that by phrasing a task as a textual prompt, the model could often perform it without explicit task- specific training. For instance, GPT- 3 demonstrated strong few- shot learning: given a few examples of a new task in the prompt, it could continue with the correct output for a new input, effectively performing translation, Q&A, or arithmetic on the fly. This emergent capability made LLMs extremely versatile, turning them into general problem- solvers for text.

Two- Stage Training Pipeline: Most LLMs undergo a two- stage training process 22 :

1. Pre-training: In the first stage, the model learns from a massive generic corpus via self-supervised objectives. For decoder-only (autoregressive) LLMs, the objective is usually to predict the next token in the sequence (as in GPT). For encoder-decoder models, a common objective is masked language modeling (as in BERT, where some words are masked and the model must predict them) or a sequence-to-sequence variation (as in T5 where a corrupted input is transformed into a clean output). Pre-training imparts general language understanding and some world knowledge. This stage uses unlabeled data – no human annotation is needed – which is crucial because it enables using orders of magnitude more data than would be feasible with labeled datasets. By the end of pre-training, an LLM has learned to model the distribution of natural language, capturing rich patterns of word usage and some latent representation of meaning.

2. Fine-tuning: In the second stage, the pretrained model is adapted to a more specific task or domain. Fine-tuning typically uses a smaller, task-specific labeled dataset 22. For example, to create a biomedical language model, one might fine-tune a general LLM on a corpus of medical texts and Q&A pairs. Fine-tuning adjusts the model's parameters to better suit the target task, often dramatically improving performance on that task. In practice, fine-tuning can also mean instruction tuning – training the model to better follow human instructions by providing many examples of prompts and desired responses. Another increasingly important fine-tuning method is RLHF (Reinforcement Learning from Human Feedback), where the model's outputs are refined using human preference judgments as a reward signal (this was used to align models like OpenAI's ChatGPT with what users find helpful and truthful 23). The combination of pretraining and fine-tuning yields a model that is both general-purpose (due to broad pretraining) and specialized (due to task/domain adaptation), achieving high accuracy on complex tasks 24.

In the case of scientific LLMs, this pipeline may be applied with a twist: the pre- training itself might be done on a corpus of scientific texts instead of general web text. Alternatively, one starts with a general pretrained LLM and then fine- tunes it on scientific content. We will see examples of both approaches (e.g., SciBERT vs. fine- tuned GPT models). In either case, the end goal is a model that understands scientific terminology, can read and generate scientific articles, and reason about scientific problems.

Capabilities and Limitations: A pretrained LLM encapsulates a vast amount of information in its parameters – including linguistic nuances and factual knowledge present in training data. They have demonstrated capabilities not just in language generation (writing coherent paragraphs, answering

questions, summarizing documents) but also in tasks like writing programming code, solving logic puzzles, and more 25 . This versatility stems from the rich pattern recognition the model has achieved: it effectively compresses the statistical structure of its training texts and can generalize to new prompts by extrapolating those patterns 26 . However, it is crucial to note (especially in the context of scientific applications) that an LLM is not a database of verified facts. As researchers have pointed out, "ChatGPT is fundamentally not an information- processing tool, but a language- processing tool. It mimics the texts - not necessarily the substantive content - found in its information base" 10 . In practice, this means an LLM might generate a fluent- sounding answer that follows the form of training sentences without guaranteeing the truth of the statements. The tendency of LLMs to produce hallucinations (incorrect statements not grounded in the input or real facts) is a well- known limitation 27 . In scientific domains, such hallucinations can be misleading or even dangerous - e.g. a model might confidently cite a nonexistent research paper or propose an incorrect chemical formula.

Therefore, a significant part of developing scientific LLMs involves techniques to mitigate these issues: providing the model with contextual grounding (e.g., retrieving relevant facts or papers to condition the answer), adding constraints or verification steps (like using calculators for numerical accuracy, or crosschecking citations), and evaluation mechanisms to detect and avoid errors. We will delve into these challenges and solutions after first reviewing how we arrived at the current generation of LLMs.

## 3 Historical Timeline of Major Milestones

The concept of language models has a long history in computational linguistics, but the age of large language models as we define them took off in the late 2010s. Below is a timeline of key milestones leading to the development of scientific LLMs:

- 1990s 
- 2000s (Statistical Era): Early language modeling focused on n-gram models (e.g., bigrams, trigrams) and statistical machine translation. For example, in 2001, a Kneser-Ney smoothed 5-gram model trained on 300 million words set state-of-the-art perplexity on benchmarks of that time 28 . These models, however, had limited ability to handle long-range context or complex structure 29 . The late 2000s saw the rise of neural language models using feed-forward or recurrent neural networks, which learned word embeddings (continuous vector representations of words) enabling generalization beyond exact word matches 30 . Notably, by 2013 Mikolov's Word2Vec showed how to efficiently learn embeddings from large corpora 30 . Still, neural networks struggled with long sentences due to the vanishing gradient problem in RNNs.

- 2017 
- Transformer Architecture: A watershed moment came with the paper "Attention Is All You Need" (Vaswani et al., 2017), introducing the Transformer model 31 . The transformer's self-attention mechanism allowed models to capture long-distance relationships in text more effectively than RNNs. Google's release of the Transformer architecture in 2017 provided the foundation for scaling up language models in depth (number of layers) and width (model parameters) without the bottlenecks of sequential RNN processing.

- 2018 
- Early LLMs (ELMo, GPT-1, BERT): Researchers quickly leveraged transformers for pretraining on large text corpora. The concept of contextual embeddings emerged 
- unlike static word vectors, contextual models produce representations that depend on the sentence context. The ELMo model (Peters et al., 2018) used LSTMs but hinted at the power of deep pretraining on language modeling. In mid-2018, OpenAI introduced GPT-1 (Generative Pretrained Transformer) with 117 million parameters, demonstrating that a single transformer decoder, pretrained on Books corpus, can be fine-tuned to various NLP tasks with excellent results 32 . Later in 2018,

Google released BERT (Bidirectional Encoder Representations from Transformers), a 340- million parameter bi- directional transformer model pretrained on Wikipedia and books  $^{33}$ . BERT achieved state- of- the- art on many NLP benchmarks via fine- tuning, popularizing the "pretrain- then- fine- tune" paradigm. It was also notable for being bidirectional (using masked token prediction to allow the model to leverage both left and right context) as opposed to GPT's unidirectional approach. By the end of 2018, ULMFiT and OpenAI GPT and BERT had firmly established that large pretrained transformers could serve as universal language features, kicking off the modern LLM revolution.

- 2019 
- Scaling and Domain-Specific Models: The year 2019 saw rapid progress: OpenAI's GPT-2 was introduced with 1.5 billion parameters, trained on 8 million web pages. GPT-2 shocked the community by generating surprisingly coherent and contextually relevant text, to the point OpenAI initially withheld the full model out of concern for misuse. Also in 2019, researchers began specializing models for scientific text. SciBERT (Beltagy et al., 2019) was a BERT-based model trained on 1.14 million scientific papers from Semantic Scholar, covering multiple scientific domains  $^{34}$ . By training on scientific literature, SciBERT provided vocabulary and embeddings better suited to scientific terminology than general BERT, and indeed it outperformed BERT on various scientific NLP tasks like sequence tagging and classification  $^{35}$ . Similarly, BioBERT (Lee et al., 2019) was released as a BERT model further pretrained on biomedical research articles, yielding improved performance on biomedical named entity recognition, question answering, and relation extraction  $^{36}$ . These were among the first domain-specific LLMs, confirming that specializing the massive knowledge of an LLM to a particular field leads to tangible gains. 2019 also saw models exploring larger contexts and different pretraining objectives (e.g., XLNet introduced a permutation-based language modeling objective).

- 2020 
- The 100B+ Scale (GPT-3 and beyond): In 2020, the scale of LLMs reached new heights. OpenAI's GPT-3 (Brown et al., 2020) was a landmark with 175 billion parameters, trained on ~500 billion tokens of Internet text. GPT-3 demonstrated remarkable few-shot learning abilities - without fine-tuning, it could perform tasks from translation to arithmetic by simple prompting  $^{21}$ . Its size and training data appeared to give it enough world knowledge to respond to a wide array of queries, including some scientific questions, purely from what it had seen in training. Another advancement was T5 (Text-To-Text Transfer Transformer) by Google, with 11 billion parameters, which treated every task (translation, summarization, Q&A, etc.) as text-to-text. While GPT-3 grabbed headlines for generative prowess, other specialized models were in development: e.g., BioMegatron (an NVIDIA Megatron-based large model for biomedical text) and jurisdiction-specific models like LEGAL-BERT for legal text, indicating the trend of applying LLMs in various domains. COVID-19 Open Research Dataset (CORD-19) also spurred the creation of models to help mine scientific papers for insights during the pandemic.

- 2021 
- Multimodality and Refined Training: By 2021, research focused not just on bigger models but also on efficient training and alignment. DeepMind's Gopher (280B params) and Google's Switch-Transformer (a sparse Mixture-of-Experts model effectively having trillions of parameters but fewer active per token) explored new scaling strategies. Importantly for science, models began tackling not just text but integrating other modalities. For instance, AlphaFold 2 (DeepMind, 2021) used transformer architectures to solve protein 3D structure prediction, although it's not a language model per se, it demonstrated transformers' power in scientific domains. OpenAI introduced Codex (2021), a GPT-3 derivative fine-tuned on source code, which could be seen as a "scientific" model in the programming domain (later powering Github's Copilot for code generation). Stanford's Foundation Model report (2021) formally articulated the concept of foundation models and discussed their opportunities and risks in domains like healthcare and science  $^{18}$ , signaling the awareness of their cross-cutting impact.

- 2022 – Explosion of LLMs & Scientific Specialization: 2022 was a pivotal year with a flurry of new LLMs and specifically several aimed at scientific knowledge:

- Open Source and Global Efforts: The BigScience project released BLOOM, a 176B-parameter multilingual LLM, as a fully open-model for research. Meta AI released OPT-175B and allowed researchers access, indicating a move toward transparency. Many smaller (but still large) models like EleutherAI's GPT-NeoX (20B) also became available. While these were general models, their availability meant researchers could fine-tune them for science (and indeed many did so for biomedical QA, etc.).

- Chinchilla & Efficiency: DeepMind's Chinchilla paper (March 2022) argued that given a fixed compute budget, model size should be balanced with training data. Chinchilla (70B params trained on  $4 \times$  more data than GPT-3) outperformed larger models like GPT-3 and Gopher on many tasks by virtue of having seen more text 38. This finding is relevant to scientific LLMs: it suggests that training on more scientific text can be as important as raw parameter count.

- Scientific LLMs: In mid-2022, Google Research unveiled Minerva, a specialized LLM for mathematical and scientific problem-solving 39. Minerva was built by taking Google's general PaLM model (which had 540B parameters) and further training it on 118GB of scientific papers and webpages containing mathematical notation 40. By preserving LaTeX formulas in the data, Minerva learned to parse and produce complex equations. It was able to solve quantitative reasoning problems – for example, questions from math competitions and physics, which typically require step-by-step reasoning and correct algebraic manipulation. Minerva achieved state-of-the-art results on STEM benchmarks (like the MATH dataset, where it solved about  $50\%$  of problems, a huge jump from prior models) 41, 42. Significantly, Minerva demonstrated techniques like chain-of-thought prompting and majority voting to improve accuracy: it was prompted to generate step-by-step solutions and then multiple solution attempts were run and the most common answer taken, reducing errors 43, 44.

- Galactica: In November 2022, Meta AI released Galactica, a large language model explicitly for scientific knowledge. 45. Galactica was trained on 106 billion tokens of scientific text, including papers, textbooks, encyclopedias, knowledge bases, etc. 46, and offered in various sizes up to 120B parameters 47. It was designed to assist scientists with tasks like literature review, question answering, citation prediction, writing code for experiments, and even solving chemistry or biology problems 48. Notably, Galactica could generate formatted scientific text – for instance, given a prompt it might output a Wikipedia-style article with references, or a LaTeX-formatted derivation. On specialized evaluations, Galactica showed strong performance: e.g., it exceeded GPT-3's accuracy on technical questions involving LaTeX equations (68% vs 49% on a set of probes) and set new state-of-the-art on tasks like PubMedQA (biomedical Q&A) and MedMCQA (medical exam questions) 8. This indicated that domain-focused training data gave it an edge on scientific tasks. Meta released Galactica's model (under a non-commercial license) and a public demo. However, the demo was withdrawn after a few days due to concerns over hallucinations – users found the model could output authoritative-sounding but incorrect or misleading scientific content (including bogus citations) if prompted incautiously 49, 50. This incident underscored the need for careful deployment and perhaps additional alignment for scientific LLMs.

![](images/57a231d958c3c3b77a7865c41b5b58c63434abf8b5dc1f8d3db1eb17ef0659f3.jpg)

Trend in training compute for AI models (2010- 2024). The amount of compute used to train leading models has followed an exponential growth (roughly  $4 - 5x$  increase per year), as illustrated by the upward curves. Notably, "Frontier LLMs" (bottom- left) saw a surge after the introduction of the Transformer in 2017, with models like GPT- 3 (2020) using orders of magnitude more compute than earlier models 51. This scaling of compute and model size has been a driving force behind the improved capabilities of LLMs.

- 2023 
- The ChatGPT Era and Democratization: Late 2022 and 2023 saw LLMs enter the mainstream consciousness with the release of ChatGPT (Nov 2022) 
- a conversational chatbot based on GPT-3.5 that was fine-tuned with human feedback to follow instructions and provide helpful answers. The public's enthusiastic adoption of ChatGPT demonstrated the practicality of LLMs for everyday queries, but researchers also began exploring its use for scientific support (like explaining concepts or brainstorming research ideas). OpenAI's subsequent GPT-4 (Mar 2023) further improved capability, reportedly passing advanced exams in law and medicine and approaching expert-level performance on some scientific reasoning tasks (e.g., high scores in biology Olympiad questions). Importantly, GPT-4 introduced a multi-modal capacity (processing image inputs in addition to text), which has implications for science 
- for instance, it can analyze charts or diagrams from papers. Meanwhile, in the open-source sphere, Meta's LLaMA models (Feb 2023) provided smaller-scale LLMs (7B to 65B parameters) that achieved strong performance and were released to researchers. Though LLaMA itself was trained on general data, its release sparked a wave of fine-tuned domain-specific models by the community. For example, MedAlpaca (based on LLaMA-7B/13B) was fine-tuned on medical question-answering data to serve as a medical assistant 52; ChemAlpaca and others soon followed. This trend significantly democratized scientific LLMs: instead of needing a  $100\mathrm{B}+$  model from a big tech lab, researchers could fine-tune a 7B-13B model on their domain data with relatively modest compute and still get useful performance. By mid-2023, we also saw specialized instruction-tuned models like SciTune or BioGPT (Microsoft) being openly available 53 54, often built on top of open base models.

- 2024 and beyond 
- Toward Next-Generation LLMs: As of 2024-2025, several trajectories are clear. One is the pursuit of longer context 
- new models can handle much longer input lengths (100k tokens or more), which could allow an LLM to ingest an entire research paper or even a collection of papers at once, greatly enhancing literature analysis. Another is better tool integration: e.g., connecting LLMs to scientific databases, calculators (for exact arithmetic), or

even lab equipment control. This is seen as essential to mitigate hallucinations by letting the model retrieve facts rather than guess. Efforts in 2024 also include more robust evaluation benchmarks for scientific LLMs to quantify their reasoning and factual accuracy in specialized domains. We are also witnessing initial multi- modal scientific models - systems that can handle text plus other data like chemical structures or genomic sequences. The community is actively addressing ethical and policy questions around the use of LLMs in research (for instance, how to credit AI assistance in writing a paper, or dealing with potential biases introduced by training data). The pace of progress remains very high, with new techniques and models appearing frequently.

This historical context sets the stage for our deeper dive into how LLMs work and how they are adapted and used for science. In the next sections, we examine the architecture and training in more detail, then explore the specific innovations that make scientific LLMs effective for their intended tasks.

## 4 Architecture and Training Paradigms of LLMs

Transformer Architecture Recap: Modern LLMs are mostly based on the Transformer, so it is worth understanding its components. At a high level, a Transformer processes text in a series of layers, each of which has two main sublayers: (1) a self- attention mechanism, and (2) a position- wise feed- forward network. The model also maintains positional encodings to keep track of word order (since self- attention itself is order- agnostic). In a decoder- only transformer (like GPT), each layer's self- attention is masked to prevent a given position from "seeing" future positions (this enables left- to- right text generation). In an encoder- decoder transformer (like BERT or T5), the encoder layers attend bi- directionally over the input, and the decoder layers attend to the encoder's outputs as well as preceding decoder tokens (for sequence- to- sequence generation).

To illustrate, the diagram below shows a classic transformer architecture with an encoder (left) and decoder (right) stack, as originally proposed by Vaswani et al. (2017):

![](images/45e43e41e7c60db4e5d8abc5c824fb1f31854879a3046ea81b365be3cbea2201.jpg)

Diagram of the Transformer model architecture (encoder on left, decoder on right). Each encoder layer (left stack) consists of a self- attention sublayer followed by a feed- forward sublayer (with residual connections and layer normalization at each step). The decoder layers (right stack) have an additional cross- attention sublayer that attends to encoder outputs, and use masked self- attention to ensure autoregressive generation. This architecture allows modeling of long- range dependencies and efficient parallel computation, enabling the training of very large language models. 55 56

The self- attention mechanism computes attention weights for each word with respect to every other word in the sequence, effectively learning which words are related or important to one another for a

given task. For example, in a scientific sentence: "The CRISPR- Cas9 system, discovered in 2012, allows precise genome editing", an attention head might learn to strongly connect "CRISPR- Cas9 system" with "genome editing" across the clause boundary, understanding that the former enables the latter. With multiple layers and heads, transformers build up complex representations - some heads might capture syntactic relations (e.g., which noun a pronoun refers to), others semantic or factual associations.

Scaling Laws: One empirical finding that guided the growth of LLMs is that larger models (more parameters) and larger training datasets lead to predictable improvements in performance, following a power- law scaling trend (until saturating) 51. Kaplan et al. (2020) and others showed that as you scale compute (by increasing model size and data), language modeling performance (perplexity) keeps improving and new capabilities emerge. This is why the field moved from millions of parameters (in 2018) to tens of billions (by 2020) and further. The trend graph above illustrates this growth in compute for training AI models, rising roughly  $10x$  every year or two (a much faster trajectory than Moore's Law) 51. Importantly, the scaling behavior also applies to scientific text: a larger model can absorb more scientific facts and jargon. However, scaling alone is not enough - the quality and relevance of data also matter, especially for scientific tasks. A model might have seen many general texts but few detailed chemistry papers; adding those to the training mix could greatly enhance its chemistry knowledge. This led to efforts like curated scientific corpora for pretraining (e.g., arXiv, PubMed, USPTO patents for chemistry, etc.).

Pretraining Data for Scientific LLMs: Scientific LLMs use specialized corpora. For instance, Galactica's 106B- token training set included: millions of research paper abstracts and full texts, textbooks and lecture notes, scientific websites (like Wikipedia's science articles), knowledge bases (e.g., protein databases), and even "semi- structured" data like tables or chemical formulas converted to text 46. The inclusion of such data ensures the model encounters domain- specific terminology ("Higgs boson", "gene expression", "Naive Bayes classifier"), formal notations (mathematical symbols, chemical formulas), and the typical discourse of scientific writing (citations, statements of evidence). One challenge is tokenizing these diverse inputs - standard subword tokenizers used in LLMs might break a DNA sequence or an equation oddly. Researchers therefore augment tokenizers to handle scientific notation (Galactica, for example, introduced special tokens for citation references [START_REF] etc., and for molecule strings or protein sequences so that they aren't fragmented arbitrarily 46).

Another example, SciBERT, didn't create a new tokenizer but did build a vocabulary based on the scientific corpus frequency, resulting in subword units more suited to scientific terms (e.g., "organizo" might be a token if "organizational" is common, etc.), which improved downstream task performance 34. The principle is that an LLM's knowledge is only as good as the data seen; thus, curating high- quality, diverse scientific data is a cornerstone of training scientific LLMs.

Fine- Tuning and Instruction Tuning: After pretraining, scientific LLMs are often fine- tuned on task- specific data. Fine- tuning can be supervised: e.g., using question- answer pairs from biomedical exams to tune the model to answer like an expert, or using a set of scientific papers with summaries to teach the model summarization. An interesting development is fine- tuning models to follow natural language instructions. Instead of formulating every task as "<input, output>" pairs, researchers created datasets of prompts and ideal responses (often written by humans) to train models to follow arbitrary instructions. For instance, one instruction might be "Summarize the following paper in one paragraph... [paper text] ...", paired with a human- written summary. Models like OpenAI's InstructGPT (on which ChatGPT was based) were fine- tuned on a wide variety of instructions, which vastly improved their usability. For scientific LLMs, a similar approach is taken: e.g., ChatGPT itself, while not exclusively trained on science, has been fine- tuned with human feedback and can often produce decent answers to scientific queries. The open source community has also created instruction- tuned versions of scientific

models; for example, SciGPT or BioAlpaca where they fine- tune a base model on instructions specific to scientific tasks (like "Explain this concept", "Classify this medical case", etc.). The result of instruction tuning is that the LLM becomes better at understanding a user's request (which may be expressed in plain English) and producing a helpful, targeted response, rather than a generic completion of text.

Reinforcement Learning from Human Feedback (RLHF): One of the most impactful training paradigms for aligning LLM behavior with human expectations is RLHF. The process in brief: after a model is pretrained (and possibly instruction- tuned), developers generate many sample outputs for various prompts, then have human annotators rank these outputs from best to worst. A separate "reward model" is trained on these rankings to predict a score for a given output. Finally, the LLM is further optimized using reinforcement learning (often Proximal Policy Optimization) to maximize the reward model's score, thereby making its outputs more preferable to humans. This was used to train ChatGPT to be more polite, correct, and refuse inappropriate requests. In scientific context, one can imagine using RLHF to discourage the model from making up answers when it is unsure. For example, human evaluators can prefer answers that say "I don't know" or provide a reference, over answers that state an unsupported claim. By incorporating those preferences, the scientific LLM can learn to be more cautious and evidence- seeking. Indeed, OpenAI noted a reduction in hallucination frequency from GPT- 3.5 to GPT- 4 partly due to more fine- tuning and feedback 27. However, RLHF is not a panacea; it might make the model too hesitant or stick to "safer" but less informative answers sometimes. It's a tuning process that requires careful design of the prompt distribution and reward.

Retrieval- Augmented Generation (RAG): A significant innovation for LLMs (especially valuable in scientific use- cases) is integrating them with information retrieval systems. The idea is to give the model access to an external knowledge base or search engine: when a query comes, first retrieve relevant documents (e.g., relevant papers or database entries), provide those as additional context to the model, and then have the model generate an answer using both its parametric knowledge and the retrieved evidence. This can greatly improve factual accuracy and allow up- to- date information. In science, where new discoveries are continually published, a model frozen in 2023 might not know a 2024 discovery unless we provide it. Systems like Atlas (Izacard et al., 2022) or OpenAI's WebGPT experiments have shown that LLMs augmented with retrieval can produce more truthful and specific answers, with the ability to cite sources. We see this approach in practice in tools like the Elikit.org assistant for literature review, which uses an LLM to summarize and compare retrieved research papers. Another example is the SciQA benchmark which encourages models that can query a corpus of science texts to answer questions 57.

Tool Use and Multi- step Reasoning: An exciting frontier is making LLMs interact with tools or perform multi- step computations. In scientific problem- solving, a model might need to do arithmetic or algebra, plot a graph, or call an API for a chemistry database. Researchers have begun training LLMs to output not just the final answer but a kind of program or plan that can be executed by external tools (this is related to the idea of an "LLM agent"). For example, a model might decide: "To answer this question about statistical significance, I should perform a t- test on the given data." Then it can produce pseudo- code, which a tool executes, and the results are fed back into the model. There's active research in this area, such as IBM's ScienceQA or projects on integrating Python interpreters with the model for better math solving 58. While this goes beyond pure language modeling, it is highly relevant to making LLMs more reliable for scientific tasks that require calculation or access to dynamic data.

Emergent Abilities and Limits: As LLMs were scaled up, people observed some abilities that seemingly "emerge" around a certain scale - for instance, GPT- 3's few- shot learning is much more pronounced than in smaller models. In science, one could ask: do models suddenly get good at some task (like solving a type of physics problem) only beyond a parameter threshold? Research by Wei et al. (2022)

documented some such jumps (they coined “emergent abilities”). However, not every task magically appears; some require explicit training or new techniques regardless of scale. For example, while a large model might memorize a lot of trivia, it won’t necessarily master logical deduction without guided training. Scientific reasoning often needs more than next- word prediction; it may benefit from approaches like chain- of- thought prompting, where we prompt the model to produce a step- by- step solution rather than a direct answer. By including few examples of reasoning steps in the prompt, models like GPT- 4 or PaLM showed much better performance on math and science questions 43. This indicates the way we use the model (prompt engineering) can unlock latent capabilities. We will see an example of chain- of- thought in the next section on scientific applications.

In summary, the architecture and training paradigm of LLMs provide a flexible foundation. By pretraining a transformer on huge data and then fine- tuning or prompting it in clever ways, we obtain a system with a surprising degree of competence in language understanding and generation. The rest of this survey will examine how these systems are applied and further adapted in scientific contexts, what they can do, and where they fall short.

## 5 Scientific Domain Adaptations of LLMs

One straightforward approach to creating a scientific LLM is domain adaptation: take a general LLM and continue training it on scientific text. We’ve already mentioned SciBERT and BioBERT as early successes in this vein. Here we delve a bit deeper into notable domain- specific models and techniques for adapting LLMs to science.

- SciBERT (2019): Built on the BERT architecture, SciBERT was pretrained from scratch on a corpus of 1.14 million scientific papers across disciplines 34. Instead of using BERT’s original WordPiece vocabulary (which was derived from Wikipedia and Books text), SciBERT learned a new vocabulary from the scientific corpus. This meant common science terms got their own tokens rather than being split (for example, “COVID-19” might be a single token rather than six tokens C O V I D - 19). The result was improved representations for scientific text. In evaluations, SciBERT outperformed baseline BERT on tasks like extracting chemical named entities, classifying scientific sentences by their role, and parsing syntactic structure in scientific papers 35. SciBERT essentially demonstrated that continued pretraining on domain data can yield a better model for that domain even if the architecture remains the same. One might say SciBERT is a “BERT that speaks science”.

- BioBERT and ClinicalBERT (2019-2020): These were similar efforts focusing on biomedical and clinical text respectively. BioBERT (Lee et al.) was BERT-base further trained on PubMed abstracts and PMC full-text articles; it showed significant gains on biomedical Q&A and NER tasks 36. ClinicalBERT (Huang et al.) was fine-tuned on clinical notes (EHI data) to better handle the idiosyncratic language used by clinicians. These models filled an important gap: while a general LLM might know common medical facts, BioBERT could better disambiguate terms (e.g., “HER2” in oncology context or “ECG” in cardiology) and capture domain-specific context that general models miss.

- Galactica (2022): As introduced, Galactica was a high-profile attempt at a single model for a wide array of scientific tasks 48. Under the hood, Galactica’s architecture was a standard transformer decoder (similar to Meta’s earlier LLMs like OPT) but trained on a curated scientific dataset. One interesting aspect was that Galactica included not just plain text but also structured data: for example, it was trained on amino acid sequences (as text) and their properties, LaTeX equations, and citation graphs. It treated everything as a sequence of tokens, which means it could, in

principle, output a protein sequence with a desired property or predict references for a given snippet of text (by generating a citation token sequence). This approach hints at multi- modal within text capabilities - the model doesn't see an image or molecule directly, but if you describe it in text or a suitable linear notation, the model can work with it. The performance claims of Galactica were strong (e.g., outperforming general LLMs of similar size on science QA) 8, but the concerns about misinformation led to some backlash. The episode highlighted that specialized pretraining can amplify both the strengths and weaknesses: the model became very fluent in scientific rhetoric (sounding authoritative, producing references), which is great when it's correct, but potentially dangerous when it is wrong, as it may give a false sense of validity. The Galactica model weights are available and researchers continue to tinker with them, possibly combining with better filtering or feedback to reduce flaws.

- BioGPT (2022): Microsoft Research introduced BioGPT, a domain-specific generative model for biomedical text 53. BioGPT was based on the GPT-2 architecture (a transformer decoder with ~345M parameters) and trained on 15 million PubMed abstracts 54. Despite being much smaller than GPT-3, BioGPT achieved strong results on tasks like biomedical literature question answering and medical term extraction 59. This is a testament to how targeted training data can make a relatively small LLM competitive in a niche domain. For example, on a task of extracting drug-disease relationships from text, BioGPT could generate relevant outputs after being fine-tuned, sometimes better than using a generic GPT-2 which lacks domain knowledge. BioGPT was released openly, which is beneficial for the biomedical NLP community.

- Codex and Scientific Coding Assistants: A lot of science involves programming (data analysis, simulation). OpenAI's Codex (2021) and its successors (e.g., GitHub Copilot powered by GPT-4 in 2023) have become inadvertent scientific tools, as many scientists use them to write code in Python, R, MATLAB, etc. While not trained specifically on scientific code, these models trained on GitHub code have learned many scientific computing idioms (like using libraries numpy, scipy, or performing data manipulation). There are also efforts for domain-specific coding LMs, e.g., AlphaCode by DeepMind (which was more for competitive programming) and Polycoder (open-source). For scientific computing, having a model that understands both natural language and can produce correct code can greatly speed up tasks (like "simulate this differential equation" or "plot the data from this file with error bars"). As LLMs get integrated into tools like Jupyter notebooks, they act as a bridge between a scientist's intent (in plain English) and the actual code to execute analysis. This is somewhat tangential to language modeling, but it's a notable application domain for "LLMs in science".

- Domain-Specific Fine-Tunes using LLaMA (2023): As mentioned, the availability of Meta's LLaMA model led to many fine-tuned variants. For example:

- MedAlpaca (2023) 
- fine-tuned LLaMA on medical Q&A datasets, yielding a conversational doctor-like model 52. 
- ChemLLaMA 
- fine-tuned on chemistry papers or patent data (some groups likely did this to create models that can help in chemical synthesis planning or property prediction). 
- MathLLaMA 
- an attempt to fine-tune on math problems (though IIRC, more often people use GPT-4 via API for math due to its superior ability). 
- OpenOrca 
- not science-specific but an open RLHF-tuned LLaMA that included technical content in its training, making it better at detailed explanations.

The fine- tuning typically uses publicly available QA pairs, textbooks, and sometimes data generated by other models. A noteworthy project is GPT- 4's role as a teacher: some open- source fine- tunings used GPT- 4 to generate high- quality instruction- response pairs on scientific topics (since GPT- 4 is quite

knowledgeable). This way, one can distill some of GPT- 4's expertise into a smaller model. However, one must be cautious because any errors or biases in GPT- 4 outputs then propagate.

- Minerva (2022) and successors: Google's Minerva (based on PaLM) achieved SOTA on math and physics questions by specialized training 40. In 2023, we've seen Google extend this idea with models like Med-PaLM (a version of PaLM fine-tuned on medical question-answering, which reportedly achieved expert-level performance on the US Medical Licensing Exam) and Med-PaLM 2 (improved version using feedback from clinicians). These are proprietary but indicate a clear path: take a large foundation model and fine-tune it with domain experts in the loop. For Med-PaLM, they actually had human doctors evaluate and help refine answers (a form of domain-specific RLHF). Similarly, Law-trained LLMs and Finance-trained LLMs are being explored by others.

In general, the pattern is that adapting an LLM to a scientific domain can be done by continued pretraining on raw domain text, fine- tuning on curated supervised data, or a combination of both. Continued pretraining is unsupervised and cheap (just need lots of text and compute), and it gives the model domain- language familiarity. Supervised fine- tuning teaches it the formats of tasks we care about (like producing a well- formed answer or summary).

A subtle point is catastrophic forgetting: if you fine- tune too aggressively on a narrow domain, the model might degrade on general knowledge. Researchers mitigate this by either mixing some general data in, or using techniques like Low- Rank Adaptation (LoRA) which add new weights for domain knowledge while keeping original weights mostly intact. This way, a model can "switch" to being a scientist when needed but still recall general facts when relevant.

Now, it's worth noting that scientific LLMs not only understand text, but can sometimes be extended to predict scientific data. For example, some works treat gene sequences or chemical formulas as a language and train LMs on them to generate new sequences with desired properties (a form of generative design). While not the focus of this survey, it's a fascinating crossover: language model techniques being used for protein design (like in ProtGPT2 for protein sequences) or materials discovery, where the "grammar" is a molecular representation.

To conclude this section: We have a variety of LLMs either derived from general ones or trained from scratch that cater to different scientific fields. Table 1 below summarizes a few examples of scientific LLMs and their key characteristics:

<table><tr><td>Model</td><td>Year</td><td>Base Architecture</td><td>Domain</td><td>Parameters</td><td>Training Data Highlights</td></tr><tr><td>SciBERT</td><td>2019</td><td>BERT (encoder)</td><td>Broad Science</td><td>110M (base)</td><td>1.14M scholarly articles (multi-domain) 34</td></tr><tr><td>BioBERT</td><td>2019</td><td>BERT (encoder)</td><td>Biomedical</td><td>110M (base)</td><td>PubMed abstracts, PMC articles (±18B words)</td></tr><tr><td>Galactica</td><td>2022</td><td>Transformer decoder</td><td>Science (general)</td><td>120B (largest)</td><td>106B tokens (papers, textbooks, references, etc.) 46</td></tr></table>

<table><tr><td>Model</td><td>Year</td><td>Base Architecture</td><td>Domain</td><td>Parameters</td><td>Training Data Highlights</td></tr><tr><td>BioGPT</td><td>2022</td><td>Transformer decoder</td><td>Biomedical</td><td>345M</td><td>~15M PubMed abstracts 54</td></tr><tr><td>Codex</td><td>2021</td><td>Transformer decoder</td><td>Programming (Sci)</td><td>12B (GPT-3 var)</td><td>54M GitHub files (source code)</td></tr><tr><td>Minerva</td><td>2022</td><td>Transformer decoder</td><td>Math &amp;amp; STEM</td><td>540B (PaLM)</td><td>118 GB of math/science papers + web 40</td></tr><tr><td>MedAlpaca (LLaMA-13B)</td><td>2023</td><td>Transformer decoder</td><td>Medicine</td><td>13B</td><td>Medical Q&amp;amp;A pairs, dialogues 52</td></tr><tr><td>GPT-4 (Med-PaLM2, etc.)</td><td>2023</td><td>Transformer (multi-modal)</td><td>Medicine (variant)</td><td>? (proprietary)</td><td>General web + medical Q&amp;amp;A fine-tuning (human feedback)</td></tr></table>

Table 1: Examples of Large Language Models adapted for scientific domains.

These examples illustrate how the community has engineered models for different specializations. Next, we move on to discuss how such models are used in practice across various scientific tasks and what capabilities they demonstrate.

## 6 Applications of LLMs in Scientific Research

Large language models have begun to impact many areas of scientific work. We organize the discussion of applications into several categories of research activities: literature search and synthesis, explanation and tutoring, hypothesis generation and reasoning on scientific problems, domain- specific Q&A and decision support, and scientific writing assistance. Throughout, we will highlight examples of how LLMs are used and any known evaluations of their performance.

### 6.1 Literature Discovery and Summarization

One of the most immediate applications of LLMs in science is helping researchers cope with the flood of literature. LLMs can be used to summarize papers, compare findings across papers, and even generate structured literature reviews.

Document Summarization. Given a single research article, an LLM can produce a summary in plain English, highlight key findings, or simplify the content for a wider audience. For instance, the SciTLDR dataset introduced in 2020 provided a benchmark for summarizing AI research papers into a few sentences. Models like BART and Pegasus fine- tuned on such data could generate decent paper summaries. Now with LLMs like GPT- 4, many researchers simply paste an abstract (or even full text in chunks) into ChatGPT and ask for a summary or for an explanation of the paper. Anecdotally, ChatGPT's summaries are coherent and often accurate for well- written papers, though it may miss nuances or caveats. Some tools have built wrappers around this: Semantic Scholar's TLDR feature, and ScholarAI, etc., offering on- click summaries.

Comparative Analysis: More challenging is summarizing multiple papers together - e.g., "What are the differing approaches and conclusions of these 5 papers on quantum computing?" This requires the model not only to digest long texts but also to identify similarities and differences. The ChatCite system (2023) was specifically designed for comparative literature summarization 60. It introduces a method with a reflective incremental mechanism: essentially, it processes papers one by one, extracts key points, and then incrementally builds a comparative summary, refining it with a "reflective memory" to ensure consistency 61 62 . In experiments, ChatCite outperformed even GPT- 4 on generating comparative summaries of 1000 research papers (per their custom dataset), producing more coherent and insightful overviews of research topics 63 64 . The ability to maintain a "long context" of multiple documents was critical. ChatCite' s model incorporated a specialized long- context transformer and was fine- tuned to prefer outputs that mention contrasting findings, methodological differences, etc., rather than just generic summaries 65 66 .

A notable challenge here is the context window limit: many LLMs can only handle a few thousand tokens of input, which is often less than one full paper, let alone multiple papers. Techniques to overcome this include chunking (summarizing parts and then summarizing the summaries), long- context models (like Transformer variants Longformer, BigBird which allow 16k+ tokens), or retrieval (fetch relevant parts of each document dynamically). The ChatCite approach implements a tailored long- context memory for this 67 . As of 2025, we' re seeing models with 100k token windows (e.g., Claude from Anthropic offers this) which could directly take in  $\sim 75$  pages of text. That means in principle one could feed an entire thesis or a set of articles into a single model prompt. Early explorations show that these extended context models can indeed produce integrated summaries, though with diminishing returns as more data is stuffed in (the model may lose focus or give broad strokes only).

Literature Review Generation: Taking summarization a step further, LLMs can draft portions of a literature review article. A researcher can prompt: "Summarize recent advances in nanomaterials for energy storage, citing key papers." The model might produce a few paragraphs that synthesize known information. However, caution is needed: models may fabricate citations or misattribute findings if they don't recall correctly 68 69 . One approach is to use retrieval: first find relevant papers via a search, then feed their abstracts or conclusions to the LLM to ground its summary. This hybrid approach is used by tools like Elicit, which lets you enter a question and then returns a summary with references. The summaries are generated by GPT- 3.5/4 type models but each sentence is linked to a source paper, mitigating the risk of hallucination.

In a 2022 study, an "automated review- generation method" was proposed that combined an LLM with topic modeling: it identified key topics from a corpus of papers and then had the LLM write paragraphs about each topic 69 . The output was a draft literature review that a human could then refine. This hints at future workflows where much of the grunt work of synthesizing literature could be machine- assisted.

Evaluation of Summaries: How good are these LLM- generated summaries? Human evaluations generally show they are understandable and capture many main points, but might lack nuance or include minor inaccuracies. For example, an LLM might omit a paper's limitations or the confidence level of results. There is also the risk of oversimplification: a summary might state a conclusion strongly whereas the paper was cautious. Evaluation metrics like ROUGE (which measures overlap with reference summaries) are used, but in scientific domain, factual consistency is equally important. Some benchmarks specifically check if generated summaries contain unsupported claims. Improvements continue, with some research focusing on letting LLMs themselves critique and refine their summaries (like asking the model "does the above summary accurately reflect the source?" and having it adjust if not).

### 6.2 Scientific Question-Answering and Knowledge Bases

Scientific Question- Answering and Knowledge BasesLLMs naturally excel at question- answering (QA) tasks, including those in scientific domains. Scientific QA can range from straightforward factual queries (e.g., "What is the chemical formula for caffeine?") to complex explanatory questions ("How does quantum entanglement differ from classical correlation?"). LLMs, with their vast embedded knowledge, can attempt answers to many such questions.

Performance on Scientific QA Benchmarks: There are several datasets designed to test models on scientific QA: - PubMedQA: Biomedical questions requiring understanding of research findings. Galactica set a new state- of- the- art on PubMedQA (77.6% accuracy on the dev set) 70, outperforming prior specialized models. This suggests the model learned to reason about medical paper content effectively. - MedMCQA: multiple- choice medical exam questions. Again, Galactica (52.9% dev accuracy) exceeded previous models by a margin 70. Later, Google's Med- PaLM reported >85% accuracy on USMLE- style questions (surpassing the passing threshold) - a dramatic leap, attributed to fine- tuning with expert feedback. - MATH and STEM Benchmarks: These involve short problems in math, physics, etc. Minerva's success here is notable: it scored 50% on MATH (competition- level problems) vs previous state of ~8% by PaLM 71. GPT- 4 in 2023 also scored around 40% on MATH and did well on others like GSM8K (grade school math). These models often employ chain- of- thought solutions, meaning they generate a step- by- step reasoning path, which is crucial for getting the correct numeric answers 43.

It's worth noting a general finding: for factual questions (like definitions, properties, who/when, etc.), LLMs are quite strong because they likely memorized or can recombine relevant facts from training data. For deep reasoning or multi- step problems, the largest models with special prompting or fine- tuning do best (e.g., GPT- 4, Minerva). For common- sense science questions (like intuitive physics, or everyday science trivia), models do reasonably well but can be tripped up by trick questions or the need for real- world experience.

Open- Domain vs Closed- Book QA: In open- domain QA, the model can access external information (like retrieving documents). In closed- book, it must rely solely on what's in its parameters. LLMs like GPT- 3 were evaluated in a closed- book fashion on things like biomedical QA and did decently. But as knowledge domains get more niche or recent (post- training), closed- book hits its limits. That's why systems like SciQA incorporate retrieval 57. SciQA (2023) set up a challenge where the model had to answer questions by drawing from a provided scientific corpus (like a mini version of a research library). Models using retrieval augmented generation outperformed those that tried to answer from memory, especially when precise details were needed (like "According to the 2018 study by X, what was the sample size?").

Examples of QA Assistants: - IBM's Project Debater tech was used to create an AI called "Dr. Evidence" that would answer medical questions with cited sources. - Google's Search Generative Experience (SGE) in 2023 started giving short synthesized answers (though not specifically for academic questions, it includes some scientific queries, citing websites). - WolframAlpha Plugin + LLMs: For math and physics questions requiring calculation, hooking up an LLM with WolframAlpha (for exact computation) yields powerful results. The LLM can parse a question, formulate a query to WolframAlpha, get a result (say a differential equation solution or an integral value), and then present it in context.

Knowledge Base Construction: Another angle is using LLMs to help build or populate knowledge bases. For instance, extracting structured facts from papers (information extraction) can be aided by LLMs that do zero- shot relation extraction. Some researchers have prompted GPT- 3 to read a paragraph and output a triple like (Chemical X, treats, Disease Y). The results are mixed; LLMs can identify obvious relations but may hallucinate some if not explicitly trained. But when fine- tuned on a small annotated set, they often surpass older IE methods that required heavy feature engineering.

A cautionary tale: LLMs often overestimate their knowledge. They may not know something but will still produce an answer. For example, ask ChatGPT a very obscure scientific fact, if it doesn't know, it might still guess and present the guess as fact. This is problematic - it requires either a mechanism for the model to express uncertainty ("I'm not sure" which base GPT- 3 didn't do much, but GPT- 4 does slightly better) or to ensure factual grounding via sources. The Editorial in Scientific Reports (2023) stressed this: "Researchers must learn the extent to which we can trust these tools" and not blindly accept their answers 72.

### 6.3 Reasoning and Problem Solving in Science

Beyond retrieving facts, we want LLMs to help solve novel problems - whether it's working through a physics exercise, suggesting an experiment design, or proving a theorem. This is arguably the frontier of scientific LLM capability, requiring reasoning, not just recall.

Mathematical Problem Solving: LLMs like Minerva and GPT- 4 have shown an aptitude for solving math problems by generating step- by- step solutions 42 43 . They can derive equations, perform algebraic manipulations, and arrive at answers, as illustrated by Minerva solving a line equation problem:

In the example below (an LLM solution to a math problem), the model receives a question and produces a multi- step reasoning:

Question: A line parallel to  $y = 4x + 6$  passes through (5, 10) . What is the  $y$  - coordinate of the point where this line crosses the  $y$  - axis?

Model output: The line is parallel to  $y = 4x + 6$  , which means it has the same slope  $4x$  (slope 4). Since the line passes through (5, 10), we can use the point- slope form to find the equation:  $y - 10 = 4(x - 5)$  . This gives  $y - 10 = 4x - 20$  , so  $y = 4x - 10$  . Now, the  $y$  - intercept is when  $x =$  0, so  $y = 4(0) - 10 = - 10$

This reasoning chain leads to the correct answer:  $- 10$  . Models achieve this by internalizing some algebra rules and being prompted to show their work. However, they are not infallible - they can slip on more complex algebra or when several reasoning steps are needed. One strategy to improve reliability is self- consistency: have the model do the problem multiple times (with slight randomness) and see if answers converge (majority voting) 73 . If 3 out of 5 attempts yield the same result, that' s likely correct; if they diverge, the model is not consistent and another method or external tool might be needed.

Scientific Explanations: LLMs can also provide explanations to scientific phenomena or solutions, like a tutor. For instance, one might ask, "Why does increasing the temperature increase the rate of a chemical reaction (according to collision theory)?" A well- trained LLM can give a coherent explanation: "Raising temperature gives molecules more kinetic energy, so collisions occur more frequently and with greater energy, increasing the chance that collisions overcome the activation energy barrier, thus speeding up the reaction 73. " (This is a hypothetical output - the citation is just an example if that info was in a source.) Such explanatory power is great for education. In fact, people are using ChatGPT as a kind of interactive textbook or personal tutor in various subjects. Early studies show mixed results: the AI might occasionally explain something incorrectly or without nuance, but overall it can articulate many concepts at a level appropriate for, say, a high school or early college student.

One must be careful of illusory explanations, though. An LLM might give a superficially plausible explanation that is subtly wrong or circular. Thus, experts need to verify and possibly correct the AI's output.

Hypothesis Generation: There's speculation and some attempts at using LLMs to suggest new hypotheses or research directions. For example, feeding it a set of observations or a partial result and asking "what could explain this?" or "what experiment would test this phenomenon?" An LLM might come up with ideas by analogies to known science in its training. This is hard to evaluate because if it suggests something truly novel, we wouldn't know immediately if it's valid. But at the very least, it can help brainstorm hypotheses that a human might then investigate. A 2023 paper by Zheng et al. even showed that an LLM can help in scientific inference by connecting data and literature 74. They used GPT- 3 to look at some molecular property data and related literature and the LLM pointed out patterns that, when combined with a standard machine learning model, improved predictions 75. Essentially, the LLM synthesized a piece of theory from the literature which, added to a model's features, gave better results. This is a promising hybrid: using LLMs to infuse theoretical insight into data- driven models.

Theorem Proving and Formal Reasoning: An extreme end of scientific reasoning is formal mathematics. Efforts exist to pair LLMs with proof assistants (like Lean, Coq). While GPT- 4 can sometimes write short proofs in natural language, formal proof checking requires a different interface. Some projects train LLMs on the tokens of formal proofs to suggest the next step in a proof (OpenAI's GPT- f for Lean, for example). Progress is there, but human mathematicians still far outperform AI in coming up with creative proofs. LLMs might assist by quickly retrieving relevant lemmas or suggesting approaches based on similarity to known proofs. In 2022, a mini milestone was that a formal proof system guided by a neural model solved some Olympiad- level problems (which require multi- step original proofs). This is still experimental but hints that future "scientific LLMs" could one day be collaborators in pure reasoning tasks, not just empirical ones.

### 6.4 Scientific Writing and Communication

Another domain of application is in assisting the writing and reviewing of scientific texts:

- Drafting and Editing: LLMs can generate drafts of certain sections of papers. For instance, given methods and results, it might draft an abstract. Or it can help rephrase sentences for clarity. Nature recently had an article "Three ways ChatGPT helps me in my academic writing" 76, where the author described using it for editing text to improve readability, grammar and even to suggest alternate phrasings. Journals are cautious about this – many have policies that AI cannot be listed as an author (since it can't take responsibility for the content) 77, but they acknowledge it as a tool. If properly used (with human verification of content), LLMs can save researchers time in polishing language, especially for non-native English speakers.

- Translation: Scientific LLMs can translate technical content between languages, often better than generic translation engines for domain-specific terms. E.g., a Chinese physics paper to English, preserving terminology. One of the earliest successes of machine learning in the 2010s was machine translation, and LLMs have absorbed that ability too.

- Reviewing and Proofreading: An interesting use is to have an LLM "review" a paper – i.e., read it and provide feedback like a peer reviewer. There have been informal tests where someone gave GPT-4 a paper and asked for critical comments. It can catch some logical inconsistencies, unclear explanations, or missing references, much like a junior reviewer might. It might not fully grasp novelty or subtle domain issues, but as a second pair of eyes for clarity and even checking arithmetic in tables, it's quite useful. Some conference submissions have apparently used GPT-based tools to check their drafts before submission.

- Citation Assistance: Tools are emerging that, given a claim or sentence, suggest relevant citations (similar to how Zotero or SemanticScholar's beta features work). An LLM can take a sentence like "Transformer models have revolutionized NLP in recent years" and suggest seminal references (Vaswani et al 2017 for transformer, maybe Devlin 2018 for BERT) which the user can verify and include. However, as discussed, a naive LLM might just invent a citation that looks plausible 78 79. So specialized training or linking to a database of papers is needed. Some LLM-powered systems index a database so that the suggestions come with actual MOIs. The fabrication of citations by ChatGPT-3.5 was alarmingly high in studies (often  $50\% +$  of the citations it produced were fake) 80, hence any citation feature must ensure realness, typically by retrieving titles from a library and matching them.

- Automating Parts of Writing: Even tasks like writing the Methods section from code or data analysis steps could be partly automated. If you have a lab procedure in bullet points, an LLM can turn it into prose. Or from a piece of code that produced a figure, it can write a description of what the code did.

On the flip side, this raises concerns: if everyone starts using AI to write papers, could it lead to uniformity or even misinformation if not carefully fact- checked? There's concern over "AI- generated science nonsense". Already, journals have retracted some papers that turned out to be partly nonsense likely generated by tools (there was news of some fake cancer trial papers possibly with AI- written parts). The community is thus discussing guidelines: e.g., don't use LLMs to generate content that you don't fully understand or cannot verify. Use them to improve language, not to generate scientific claims.

### 6.5 Domain-Specific Support Systems

Finally, consider specific scientific fields and how LLMs are applied:

- Biomedicine & Healthcare: LLMs are being tested as clinical assistants (e.g., for suggesting diagnoses from notes, or patient Q&A via something like Dr. GPT). Already, GPT-4 scored in top  $10\%$  of a US medical licensing exam, showing it has absorbed a lot of medical knowledge. Companies are exploring using LLMs to summarize patient interactions, draft clinical reports, or sift through electronic health records to find relevant info. In drug discovery, language models are used to predict interactions (treat it like language: "molecule interacts with protein" as a sentence). BioGPT was used for relation extraction, and there's also ChatGPT usage by patients (with doctors then verifying). It's a hot area but with high stakes, so thorough validation is needed.

- Chemistry and Materials: There's the notion of "molecular language" 
- sequences like SMILES strings for molecules can be modeled with LMs to generate novel candidate compounds. A model might generate new chemical formulas that are likely stable or bind to a target, which then need to be tested. Similarly, polymer formulas or crystal descriptions can be treated as text. Some success in generating valid new compounds has been reported, essentially treating chemical space exploration as a language generation task constrained by chemistry rules (which the model can learn from a database of known compounds).

- Physics and Engineering: These often require solving equations or running simulations, which LLMs can't directly do, but they can assist in setting up those problems. For example, describing a scenario to an LLM and asking which equations or method apply. It can also help in coding simulation scripts (many scientists have used ChatGPT to help write MATLAB or simulation code).

There's emerging work on combining LLMs with symbolic math tools - for instance, an LLM could turn a physics word problem into a set of equations, then feed that to a solver.

- Environmental and Earth Sciences: LLMs can summarize sensor reports, help in understanding climate model outputs by explaining them in plain language, or compile knowledge about ecosystems from literature. They can also assist in collaborative projects by facilitating communication: e.g., an ecologist might ask an LLM to explain a genetics paper in simpler terms if it's relevant to their work, breaking disciplinary barriers.

- Interdisciplinary Research: LLMs have a broad knowledge base, which is useful in interdisciplinary areas where no single human can be expert in all components. For example, consider research in bioinformatics: it spans biology and computer science. An LLM could help a biologist understand some computing concept (like neural networks for genomic data) or vice versa, essentially acting as a knowledge translator between fields.

- Education and Training: While not exactly an "application for research", the use of LLMs in STEM education will indirectly influence research by training the next generation. Tools like Khan Academy's AI tutor (based on GPT-4) are being used to teach science and math interactively. Students can ask "dumb" questions they might hesitate to ask a professor and get immediate answers. Over time, if AI tutors become prevalent, we might see a generation of scientists who are very comfortable using AI to double-check their logic or explore ideas from an early stage.

In summary, applications of LLMs in science are diverse and rapidly expanding. They range from relatively straightforward uses (summarizing text, answering questions) to ambitious ones (solving new problems, suggesting experiments). A common thread is that LLMs act as amplifiers of human capability: they can sift through more information than a person can in a short time, they can produce first drafts that save time, and they can serve as a knowledgeable sounding board available 24/7. However, they are not omniscient or perfectly reliable, so human oversight, domain expertise, and critical thinking remain crucial when using LLM outputs in serious scientific work.

## 7 Open Challenges and Limitations

Despite the impressive achievements of large language models in scientific applications, significant challenges remain. We highlight some of the key open issues that are the focus of current research and are crucial to address for the safe and effective use of LLMs in science.

1. Factual Accuracy and Hallucinations: As repeatedly noted, LLMs can produce factually incorrect statements with great confidence. In scientific contexts, these hallucinations can be especially problematic. A model might, for example, cite a non-existent study to support a claim, or misreport a statistic from a paper. One study found that across multiple evaluations, about 47-69% of citations produced by ChatGPT were fabricated (not real articles) 80. Even when citations corresponded to real papers, a majority contained some error in details (wrong author, title, etc.) 81 82. This level of inaccuracy is clearly unacceptable for published work. Researchers are exploring methods to reduce hallucinations: improved training (e.g., include more structured factual data), post-hoc verification (have the model check its own answers against a database), or user interfaces that encourage verification (like showing source documents). OpenAI reported that GPT-4 has a lower hallucination rate than GPT-3.5 27, but it's not zero. For scientific LLMs, an important development is the integration of retrieval augmentation, so the model can say "According to [source], the value is X" instead of guessing. Another idea is to have the model express uncertainty: it's better for an LLM to say "I'm not sure, I

couldn't find that data" than to fabricate. Some alignment training encourages refusal to answer if unsure, but calibration of this is hard (we don't want the model to refuse too much either).

2. Reasoning and Consistency: While LLMs can perform reasoning tasks to an extent, they often lack robust logical consistency. They might make a correct inference in one context and fail a very similar one in another context (especially if worded differently). Their reasoning can also be superficial - they might use pattern matching to appear as if they're reasoning. For example, one might catch a model giving a tautological explanation or circular logic. In math or causal reasoning, models still make errors that a human with basic training wouldn't. For instance, an LLM might confuse correlation and causation in an explanation, or misuse a physics formula because it only remembered the formula but not the conditions of applicability. Ensuring that scientific LLMs truly understand the material, rather than just mimic the form of explanations, is an ongoing challenge. Approaches to improve reasoning include chain-of-thought prompting (discussed earlier), training on more step-by-step solutions, and even incorporating symbolic modules. There's a line of research on neuro-symbolic models: using the neural LLM for the language part and a symbolic solver for the logic part. However, integrating these seamlessly is non-trivial.

3. Context Length and Information Overload: Although context lengths are increasing, an LLM still can't truly read and memorize thousands of pages the way a team of researchers could over time. If asked to integrate information from 50 papers, it might miss nuances due to limited attention span or summarization compression. Current 100k-token models could take  $\sim 75$  pages in one go, which is great, but some literature reviews involve hundreds of sources. One solution is iterative reading: have the model summarize groups of papers, then summarize the summaries, etc. But this iterative process can accumulate errors or omit critical details if not carefully managed. There's also the challenge of forgetting: within a long context, models tend to focus more on the recent tokens (due to how attention is typically weighted or due to positional encoding issues). Research into long-context learning and new architectures (like hierarchical attention, segmenting memory, etc.) is active. For now, practical use might involve humans curating subsets of information for the AI to process sequentially, rather than expecting the AI to handle everything in one shot.

4. Biases and Ethical Concerns: LLMs inherit biases present in their training data. Scientific data isn't free of biases: e.g., biomedical literature might over-represent certain demographics, leading a model to be less accurate for under-represented groups. Or historical scientific texts contain sexism/racism which an unfiltered model might reproduce (e.g., using only male pronouns for scientists, etc.). There's also an issue of citation bias: if an LLM is asked for relevant papers and it was trained mostly on English sources or certain journals, it might overlook important work from other languages or less famous journals, thus perpetuating a skewed canon. Ensuring diversity in training data and maybe actively debiasing outputs is important. Ethically, the ease of generating text can lead to misuse: generating fake scientific papers or reviews, for instance. Already journals saw a surge of auto-generated submissions of low quality. There needs to be detection or verification mechanisms. Some ideas include requiring raw data or code as proof of results (hard to fake with just text) and developing AI-detection tools (though these are always in an arms race with AI that can evade detection).

5. Integration with Scientific Workflows: LLMs currently operate largely outside traditional scientific software. Integrating them with tools like lab equipment, simulation software, or databases requires new interfaces. For example, in a wet lab, a scientist might want to query the AI, "Based on these results, which experiment should I do next?" The AI could suggest something, but can it also directly interface with, say, a lab management system to set up that experiment? Not yet, but maybe in the future. Similarly, linking LLMs with computational notebooks (like an AI that lives in your Jupyter Notebook, reading your data and code and assisting) is in progress. Microsoft's Azure ML now has some GPT integration, and tools like Wolfram plug into ChatGPT, which moves in that direction. The challenge is

making sure the AI's suggestions or actions are correct and don't break things - essentially aligning AI outputs with the rigorous demands of scientific reproducibility and correctness. An AI suggesting code that subtly has a bug could derail an analysis if the user trusts it blindly. Thus, validation steps (like unit tests for AI- generated code, or cross- checks of AI- generated hypotheses with known literature) are needed in the workflow.

6. Intellectual Property and Privacy: Scientific data often isn't fully open - e.g., patient data, proprietary research. Training or using LLMs with such data raises privacy concerns. If an LLM trained on private medical records leaks some of that info in output, that's a big problem. OpenAI and others have guidelines to not train on sensitive data unless anonymized, but once a model is trained, it's hard to guarantee it won't indirectly reveal something. Techniques like fine-tuning on private data locally (so the base model is general and only the adaptation sees the private data) are used. Also, some institutions ban using cloud-based LLMs on confidential info. On the IP side, using text from published papers might be copyrighted - there's an ongoing debate whether training on copyrighted text is fair use. The legal landscape is uncertain. Researchers making domain models typically use open-access corpora (like arXiv, PubMed Central Open Access subset) to avoid this issue. But if an LLM were to spit out a paragraph verbatim from a paywalled paper that was in its training set, that's a copyright violation potentially. Monitoring for that (maybe prompting the model to avoid long verbatim outputs, or to always rephrase) is something to consider.

7. Validation and Trust in Results: In science, any new method or tool must be validated. With LLMs, how do we validate their "outputs" which might be literature reviews or answers? We likely need new protocols. For instance, if an AI assistant helped write a paper's discussion, perhaps that section should be carefully reviewed by multiple humans or cross-checked against sources, more than usual. If an AI suggests an experiment, one should evaluate if it actually aligns with domain theory. Essentially, incorporate AI suggestions but still apply the scientific method rigorously to them. Some suggest the concept of an AI audit trail keep track of what AI wrote or decided, so that human team members can double-check those parts specifically. Over time, as confidence builds in certain narrow tasks (say, an AI that reliably converts spectra data to a summarized table), those might be trusted more, but initial oversight is key.

8. Model Interpretability: LLMs are black boxes in many ways. For critical scientific decisions, it's often not enough to have an answer - scientists want to know why or how the model arrived at that. Work on interpreting neural networks (like seeing which neurons/attention heads correspond to which concept) is being applied to LLMs. There have been some findings, e.g., some GPT-2 neurons clearly correspond to whether text is in quotes or not, etc. But for complex factual stuff, it's hard to trace. For domain models, maybe we can index training data and if the model answers "The optimal catalyst is X", one could trace that it likely saw that in a particular paper during training. There's research into attribution for language models, which tries to find which training data influenced a given output. This could help trace errors or give credit to original sources. Until we have that, many scientists remain rightly wary - they prefer an interpretable model (like a regression or smaller ML model) over an inscrutable LLM for making critical predictions. In some fields (like clinical risk prediction), simpler models still dominate because of regulations requiring explainability. Bridging that gap will require either making LLMs more transparent or wrapping them in frameworks that provide explanations (like the model generating a justification that can be independently verified).

9. Sustainability and Access: Training and running huge LLMs is resource-intensive. Not all scientific groups have access to the compute needed. There's a concern that science could become dependent on tools only tech giants can build, which might create inequality or even influence research agendas (if a model has a subtle bias towards certain topics because that's what it saw more). Efforts like open-source models (BLOOM, LLAMA, etc.) help democratize it. Also, techniques to compress models or use

smaller models effectively (like knowledge distillation, quantization) are important so that one doesn't need a server farm to use a scientific LLM. Environmental impacts of training are also a concern, though for many domain- specific models, they leverage an existing base model and fine- tune on a smaller set (which is much less costly than training from scratch).

10. Keeping Up-to-Date: Scientific knowledge evolves. An LLM from 2021 won't know 2023's breakthroughs unless retrained or fine-tuned on new data. Continuous learning is tricky for LLMs; fine-tuning on new data could degrade performance on older knowledge (catastrophic forgetting). There's research into lifelong learning for LMs, such as periodically updating them without retraining from scratch. One pragmatic solution is just frequent re-training (like a new GPT model each year that includes the last year's papers), but that's expensive. Alternatively, rely on retrieval: the model can pull recent info from a database of latest papers. That mitigates the need to bake every new fact into weights. However, retrieval only works if the data is well-indexed and the model knows when to use it. It may still answer from memory something that's outdated. We might see something like "scientific LLM + literature search engine" hybrids as the norm: always checking its answers against a dynamic knowledge base.

In summary, while large language models offer tremendous opportunities for accelerating science, addressing these challenges is essential to integrate them responsibly. A recurring theme is augmented intelligence rather than fully autonomous AI in science: use LLMs to assist and enhance human researchers, but keep humans in the loop to ensure quality and integrity. With proper checks and improvements, many of these challenges can be mitigated, and the benefits of LLMs can be safely realized in scientific domains.

## 8 Conclusion and Future Outlook

Scientific Large Language Models represent a convergence of cutting- edge AI with the needs of the scientific community. In this survey, we have traced their development, from the general foundations of large language models to specialized adaptations for scientific knowledge and tasks. We reviewed how models like SciBERT, Galactica, BioGPT, and others have been engineered to absorb vast amounts of scientific literature and assist in everything from answering questions to drafting summaries and solving problems. The capabilities of these models are impressive - they can generate human- like explanations, unify information across sources, and even perform certain reasoning tasks previously thought to require human intuition.

We have also emphasized that these models are far from perfect. They lack true understanding, can falter on complex reasoning, and may produce incorrect or fabricated outputs if used naively. The current state of the art suggests that LLMs work best as assistive tools: for example, a scientist can use an LLM to get a quick summary or code snippet, then refine it using their expertise. In this symbiosis, the LLM provides breadth (scanning huge knowledge bases quickly) and speed, while the human provides depth of understanding and critical judgment.

Looking forward, there are several key trends and speculative future developments worth noting:

- Increased Interactivity and Agency: Future scientific LLMs might not just wait for prompts but could act as semi-autonomous agents. For instance, an LLM agent could monitor new publications in a researcher's field and proactively alert: "A new paper just came out that potentially contradicts your last experiment's results." It might even attempt to replicate calculations from that paper to verify consistency. This kind of agency would rely on connecting LLMs with pipelines

that fetch data, run analyses, and then summarize findings - effectively an AI research assistant that never sleeps.

- Personalized AI Collaborators: Just as individual scientists have particular styles or focus, one could fine-tune an LLM on a particular researcher's publications and notes, creating a "personal scientific assistant" attuned to that person's domain and approaches. Such a model might remember the specific context of one's lab, ongoing projects, and even communication style, making its assistance more contextually relevant. There are already hints of this with smaller models that teams train on their internal data.

- Multi-modal Scientific Models: We have begun to see models like GPT-4 that accept images; for science, this could mean analyzing graphs, microscopic images, spectra, etc. A future model might take in a plot of experimental data and the corresponding text, and help interpret the results or point out anomalies. Combining text with other modalities (tables, figures, code, etc.) will give a more holistic AI assistant. For example, an AI might scan a chemistry paper's diagram of a molecular structure and be able to discuss it in text form, bridging visual and textual understanding.

- Closing the Loop in Discovery: There is an aspirational vision where AI not only parses existing knowledge but also helps generate new hypotheses and even design and execute experiments - an AI-driven cycle of scientific discovery. For instance, in materials science, some efforts combine language models (to read papers and suggest promising materials) with robotic labs (to synthesize and test those materials). If successful, this could greatly accelerate discovery by rapidly iterating proposals and experiments. LLMs would be a component, providing the knowledge integration and hypothesis generation steps.

- Better Alignment with Scientific Values: Efforts to align LLMs with factuality and rigor will likely intensify. We might see specialized training objectives that reward a model for providing sources, or penalties for inconsistencies with known laws (e.g., a physics-aligned model would be penalized if it suggests a violation of energy conservation in an answer). The models might incorporate symbolic constraints or use modules that enforce consistency with scientific theory where applicable.

- Collaborative Knowledge Bases: LLMs might become interfaces for dynamic knowledge bases like Wikidata or specialized databases (Protein Data Bank, etc.). Rather than reading a static article, a researcher could query the AI which under the hood queries a database and then explains the result. The line between a database query language and natural language will blur, making access to structured scientific data more conversational. Conversely, LLMs might help update those databases by extracting new facts from papers as they're published.

- Education and Democratization of Expertise: By 2025, we already see that tools like ChatGPT can help novices grasp complex scientific concepts, essentially lowering the barrier to entry for learning advanced topics. In the future, someone with curiosity but limited formal training might leverage LLMs to conduct meaningful research much more easily - the AI can guide them through learning prerequisites, suggest methodologies, and warn of common pitfalls. This could democratize science, allowing more diverse participation, which historically has been limited by access to education and information. The flip side is that it also demands critical thinking education, so that people using these tools understand their limitations.

- Preventing the Spread of Misinformation: On a community level, there will likely be norms or systems for verifying AI-generated content in science. For example, arXiv or journals might develop AI-check pipelines: when a paper is submitted, an AI could flag “These 3 sentences seem to assert something not supported by the references” or “The text here appears AI-generated without proper verification.” Conversely, journals might accept certain AI contributions if labeled (like “This summary of related work was assisted by an AI”). Clear disclosure and verification protocols may become part of the publication process.

In concluding, it’s worth reflecting that the relationship between AI and science is bidirectional: AI is accelerating science, and scientific inquiry is needed to advance AI. Large language models at their core are a scientific achievement – a result of research in machine learning, linguistics, and cognitive science. Now, as they are applied to other sciences, they become tools for further discovery. The ideal scenario is one of synergy, where human creativity and intuition work alongside machine speed and breadth.

The path ahead will require multidisciplinary collaboration: AI experts, domain scientists, ethicists, and others working together to refine these models. By addressing current limitations and focusing on beneficial applications, Scientific LLMs could profoundly enhance our ability to process knowledge and innovate. They might not replace the fundamental role of human scientists – curiosity, conceptual leaps, and value judgments remain human strengths – but they can augment our capabilities and free us to focus more on high- level thinking. In the history of science, tools from the printing press to computers have expanded our intellectual reach. Large language models can be seen as the next such tool – a very general- purpose one – that, if used wisely, may help usher in new scientific breakthroughs and a more connected, comprehensible body of knowledge for humanity.

## References

1 3 14 15 16 17 22 24 29 73 Industrial applications of large language models | Scientific Reports https://www.nature.com/articles/s41598- 025- 98483- 1?error=cookies_not_supported&code=e5e26eec- ad0b- 44ce- 8cba- f321587d807e

2 12 20 21 23 The history, timeline, and future of LLMs https://toloka.ai/blog/history- of- LLMs/

4 6 7 8 38 70 71 [2211.09085] Galactica: A Large Language Model for Science https://arxiv.org/abs/2211.09085

5 60 61 62 63 64 65 66 67 Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms https://arxiv.org/html/2412.02149v1

9 40 42 43 44 58 Minerva: Solving Quantitative Reasoning Problems with Language Models https://research.google/blog/minerva- solving- quantitative- reasoning- problems- with- language- models/

10 11 27 68 72 78 79 80 81 82 Fabrication and errors in the bibliographic citations generated by ChatGPT | Scientific Reports

https://www.nature.com/articles/s41598- 023- 41032- 5?error=cookies_not_supported&code=22307ccb- 820f- 4619- bddc- c93c51398fc5

13 26 28 30 31 Large language model - Wikipedia https://en.wikipedia.org/wiki/Large_language_model

18 19 What Are Generative AI, Large Language Models, and Foundation ... https://cset.georgetown.edu/article/what- are- generative- ai- large- language- models- and- foundation- models/

25 74 75 [2310.07984] Large Language Models for Scientific Synthesis, Inference and Explanation https://arxiv.org/abs/2310.07984

32 33 Timeline of AI and language models - Dr Alan D. Thompson - LifeArchitect.ai https://lifearchitect.ai/timeline/

34 SciBERT: A Pretrained Language Model for Scientific Text https://www.researchgate.net/publication/336995622_SciBERT_A_Pretrained_Language_Model_for_Scientific_Text

35 [1903.10676] SciBERT: A Pretrained Language Model for Scientific Text https://arxiv.org/abs/1903.10676

36 BioBERT: a pre- trained biomedical language representation model ... https://academic.oup.com/bioinformatics/article/36/4/1234/5566506

37 BioBERT: a pre- trained biomedical language representation model ... https://sh- tsang.medium.com/brief- review- biobert- a- pre- trained- biomedical- language- representation- model- for- biomedical- text- 4b5cf07efddd7

39 Google's Minerva Can Solve Mathematical and Scientific Problems ... https://thesequence.substack.com/p/edge208

41 Minerva : a language model capable of solving mathematical ... https://www.reddit.com/r/singularity/comments/voo7pi/minerva_a_language_model_capable_of_solving/

45 Galactica's dis- assemblage: Meta's beta and the omega of post ... https://link.springer.com/article/10.1007/s00146- 024- 02088- 7

46 47 48 facebook/galactica- 6.7b • Hugging Face https://huggingface.co/facebook/galactica- 6.7b

49 New Meta AI demo writes racist and inaccurate scientific literature ... https://arstechnica.com/information- technology/2022/11/after- controversy- meta- pulls- demo- of- ai- model- that- writes- scientific- papers/

50 ChatGPT hallucinates fake but plausible scientific citations ... - PsyPost https://www.psypost.org/chatgpt-hallucinates-fake-but-plausible- scientific-citations-at-a-staggering-rate-study- finds/

51 File:Trends in AI training FLOP over time (2010- 2025).svg - Wikipedia https://en.wikipedia.org/wiki/File:Trends_in_AI_training_FLOP_over_time_(2010- 2025).svg

52 Me- LLaMA: Medical Foundation Large Language Models for ... https://pmc.ncbi.nlm.nih.gov/articles/PMC11702601/

53 What is BioGPT and what does it mean for healthcare? https://www.clinicaltrialsarena.com/news/biogpt- healthcare/

54 BioGPT - Hugging Face https://huggingface.co/docs/transformers/en/model_doc/biogpt

55 56 File:The- Transformer- model- architecture.png - Wikimedia Commons https://commons.wikimedia.org/wiki/File:The- Transformer- model- architecture.png

57 [PDF] Large Language Models for Scientific Question Answering https://2024. eswc- conferences.org/wp- content/uploads/2024/04/146640194. pdf

59 microsoft/biogpt - Hugging Face https://huggingface.co/microsoft/biogpt

69 Automated literature research and review- generation method based ... https://academic.oup.com/nsr/advance- article/doi/10.1093/nsr/nwaf169/8120226

76 Three ways ChatGPT helps me in my academic writing - Nature https://www.nature.com/articles/s41586- 024- 01042- 3

77 ChatGPT is fun, but not an author - Science https://www.science.org/doi/10.1126/science.adg7879