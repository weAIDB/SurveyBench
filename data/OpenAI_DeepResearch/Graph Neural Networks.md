# Graph Neural Networks: Foundations, Methods, Applications, and Challenges

## 1 Introduction

Graph Neural Networks (GNNs) are a family of deep learning models tailored for data that can be represented as graphs 1. In a graph, entities are represented as nodes (or vertices) and relationships between them as edges. GNNs leverage this structure by allowing information to flow along the edges, so that each node can iteratively update its representation by exchanging "messages" with neighboring nodes 2 1. By capturing relational structure, GNNs can make informed predictions about entities in a network that traditional neural networks (which expect fixed- size, grid- like inputs) would struggle with 3 4 .

Originally proposed in the mid- 2000s, GNNs remained a niche for about a decade. In recent years, however, they have exploded in popularity and capability, becoming one of the fastest- growing areas of machine learning 5 . This surge is driven by their success in a wide range of domains: social networks, knowledge graphs, recommender systems, computer vision, natural language processing, drug discovery, physics simulation, program analysis, cybersecurity, and more 5 6 . GNN- based approaches now achieve state- of- the- art results in tasks like molecule property prediction (modeling atoms and bonds as graphs), traffic forecasting (roads and intersections as graphs), fraud detection (transactions network), and recommendation engines (user- item interaction graphs) 7 . Large tech companies have incorporated GNNs into production (e.g. for content recommendation and anomaly detection), and open- source libraries such as PyTorch Geometric and Deep Graph Library (DGL) have made GNNs accessible to practitioners 8 . In short, graph neural networks have become a powerful paradigm for "deep learning on graphs" that bridges graph theory and neural computation.

This survey provides a comprehensive overview of graph neural networks, from the fundamental concepts and definitions to advanced methods, historical milestones, and emerging challenges. We begin by formally defining graphs and the GNN computation model. We then trace the evolution of GNN research in a historical timeline, highlighting major milestones. Next, we discuss key GNN architectures and algorithms - including convolution- based and attention- based GNNs - and theoretical foundations such as the message- passing framework. We also cover specialized topics like heterogeneous graphs, dynamic graphs, and graph generative models. Afterward, we explore applications of GNNs across multiple domains, illustrating how GNNs are applied in practice. Finally, we examine open challenges and future directions: issues like scalability to massive graphs, handling long- range dependencies, improving theoretical understanding, and integrating GNNs with other AI paradigms. Throughout, we aim for a balance of technical depth and accessible explanation, making this survey useful for both newcomers and experienced researchers. Key concepts are defined formally, and diagrams are used to illustrate important architectures and processes. We emphasize recent developments (especially post- 2020) to capture the state- of- the- art in this rapidly evolving field, building the discussion from fundamental principles rather than relying on existing surveys.

## 2 Preliminaries: Graphs and Graph Neural Networks

Graphs: Formally, a graph \(\) 0\\(s\) is defined as a pair \(\) 0\(E\) where \(\) 0\\(s\) is the set of nodes (vertices) and \(\) 0\\(s\) is the set of edges connecting pairs of nodes. An edge \(\) 0\(|uv\)\)in E\( may be undirected (an unordered pair\)\S(\mathsf{u},\mathsf{v})\S)\(or directed (ordered pair\)\S(\mathsf{u},\mathsf{v})\S)\(, indicating a relationship or interaction from node\)\S\mathsf{u}\S\(to node\)\S\mathsf{v}\S\(. The set of neighbors of a node\)\S\mathsf{v}\S\(is denoted\)\S\mathsf{N}(\mathsf{v})=\{\mathsf{u},\mathsf{v}\mid\mathsf{in}\mathsf{E}\}\(\)\S\( (for directed graphs, one may distinguish inbound vs. outbound neighbors). We often represent a graph with an adjacency matrix\)\S\mathsf{I}\(mathbf{f}(A)\) of size \(\) 0\(|v|\)\(times\)|v|\(\), where \(\) 0\(|ij)\(is nonzero if an edge exists from node\)\S\mathsf{i}\S\(to node\)\S\mathsf{j}\S\(. Each node\)\S\mathsf{v}\S\(can have an associated feature vector\)\S\mathsf{I}\(\)\mathsf{mathbf{f}(x)}_{- }\mathsf{v}\S\( capturing its attributes (for example, profile information in a social network, or atom type in a molecular graph). Similarly, edges can have features (e.g. bond type in a molecule), and the graph as a whole may have a feature or context vector. A graph with features is sometimes called an attributed graph. GNNs operate on such attributed graphs to produce useful outputs.

Graph Neural Networks: A GNN is a neural network model that takes a graph as input and produces outputs informed by the graph structure. The hallmark of GNNs is the use of message passing (also known as neighborhood aggregation): nodes iteratively exchange information with their neighbors and update their own embeddings (hidden states) based on these messages 21. After  $\) 0\\(s$  rounds (layers) of message passing, a node' s embedding encodes information about its  $\) 0\\(s$  - hop neighborhood in the graph 9 10 . This allows the GNN to capture both local and larger- scale graph structure. The learned node embeddings can be used for node- level tasks (like classifying nodes into categories in a social network), or they can be aggregated to produce a whole- graph representation for graph- level tasks (like predicting a property of an entire molecule). GNNs are designed to be permutation invariant/ equivariant to the graph' s node ordering: shuffling the order of input nodes does not change the output, which is appropriate because graph nodes have no canonical order 11 12 . This is achieved by using symmetric functions (like sum, mean) to aggregate neighbor messages, ensuring the model depends only on the graph topology and not on arbitrary labeling of nodes.

Formally, a basic GNN layer can be described as an operation that for each node  $\) v\(in$ \mathsf{V}\mathsf{S} $updates its feature vector (embedding)$ h_{- }v\mathbb{S}$ . One common formulation is:

Message computation: Each neighbor \(\) u\(in N(v)\) produces a message to \(\) 0\\(s\) based on \(\) u\mathbb{S}^{\ast}\(s current state and the edge features. For example, message\)\S\mathsf{m}_{- }\{\mathsf{u}\backslash\mathsf{to}\mathsf{v}\}\(=\)\mathsf{M}(\mathsf{h}_{- }\mathsf{u}^{\wedge}\{\mathsf{i}(\mathsf{k})\}\(, h_- v^{\wedge}\{\)(k)}, e_{- }{uv})\)\S\(for some trainable message function\)\S\mathsf{M}\S\(In many simpler GNNs,\)\\(m_{- }\{\mathsf{u}\backslash\mathsf{to}\mathsf{v}\}\) is just a linear transformation of \(\) h_{- }u^{\wedge}\{\mathsf{i}(\mathsf{k})\}\(\)the feature of neighbor \(\) u\mathbb{S}\(at layer\)\S\mathbb{K}\S)\(or even just\)\S\mathsf{h}_{- }\mathsf{u}^{\wedge}\{\mathsf{i}(\mathsf{k})\}\(\)itself.

Message aggregation: Node  $\) 0\\(s$  collects messages from its neighbors  $\) 0\\(v)\S$  and aggregates them into a single combined message. The aggregation is typically an order- invariant operation like summation or averaging:  $\S \mathsf{m}_{- }\mathsf{v}^{\wedge}\{\mathsf{i}(\mathsf{k})\} = \{\mathsf{t}\mathsf{e}\mathsf{x}\mathsf{t}\{\mathsf{A}\mathsf{G}\mathsf{G}\} \{\mathsf{i},\mathsf{m}_{- }\{\mathsf{u}\} \mathsf{t}\mathsf{o}\mathsf{v}\} :\mathsf{u}\backslash \mathsf{i}\mathsf{n}\mathsf{N}(\mathsf{v})\backslash \mathsf{i}\}$  , where AGG could be  $\) 0\\(u\mathsf{m}\mathsf{S},$ $\) 0\\(u\mathsf{m}\mathsf{a}\mathsf{n}\mathsf{S}$  , or a more complex pooling. (Attention- based GNNs use a weighted sum - more on that later.)

- Node state update: Node  $\) 0\\(s$  updates its embedding using the aggregated message and its previous state:  $\) h_{-}v^{\wedge}\{\langle k+1\rangle\}=\mathsf{U}!\backslash\mathsf{B}\mathsf{g}(h_{-}v^{\wedge}\{\langle k\rangle\},\mathsf{m}_{-}v^{\wedge}\{\langle k\rangle\}\backslash\mathsf{B}\mathsf{g})\S\(, where$ \S\mathsf{U}\S $is a trainable update function, often an MLP (multilayer perceptron) or a simple nonlinear transformation. This update step typically includes adding the aggregated neighbor message to$ \S 0\ $s$  s own current state (a form of skip-connection) and applying an activation function. In many GNNs, the update can be written as:

$$
\S \S h_{-}v^{\wedge}\{\langle k + 1\rangle \} = \backslash \mathsf{sigma}\backslash \mathsf{Big}(\mathsf{W}_{-}\backslash \mathsf{1}\backslash \mathsf{dot}\mathsf{h}_{-}v^{\wedge}\{\langle k\rangle \} +\mathsf{W}_{-}\backslash \mathsf{dot}\mathsf{m}_{-}v^{\wedge}\{\langle k\rangle \backslash \mathsf{Big}),\S \S
$$

where \(\) W_{- }1\(, W_2\) are weight matrices and \(\) \text{sigma}\(is an activation like ReLU. Some formulations omit the\)\\(W_{- }1\) h_v^{\text{A}}\(\)\) self- term, relying only on neighbor info, while others include it to preserve a node's own features.

By stacking such layers  $\) k=1\(dots,$ K $), information from progressively farther nodes is integrated. The above paradigm is often called the Message Passing Neural Network (MPNN) framework 13. It generalizes many specific GNN models. For instance, the popular Graph Convolutional Network (GCN) can be seen as a special case of this message- passing formula with a particular choice of$ \ $M$  and \(\) US$ (essentially a weighted sum of neighbor features plus self- feature) and with weight sharing constraints corresponding to a convolution operation 14 15. Likewise, other variants like GraphSAGE, GAT, etc., fit into this framework with different aggregation functions and update mechanisms.

Representation and output: After  $\) K\() layers of message passing, we obtain final embeddings$ \ $h_{- }v^{\wedge}\{\langle K\rangle \}$ $\) for each node \($  v\(\). These can be fed into a task- specific prediction layer. For node- level tasks (e.g. node classification), one can apply a final linear classifier or regression on each node's embedding 16. For graph- level tasks (e.g. graph classification or regression), a readout or global pooling function is applied to aggregate node (and possibly edge) embeddings into a single vector for the whole graph, which is then fed to a predictor 17 18. Common readout functions include simple sum/mean of node embeddings or more sophisticated attention or set pooling mechanisms. Figure 1 illustrates the basic "graph- in, graph- out" architecture of a GNN layer, which updates all nodes (and edges, if applicable) in parallel and produces a new graph representation 19.

Figure 1: A single GNN layer (block) transforms an input graph into an updated output graph. Each component of the graph - node features (V), edge features (E), and an optional global feature (U) - is updated by a learned function (such as an MLP). This diagram illustrates a graph- independent update where each node and edge is processed in parallel without considering neighbors; subsequent sections will introduce neighbor- dependent updates 19. Colors indicate distinct learnable transformations for nodes, edges, and globals.

Several important properties make GNNs effective and distinct: (1) Locality: Each layer's computations involve a node and its immediate neighbors, respecting the graph's sparse connectivity. (2) Permutation invariance: Aggregation ensures that the model's output doesn't depend on how we order the neighbors or label the nodes 11 12. (3) Depth vs. receptive field: By stacking multiple layers, a node can gradually incorporate information from farther parts of the graph (its  $\) K\() - hop neighborhood after$ \ $K$ \) layers) 20. However, deeper is not always better, as we will discuss (issues of over- smoothing and long- range dependency arise when \(\) K\() is large). (4) Parameter sharing: Many GNN variants share weights across different parts of the graph (analogous to convolutional kernels being reused across image locations). For example, spectral convolutional GNNs use a filter defined in graph Fourier space that applies globally. This sharing leverages the idea that the same local operation can apply to any part of the graph. (5) Expressivity: A GNN can, in theory, approximate many graph algorithms by learning appropriate message and update functions. In practice, the expressiveness of standard message- passing GNNs is bounded by certain theoretical limits (related to the Weisfeiler- Lehman graph isomorphism test, discussed later).

Having established what graphs and GNNs are, we now move on to see how GNNs evolved historically, and then dive into the varieties of modern GNN models and techniques.

## 3 A Brief History and Timeline of Graph Neural Networks

Research on neural networks that operate on graphs began in the mid- 2000s, but only in the last few years did GNNs become mainstream. Table 1 presents a timeline of notable developments and milestones in the history of graph neural networks:

- 2005: Birth of GNN concept. The concept of graph neural networks was introduced by Gori et al. (2005) as a new model for learning in graph domains 21. Around the same time, Scarselli and colleagues began formulating recursive neural networks for graphs, which led to the first formal GNN model. These early GNNs were based on recursive message passing and often involved iterating until a stable node embedding (fixed-point) was reached, due to using recurrent neural units. The initial applications included web page ranking and XML document mining using graph representations 22.

- 2009: First GNN formalization. Scarselli et al. (2009) published the seminal paper "The Graph Neural Network Model" 23, which provided a theoretical framework for GNNs. They defined a GNN as a family of functions that maps graph nodes to d-dimensional vectors (states) by iteratively applying a transition function until reaching equilibrium. This work established that a GNN can approximate broad classes of functions on graphs and introduced the idea of using neural networks to learn node state updates. Another 2009 paper by Micheli introduced a related model for neural learning on graphs 24. These can be seen as the precursors of today's GNNs, though they were limited by the hardware and datasets of the time.

- 2012-2014: Graph embeddings and feature learning. A parallel thread in these years was the rise of graph embedding methods that learn vector representations for nodes (or entire graphs) using techniques not initially framed as GNNs. For example, DeepWalk (Perozzi et al., KDD 2014) was a breakthrough that applied random walks on graphs and learned node embeddings using a neural language model (Skip-gram) 25. It was the first practical method to combine graph structure with neural feature learning, and it kicked off a wave of interest in graph-based deep learning. Following DeepWalk, node2vec (Grover & Leskovec, 2016) extended this approach with biased random walks to capture diverse network connectivity patterns. Although these methods are sometimes considered "shallow" (since they don't use deep neural architectures on the graph directly), they demonstrated the power of representation learning on graphs and paved the way for GNNs. Around the same time, early forms of graph-aware autoencoders and Boltzmann machines were also explored for link prediction and node clustering on networks.

- 2013-2016: Spectral graph convolution. Researchers sought ways to generalize the success of Convolutional Neural Networks (CNNs) from grids (images) to graphs. A key development was the formulation of graph convolution in the spectral (frequency) domain using graph signal processing. Bruna et al. (2013) and Henaff et al. (2015) proposed convolutional architectures based on eigen-decomposition of the graph Laplacian, which defines a Fourier basis on graphs. These spectral CNNs were proof-of-concept that convolution-like operations can be defined on irregular graph structures. However, they were computationally expensive due to the eigenvector computation. The breakthrough came with ChebNet by Defferrard et al. (NeurIPS 2016), which used Chebyshev polynomials to approximate spectral filters, avoiding explicit eigendecomposition 26. This made spectral graph convolution efficient and localized (K-hop support). Building on this, Kipf & Welling (2017) introduced the Graph Convolutional Network (GCN) in a simplified form: they used a first-order approximation of ChebNet, resulting in a very simple propagation rule that averages a node's neighbors' features (with normalization) and applies a linear transform 14. The GCN paper demonstrated strong performance on semi-

supervised node classification (e.g. classifying academic papers in a citation network) with a remarkably simple model. GCNs quickly became a default baseline for GNN research due to their simplicity and efficiency.

- 2016-2017: Spatial/domain-specific GNNs and pooling. Concurrent with spectral approaches, other researchers developed spatial GNNs that directly perform message passing on graph nodes (sometimes called “neighborhood aggregation” methods). These include the Message Passing Neural Network (MPNN) framework by Gilmer et al. (ICML 2017) which unified various earlier models in a common view  $^{27}$ , and GraphSAGE (Hamilton et al., NeurIPS 2017) which introduced the idea of inductive node representation learning by sampling and aggregating neighbor information (mean, LSTM, or pooling aggregator)  $^{28}$ . GraphSAGE allowed GNNs to scale to large graphs by sampling a fixed number of neighbors rather than using all neighbors, and to generate embeddings for previously unseen nodes. Another notable development was Gated Graph Neural Networks (GGNN) by Li et al. (2016), which applied recurrent gating (GRU units) to the propagation steps, enabling longer-range information flow with stable gradients. In addition, 2017 saw the emergence of graph pooling and hierarchical network ideas: for example, Ying et al. (2018) proposed DiffPool, a differentiable graph pooling that learns to cluster nodes, enabling GNNs to produce coarse-grained summaries of graphs for graph-level tasks. These advances addressed tasks like graph classification by mimicking how CNNs combine local features into global ones.

- 2018: Attention and general frameworks. A landmark of 2018 was the introduction of Graph Attention Networks (GAT) by Velickovic et al.  $^{29}$ . GAT employed a self-attention mechanism on graph nodes: each node learns to attend over its neighbors with learned attention weights, so that important neighbors have larger influence. This allowed the model to focus on the most relevant parts of the neighborhood adaptively  $^{29}$ , improving performance in scenarios with noisy or heterophilous neighbors. GAT quickly became popular due to its strong performance and flexibility (attention weights provide interpretability by highlighting which neighbors are important). 2018 also saw the release of DeepMind’s Graph Networks framework (Battaglia et al.)  $^{30}$ , which presented a comprehensive graph-to-graph neural network schema with updates for node, edge, and global attributes. This framework encapsulated many prior models (such as MPNNs) and extended them to include global state and more complex interaction blocks, and it showcased applications like learning physical dynamics (e.g. modeling n-body gravitational interactions) entirely through learned message passing  $^{31}$ . The concept of relational inductive biases in learning, as expounded in that work, underscored how GNNs can incorporate structured knowledge about relationships. By this time, the core idea of GNNs – trainable message passing – had matured into a robust toolkit.

- 2019: Expressiveness and theoretical insights. As GNN models proliferated, researchers began studying their theoretical capabilities and limits. Xu et al. (ICLR 2019) introduced the Graph Isomorphism Network (GIN), which is a simple GNN architecture using sum aggregation and MLP updates. They proved that GIN is as powerful as the Weisfeiler-Lehman (WL) graph isomorphism test in distinguishing graph structures, whereas most previous GNNs were less powerful. This result connected GNN expressiveness to a known graph theoretic procedure (the WL test) and spurred interest in making GNNs more expressive. Other works examined the limits of message passing (e.g. proving that standard GNNs cannot distinguish certain graph pairs that WL cannot) and proposed higher-order GNNs or adding features like random node identifiers to increase discriminative power. 2019 also saw many new GNN variants (e.g. EDGE Conv for point cloud graphs, Graph U-Nets with adaptive pooling, recurrent relational networks, etc.) and the expansion of GNNs into new domains. For instance, Relational GCN (Schlichtkrull et al., 2018) extended GCNs to multi-relational knowledge graphs (with different edge types), enabling

applications in knowledge base completion. In practice, GNNs began to be used for modeling social influence, program analysis (learning over code's graph representations), and combinatorial optimization problems (like the Traveling Salesman Problem) by 2019.

- 2020: Scaling up and new frontiers. With growing usage came the challenge of scaling GNNs to massive graphs (with millions of nodes/edges). Techniques like GraphSAGE sampling were improved upon: GraphSdN(T Zeng et al., 2020) and Cluster-GCN (Chiang et al., 2019) introduced efficient training by sampling subgraphs or clusters as mini-batches 32. These methods drastically improved the scalability of GNN training on large networks (e.g. social network graphs with millions of users). Meanwhile, there was increasing interest in dynamic graphs - graphs that evolve over time. Models like Temporal GNNs (e.g. TGN by Rossi et al., 2020) were proposed to handle streaming edges or node updates, using memory modules or time encodings to incorporate temporal order into GNNs. On another front, researchers started merging GNNs with probabilistic reasoning and logic, exploring how to do causal inference or relational reasoning on graphs. Libraries and benchmarks also matured: the Open Graph Benchmark (OGB) was introduced in 2020 to provide large-scale graph datasets and standardized evaluation for GNNs, accelerating research. GNNs continued expanding into domains like computer vision (for example, modeling relationships between objects in a scene as a graph), NLP (modeling dependency trees or entity graphs), and reinforcement learning (using GNNs to model state in multi-agent systems or combinatorial action spaces).

- 2021: Graph Transformers and large models. Inspired by the success of Transformers in NLP and vision, 2021 brought GNN variants that incorporate transformer-style attention over the whole graph. One notable model was Graphormer (Ying et al., NeurIPS 2021), which demonstrated that with the right positional encodings and structural bias, a Transformer can excel at graph tasks 33. Graphormer introduced centrality and spatial encodings to inform the attention mechanism of graph distances, achieving state-of-the-art on many graph benchmarks. Other efforts, like SAN (Structural Attention Network) and Graph-BERT, similarly explored fully-connected attention on graphs combined with encodings of node positions (e.g. via random walks or Laplacian eigenvectors). These approaches addressed the short-range bias of typical GNNs by allowing even distant nodes to attend to one another (effectively alleviating the need for many message-passing hops) 34 35. However, they also had to confront efficiency challenges since full self-attention on a graph with  $\) 15\(nodes is$ \ $0(n^2)$  , which is infeasible for large \(\) 15$ 35 . 2021 also saw GNNs being used as components in larger architectures, such as in neural symbolic reasoning (combining logical rules with GNN modules) and in powering parts of large pipelines (e.g. Pinterest' s Pixie recommender system uses GNN embeddings for candidate generation). Framework development continued: DeepMind' s Graph Networks library and Microsoft' s Deep Graph Library (DGL) gained users, and new frameworks like PyTorch Geometric became widely adopted for rapid prototyping of GNNs 8 .

- 2022: Refinement and specialization. By 2022, the research community was addressing some GNN pain points. One issue is the over-smoothing phenomenon: as one stacks many GNN layers, node embeddings tend to become indistinguishable (converging to a common vector) because of repeated averaging over neighbors 36. Various strategies to mitigate over-smoothing were proposed, such as residual connections, normalization techniques, or using disentangled propagation. Another related issue identified is over-squashing, where exponentially growing neighborhood information is "squashed" into fixed-size embeddings, limiting a GNN's ability to propagate signals from distant nodes 37. New studies analyzed this as a bottleneck and suggested using higher-capacity aggregation or network rewiring (adding virtual links or hierarchy) to address it. 2022 also saw heterophilous graphs getting attention - graphs where connected nodes tend to have different labels or features (the opposite of homophily). Classic

GNNs often assume homophily (neighbors are similar), so new architectures (like H2GCN, GeomGCN, FAGCN, etc.) were designed to handle heterophily by adjusting how messages are passed or by incorporating sign inversion, etc. Moreover, application- focused GNN research blossomed: e.g. in biology, GNNs were applied to protein interface prediction and brain connectivity analysis; in chemistry, they were integrated with physics simulations for molecular dynamics; in finance, GNNs were used for fraud detection and trade network analysis. A notable trend was the emergence of graph self- supervised learning and pre- training - techniques to pre- train GNNs on large unlabeled graphs using objectives like contrastive learning (e.g. DGI, GraphCL) or masked prediction (inspired by BERT), to then fine- tune on downstream tasks. This was seen as a step toward graph foundation models.

- 2023: Current state and beyond. In 2023, GNNs remain a top topic in machine learning research (continuing to dominate conferences like KDD 38). The frontier includes GNNs for long-range dependencies (how to efficiently let a node access information far away in the graph) 
- Graph Transformers are one solution, but others involve intelligent architectural tweaks or multi-scale models 34 39. Scalability is still crucial: techniques for distributed GNN training, quantization, and compression are being developed to handle web-scale graphs (hundreds of millions of nodes) 40 41. Another vibrant area is GNNs combined with causal inference: researchers are investigating how to model interventions and causal relations in graphs, which is important for applications like drug repurposing or social science 42 43. Graph generation (using generative models to create new graphs, such as molecular graphs for drug discovery) is maturing with approaches like graph VAEs and diffusion models, now yielding more valid and complex synthetic graphs. Meanwhile, the synergy of GNNs with other AI paradigms is a hot topic: one question being asked is how to integrate GNN-based graph reasoning with the power of large language models (LLMs) 44 45. For example, how might an LLM use a knowledge graph (processed by a GNN) to reduce hallucinations and improve factual accuracy? Early works in 2023 explore using GNNs to inject structured knowledge into language model prompts or to post-process LLM outputs with graph constraints. Finally, GNN theory is advancing with deeper analysis of expressivity, generalization, and connections to topology (e.g. sheaf neural networks or cell complexes extending beyond simple graphs). The field continues to expand in many directions, ensuring that graph neural networks will remain an exciting research area in the years to come.

(Table 1: Timeline of key developments in graph neural networks, 2005- 2023. Each entry highlights representative milestones and publications illustrating the progress of the field.)

## 4 GNN Architectures and Techniques

Having reviewed the historical milestones, we now delve into the methodological foundations and variants of graph neural networks. We organize this section by major themes in GNN model design, highlighting both foundational models and recent methods. We start with the basic graph convolution concept, then discuss improvements like attention and skip connections, followed by specialized architectures (for heterogeneous graphs, dynamic graphs, etc.). Throughout, we use small examples or diagrams to illustrate how these models work.

### 4.1 Graph Convolution and Message Passing Foundations

At the core of many GNNs is the idea of a graph convolution - an operation that aggregates a node's neighborhood information to update its feature. The term "convolution" is used by analogy to image CNNs: in an image, a convolution filters a pixel and its local patch of neighbors; in a graph, we "filter" a node and its neighbors. There are two perspectives on graph convolution: spectral and spatial. The

spectral view defines convolution via the graph' s Laplacian eigenbasis (treating it as a signal processing problem on graphs), while the spatial view defines it directly in terms of neighbor aggregation in the graph. In practice, most modern GNNs are designed and understood in the spatial sense (message passing), but many were inspired by the spectral formulation.

In a spectral GNN (like the original Bruna model or ChebNet), one defines a filter as  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $1\mathsf{L}$ $\theta = 1 - \mathsf{D}^{\wedge}\{- 1 / 2\}$  A  $\mathsf{D}^{\wedge}\{- 1 / 2\}$ $\S$  for an undirected graph, for example). Then the convolution of a signal  $\S X\S$  on the graph is  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$ $\theta$ $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \  $\theta$ $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{+}$  \theta  $\S g_{+}$  \theta  $\S g_{+}$  \theta  $\S g_{+}$  \theta  $\S g_{+}$  \theta  $\S g_{+}$  \theta  $\S g_{+}$  \theta  $\S g_{+}$  \theta  $\S g_{+}$  \theta  $\S g_{+}$  \theta  $\S g_{+}^{\wedge}$  \theta  $\S g_{+}^{\wedge}$  \theta  $\S g_{+}^{\wedge}$  \theta  $\S g_{+}^{\wedge}$  \theta  $\S g_{+}^{\wedge}$  \theta  $\S g_{+}^{\wedge}$  \theta  $\S g_{+}^{\wedge}$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }^{\wedge}.$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- }^{\wedge}$  \theta  $\S g_{- - }$  \theta  $\S g_{- - }$  \theta  $\S g_{- - }$  \theta  $\S g_{- - }$  \theta  $\S g_{- - }$  \theta  $\S g_{- - }$  \theta  $\S g_{- - }$  \theta  $\S g_{- - }$  \theta  $\S g_{- - }$  \theta  $\S g_{- - }$ 

where  $\S d\_ v\S$  is the degree of node  $\S v\S$  (used for normalization) and  $\S W\S$  is a trainable weight matrix shared by all nodes 14 15 . In matrix form, this is often written as  $\S H^{\wedge}\{k + 1\} = \backslash \mathsf{sigma}\backslash \mathsf{hat}\{D\}^{\wedge}\{- 1 / 2\}$ $\backslash \mathsf{hat}\{A\} \backslash \mathsf{hat}\{D\}^{\wedge}\{- 1 / 2\} \mathsf{H}^{\wedge}\{(k)\} \mathsf{W}$ $\S ,$  where  $\S \mathsf{hat}\{\mathsf{A}\} = \mathsf{A} + \mathsf{I}\S$  (with self-loops added) and  $\S \backslash \mathsf{hat}\{\mathsf{D}\} \S$  is its degree matrix. This simple GCN layer performs normalized averaging of neighbor features, followed by a linear transform and nonlinearity. Despite (or because of) its simplicity, the GCN formulation became very influential.

$\S \S h\_ v^{\wedge}\{(k + 1)\} = \backslash \mathsf{sigma}\backslash \mathsf{Big}\backslash \mathsf{sum\_}\{u\backslash \mathsf{in}\mathsf{N}(v)\backslash \mathsf{cup}\{1\} \backslash \mathsf{sqrt}\{d\_ v d\_ u\} \} \backslash ,W\backslash ,h\_ u^{\wedge}\{(k)\} \backslash \mathsf{Big}),\S \S$

where  $\S d\_ v\S$  is the degree of node  $\S v\S$  (used for normalization) and  $\S W\S$  is a trainable weight matrix shared by all nodes 14 15 . In matrix form, this is often written as  $\S H^{\wedge}\{k + 1\} = \backslash \mathsf{sigma}\backslash \mathsf{hat}\{D\}^{\wedge}(- 1 / 2)$ $\backslash \mathsf{hat}\{A\} \backslash \mathsf{hat}\{D\}^{\wedge}\{- 1 / 2\} \mathsf{H}^{\wedge}\{(k)\} \mathsf{W}$ $\S ,$  where  $\S \mathsf{hat}\{\mathsf{A}\} = \mathsf{A} + \mathsf{I}\mathsf{S}$  (with self- loops added) and  $\S \backslash \mathsf{hat}\{D\} \S$  is its degree matrix. This simple GCN layer performs normalized averaging of neighbor features, followed by a linear transform and nonlinearity. Despite (or because of) its simplicity, the GCN formulation became very influential.

From the spatial perspective, the GCN formula can be seen as a specific message- passing instantiation: each neighbor' s message is just its feature  $\S h\_ u\S$  scaled by  $\S 1 / \backslash \mathsf{sqrt}\{d\_ v d\_ u\} \S$  aggregated by sum, and then  $\S W\S$  and  $\S \backslash \mathsf{sigma}$  are the update function. Many other GNNs differ only in the choice of how to compute and weight these messages:

GraphSAGE (Hamilton et al. 2017): Each node  $\S v\S$  aggregates neighbor messages by a (possibly non- linear) aggregator like mean, LSTM, or max- pooling:  $\S m\_ v = \backslash \mathsf{ext}\{\mathsf{AGG}\} \{1h\_ u:\mathsf{u}\backslash \mathsf{in}\mathsf{N}(v)\} \S$  Then  $\S h\_ v^{\wedge}\{(k + 1)\} = \backslash \mathsf{sigma}\backslash \mathsf{W\_1}\mathsf{h\_v^{\wedge}\{(k)\} + W\_2\mathsf{m\_v\backslash\S}}$  . GraphSAGE introduced the idea of inductive learning: the weights  $\S W\_ 1,W\_ 2\S$  are learned on a training graph and can be applied to unseen nodes or new graphs, since the aggregator does not depend on node identities. It also samples a fixed number of neighbors for efficiency. This was important for scaling and for dynamic graphs.

Gated GNN / GRU- based updates: Some models (e.g. Li et al. 2016' s GGNN) keep a hidden state per node and update with a Gated Recurrent Unit:  $\S h\_ v^{\wedge}\{(k + 1)\} = \backslash \mathsf{t e x t}\{\mathsf{G R U}\} (h\_ v^{\wedge}\{(k)\}$ $m\_ v^{\wedge}\{(k)\}$ $\S$  This introduces a memory effect and can stabilize training for deeper GNNs, at the cost of more parameters.

Jumping Knowledge Networks (Xu et al. 2018): JK- Nets allow adaptive aggregation of information from different layers (hops) for each node, instead of just the final layer. A node can "jump" to an earlier layer' s representation if needed, which helps preserve local info even in a deeper network. It' s essentially a skip- connection mechanism specific for GNNs.

Neighborhood normalization and weighting: Many GNN variants differ in how they weight neighbor contributions. GCN uses  $\S 1 / \backslash \mathsf{sqrt}\{d\_ u d\_ v\} \S$  . Others like PinSage (Ying et al. 2018, for Pinterest recommendation) use personalized PageRank matrices for propagation. Some use learned weights per edge or per node degree. Ensuring proper normalization is important to

prevent numerical instabilities and to manage scale of features (the GCN normalization ensures the scale of features doesn't blow up with many neighbors).

Figure 2 illustrates the concept of a one- hop graph convolution on a small graph: each node's new representation is computed from itself and its direct neighbors' features, pooled with some weighting.

Figure 2: Graph Convolution (GCN) layer schematic. Each node (colored circle) updates its representation by aggregating features from its neighbors at distance 1. The diagram shows an example where a target node (in red) receives information from its immediate neighbors (blue) via a pooling ( $\Sigma$ ) operation 46. This corresponds to one "message passing" step and is analogous to an image convolution that mixes a pixel with its surrounding pixels.

The message passing framework unifies these ideas. To reiterate in simple terms: at each layer, each node looks at its neighbors' current embeddings, applies some function to those neighbor embeddings (and possibly the connecting edge features), sums or averages them, and then uses that aggregated message along with its own embedding to compute a new embedding. This general pattern covers GCN, GraphSAGE, GIN, and many others. The differences lie in the choice of the message function  $\S M(\backslash \mathrm{cdot})\S$ , the aggregate function, and the update function  $\S U(\backslash \mathrm{cdot})\S$ . For example, GIN (Graph Isomorphism Network) uses sum aggregation without normalization (plus a learnable  $\S$  epsilon term for the node's own feature) to maximize discriminative power 21. This heavy aggregation makes it theoretically as powerful as the first- order WL test, but can magnify feature scale. Conversely, SGC (Simple Graph Conv, Wu et al. 2019) showed that you can remove all nonlinearities and just do repeated neighbor averaging (essentially  $\S \Delta \S$  power iterations) followed by a logistic regression, and still get reasonably good performance - implying that much of GCN's power came from the linear neighbor mixing, not the nonlinearity. However, nonlinearity and deeper architectures are needed for more complex decisions.

In terms of computational design, GNN layers are often implemented by sparse matrix operations. If \(\S \S\) is the \(\S \cap\) \times \times d\S\) matrix of node features (n nodes, d feature dim), a message passing layer can be implemented as: \(\S H_{- }\{\backslash \mathrm{text}\{a\mathrm{gg}\} \} = \mathsf{A}\backslash \mathrm{cdot}\mathsf{H}\S\) (this gives the summed neighbor features for each node, using adjacency matrix multiply), then \(\S H_{- }\{\backslash \mathrm{text}\{\mathrm{out}\} \} = \backslash \mathrm{sigma}\) (H_\_text{agg}) W) \(\S\) (linear transform on the aggregated messages). Variants add tweaks to this basic scheme.

### 4.2 Extensions: Attention Mechanisms and Beyond Message Passing

One of the most influential extensions to the basic GNN framework was the incorporation of attention mechanisms. Traditional message passing treats all neighbors either uniformly or based on fixed weights (like degree- normalization). But not all neighbors are equally important. In many graphs, a node might have some irrelevant or noisy connections alongside crucial ones. Graph Attention Network (GAT) addressed this by learning attention weights for neighbor contributions 29.

In GAT, for each directed edge  $\S \cup \backslash \mathrm{to} \backslash \S$ , an attention coefficient  $\S$  alpha_\{uv\} is computed using the two nodes' features: for example,  $\S \backslash \mathrm{alpha\_{[uv]}} = \backslash \mathrm{text}\{\mathrm{softmax}\} \{\mathrm{uv}\} \backslash \mathrm{in}\mathsf{N}(\mathrm{v})\} \backslash \mathrm{big}\backslash \mathrm{text}\{\mathrm{LeakyReLU}\} (\mathrm{a^{\wedge}T}[\mathsf{W}\mathsf{h_{- }u}}$ $\backslash ,\vert \backslash ,\mathsf{Wh_{- }v}]\backslash \mathsf{big}\} \S$ , where  $\S \Delta \S$  is a learnable vector and  $\S |\S$  denotes concatenation 47 48. This is similar to the Q- K- V attention in Transformers but done over graph neighbors. The softmax normalizes the weights across neighbors  $\S \backslash \mathrm{in}\mathsf{N}(\mathrm{v})\S$ . Then neighbor features are aggregated as  $\S \mathsf{m\_v} = \backslash \mathsf{sum}\S$  scales each neighbor's contribution based on content similarity. Multi- head attention is often used (computing multiple sets of  $\S \backslash \mathrm{alpha}\S$  with different  $\S \Delta \S$  vectors and concatenating results) to stabilize learning 48. GATs brought improvements especially on heterophilic graphs or when some neighbors are not useful - the model can learn to down- weight those. They also maintain permutation invariance

(softmax is applied to an unordered set of neighbors).} \alpha_{\alpha_{\beta}}\{uv\} \backslash , W h_{\alpha_{\beta}}\{s\} . So  $\) 1\($  is a weight matrix like before, and \(\) \alpha_{\beta}=\{uv\}$

Attention vs. fully- connected graph Transformer: It's worth noting the distinction between GAT and a fully- connected self- attention as in a Transformer encoder. In GAT, each node attends only to its immediate neighbors, not to every node in the graph. It's a localized attention, which preserves sparsity and scalability for large graphs (complexity linear in edges). A Graph Transformer, on the other hand, might allow any node to attend to any other (a complete graph of attention), which increases modeling power (global context in one hop) but at great computational cost (quadratic in number of nodes). Recent models like Graphormer have introduced structural encodings so that even a fully- connected attention can be aware of graph distances - effectively encoding some notion of the adjacency matrix in the attention biases 49 50 . This blurs the line between pure MPNNs and Transformers. One way to see it: GAT is a special case of a graph Transformer where the attention graph is the same as the input graph structure. Researchers continue to experiment with hybrid models that expand the receptive field selectively (e.g. adding a few long- range links in the attention graph, or hierarchical attentions).

Another extension is to incorporate edge features and edge updates. The basic message passing can be enriched if edges have attributes (e.g. relationship type or weight). Some frameworks like the Graph Networks (Battaglia et al.) explicitly include an edge update step: before aggregating neighbor info to a node, you first compute a transformed edge feature  $\mathsf{S e\_ \{uv\}^{\wedge}\{(k + 1)\} = \backslash p h\_ e(e\_ \{uv\}^{\wedge}\{(k)\}}$ $\mathsf{h\_ u^{\wedge}\{(k)\}}$  h_v\{k\})\(\) for each edge 51 . Then messages could be a function of these updated edge features as well. This yields a more powerful model that can, for example, learn different interaction effects based on edge types. Relational GCN and some chemistry GNNs (for bonds) use such edge conditioning. Figure 3 shows a message- passing layer that involves an edge- to- node information flow.

Figure 3: Message Passing with edge features. In this schematic, an edge's feature (orange square) connecting a neighbor node (blue circle) to the target node (red) is used in computing the message 52 . The first step "prepares" a message by combining the neighbor node's embedding with the edge attribute, then the message is passed and aggregated at the target node 52 . This allows the model to learn how different types of connections influence the node update.

There are myriad other architectural tweaks proposed in literature, including: using higher- order neighborhoods (beyond direct edges, e.g. two- hop terms in one layer), jump connections as mentioned, normalization layers (some use batch norm or pairnorm specifically to avoid oversmoothing by re- scaling node features), and activation functions (some works suggest using preactivation residual blocks, like in ResNets, to allow very deep GNNs). In 2021, researchers managed to train deep GNNs with  $100+$  layers using such tricks (e.g. combining residual connections, normalization, and selective connections) - something previously thought infeasible due to over- smoothing and vanishing gradients.

### 4.3 Specialized GNNs: Heterogeneous, Dynamic, and Higher-Order Graphs

Real- world graphs often have additional complexities: multiple types of nodes and edges (heterogeneous graphs), or changes over time (dynamic graphs), or more complex structures like hyperedges. Specialized GNN architectures have been developed for these cases:

- Heterogeneous GNNs: In a heterogeneous graph (a.k.a. multi-relational graph), nodes have different categories (e.g. users vs. items in a recommender system; authors vs. papers in a citation network) and edges have types (friendship, co-authorship, citation, etc.). A single aggregate of all neighbors may not suffice since different relations carry different semantics. Relational GCN (R-

GCN) is an example that introduced separate weight matrices  $\)W_{- }r\$\(for each relation type$ \ $r$ 53, so that messages are computed and summed per relation. Other models use meta- paths (sequences of edge types) to guide neighbor aggregation in heterogeneous networks - this is common in knowledge graph tasks. Heterogeneous Graph Attention Network (HAN) uses attention both on the neighbor level and between meta- paths. The general idea is to treat each relation- neighbor type as its own channel of information. As a result, these models can capture the rich interactions in, say, a user- item bipartite graph with extra context (user- user and item- item relations). Applications in recommendation and knowledge graphs have benefitted from such tailored GNNs.

- Dynamic and Temporal GNNs: Many graphs are not static 
- e.g., a social network gains new users and connections over time, or communication networks have time-stamped interactions. Two main approaches exist to handle dynamics: (1) Discrete-time dynamic GNNs, where you have graph snapshots at different times and possibly want to predict future states. Here one can either use a sequence model (like an RNN over GNN outputs at each time) or incorporate temporal features into the GNN (e.g. an edge has a time attribute influencing its message). (2) Continuous-time event-based GNNs, where edges (and nodes) come as a stream of events. Temporal Graph Networks (TGN) is a 2020 architecture that maintains a memory for each node which is updated when events (edges) involving that node occur. It uses a combination of GNN and temporal point process modeling to update and query node embeddings over time. Another example is Dynamic Graph Attention (DysAT) which uses self-attention both over structural neighbors and temporal neighbors (previous time instances of the same node). Challenges here include efficiency (many events) and non-stationarity (the graph structure evolving). But dynamic GNNs have shown success in predicting things like future links (e.g. which users will connect next month), or forecasting time-sensitive labels.

- Higher-order and geometric graphs: Some data is better modeled not as simple graphs but as hypergraphs (an edge can connect more than two nodes) or as structures like meshes and manifolds. Variants of GNNs have been extended to these cases. For hypergraphs, one can treat hyperedges as special nodes or do two-step message passing (node-to-hyperedge to node). For example, Hypergraph neural networks compute embeddings for hyperedges as well and allow messages through them. In geometry and point clouds, GNNs appear as PointNet++ and DGCNN, which connect points that are spatially close (k-nearest neighbors graph) and apply message passing to learn geometric features. Graph GNNs on manifolds (geodesic neighborhoods) also tie into this. Additionally, recent work on cellular and simplicial complex networks extends GNNs to higher-dimensional simplices (faces, volumes) beyond graph edges, which is useful in some physics and topology applications.

- Combining GNNs with physical models: A noteworthy sub-area is physics-informed GNNs, where GNNs are used to simulate physical systems by treating particles or objects as nodes. The Interaction Network (Battaglia 2016) and later MeshGraphNets use GNNs to learn the time evolution of systems (like n-body systems, fluids, etc.) by encoding physical laws as graph constraints. These models often incorporate global state to represent conserved quantities or global fields.

- Graph Autoencoders and Generative Models: While not a separate architecture for message passing, it's worth noting GNNs used in unsupervised settings. Graph autoencoders (GAEs) and variational graph autoencoders (VGAEs) use an encoder GNN to produce low-dimensional embeddings, and then a decoder (sometimes another neural network) to reconstruct graph structures or features (e.g. predict if an edge exists between two nodes given their embeddings) 16 17. These are used for tasks like link prediction, where you train the autoencoder to

reconstruct known links and then use it to score potential new links. Generative models for graphs, like GraphRNN or graph normalizing flows, often use GNNs internally or as a component to ensure generated graphs have realistic local structure. Diffusion models on graphs (an emerging theme in 2023) also employ GNN message passing during the denoising steps to gradually generate a graph adjacency matrix.

### 4.4 Training Techniques and Efficiency Considerations

Training GNNs brings unique challenges compared to standard CNNs or NLP models. A graph's size can be huge, and neighbor explosion makes naive full- batch training impossible for large graphs. We highlight some training techniques and systems advances:

- Neighborhood sampling: As introduced by GraphSAGE, instead of using all neighbors in a layer (which could lead to exponentially many nodes after a few layers in a dense graph), sample a fixed number of neighbors per node. For instance, sample 10 neighbors at random (or by some criterion) for each node in each layer. This controls the computational footprint and is analogous to dropout on edges. It introduces variance but works well in practice.

- Layer-wise vs. batch-wise sampling: PinSage and others use a layer-wise sampling (like GraphSAGE described). GraphSAINT (ICLR 2020) does graph sampling for whole mini-batches: it samples a subgraph (by random walk or other strategy) and then trains on that subgraph as a mini-batch, as if it were the whole graph. This retains more of the graph structure per batch and empirically can reduce variance. Cluster-GCN partitions the graph into clusters (using something like METIS graph partitioning) and then uses these as mini-batches to maximize locality (edges mostly within batches) 32.

- Stochastic training and streaming: For very large or streaming graphs, one may use dynamic mini-batch creation and even asynchronous updates. There's ongoing research on online learning with GNNs where the graph grows over time and the model updates without retraining from scratch.

- Normalization and regularization: Aside from sampling, techniques like  $\mathfrak{SL\_2S}$  regularization on weights, dropout on node features or edges, and pairwise loss (for link prediction) are common. Due to the possibility of information leakage in graph transductive learning (where training nodes can influence test nodes through edges), careful data splitting and sometimes edge dropout (removing some edges randomly during training) are used to improve generalization.

- Hardware and systems: GNNs can be memory-bound because storing adjacency info and doing irregular memory access (gather neighbors) is intensive. To scale, recent systems research has looked at distributed GNN training (partitioning the graph across GPUs or machines and cleverly managing communication) 41, as well as specialized hardware. There are now graph learning frameworks that integrate with Apache Spark or Parameter Server for giant graphs. And hardware like Graphcore IPUs or even custom ASICs are being explored to accelerate the sparse operations. Techniques like quantization and knowledge distillation can compress GNNs for fast inference 54. For example, one can distill a large GNN's output into a simpler model (even a multilayer perceptron) on the same graph, to reduce inference latency at some accuracy cost 54. These system-level optimizations are crucial for deploying GNNs on web-scale applications.

- Benchmarking and evaluation: With the introduction of OGB and other benchmarks, there's been effort to standardize training protocols. Evaluation on graphs also brings nuances: for

example, if the task is node classification, one must be careful to avoid using neighbor information from test nodes during training (to prevent test leakage). Inductive vs transductive settings (whether test nodes were seen during training graph) greatly affect evaluation design. In inductive tasks, the model must generalize to entirely new graphs or new nodes.

Having surveyed GNN architectures and training techniques, we next look at how these models are applied across different domains, and then discuss current challenges and future directions.

## 5 Applications of Graph Neural Networks

One reason GNNs are so exciting is their wide applicability. Many problems across science and industry can be naturally modeled with graphs, and GNNs have proven to be effective tools in these settings. We highlight several key application areas and provide examples of how GNNs are used:

- Social Networks and Web: Social media graphs (Facebook, Twitter, etc.) are a classic use case. Nodes represent people or accounts, edges represent friendships or interactions. GNNs have been used for node classification (e.g. detecting malicious accounts or classifying users by interests), link prediction (recommending new connections), and community detection. For example, Pinterest's PinSage algorithm uses a GraphSAGE-like GNN on a user-item bipartite graph to generate embeddings that improve their recommendation system . Twitter has used GNNs to detect fake or bot accounts by aggregating signals from one account's neighborhood. These networks are huge (thundreds of millions of nodes), so these applications push the limits of scalable GNN training and often rely on heavy sampling or distributed computing.

- Knowledge Graphs and Recommender Systems: In a knowledge graph, nodes are entities (people, places, concepts) and edges are relations (e.g. lives_in, friends_with, part_of). GNNs like R-GCN have been used for knowledge base completion: predicting new facts by learning from the graph structure. Knowledge graphs are also used in recommendation 
- for example, e-commerce or movie recommendation can be enhanced by a knowledge graph of items and their attributes/ relations. Graph neural recommenders take advantage of meta-paths in these graphs to connect users to items through various contexts, and use attention to emphasize certain connection types. Alibaba and Amazon have reported using GNN-based recommenders to model relationships in their product graphs or query-item graphs, achieving better personalization 55. The Heterogeneous GNN models are particularly relevant here due to multiple node/edge types.

- Chemistry and Biology: Arguably one of the most impactful domains for GNNs has been molecular biology and drug discovery. Molecules can be represented as graphs (atoms = nodes, chemical bonds = edges, with edge types like single, double, aromatic). GNNs naturally handle these and can learn to predict a molecule's properties 
- e.g. physical properties, bioactivity, toxicity 
- more accurately than previous fingerprint methods 56, 57. In 2017, Gilmer et al. showed that an MPNN could outperform handcrafted features on quantum chemistry regression tasks, inaugurating a series of works applying GNNs to chemistry 58, 59. Since then, GNNs have been used to predict drug-target interactions, design new molecules (generative models that tweak graph structures to optimize a property), and even to model reactions (as graphs transforming into other graphs). In biology, graphs are used to model protein structures (residues as nodes, with edges if within some distance 
- GNNs then predict properties like function or stability) and interaction networks (e.g. gene regulatory networks, where GNNs can help identify important genes in a disease). A recent trend is applying GNNs to medical research 
- e.g., modeling brain connectivity as a graph of brain regions, using GNNs to detect abnormal connectivity patterns in neurological disorders 60. These scientific applications benefit from

GNNs' ability to respect complex relational structure and often involve graph sizes that are manageable (molecules are usually tens to hundreds of nodes), which allows deep and tailored GNN architectures to be used.

- Vision and Graphics: Although images and videos are grid-structured (so CNNs suffice in many cases), there are scenarios in vision where graphs arise. One example is scene graphs: given an image, one can construct a graph where nodes are detected objects and edges represent relationships (e.g. person-on-bicycle, dog-next_to-person). GNNs have been used to reason on these scene graphs for tasks like image captioning or visual question answering – essentially relational reasoning about objects 61 62. Another area is point cloud analysis: a point cloud (set of points in 3D space) can be turned into a graph (connecting each point to its k nearest neighbors), and then a GNN (EdgeConv, DGCNN, etc.) can classify the shape or segment it. GNNs have also been applied to 3D meshes (graph of vertices connected by edges forming a mesh) for tasks like shape deformation, mesh segmentation, or physics simulation. In graphics, one fascinating use was in character animation: representing the skeleton as a graph and using a GNN to learn motion patterns. More broadly, any problem requiring relational reasoning in vision (e.g. tracking multiple objects and their interactions over time) could leverage a GNN to aggregate information over a relational graph structure.

- Natural Language Processing: NLP tasks have also found uses for GNNs, especially when incorporating structured knowledge. For instance, a dependency parse tree of a sentence is a graph (tree) where words are connected by grammatical relations. A GNN can spread information along this parse tree, which has been used to improve tasks like relation extraction or semantic role labeling. Another NLP application is in text generation from graphs: e.g., generating a sentence description from an input knowledge graph (used in tasks like WebNLG). Conversely, there's graph extraction from text (like AMR parsing, where a GNN can refine a candidate graph structure). Also, knowledge graph-based question answering uses GNNs to propagate information on the subgraph relevant to the question. In summary, whenever language tasks involve some graph-structured auxiliary data (like a knowledge base or a parse), GNNs serve as a good interface between the neural text representations and the structured info.

- Reinforcement Learning and Planning: GNNs are increasingly used in RL when the environment or state has an underlying graph structure. For example, in multi-agent systems, you can model agents as nodes and their interactions as edges, and use a GNN to produce an embedding for each agent that informs its policy. This relational reinforcement learning helps when agents must cooperate or compete with partial observations. GNNs have also been used for planning problems: e.g., for the classic Travelling Salesman Problem (TSP), a graph (cities and distances) is the input, and some works use a GNN to produce a solution or to guide a search algorithm. A notable result was that a GNN combined with beam search could find very good TSP tours, showcasing that learned heuristics on graphs can tackle NP-hard problems to an extent. Similarly, in robotics, GNNs have been used to reason over configurations of objects (connectivity graphs in a planning problem). The relational inductive bias of GNNs suits these scenarios by encouraging generalization over different graph instances (different numbers of agents, cities, etc., can be handled by the same network).

- Physical Sciences: Beyond the molecular applications mentioned, GNNs are used in physics simulations as surrogate models. For example, learning fluid dynamics: particles in a fluid can be nodes, their interactions edges, and a GNN can learn to predict forces and next states (see e.g. the Accel model by Sanchez-Gonzalez et al.). Graph networks have been used by DeepMind to model things like cloth deformation, water flowing, or gravitational dynamics with impressively accurate results that obey physical laws like momentum conservation (especially when combined with

techniques to enforce those). In traffic and logistics, road networks can be graphs and GNNs help in traffic forecasting or optimizing delivery routes by learning city traffic patterns  $637$ . In power systems, the electrical grid is a graph and GNNs have been tested for fault detection and load forecasting. Even in cosmology, the large- scale structure of the universe (galaxies connected by filaments) has been represented as a graph for analyzing via GNNs.

As we can see, GNNs have a remarkably broad reach. Often, domain- specific adjustments are made (like for molecules, adding features for atom types; for social networks, maybe combining with text or images associated with nodes, etc.), but the underlying idea of neighbor- based representation learning is common. This breadth also means evaluation criteria vary - success might be classification accuracy (for node or graph labels), precision@K (for recommendations), RMSE (for regression of properties), etc., depending on the field.

To give an example in practice: a team working on drug discovery might use a GNN to predict a drug molecule's probability of binding to a target protein. They would train the GNN on a dataset of molecules with known activities, then use the trained model to screen large libraries of candidate molecules, which is far faster than running wet- lab experiments on all candidates. Another example: a social network might use a GNN to embed users and then find suspicious accounts by their embedding patterns (e.g., a spam bot may stick out because its neighbor aggregation yields an embedding far from normal users). These illustrate the real- world impact GNNs are starting to have.

## 6 Open Challenges and Research Directions

Despite their rapid progress and adoption, graph neural networks still face several open challenges. We outline some key issues and active research directions, indicating why they are important and how the community is addressing them:

- Scalability to Very Large Graphs: While GNNs have shown great success on moderate-sized graphs, scaling to graphs with hundreds of millions or billions of nodes (like full social networks or web graphs) is non-trivial. Memory becomes a bottleneck (storing node states and adjacency), and communication costs are high in distributed settings. Techniques like neighbor sampling, graph partitioning, and minibatch training (discussed earlier) only partially solve this. Distributed training frameworks (e.g. PaGraph, Euler, etc.) have been developed to train GNNs on billions of edges by splitting across machines, but maintaining model accuracy while partitioning the graph is tricky (edges across partitions cause communication). Another facet is scalable inference: deploying a GNN on a giant graph in real-time (e.g., every time a web user comes, computing their updated recommendations via a GNN) requires fast propagation. Methods like significant neighbor pruning (sparify the graph by keeping only the top- $K$  similar neighbors)  $64$ , or precomputing multi-hop features (as in SIGN or SGC, which reduce the model to a linear classifier with pre-aggregated features), are being explored. There is also interest in graph streaming algorithms for GNNs, where graph updates are processed in an online fashion to update predictions without retraining from scratch.

- Handling Long-Range Dependencies and Over-Squashing: Classic GNN message passing is inherently local 
- a node's representation after  $K$  layers only reflects information within  $K$  hops. Some tasks require integrating information from far-apart nodes (for example, in a citation network, two authors from different communities might need many hops to connect). Stacking many layers is problematic due to over-smoothing (discussed below). Additionally, as messages travel through intermediate nodes, they get "squashed" into fixed-size vectors, losing distinct info (over-squashing problem)  $37$ . Potential solutions include graph rewiring (adding a few long-

range edges or a global node that connects to all nodes  $^{65}$  ), hierarchical models (like using graph pooling to make a multi- scale representation), or graph Transformers that directly connect distant nodes via attention  $^{66}$ $^{67}$ . Some recent approaches add learned virtual nodes that serve as a hub to accumulate information (a "master node" that's connected to all nodes  $^{68}$ ). Others incorporate position encodings - for instance, encoding each node's coordinates in a learned diffusion map or using random features - so that even in a fully- connected attention, the model knows which nodes are originally near or far  $^{49}$ $^{50}$ . This is a very active area: making GNNs "see the bigger picture" without blowing up complexity.

- Over-Smoothing in Deep GNNs: Over-smoothing refers to the tendency of node representations to become nearly identical when many graph convolution layers are applied  $^{36}$ . Essentially, features get mixed so many times that all distinguishable info is lost - every node's embedding converges to something like the global average in a connected component. This drastically hurts classification performance beyond a certain depth (often GNNs work best with only 2-4 layers in homophilous graphs). Strategies to counter over-smoothing include adding residual connections (so each layer doesn't fully rely on neighbor info, preserving some original info)  $^{69}$ , using norm layers (like PairNorm which periodically normalizes embeddings to keep variance across nodes), and dropout or stochastic depth (training with some layers or edges dropped to prevent oversmooth). Some theoretical works characterize over-smoothing as a kind of Laplacian diffusion process that converges to a constant vector; they suggest modifications like adding negative edges or inverse Laplacian terms to counteract it. Another idea is decoupling depth from message passing: e.g., APPNP (Klicpera 2019) runs many power-iteration like propagations of features (like a PageRank scheme) but with a fixed transform, then applies a MLP at the end - this was shown to mitigate over-smoothing by retaining a direct path for initial features to the final layer. Despite these efforts, training a 100-layer vanilla GCN is still not really feasible without special tricks. Understanding how to build truly deep GNNs that reliably learn long-range patterns without node representations collapsing is an ongoing challenge.

- Graph Heterogeneity and Complex Relations: While there are models for heterogeneous graphs, a challenge remains in automatically dealing with many different relationship types and node types. For example, a knowledge graph might have thousands of relation types - we cannot assign each a separate weight matrix due to parameter blow-up and sparsity of each relation. More parameter-efficient or meta-learning based approaches are needed to generalize across relations. Also, combining information from different types is non-trivial: a user in a graph might be connected to other users, to items, to tags, etc. - how to fuse these different aspects? Current models often do a weighted sum of relation-specific embeddings (with attention to weigh importance of different neighbor types), but finding the best architecture for each scenario can be an art. The trend of using automated machine learning (AutoML) for GNNs attempts to learn the best aggregation architecture given the graph type.

- Dynamic Graphs and Continual Learning: For temporal graphs, GNNs need to continually update as new nodes and edges arrive. Standard training (which assumes a static training set) doesn't directly apply. Continual or lifelong learning for GNNs is still nascent - models like TGN address it with memory modules, but there's the challenge of how to avoid catastrophic forgetting of past patterns when the graph evolves. Another issue is evaluation: say we train on the graph up to time  $T$  and then test on events in  $T + 1$  to  $T + H$  how to ensure our model didn't implicitly peek at future info via the graph structure? It requires careful building of training sequences and often one has to retrain periodically as new structure comes in (which can be costly for large graphs). Efficiently handling graph growth, especially in an online setting (like social network updates streaming every second), is a practical challenge.

- Expressiveness and Theoretical Understanding: There are open theoretical questions about GNN capabilities. We know from Xu et al. 2019 that message-passing GNNs are bounded by the 1-dimensional WL test in their ability to distinguish non-isomorphic graphs (i.e., two different graphs that WL views as identical will fool any message-passing GNN under certain conditions). To go beyond this, one line of research is designing higher-order GNNs that mimic higher-dimensional WL tests (e.g. considering pairs of nodes jointly, etc.), but these quickly become expensive. Another approach is augmenting GNNs with attributes that break symmetries (like random node IDs or positional encodings) – this can make them more powerful but at risk of losing generalization if not done carefully. There's also interesting work connecting GNNs to logic (like showing certain logical queries can be answered by bounded GNNs, and conversely some can't), and to computational complexity (what classes of problems can polynomial-size GNNs solve?). As GNNs are used in more critical domains, understanding their limitations theoretically will be important. For example, can a GNN learn to count certain substructures in a graph? It's been shown that vanilla GNNs cannot count cycles of certain lengths, whereas specially designed networks (using random features or Fourier features) might. The expressiveness vs. efficiency trade-off is a key theme: more expressive models often blow up in computation (like considering tuple of nodes for WL of higher order).

- Robustness and Security: Like other deep models, GNNs can be susceptible to adversarial attacks. There have been studies on adversarial perturbations on graphs – e.g., an attacker might add a few fake nodes or edges to trick a GNN into misclassifying a target node. Because graph connectivity has a discrete and combinatorial nature, even a small change (like adding an edge between a spam node and a popular node) can significantly alter message passing outcomes. Robust GNN methods attempt to detect or resist such perturbations, perhaps by limiting reliance on any single neighbor or by using Bayesian approaches to model uncertainty. There is also the area of graph explainability: how to explain why a GNN made a certain prediction (e.g., which subgraph or neighbors were most influential). Some methods like GNNExplainer (Ying et al. 2019) find a subset of nodes/edges that were crucial for the decision. However, providing intuitive explanations for decisions on graph data to end-users is challenging and important, especially in domains like medicine or finance.

- Combining GNNs with Other Modalities: Many real-world tasks involve more than just a graph. For example, in a recommendation system, you might have a user-item graph but also textual reviews (NLP) and product images (vision). A challenge is how to combine GNNs with CNNs, LSTMs, Transformers, etc., in a unified model. Some recent architectures feed output of GNN as input to an LLM to ground it in facts (for knowledge-augmented QA), or conversely use LLMs to predict relations and then refine with GNN. Designing multi-modal models that handle graphs plus sequences, grids, etc., is non-trivial because of different data structures. It often ends up with two-tower models (one network for text, one for graph, then combine at some point). A more integrated approach might be to encode everything into a graph (e.g., treat words as nodes too and connect to knowledge graph entities) and use a GNN, but then the GNN must scale to that combined structure. This cross-domain integration is an open field – success could lead to what some call “foundation models” for graphs, analogous to GPT for language or ViTs for images, which could handle arbitrary connected data.

- Causality and Counterfactuals: Graph data often comes from complex processes where causal relationships might be present (e.g., in a social graph, an edge might signify influence). GNNs typically are correlation learners like any ML model, and they can be confounded by latent causes. A burgeoning area is causal GNNs – incorporating causal inference into graph learning. For instance, in drug repurposing, treating a patient (one node) might affect another through a network (spread of disease). GNNs that can model interventions (like removing a node or an edge

and seeing effect) and answer counterfactual questions ("what if this connection didn't exist?") would be extremely valuable 42 43. Initial work combines structural causal models with GNNs or uses GNNs to learn causal structure themselves (like inferring hidden links that best explain the data). This is very challenging and still in early stages.

- Data scarcity and few-shot learning: Labeling graph data can be expensive. Sometimes you have a huge graph but labels on only a few nodes (typical semi-supervised scenario). GNNs usually can leverage unlabeled structure well (through propagation), but if the patterns are complex, they might still overfit the few labeled examples. Techniques like self-supervised pretraining (mentioned earlier) aim to learn good representations without labels and fine-tune on small labeled sets. There's also meta-learning on graphs, e.g. learning how to adapt a GNN to a new graph with few labels by training on many small graph tasks. Few-shot node classification is another setting under exploration: e.g., classify nodes of a new type given only a couple of examples by propagating from known nodes. This ties into transfer learning for GNNs – not widely solved yet, as graphs can differ a lot in structure across domains (unlike images, where natural images share some features, graphs from biology vs. social can be very different).

In summary, while GNNs have come far, there is ample room for improvement in making them deeper, faster, more robust, and more intelligent. Each challenge above is an active research frontier. Overcoming these will likely involve drawing ideas from other areas (e.g., Transformers for long- range issues, causality for robustness, distributed systems for scalability) and inventing new theory specific to graphs.

## 7 Conclusion

Graph Neural Networks have emerged as a powerful and versatile framework for deep learning on structured data. In this survey, we reviewed the fundamental concepts of GNNs – from their message- passing mechanics and formal definitions to the diverse array of model variants developed over the past several years. We outlined the historical trajectory of GNN research, noting how initial ideas in 2005–2009 set the stage for the explosive growth post- 2017 when practical models like GCN, GraphSAGE, GAT, and others unlocked state- of- the- art performance on many tasks. GNNs have unified and surpassed earlier graph analysis methods (like random walk embeddings and graph kernels) by offering a learnable, end- to- end approach to extracting information from graphs 70.

We discussed a range of GNN architectures: spectral and spatial graph convolutions, attention- based networks, recurrent and gated models, and advanced designs for heterogeneous and dynamic graphs. Key themes such as neighborhood aggregation, attention, and hierarchical pooling were highlighted, along with illustrative diagrams to build intuition. We also touched on training strategies and infrastructure that enable GNNs to run on larger graphs and complex scenarios.

GNNs have proven their value in an impressive span of application domains – from technology (social networks, recommender systems, knowledge graphs) to science (chemistry, biology, physics), from vision and NLP to combinatorial optimization. They excel wherever relationships matter as much as individual data points. By leveraging connectivity patterns, GNNs can reveal insights like functional groups in molecules, communities in social networks, or important influencers in a knowledge graph that would be missed by methods ignoring graph structure. We cited examples ranging from traffic flow prediction to drug discovery where GNNs are making an impact 63 7.

At the same time, we emphasized that the field is very much in development. Challenges like scaling to web- scale graphs, handling long- range dependencies without information dilution, and ensuring

robustness against noise or attacks are subjects of intense research. The limits of GNN expressiveness are being probed, and new hybrid architectures (such as combining GNNs with Transformers or with logical reasoning modules) are on the horizon. Particularly intriguing is the intersection of GNNs with emerging AI frontiers: how can we integrate graph- based reasoning into large language models? How can GNNs help AI systems not just see and read, but also relate and reason? These questions hint at a future where graphs and GNNs play a central role in more general intelligence systems, given that many aspects of human knowledge can be viewed as graphs (knowledge graphs, parse trees, scene graphs, etc.).

In concluding, we note that the pace of GNN research is rapid, with new methods and theoretical insights emerging continuously. The prospects for GNNs are bright: there is potential for breakthroughs in how we model complex systems (like multi- agent dynamics, or the brain's connectivity) and in how we fuse structured knowledge with data- driven learning. Open- source projects and an active community ensure that advances spread quickly from research to practice. We expect that in the coming years, graph neural networks will become even more ubiquitous, possibly serving as fundamental building blocks in AI solutions, much like CNNs and Transformers today.

For researchers and practitioners entering this field, the takeaway is that GNNs offer a rich toolkit - one that requires understanding both the theory (graphs, linear algebra, optimization) and the nuances of implementation (efficient graph operations, parallelism) - but the reward is the ability to tackle problems that were once out of reach for neural networks. In an increasingly connected world, GNNs provide a natural and powerful way to model the interconnectedness of data. As Euler's 18th- century bridges problem gave birth to graph theory, today's challenges in our networked systems are giving birth to new generations of graph neural networks, pushing the boundary of what AI can achieve on relational data 71. The survey presented here serves as a foundation, and we anticipate that readers will build on this knowledge to contribute to the next wave of innovations in graph- based learning.

## References

1 3 4 11 70 Understanding Convolutions on Graphs https://distill.pub/2021/understanding- gnns/

2 8 12 23 24 56 57 Graph neural network - Wikipedia https://en.wikipedia.org/wiki/Graph_neural_network

5 GNNBook@2023

https://graph- neural- networks.github.io/

6 13 16 17 18 19 20 32 46 51 52 61 62 65 68 A Gentle Introduction to Graph Neural Networks https://distill.pub/2021/gnn- intro/

7 14 25 27 28 29 30 31 45 46 63 71 The evolution of graph learning

https://research.google/blog/the- evolution- of- graph- learning/

9 10 34 35 38 39 40 41 42 43 44 49 50 54 64 66 67 KDD 2023: Graph neural networks' new frontiers - Amazon Science https://www.amazon.science/blog/kdd- 2023- graph- neural- networks- new- frontiers

15 21 22 47 48 53 A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions | Journal of Big Data | Full Text https://journalofbigdata.springeropen.com/articles/10.1186/s40537- 023- 00876- 4

26 Convolutional neural networks on graphs with fast localized spectral ... https://dl.acm.org/doi/10.5555/3157382.3157527

33 Assessing Transformer Performance for Graph Representation https://gram- blogposts.github.io/blog/2024/graphormer/ 36 [PDF] Demystifying Oversmoothing in Attention- Based Graph Neural ... https://papers.nips.cc/paper_files/paper/2023/file/6e4cdfdd909ea4e34bfc85a12774cba0- Paper- Conference.pdf 37 On the Trade- off between Over- smoothing and Over- squashing in ... https://dl.acm.org/doi/10.1145/3583786.3614997 58 59 Neural Message Passing for Quantum Chemistry https://proceedings.mlr.press/v70/gilmer17a.html 60 Recent Developments in GNNs for Drug Discovery - arXiv https://arxiv.org/html/2506.01302v1 69 Analyzing the effect of residual connections to oversmoothing in ... https://link.springer.com/article/10.1007/s10994- 025- 06822- 0