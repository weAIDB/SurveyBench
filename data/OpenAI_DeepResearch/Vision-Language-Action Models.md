# Vision-Language-Action Models: Unifying Vision, Language, and Embodied Action

## 1 Introduction

Artificial intelligence has made tremendous strides in understanding images and text, but a long- standing goal is to ground this understanding in the physical world. Vision- Language- Action (VLA) models represent a new class of AI systems that integrate visual perception, natural language understanding, and action execution within a single framework. Formally, a Vision- Language- Action model can be defined as a multimodal foundation model 1 that takes visual inputs (images or video of an agent' s environment) and language inputs (e.g. a natural language instruction), and directly outputs low- level actions for an embodied agent or robot to execute in the real world 1 2 . In other words, given an observation of surroundings and a task description, a VLA model generates a sequence of motor commands that accomplish the task 1 . This end- to- end capability - perceiving the world, interpreting high- level goals, and performing physical actions - marks a significant step toward more generalpurpose, embodied Al.

Embodied Al refers to Al systems that operate within and interact with the physical world (via a body or agent) rather than just performing disembodied computations. For decades, researchers have sought to teach robots to follow human instructions in natural language, dating back to early symbolic systems like Terry Winograd' s SHRDLU in 1970 3 4 . SHRDLU allowed users to instruct a computer to move blocks around in a simulated world using English commands - an early illustration that connecting language to action was possible in constrained settings 3 . Modern VLA models build on this vision, but instead of hand- coded understanding, they leverage powerful data- driven neural networks and webscale knowledge. The rise of large language models (LLMs) and vision- language models (VLMs) has provided Al systems with rich semantic understanding of images and text. VLA models embed this knowledge into action, allowing Al to not only interpret or describe the world, but to physically act within it.

Why Vision- Language- Action? The motivation for VLA models stems from the complementary strengths and weaknesses of perception, language, and control systems. Traditional robots can precisely execute low- level commands (e.g. joint torques or fixed action sequences) but struggle with abstract goals or unseen scenarios. Conversely, LLMs have impressive reasoning and knowledge capabilities but lack grounding: they have no direct experience of physics or the environment 5 . By combining vision and language understanding with a mechanism to output actions, VLA models aim to create agents that benefit from both - the semantic and reasoning abilities of language models, and the situational awareness and feedback of embodied control. For example, a person could tell a household robot, "I just worked out, please bring me a snack and a drink" . A pure vision system might locate food and drinks; a pure robotics controller might fetch known objects; but a VLA- enabled robot can interpret the request, reason that a "healthy snack" might be e.g. a banana, use its vision to find the banana and a bottle of water, and physically bring them - all through one integrated model. Recent studies demonstrated such capabilities: a VLA robot can follow high- level natural instructions like "I spilled my drink, help me clean it" by leveraging language knowledge (what "cleaning a spill" entails) while remaining grounded in what actions it can actually perform safely 5 6 . This grounding of language

in robotic affordances was shown to greatly reduce errors compared to a language model without physical context 7.

In the following sections, we provide a comprehensive survey of Vision- Language- Action models. We begin by defining key concepts and the common architecture of VLA systems. Next, we trace a historical timeline of major milestones, from early efforts in language- guided robotics to the latest generalist models. We then discuss the architectural paradigms and training strategies that enable VLA models, including how they integrate vision and language encoders with action decoders. After covering foundational methods, we review state- of- the- art developments - recent models that push the boundaries in scale, generalization, and new domains. We also survey the applications of VLA models across robotics and related fields, and highlight open challenges (such as real- time control, data efficiency, generalization to novel tasks, and safety) that remain unsolved. Finally, we outline future directions for research, envisioning how VLA models could evolve toward more adaptive, intelligent embodied agents.

## 2 Foundations and Key Concepts

Vision- Language Models (VLMs): A VLM is typically a large neural network (often transformer- based) trained on paired image- text data, enabling it to associate visual content with natural language 8 9. Classic examples include CLIP (2021), which learns a shared embedding space for images and text, and PaLI (Pathways Language & Image model) which extends language models with image understanding 9 . VLMs can perform tasks like image captioning, visual question answering, and image- based reasoning by virtue of extensive web- scale training 8 9 . However, their outputs are usually textual descriptions or answers. To repurpose a VLM for action, one must extend or fine- tune it so that its outputs correspond to actions rather than words 9 10 .

Action Representation: In robotics, actions can be represented in various ways. Many VLA models discretize the robot's continuous motor commands into a sequence of tokens (numbers or symbols) so that they can be produced just like word tokens by a language model 10 11 . For example, Google DeepMind's RT- 2 model represents each action as a string of numbers encoding the movement of the robot's end- effector (in 6 degrees- of- freedom) and the gripper state 12 . An action might look like a sequence "1 128 91 241 5 101 127 217", where each number corresponds to a discretized motion or a flag 12 . Other approaches represent actions as continuous vectors output by a policy network (e.g. joint velocity commands at each time step). There is a design trade- off here: Discrete tokenization allows reuse of language- model architectures and simple training of action prediction as a "next token" problem 10 , but it may limit precision or require large token sequences for complex motions. Continuous control outputs (sometimes achieved via a diffusion model or flow matching technique instead of autoregressive decoding) can directly provide fine- grained motor control, but may complicate training since the model must learn a regression in high- dimensional action space. Recent work addresses this by specialized decoders - for instance, Octo (2024) introduced a diffusion policy that generates smooth joint trajectories instead of discrete steps 13 14 , and it<sub>0</sub> (PI- zero) (2024) proposed a flow- matching model for high- frequency continuous actions up to 50 Hz 15 .

Observation and State: VLA models primarily use visual observations (camera images, and sometimes depth or semantic maps) as the state input, since vision provides rich information about the environment. Some models additionally include the agent's internal state (e.g. robot joint angles, or past memory of the scene) as part of the input. In general, we can denote the state observation at time t as o<sub>t</sub> (which may be an image or set of images, along with other sensor readings), and a language instruction or query as L. The VLA model seeks a policy  $\pi (a\mid o,L)$  that outputs an action a<sub>t</sub> (or distribution over possible actions) given the current observation and task command.

Many VLA architectures concatenate or fuse the visual inputs and text inputs into a unified latent representation before decoding the action  $216$  . The latent space is often that of a language model, meaning images are converted into a sequence of embeddings that mimic word embeddings 17 18 This ensures that a single transformer can process both modalities together.

Degrees of Freedom (DoF): A concept often mentioned with VLA action spaces is the degrees of freedom of the robot. This refers to the number of independent parameters that define the robot's configuration. For instance, a robotic arm with a gripper might have 6 DoF for the arm's end- effector pose (3 for position x,y,z and 3 for orientation roll,pitch,yaw) plus 1 DoF for the gripper open/close. Many manipulation- oriented VLA models thus have a 7- dimensional action output per time step  $19$ . Higher DoF systems, like humanoid robots with arms, fingers, torso, and head, have significantly more (dozens of DoF). A continuing challenge is scaling VLA models to handle high- DoF actions; early VLA successes were in relatively low- DoF settings (robot arms or mobile bases), whereas newer models like Helix (2025) tackle whole humanoid upper bodies by a novel architecture (discussed later)  $20$ $21$ .

Multimodal Foundation Model: VLA models are often described as foundation models because they are trained on large and broad datasets and intended to generalize to a wide range of tasks  $1$ . In essence, a VLA model is a foundation model that "speaks" the language of action in addition to vision and text. Many are built by fine- tuning a pre- trained vision- language foundation model on robot data  $22$ . This transfer learning is powerful: it allows the model to leverage internet- scale visual knowledge (e.g. understanding what an object is and what might be done with it) to perform physical tasks beyond its direct experience  $23$ $24$ . A striking example reported by DeepMind is that RT- 2 (which learned from both web data and a robot's own data) could execute commands involving objects or concepts never encountered in the robot training - for instance, the robot could be asked to "pick up the Coca- Cola can" even if it never saw a Coke can during its physical trials, because it learned from web images what a Coke can looks like and that it is a kind of graspable drink  $23$ $24$ . This ability to transfer semantic knowledge to action is a hallmark of VLA models, and is enabled by their multimodal training.

Policy vs. Planner: It is useful to distinguish between low- level policies that map immediate observations to immediate actions, and high- level planners that break down long- horizon tasks into subgoals or sequences of skills. VLA models in the strict sense often focus on the low- level policy - producing motor commands directly - but they can be incorporated into a larger system with a planner. Some research efforts treat language models as planners and VLA models as executors. For instance, one approach might use an LLM to generate a step- by- step plan from a complex instruction (in text form), and then call a VLA policy model to execute each step in the real world. An alternative is to have the VLA model itself handle multiple steps by outputting a sequence of actions conditioned on the instruction, possibly with chain- of- thought style reasoning internally  $25$ . In practice, to solve very complex tasks, hierarchical approaches are emerging: a recent survey categorizes one line of work as "high- level task planners capable of decomposing long- horizon tasks into subtasks, thereby guiding VLAs to follow more general user instructions."  $26$ . Another line focuses on the low- level control policies themselves  $26$ . In this survey, we primarily look at the VLA policy models, but we will discuss how some systems integrate planning or reasoning modules for complex scenarios.

## 3 Historical Timeline of Vision, Language, and Action Integration

Historical Timeline of Vision, Language, and Action IntegrationVision- Language- Action models are a convergence of multiple research threads. Below we present a timeline of notable milestones leading up to and including the VLA paradigm, to contextualize how the field evolved:

- 1970s 
- The Dawn of Language-Guided Action: Early AI systems like SHRDLU (1968-1970) allowed a user to converse with a computer to manipulate objects in a simplified blocks world 3 4 . SHRDLU could interpret commands such as "pick up the green cone and put it on the red block," maintaining a simple memory of the world and dialog context. Although entirely symbolic and limited to a virtual toy domain, this work demonstrated the concept of linking language understanding to action execution. It foreshadowed many challenges in grounding language to physical operations, such as reference resolution ("the cone" referring to the one just mentioned) and planning a sequence of moves to satisfy a request 4 27.

- 1990s-2010s 
- Early Embodied Language Instruction: Through the 1990s and 2000s, robotics and NLP researchers explored instructable robots using rule-based or statistical language understanding. Progress was slow and domain-specific; for example, systems were developed to instruct robots to navigate or manipulate objects using constrained grammars or pre-defined mappings from phrases to actions. In the late 2010s, as computer vision and NLP matured with deep learning, we saw embodied AI benchmarks that combined vision, language, and navigation. Tasks like Vision-and-Language Navigation (VLN) (e.g. the Room-to-Room dataset introduced in 2018) required an agent to follow natural language directions (like "turn right at the hallway and go through the door") in a simulated 3D environment. Around the same time, Embodied Question Answering and Instruction Following tasks (such as ALFRED in 2020) were proposed, where an agent must both interpret language and act in a visual environment (for instance, "find a knife and place it on the cutting board"). These benchmarks used deep neural models for perception and policy (often trained via reinforcement learning or imitation learning), but they were narrow in scope and significantly smaller in scale than today's VLA models. They did, however, validate the feasibility of end-to-end learning for language-conditioned action in simulated environments.

- 2022 
- Generalist Agents and LLM-Planned Robotics: A watershed moment came in 2022 with the introduction of large-scale multi-task models and the application of large language models to robotics. In May 2022, DeepMind unveiled Gato, dubbed a "generalist agent" that can perform hundreds of tasks across different domains 28 29. Gato is a single transformer network trained on a flat, tokenized dataset comprising text, images, and agent experience (including game playing, robotic arm control, and dialog) 29 30. Remarkably, the same set of model weights in Gato can caption images, engage in dialogue, play Atari games via button-press actions, and even control a real robot arm to stack blocks 28. This is achieved by converting all data (observations and actions) into a unified token sequence format and training the model to predict the next token given the context 30 31. Gato demonstrated the viability of a single multi-modal, multi-embodiment policy that decides whether to output text or physical actions based on context 29 31, a precursor idea to VLA models. Around the same time, Google researchers introduced PaLM-SayCan (August 2022), which took a different approach by combining a pre-trained large language model (Google's PaLM) with a robotics affordance model 6 32. In PaLM-SayCan, the language model is not directly controlling the robot; instead, it plans what high-level skill the robot should do (through dialog-like interaction), and a lower-level policy (learned via reinforcement learning on robot data) executes that skill 6 33. Crucially, the language model's suggestions are grounded by an affordance function ("Can" component) which

checks the feasibility of each action given the robot's state 33 34. This grounding step prevented the robot from attempting actions that it was incapable of, addressing an important safety concern. PaLM- SayCan was able to follow user instructions in a real kitchen environment, achieving tasks like "bring me a water bottle and a snack" by breaking them into steps and executing them with interpretable scoring of options 6 35. While not a single end- to- end model, this work demonstrated that LLMs endowed with factual and commonsense knowledge can significantly improve robotic task performance when properly integrated 5 6. Both Gato and PaLM- SayCan can be seen as precursors to VLA models: Gato showed multi- task policy learning at scale, and SayCan showed the power of language- model reasoning in robotics - the next generation (VLA) would combine these ideas by directly fine- tuning large multi- modal models to output actions.

- Early 2023 
- Emergence of Embodied Language Models: In March 2023, Google introduced PaLM-E ("Embodied PaLM), a large language model explicitly extended to handle embodied inputs and outputs 35 37. PaLM-E is essentially a PaLM-540B language model augmented with a vision front-end (a pretrained ViT transformer) so that it can take images and continuous state info alongside text as input 38 39. Importantly, PaLM-E was trained on both conventional vision-language tasks and on robotic tasks, framing them all as a form of "multi-modal sentence completion" 38 40. For example, one input to PaLM-E might be: "<img1> <img2> What happened between these two images?" and it would output a textual answer describing the change 40. For a robotics task, the input could be an instruction plus sensory readings and the output might be a sequence of textual commands or a description of the next action 40 41. PaLM-E's largest version had 562 billion parameters, making it one of the biggest models of its time 38. Impressively, PaLM-E not only achieved strong robot task performance (on multiple robot types), but also improved on pure vision-language benchmarks (like OK-VQA for answering visual questions) compared to dedicated models 38. This showed that training a single model on diverse vision-language-action data did not compromise its general AI capabilities; in fact, it enhanced them 38. PaLM-E was a milestone in that it proved an LLM can be embodied - ingesting real-world sensor data - and still retain broad knowledge and reasoning. However, PaLM-E's outputs were primarily textual. It could reason about what action to take (and even explain why), but typically a separate controller was needed to execute the described action. The stage was set for a model that goes one step further: directly outputting low-level actions.

- Mid 2023 
- The Vision-Language-Action Paradigm (RT-2): The concept of "vision-language-action model" as a distinct category took shape with Google DeepMind's Robotic Transformer 2 (RT-2), announced in July 2023 42 43. RT-2 was pioneering in that it directly unified perception, language understanding, and control in a single system 42. It builds on two advanced VLMs (PaLLX 44 and PaLM-E) by fine-tuning them on a large dataset of real robot demonstrations 45. The resulting model is a transformer that takes as input one or more camera images plus a textual task instruction, and outputs a sequence of action tokens for a robot 1 23. Figure 1 illustrates this general VLA model architecture. Notably, RT-2 was trained on the same multi-task robotic dataset as its predecessor RT-1 (which contained tens of thousands of real robot episodes across 13 robots and 700+ tasks in a kitchen) 46, but with the addition of vast internet data via the VLM pretraining. This gave RT-2 a form of "web-scale common sense" that allowed it to generalize beyond the exact scenarios in its robot data 24. Early results were striking: RT-2 could interpret novel commands and demonstrated rudimentary reasoning, like using object categories (e.g. recognizing a toy dinosaur as an animal and correctly responding to "pick up the animal") 24. The model could also leverage abstract knowledge - for example, deciding which object could serve as an improvised hammer when instructed (it chose a rock), or identifying an energy drink as a suitable choice when asked to find a drink for a tired person 47.

These require connecting visual perception with learned concepts about object uses and human intentions, something not explicitly programmed in the robot's data. RT- 2 achieved this through its VLM core. Additionally, the RT- 2 team showed that incorporating chain- of- thought prompting during inference enabled multi- step reasoning on the fly 47. In a sense, RT- 2 firmly established the VLA model paradigm: a large pre- trained vision- language model adapted to output actions, thereby translating internet- scale knowledge into robot behavior 43 48. This work received significant attention and inspired many subsequent efforts in the community.

Figure 1: Illustration of a vision- language- action model in a robotic manipulation scenario. The system receives an image observation (left) and a text instruction, and produces a sequence of low- level actions to be executed by the robot (right). For example, given a command "pick up the dinosaur toy", a VLA model can identify the toy in the visual scene and output the appropriate arm motions to grasp it. This end- to- end process unifies visual perception, language understanding, and motor control. 1 23

- Late 2023 
- Further Advances: The latter half of 2023 saw rapid progress. Many groups began developing their own VLA models or variations. Large academic collaborations started to assemble the necessary multimodal robot datasets. For instance, the Open X-Embodiment project (spearheaded by Stanford and others) collected over 1 million robot episodes across 22 different robotic embodiments to fuel generalist policy learning 49. By leveraging such data, researchers aimed to train VLA models that are open-source and accessible to the community (in contrast to RT-2, which was proprietary). We also saw extensions of VLA concepts to other fields: in autonomous driving, Wayev introduced LINGO-2 (April 2024), the first vision-language-action model for driving a car on public roads 50 51. LINGO-2 can take in visual sensor data and high-level textual goals (or even conversational input from a passenger) and output driving controls, while simultaneously generating a natural-language commentary explaining its decisions 51 52. This "explainable VLA" approach helped align the car's actions with human-understandable reasons, e.g. "Slowing down because there are pedestrians crossing" - a key step for trust in AI-driven vehicles 53 54. Such developments indicate how VLA ideas began permeating beyond just robot arms to other embodied domains by 2024.

- 2024 
- Open-Source and Specialized VLA Models: By 2024, the landscape included both industry-led and community-led VLA projects. In mid-2024, Stanford researchers announced OpenVLA, a 7-billion-parameter vision-language-action model released openly 49. OpenVLA was trained on the Open X-Embodiment dataset and uses a combination of pre-trained encoders (DINOv2 and CLIP for vision, LLaMA-2 for language) fused together 49. Despite being much smaller than RT-2, OpenVLA reportedly outperformed RT-2 on a suite of robot manipulation tasks, likely due to the breadth of training data and careful design 49. It also was designed for efficiency, supporting parameter-efficient fine-tuning and quantization for deployment on moderate hardware 55. Around the same time, Octo was introduced by UC Berkeley 
- a "lightweight generalist robot policy" focusing on efficient architecture 56. Octo came in extremely compact sizes (27M and 93M parameters) and explored a new idea: instead of generating actions token-by-token, it used a diffusion model to generate an entire continuous motion trajectory in one go 13. This enabled much smoother and faster control, which is beneficial for responsive robot behavior 13. Another 2024 entrant was TinyVLA, explicitly aiming at fast inference and data-efficient training by using a smaller multimodal backbone and fine-tuning it on robotics data 57. TinyVLA demonstrated that one could achieve competent VLA performance without the heavy computational cost of the largest models by clever architecture and curation of training data 57. Meanwhile, a startup named Physical Intelligence announced π<sub>0</sub> (Pi-Zero), a large-scale VLA model that pushed toward cross-embodiment generalization 15. π<sub>0</sub> used a new pre-trained VLM backbone ("Paligemma") and

was trained on data from 8 different robot types, enabling it to control various robotic arms (single or dual- arm) and handle a wide variety of tasks 15. It also incorporated advanced techniques like flow- matching continuous control and a diffusion action head to achieve high- frequency  $(50\mathrm{Hz})$  control needed for smooth manipulation 58. The flurry of activity in 2024 underscored a trend: VLA research was not limited to tech giants; academia and startups significantly contributed, focusing on openness, efficiency, or specialized capabilities.

- 2025 
- Toward Generalist Humanoids and Advanced Reasoning: In 2025, Vision-Language-Action models have continued to advance, with a notable emphasis on scaling to more complex embodiments (like humanoid robots) and improving the integration of high-level reasoning with low-level control. In early 2025, Figure AI unveiled Helix, a VLA model for their humanoid robot that introduced a dual-system architecture 20 21. Helix is actually composed of two cooperating neural networks: System 2 (S2) is a large vision-language model (an "internet-scale" VLM, open-source 7B model) that handles perception and language comprehension at a relatively low frequency  $(\sim 7 - 9\mathrm{Hz})$ , and System 1 (S1) is a reactive visuomotor policy that runs at high frequency (up to  $200\mathrm{Hz}$ ) to translate  $S2^{\prime}$  s intentions into fine motor commands 59 60. They are trained end-to-end to communicate via a shared latent space 59 61. This design allowed Helix to achieve both broad generalization (through  $S2^{\prime}$  s powerful understanding of varied objects and instructions) and fast, precise control (through S1' s high-rate adjustment of motions) 60 61. Helix was a first to demonstrate continuous, real-time control of a full humanoid upper body (arms, hands, torso, head, fingers) via a single AI model 62. In a demo, Helix even controlled two humanoid robots simultaneously in a collaborative task - each robot running an instance of the same policy network - to jointly put away unseen objects, like two humans helping each other store groceries 63 64. This marked a leap in complexity, as it required spatial coordination and communication implicitly through the model. Helix also highlighted zero-shot generalization: without explicit training on particular items, the robots could pick up essentially any small household object upon request 65. Shortly after, NVIDIA Research introduced GR00T N1 (March 2025), another VLA model for humanoids that independently adopted a similar two-system (S1/S2) approach as Helix 66. GR00T' s novelty was in its training data - a heterogeneous mix of not only robot teleoperation trajectories but also human videos and synthetic data 67 - indicating the use of diverse sources (possibly leveraging video understanding to teach robots).

- Mid/Late 2025 
- Generalist Foundation Meets Robotics: Google DeepMind, building on their success with large multimodal models, introduced Gemini Robotics in 2025, an advanced VLA model built on top of their Gemini 2.0 foundation 68. Gemini 2.0 is a multimodal model (capable of text, images, video, audio) and Gemini Robotics extends this to the physical action domain 69. The notable aspect of Gemini Robotics is the level of reasoning and dexterity it exhibits: thanks to Gemini' s powerful core, the robot can perform intricate tasks like folding origami and playing card games, which require fine manipulation and understanding of sequential, goal-directed procedures 70. Moreover, Gemini Robotics was shown to adapt to entirely new robot platforms - essentially demonstrating embodiment-agnostic control 71. By mid-2025, DeepMind also released Gemini Robotics On-Device, a lightweight variant optimized to run locally on a robot' s onboard computer with low latency 72. This addresses a practical need: for real-world deployment, robots often cannot rely on datacenter-scale models due to latency or connectivity issues. An on-device model that still preserves the skills (like dexterous folding) is a significant engineering achievement 71. In parallel, the open-source community delivered SmoVLA (2025), a vision-language-action model with only 450 million parameters 73. Though "smol" (small) in size, this model was trained on a fully open dataset called LeRobot (curated from community contributions) and achieved performance comparable to models  $10 - 100\times$  larger on standard manipulation tasks 73. SmoVLA' s design incorporates efficient features like flow-matching for

continuous control and asynchronous perception- to- action coupling 74. Its existence underscores the democratization of VLA research - you don' I need a supercomputer or proprietary data to experiment at the cutting edge.

This timeline shows how quickly the field has moved: from conceptual prototypes to internet- scale models in just a few years. We now turn to examining the technical underpinnings of these Vision- Language- Action models - their architectures, training methods, and variations - in more detail.

## 4 Common Architectures and Design Paradigms

Despite the variety of VLA models introduced, most share a common two- stage architecture: (1) a vision- language encoder that processes inputs into a latent representation, and (2) an action decoder that generates outputs executable by the agent 22 75 . Figure 2 sketches this generic architecture. Below, we discuss major architectural choices and paradigms, including the distinction between single- model vs. dual- model designs, choices of action decoding, and how researchers incorporate reasoning or planning modules. We will illustrate each concept with examples from the models surveyed above.

Vision- Language Perception Backbone: Almost all VLA systems leverage a pre- trained VLM or a combination of pre- trained vision and language models for the perception stage. For example, RT- 2 used PaLI- X (an advanced multilingual vision- language model) and PaLM- E as backbones 9 . OpenVLA similarly used DINOv2 + CLIP for image features and LLaMA- 2 as the language model 49 . The choice of backbone can affect the model' s capabilities: a larger VLM can encode more general knowledge and handle diverse inputs (e.g., multi- lingual instructions, as PaLI- X allowed RT- 2 to respond to commands in different languages 8 ). The backbone typically encodes the image(s) and text into a unified sequence of embeddings or tokens 16 . One method is to project image features into the same vector space as the language tokens (as done in PaLM- E 18 76 ). Another method is to simply intersperse visual tokens and text tokens in a single sequence if the model architecture supports it (this is common in transformers adapted for multimodal input). The output of the backbone is a latent representation (often the hidden states of the transformer) that contains both the semantic understanding of the instruction and the context of the visual scene 16 .

Action Decoder (Policy Head): On top of the backbone, a decoder module produces actions. In single- model architectures, this decoder is actually just an extension of the same transformer that encodes the inputs - effectively, the model is one big transformer network that was trained end- to- end to output action tokens. This is the case for RT- 2: they converted robot actions into tokens that look like words to the model, and trained the whole transformer to predict those just as it would predict the next word in a sentence 10 . The advantages of this approach are simplicity and tight integration: the model can potentially use the same attention mechanisms to attend to language and vision context when deciding on an action. However, a single huge model has to compromise between speed and generality, as noted earlier. In contrast, dual- system architectures explicitly separate the high- level perception/reasoning from the low- level control. Helix' s S2 and S1 networks are a prime example 59 60 . In Helix, the VLM (S2) processes vision and language and outputs a single latent vector representing the "intent" or "task context" for the current situation 77 . That vector is then fed into the smaller control policy network (S1) as a condition, and S1 takes the robot' s current sensor input to output motor commands at each control cycle 78 79 . The two networks operate at different frequencies and on different time scales, but because S2' s output influences S1, the robot' s actions are guided by semantic understanding. The dual- system design effectively decouples the cognitive part of the problem (vision- language comprehension) from the reactive control part. This resolves a fundamental trade- off: large VLMs are not fast enough for real- time servoing, while small reactive policies lack semantic smarts 80 61 . By letting S2 "think slow" (a few times per second, updating its goal latent) and S1 "think

fast" (hundreds of Hz, adjusting motions), Helix achieves both broad task generality and precise motor control 61 81 . Another benefit is modularity: S2 and S1 can be trained somewhat independently or improved separately (e.g., swapping in a better VLM in the future, or retraining S1 on more data) 82 . NVIDIA' s GRo0T N1 followed a similar dual approach, suggesting this may become a standard for complex robots 83 .

Hierarchical and Hybrid Planning: Some VLA frameworks incorporate an explicit notion of hierarchical control. The dual architectures already hint at this (S2 setting intermediate goals for S1). Beyond that, certain systems use LLMs as high- level planners or commentators while the VLA model handles low- level execution. Wayve's LINGO- 2, for instance, outputs both actions and an explanatory commentary while driving 51 . This is effectively a multi- head decoder: one head for the steering/brake commands, one head for text. The text acts like a "thought bubble" from the model, increasing transparency and perhaps allowing feedback. Similarly, some research has looked at using an LLM to generate a sequence of subtasks for the VLA model to carry out. In the survey by Ma et al. (2024), this is characterized as combining a task planner with the VLA policy to tackle long- horizon tasks 26 . An example might be: Instruction - "Prepare a cup of coffee." A planning module (could be an LLM or a symbolic planner) breaks this into: "1) pick up a cup, 2) put a coffee pod in machine, 3) press button, 4) serve coffee." Then each step is executed by a VLA policy capable of those atomic actions (or by invoking appropriate skill policies). While fully integrated planning is still an emerging area, the modular approaches like PaLM- SayCan demonstrated that you can successfully have an LLM in the loop guiding a low- level controller 6 33 . A challenge is ensuring the high- level planner' s assumptions stay within the capabilities of the low- level model; grounding (like the affordance check in SayCan) is one solution 33 34 . Looking forward, we might see more unified models that intrinsically handle both planning and execution - e.g. a VLA model that can internally generate a plan if needed and carry it out, all in one sequence of latent reasoning. Some evidence of this is the chain- of- thought prompting in RT- 2 which allowed it to handle queries requiring reasoning steps by producing intermediate "thought" tokens (not executed) before final action tokens 47 .

Discrete vs. Continuous Action Decoding: We touched on action representation earlier; here we elaborate in context of architectures. The majority of first- generation VLA models (RT- 2, OpenVLA, etc.) use discrete action tokens. This means the same transformer can generate a token like "pick" or "place" or a number representing a joint movement, analogous to generating a word. Discrete outputs simplify the learning problem (turning control into a classification at each step) and leverage the powerful sequence modeling of transformers 10 . However, a drawback is that fine motor control might require predicting many small token increments, and tokenization itself can be tricky for high- DoF robots (how do you quantize continuous joint angles? RT- 1/2 did this by a learned codebook of discrete values). As researchers pushed to higher DoF and smoother skills, alternative decoders emerged. The diffusion policy used by Octo (2024) is one such innovation 13 . Instead of predicting one action token at a time, Octo' s decoder treats the entire sequence of future actions (e.g., a trajectory of joint positions for the next few seconds) as a data sample and uses a diffusion generative model to produce it in one go 13 . Essentially, it generates a motion path that the robot should execute, which can then be quickly adjusted or replanned at each time step. This led to smoother motions, because the model could optimize the trajectory as a whole (for continuity) rather than one step at a time. Another approach is flow- matching (as in  $\pi < \sin \theta >0< /\sin \theta >$  and SmolVLA), which is a technique for continuous trajectory generation that avoids needing a large discrete token space 84 73 . Flow matching trains a model to output a continuous function (flow) that transforms an initial state into a target state over time, producing natural motions. These techniques often require more complex training regimes (e.g., denoising diffusion loss or matching flow fields), but they hold promise especially for high- frequency control. Helix' s approach, interestingly, is to sidestep discrete representation altogether at low level: its S1 outputs continuous joint commands directly at 200 Hz 60 61 . The burden of ensuring those commands make sense in context is on  $52^{,}$  s latent guidance. The Helix team argued that avoiding tokenization overhead was key to scale to

humanoid control, citing that previous VLAs' discrete action schemes worked in low- dimensional tasks (like gripper open/close) but would "face scaling challenges" if extended to dozens of joints 85. It' s likely that ongoing research will continue to explore these decoder designs - perhaps even hybrids (discrete high- level decisions plus continuous fine motions).

Training Strategies: Training a VLA model is non- trivial due to the diverse data needed (visual, language, action triplets). The dominant strategy so far is behavioral cloning on large demonstration datasets - i.e., supervised learning to imitate actions given observations and instructions. This was how RT- 1 and RT- 2 were trained, using tens of thousands of human- teleoperated or scripted demonstrations in a kitchen environment 46 . Datasets like Open X- Embodiment aggregate many such demonstrations across labs 49 . A key consideration is data alignment: the visual and language inputs must correspond to the actions. In some cases, language annotations (instructions) are not naturally available for every trajectory and have to be generated. Helix, for example, used an auto- labeling VLM to generate textual descriptions of teleoperated behaviors after the fact ("hindsight instructions") to create training pairs 86 87 . This dramatically scaled up the language- labeled data without requiring humans to write an instruction for each demonstration. Another strategy is mixing simulation and real data. Real robot data is costly, so methods like training in simulation then fine- tuning on real (sim2real transfer) are used. Some projects used human video datasets: e.g., GR00T incorporated human demonstration videos to expose the model to a wider range of actions and scenarios, presumably using imitation learning or some form of video- action alignment 67 . Reinforcement learning (RL) has been used less often directly in VLA training (because of the difficulty of online training with large models), but hybrid approaches exist. For instance, after pretraining a model with behavior cloning, one could fine- tune it with RL to further improve performance on specific goals or to encourage exploration of new behaviors. An interesting idea from research is using pretrained VLMs to provide reward signals for RL in cases where explicit rewards are hard to design 88 . This is an area of active exploration (sometimes called "VLM+RL" or "VLM as reward model" in robotics). Overall, the training of VLA models is evolving towards being more data- efficient (through better use of prior knowledge and synthetic data) and more task- aware (through techniques like instruction tuning or RL fine- tuning to hone the model on what matters for control).

System Evaluation and Feedback: Another architectural consideration is how to integrate feedback and guarantee safety. Some VLA models are purely reactive policies that map the current observation to the next action, without an explicit memory of past observations (beyond what the transformer' s context window can hold). For tasks that require memory or recurrence (like remembering where an object was earlier, or handling partial observability), researchers have tried providing sequences of observations as input (e.g., multiple camera frames or a short video clip) so the model can infer temporal context 9 . Others incorporate a memory buffer of recent actions and states into the input tokens (Gato did this by always conditioning on previous tokens up to a window 31 89 . If the context window is large (some models have thousands of token capacity), a VLA model could conceivably maintain an internal state over a fairly long horizon. However, long- term planning still might require either breaking tasks or some higher- level supervisory signal. Regarding safety and constraint adherence: one approach is to bake rules or constraints into the action decoding (e.g., forbid certain dangerous actions, similar to how LLMs are instruction- finetuned to avoid harmful outputs). Another approach is to have an external monitor or verifier that checks the model' s proposed action against safety constraints. This is analogous to how PaLM- SayCan multiplied language likelihood and affordance feasibility to ensure an action is both sensible and possible 33 34 . Some recent models, like those for autonomous driving, focus heavily on alignment between the model' s decisions and understandable justifications 51 53 - an approach that could be valuable in general robotics for safety (the robot effectively "explains itself" so we can spot if it reasons incorrectly before a catastrophe). As VLA models become more autonomous and take on openended tasks, expect to see integrated safety layers (either as part of the model or as sidecar processes) to ensure they operate within desired limits.

In summary, current VLA model architectures range from monolithic Transformers that treat action like another language, to multi- component systems that separate slow semantic reasoning from fast control, to hierarchical planners coupling with policies. Each has trade- offs: monolithic models are elegant and perform impressively on broad knowledge generalization  $^{24}$ , while two- model systems excel at real- time performance and physical realism  $^{61}$ . The field has not converged on a single "best" architecture yet; instead, the design often depends on the target platform and use case (e.g., a factory robot requiring precise rapid motions might benefit from a dual- system approach, whereas an assistive robot that engages in conversation and occasional actions might lean towards a single large model for richer interaction). Next, we look at how these architectures are employed across different application domains and tasks, and what unique challenges each domain poses.

## 5 Applications and Related Domains

Vision- Language- Action models are a general paradigm that can be applied wherever an agent needs to see, talk, and act. The prototypical application has been robotic manipulation and household robotics, but VLAs have begun to influence several other areas of embodied AI. We provide an overview of key domains where VLAs are making an impact, along with examples of how they are used.

### 5.1 Robotic Manipulation and Household Assistance

This is the core scenario for many VLA models: a robotic arm or mobile manipulator in a home or kitchen environment, receiving verbal instructions to perform tasks. Applications include fetching items, cleaning up, organizing objects, food preparation, etc. For example, the RT- 2 model was demonstrated in a kitchen setting, responding to commands like "throw away the trash" (it would identify trash objects and operate the bin) or "hand me the SpongeBob toy" (requiring recognizing a character on an item)  $^{24}$ . In research labs, tasks like setting a table, making a snack, or assembling simple objects are often used to benchmark these models. Vision is crucial here due to clutter and variety of objects, and language allows specifying flexible goals ("pour a glass of water" vs "bring a bottle of water" vs "find any snack"). A big appeal of VLAs in this context is generalization: Instead of programming each possible command, a single model can cover a wide range of user requests, even those it hasn't seen verbatim before, as long as it can connect them to similar concepts in its training (thanks to language understanding). Many see home robots as a "killer app" for VLA technology. Companies like Figure AI with Helix are explicitly targeting household chores with humanoid robots, using VLA models to enable "pick up virtually any small household object" on the fly by name or description  $^{65}$ . Similarly, Google's Everyday Robots project (which merged with DeepMind) used language- conditioned policies to get robots to tidy office spaces and bring people items, in some cases leveraging LLM- based planners (PaLM- SayCan) for complex requests  $^{6}$ .  $^{90}$ . These efforts suggest that VLA models could finally unlock a long- envisioned application: personal assistant robots that one can instruct in natural language to do various daily tasks.

### 5.2 Vision-Language Navigation and Mobile Robots

Another major domain is navigation - moving through an environment under language guidance. This includes wheeled robots or drones that you might say, "Go to the living room and check if the windows are closed" or "Find Alice and tell her the meeting is starting." For such tasks, the agent needs to interpret spatial and semantic concepts in language (rooms, objects, people's names) and relate them to its visual observations as it moves. Prior to VLA models, this domain was tackled by navigation- specific models (often using RL in simulators). Now, researchers are integrating VLA approaches to handle navigation with richer reasoning. One example is vision- language navigation for assistive robots - guiding a robot in a house through voice commands. A VLA model can take the command and the robot's egocentric camera view and directly output motion actions (like turn left, go forward steps,

etc.). Early experiments with large models in navigation show that pretraining on image- text pairs can help the robot understand landmarks mentioned in instructions (e.g., "go to the kitchen, the room with a refrigerator") which purely geometric methods struggled with. Some VLN benchmarks have started to incorporate transformer- based VLAs that jointly reason on maps and language instructions. Additionally, augmented reality (AR) applications can be seen as a variant: an embodied agent (which could be the user themselves) needs guidance, and an AI sees what they see (via AR glasses) and converses. One could imagine a future VLA assistant that, through AR, perceives your surroundings ("vision"), listens to your goal ("language"), and then acts by giving you guidance or even manipulating virtual affordances. While this is slightly different (the human still performs the physical action), the AI's output (instructions or highlighting objects in AR) is effectively its "action" in the environment. VLAs could power such assistants to naturally guide people in tasks like repairing appliances (seeing the parts and telling the user what to do next). This area is still emerging, but it aligns well with the strengths of VLAs - understanding context and dialog, with visual grounding.

### 5.3 Autonomous Driving and Vehicles

Autonomous vehicles traditionally rely on engineered perception pipelines and planning algorithms. However, with the advent of robust vision- language models, there's growing interest in making self- driving cars more interpretable and interactive using language. As mentioned, Wayve's LINGO- 2 is a pioneering example: it augments a driving policy with natural language understanding and generation, effectively creating a car that can "think out loud" in English about its driving decisions 51 53. The practical benefit is that the car can explain, in real- time, why it is doing something - e.g., "Slowing down because there is a pedestrian crossing" 53 - which helps developers and passengers trust and debug the system. Another use of VLAs in driving is following spoken directions. Imagine telling your autonomous car "Drop me off near that red sculpture" - a conventional AV might not know what you mean by "red sculpture," but a VLA- equipped AV that has seen images and text from the web could recognize the sculpture via vision and align it with the phrase. There's research on vision- and- language command datasets for driving (such as the Talk2Car dataset) where an external instruction like "turn right at the next traffic light" or "stop by the blue building" is given to the car. A VLA model can integrate this command with its camera input to adjust its driving plan accordingly. In a broader sense, personalization of driving through natural language is being explored 91. For example, a user might say "I'm feeling patient, take the scenic route" or "I'm in a rush, prioritize speed," and a VL model could interpret these high- level preferences and feed them into the driving policy. Autonomous shuttle or delivery robots could benefit similarly: you could converse with a delivery drone if something changes ("I'll be at the back door instead, meet me there"), and the drone's VLA system would adapt the route. We're also seeing interest in using language models to improve driving safety by providing commonsense constraints - e.g., an LLM might flag that driving on the sidewalk is not allowed even if pure sensor data might not forked it. In summary, while vehicles don't manipulate objects with arms, they do act (steering, accelerating) based on visual input, and language integration offers new levels of interaction and reasoning. Surveys in 2025 specifically talk about VLA models for autonomous driving, indicating it as a budding subfield 92.

### 5.4 Industrial and Medical Robotics

In industrial settings, robots often perform repetitive tasks in structured environments (factories, warehouses). Traditionally they are programmed with specific instructions or trajectories. VLA models could bring more flexibility - for instance, a factory worker could verbally instruct a robot arm to adjust its operation: "If you see any defective part with a crack, place it in the discard bin instead of the output crate." The robot's vision would need to detect a crack (a visual anomaly), language encodes the new rule, and action is sorting the part accordingly. Instead of hardcoding such conditional behaviors, a VLA system can interpret these on the fly. Another scenario is programming by demonstration with

language annotations: an engineer demonstrates a new assembly task and explains it in words; the VLA model learns the task and can reproduce it and even generalize it to slight variations. Some work in 2025 has looked at vision- language- action in precision agriculture and medical robotics  $^{93}$ . For example, in agriculture, a rover or drone could be instructed "Go to field section C, identify any weeds taller than the crops, and spray them." The robot would use vision to differentiate crop vs weed and language understanding to know what threshold is meant by "taller than" and what action "spray" entails. In medical robotics, consider a robot assistant in a hospital that can understand a nurse's spoken request like "Bring me the sterile tray from the cabinet" or even guide a surgeon: some robotic surgery platforms are exploring AI that can listen to a surgeon's commands ("hold this tissue here") and comply. While safety- critical, these areas can benefit from natural interfaces - surgeons and field workers don't want to use a keyboard or joystick; they prefer speaking or gesturing. VLAs could combine speech (as language input), computer vision (to recognize relevant objects like a specific instrument or plant), and robotic control (moving an arm or vehicle). Some hospital robots already do simple tasks like delivery and fetch, but adding language understanding would make them far more versatile to staff requests. A challenge in these domains is the need for high reliability and adherence to protocols (e.g., a medical robot must strictly follow safety rules). So, incorporating expert knowledge and constraints via language (maybe a built- in knowledge base of medical procedures) might be necessary. Nevertheless, the potential of VLAs is recognized - e.g., a recent review listed medical and industrial robotics among fields poised to be transformed by VLA models  $^{93}$ .

### 5.5 Multi-Agent Collaboration and Human-Robot Interaction

Vision- Language- Action models also open up possibilities in multi- agent systems. If each agent (robot) has a VLA model, they could communicate with each other in natural language while sharing visual context. Helix's example of two humanoid robots working together to put away groceries hints at this, although in Helix the coordination emerged implicitly by training one policy to control two robots in sync  $^{64}$ . Alternatively, one could have separate agents with VLAs that talk: e.g., one robot might say (verbally or via a messaging protocol), "I'll hold the object, you faster the screw," and the other interprets that and acts. Because language is a flexible interface, this could be easier than designing a proprietary coordination protocol. In terms of human- robot collaboration, VLAs are inherently well- suited for it: the robot can not only execute commands but also ask questions and provide feedback in language. Imagine assembling IKEA furniture with a robot helper - you could speak to it as you would to a human partner ("hold this piece here - no, the other way around"). A sufficiently advanced VLA model could watch (through cameras) the pieces, understand your language instructions, and perform the physical actions, while also possibly asking for clarification if needed ("Do you mean this plank?") or warning if it foresees an issue ("That screw seems too short for this part"). Such fluid interaction is the dream of human- robot interaction research, and combining vision and language in one model is a step toward that. We're already seeing robots that use dialogue to resolve ambiguities in commands (e.g., if you say "pick up the cup" in a room with multiple cups, the robot can respond "Which cup do you mean, the red one or the blue one?"). Those dialogue capabilities often involve an LLM; integrating it with the vision and action pipeline makes it end- to- end.

### 5.6 Summary of Domains

To summarize the reach: Humanoid service robots, domestic helper robots, autonomous vehicles, interactive drones, industrial manipulators, surgical assistants, and even AR guide systems are all within scope for Vision- Language- Action models. Each brings its own challenges (e.g., driving has high- speed constraints, surgery has precision and sterility constraints, home robots have huge variety of objects and open environments), but the unifying theme is the need for contextual understanding and adaptability. VLAs offer a way to program robots with words and examples rather than low- level code, which could dramatically expand who can instruct robots (democratizing robotics to non- programmers)

and how easily new tasks can be introduced. It's important to note that many of these applications are still in research or prototype stage - robust deployment of VLA models in the wild will require overcoming the challenges we discuss next.

## 6 Open Challenges and Research Directions

While Vision- Language- Action models have made impressive progress, there remain significant challenges on the road to truly general, reliable embodied AI. We highlight some of the major open issues and research directions:

- Real-Time Performance and Control: Achieving real-time, fine-grained control with large models is difficult. A huge transformer might output high-level correct actions but too slowly (a few seconds per inference is unacceptable for responsive control). Helix's dual-system solution is one approach, but it introduces complexity of training two networks. 80 61. Others like Octo and  $\pi < \text{sub} > 0 < /\text{sub} >$  focus on lightweight policies and special decoders to increase control frequency. 56 15. However, even with those advances, controlling highly dynamic or fast processes (like catching a thrown object or balancing) might be beyond current VLA capabilities. Ensuring stability of closed-loop control when a learning model is in the loop is also a concern 
- small errors in output can compound. Research into integrating classical control theory with learned policies (for example, using model-predictive control as a safeguard layer around a VLA action output) is ongoing. Moreover, optimizing model inference (through quantization, distillation, or new model architectures) to bring down latency is an engineering priority so that future VLA models can run on embedded hardware at, say,  $100\text{Hz}$  or more. Impressively, Helix and Gemini Robotics On-Device claim to run on onboard GPUs in real robots. 95 96, indicating progress, but generally those are smaller models than the cloud-based counterparts. Scaling down big models without losing capability (as shown by SmolVLA being competitive at 450M params 73) is a promising sign here.

- Data Efficiency and Coverage: VLA models still largely rely on large datasets of demonstrations which are expensive to collect (e.g., RT-1 had 17 months of robot data 46, Open-X collected a million episodes 49). There's an open question: how to generalize to new tasks or embodiments without requiring massive new data collections? One direction is sim2real transfer 
- using simulators and generated data to augment real data. But simulators can't capture the full complexity of the real world, and models might learn simulator biases. Another approach is one-shot or few-shot learning with VLA models: given a new task, can the model learn it from just a single demonstration plus a description? Some research in 2025 discussed agentic adaptation 
- i.e., enabling the model to autonomously practice or refine its skills, possibly by exploring or by reading manuals or watching videos (self-directed learning). 97. Also, techniques like meta-learning or prompting could allow a base model to be quickly conditioned to perform a new task described in words, without full retraining. Open vocabulary and combinatorial generalization are also challenges. a VLA model might know 100 verbs it was trained on, what if asked to do a novel verb? Ideally, if it has a grounding of that verb from language pretraining, it could attempt to execute it by analogy. There have been glimpses of this (like RT-2 doing tasks involving novel objects by leveraging concept knowledge 24). To systematically extend this, covering broader verb/action spaces and abstract goals is needed. For instance, "decorate the cake" is an abstract instruction requiring creative combination of actions 
- no dataset will have all such instructions, so generalization is key. Multimodal datasets that include outcomes of tasks (images of before/after) might help models understand effects, not just motions.

- Multimodal Reasoning and Memory: A challenge is getting models to perform complex reasoning or multi-step decision-making internally while still acting in the environment. Chain-of-thought prompting is a rudimentary method where the model basically produces text internally 47, but more robust methods (like integrating a symbolic reasoning module or an explicit memory of past events) may be necessary for long missions (imagine a household robot tasked with spring cleaning - it requires planning and remembering what has been cleaned already). One possible direction is neuro-symbolic hybrids: using neural nets for perception and low-level actions, but symbolic planning or logical reasoning for high-level decision. There's mention of "unified neuro-symbolic planning" as a forward-looking idea 98. In practice, that could mean the VLA model interfaces with a planning algorithm or knowledge base. Another aspect is learning from feedback and corrections: if a VLA model makes a mistake or misunderstands, how can it be corrected on the fly? Interactive fine-tuning (reinforcement learning from human feedback, RLHF, which was used to align chatbots) could be used to align robots as well - e.g., a human says "No, that's not what I meant, do X instead" and the model updates. This is largely unexplored but important for safety.

- Generalization to Unseen Tasks and Environments: While VLAs show some zero-shot generalization, systematically adapting to entirely new tasks or physical setups remains hard. Gemini Robotics emphasizes adaptability to new robots 70 99 - presumably by training on diverse embodiments and using a very general reasoning core. But what about new tasks? The field would benefit from a standard benchmark for generalization - for example, a robot trained in one kitchen should work in another with different layout and objects, or a model trained on pick-and-place tasks should handle a simple assembly task if described. Progress here might involve modular skills that the model can compose. Some researchers look at combining VLAs with libraries of primitive skills (like navigation, grasping) that can be invoked. If the model doesn't know how to do something, it might call an external API or tool - analogous to how some AI systems use tool use (like calling a calculator). A robot might have a library of controller primitives and the VLA model's job could be to sequence or parameterize them given a high-level command (this revisits the planner-executor structure, but potentially more integrated). Ensuring robust generalization also means tackling distribution shifts: lighting changes, moving from simulation to reality, encountering unknown objects. Using massive web data was one strategy (the belief being that internet training gives a model enough variety to handle unusual cases). Indeed, RT-2's web pretraining conferred surprising robustness to odd inputs 23 24. Future models might incorporate even more modalities (audio, tactile) to be resilient - e.g., understanding sounds or using touch sensors for feedback when vision is uncertain.

- Multimodal Action Representations: We discussed action representation challenges; the field is still seeking the best way to represent actions that is both learnable and expressive enough. Discrete tokenization worked for robot arms with limited DoF, but as we go to humanoids or legged robots, representing whole-body motions, gaits, etc., is tough. Perhaps hierarchical action spaces (where high-level tokens represent subroutines or parameterized skills) could compress the problem. One interesting direction is using natural language as the action space for certain levels of control - for instance, an LLM could output "move base forward 1m, then turn 90 degrees" as text, which is then parsed by a lower controller. This uses language as an intermediate, which is human-readable and possibly easier to correct. Some robotics platforms do have APIs that accept high-level text commands (like "go to room B") which are then internally mapped to motion; a VLA model could produce those. However, for fine motions, we likely need continuous outputs - so a hybrid of discrete high-level steps and continuous refinement might be ideal.

- Safety and Ethical Considerations: Embodied AI introduces physical safety risks. A language model making a mistake might produce an offensive sentence; a VLA model making a mistake could knock over a vase, or worse, injure someone. Ensuring safety is paramount, which means building fail-safes and constraints into the model. For example, if a command is unclear or seems dangerous ("VLA model, please knock over that shelf"), the system should refuse or ask for confirmation. This brings in aspects of AI alignment and policy governance that have been discussed in pure LLMs, now in a physical context. DeepMind's Gemini Robotics page explicitly mentions a comprehensive approach to safety with expert collaborations 100. Some techniques might include geofencing the action space (robot won't apply more force than X, won't enter restricted zones, etc.) and using language understanding to detect problematic instructions (like anything that could cause harm, or violate laws or ethical norms). There's also the risk of biases – if a VLA model learned from internet data, it might have cultural biases or incorrect assumptions that could manifest in physical actions (e.g., only handing objects to certain people if it picked up bias on roles). The community is aware of these issues, and applying fairness and ethics auditing to embodied AI is an emerging area. Additionally, user privacy can be a concern – a VLA robot in a home has cameras and microphones; ensuring that the model and data handling respect privacy (perhaps through on-device processing, as Gemini On-Device suggests 72, so images don't leave the home) will be important for public acceptance.

- Evaluation and Benchmarking: Unlike static tasks (like image classification or QA with a known dataset and accuracy metric), evaluating VLA models is complex. How do we measure "success" of a generalist robot? It might involve task completion rates on a battery of tasks, safety incident rates, human satisfaction scores, etc. Creating standard benchmarks that cover a wide range of tasks and scenarios is an open challenge. There are some efforts: e.g., CALVIN benchmark for language-conditioned continuous control, or the BEHAVIOR benchmark for household activities with a simulator, etc. But these only cover slices. The field may need something akin to an "AI embodied IQ test" – a comprehensive exam where the robot must solve various instructed tasks it hasn't seen, perhaps in a standardized simulated environment (to be repeatable). As VLA models become more capable, curriculum tests could be devised to probe their understanding of spatial relations, tool use, causality, etc., via language instructions. Another angle is evaluating how well the model's explanations align with actions (like in driving, does the commentary truly reflect the decision process?). Alignment between thought and action can build trust. We might see regulations or standards emerge, for example requiring that an assistive robot explain its reasoning in natural language for transparency – which would push evaluation of that aspect too.

- Collaboration and Social Intelligence: A frontier challenge is endowing VLAs with more social awareness – understanding not just physical tasks, but social cues and language pragmatics when interacting with humans. For instance, if a human is gesturing or pointing (non-verbal cues), the "vision" part needs to interpret that and combine it with language. Models like SayCan were purely verbal; future robots might see a human looking at an object and infer that's the object of interest. Integrating such capabilities will require extending the multimodal inputs (vision not just for objects but for human body language) and perhaps using theory of mind reasoning (keeping track of what the human knows or intends). Some initial work uses LLMs to improve human-robot collaboration by predicting human intent. This is a rich area overlapping with HRI (human-robot interaction) research. Also, multi-robot collaboration scenarios will push the envelope on the model's ability to coordinate plans in language or other communication.

In summary, while Vision- Language- Action models have opened a path to more general and intelligent embodied agents, many scientific and engineering hurdles remain. The community is actively addressing issues of speed, generalization, safety, and interaction. The challenges are multifaceted – requiring advances in model architectures (e.g. for real- time control), training methodologies

(better leveraging of unlabeled data, simulations, etc.), and even cross- disciplinary input (ethics, cognitive science for human- like reasoning, etc.). Overcoming these will not only make robots more capable but also more trustworthy and useful in everyday life.

## 7 Conclusion and Outlook

Vision- Language- Action models represent a convergence of the previously distinct fields of computer vision, natural language processing, and robotics control into a unified learning paradigm. In this survey, we have traced their development from early conceptual systems to the latest generalist models, outlined their architectures, and examined their emerging applications. A recurring theme is generalization through unification: by training on diverse multimodal data, a single VLA model can exhibit competencies that traditionally required separate specialized systems - seeing, speaking, reasoning, and acting all at once. This holds promise for creating more versatile and adaptive AI agents. A household robot no longer needs to be pre- programmed for every possible task; a sufficiently trained VLA model could interpret novel instructions and leverage its broad knowledge to figure out how to comply 24 65 . Similarly, an autonomous car with a VLA system can interact with humans in a more natural way and justify its behavior in human terms 53 .

Despite the progress, today's state- of- the- art VLAs are still stepping stones toward the long- term vision of general- purpose embodied AI. However, each generation is rapidly closing the gap. Models like Gemini Robotics hint at an AI that can understand the world in rich multimodal terms and apply that understanding to any robot, any task given the right prompt 68 101 . In the coming years, we anticipate several trends:

- Larger and Smarter Models: Just as vision and language models exploded in scale and capability (e.g., GPT-3 to GPT-4, or ViT to billions of parameters), VLA models are likely to grow in scale and incorporate even more of the world's knowledge. They may also integrate more modalities – for instance, adding audio (so the robot can respond to spoken language and even recognize sounds in the environment) or tactile inputs for fine manipulation. Google's Gemini is already multimodal in vision, audio, etc., so a Robotics version is naturally heading there 102. With scale and better algorithms, we might see emergent capabilities in VLAs such as commonsense physical reasoning (predicting what will happen if they take an action) and perhaps rudimentary emotional intelligence (gauging if a human is frustrated or pleased with its actions through visual cues).

- Improved Training Paradigms: We will likely move beyond purely imitation learning on fixed datasets. Models could be trained in open-ended environments where they have to learn by trial and error, guided by high-level language goals – combining reinforcement learning or self-play with the language-conditioned objective. There's also interest in lifelong learning for VLAs: allowing a deployed robot to continuously learn new things on the job (while not forgetting old skills). Techniques like memory rehearsal or on-policy fine-tuning in simulation after deploying in real might come into play. Cloud robotics frameworks could allow a fleet of robots to collectively improve a shared VLA model – each robot's experience (and the associated language context) feeding into a central update, akin to how self-driving car companies pool data. Privacy and safety will need to be managed in that loop.

- Standardization of Interfaces: As VLAs mature, we might see common API or interface standards. For example, a robot operating system (ROS) might include a VLA module with a defined way to send it a language command and get back motor commands or a plan. This would allow different robots to plug in the same brain. Already, efforts like the "Gemini Robotics SDK" are hinting at

tools for developers to adapt a foundation model to their robot easily 103 . If successful, this could lead to a proliferation of robots in various industries all using variations of a handful of foundation VLA models - much like many applications today use a common large language model as a base.

- **Ethical Frameworks and Policies:** With robots interacting more closely with people, expect development of guidelines akin to Asimov's laws but implemented in AI policy. For example, VLAs might be trained with explicit "do no harm" directives and be tested extensively for edge cases (e.g. if a child tells a home robot to do something dangerous, the robot must refuse and perhaps alert an adult). We may also see regulatory oversight – perhaps requiring certification for AI that controls machinery around humans. This could drive research into verification of VLA behavior: how do we formally or empirically verify a learning-based policy will not take unsafe actions? It's a hard problem but critical for deployment in sensitive areas (like eldercare robots or autonomous vehicles in cities). Collaboration between AI researchers, ethicists, and policymakers will be crucial to ensure these powerful systems are deployed responsibly.

- **Fusion of VLA with "Agentic AI":** A concept emerging is agentic AI – AI that has more autonomy in deciding its goals or solving problems proactively. Some authors suggest VLAs will converge with this, meaning future embodied agents might not just wait for commands, but could take initiative to help. For example, a home robot noticing the floor is wet might say, "I see a spill; would you like me to clean it up?" unprompted. Achieving this requires the AI to have a model of human preferences and a mechanism to balance proactivity with correctness (you don't want a robot doing something you explicitly don't want). The convergence with agentic AI also implies integration of planning, long-term memory, and even navigation of knowledge bases or the internet for information. For instance, a robot might encounter an unknown appliance and quickly query an online manual (all through its language model) to figure out how to operate it. In essence, the robot becomes an autonomous problem-solver, not just a command follower 30.

- **Human Augmentation and Collaboration:** Over time, VLAs might not just be about replacing human effort but enhancing it. Consider exoskeletons or assistive arms that amplify what a human can do – a VLA model could interpret the user's natural motions and verbal guidance to coordinate with them, effectively creating a human-robot team acting as one. In less direct ways, a VLA agent in AR (augmented reality) could serve as a second pair of eyes and an advisor in complex tasks (like a technician fixing an engine with an AR assistant that points out parts and warns of mistakes). These scenarios hinge on fluid interaction and trust, which VLA's multimodal understanding can facilitate.

In conclusion, Vision- Language- Action models have made substantial leaps toward the enduring goal of embodied AI that can learn and communicate in human- like ways. By blending the strengths of vision (grounding in reality), language (high- level abstraction and interaction), and action (causal impact on the world), they inch closer to the notion of robots that "understand what we mean and do what we intend." The journey is far from over – as our survey discussed, many challenges remain – but the progress in just a few years has been remarkable. Continued interdisciplinary research and responsible development will determine how quickly and how safely these technologies mature. If successful, VLA models could power a new generation of robots and autonomous systems that profoundly transform daily life, working alongside humans in homes, hospitals, roads, and beyond – not as mindless automatons, but as perceptive partners endowed with the ability to see, listen, and act to fulfill our needs. The excitement around recent breakthroughs suggests that this vision, once confined to science fiction, is steadily moving into the realm of possibility, guided by the combined advances in vision, language, and action understanding.

References: The content above references key developments and sources in the field, including foundational research papers, surveys, and official blog posts from research labs and companies. Notable sources include DeepMind's reports on Gato 28 29 and RT- 2 43 48, Google Research blog posts on PaLM- SayCan 6 33 and PaLM- E 36 37, the Wikipedia synopsis of VLA models and their history 104 49, the Figure Al Helix announcement 21 59, Wayve's LINGO- 2 blog for autonomous driving VLAM 50 51, and several academic surveys 105 106 that outline taxonomy and future directions. These sources collectively provide the evidence and examples underpinning this survey of Vision- Language- Action models.

## References

1 2 13 14 15 16 17 19 20 22 42 44 45 49 55 56 57 58 62 66 67 68 69 70 71 72 73 74 75 83 84 104 Vision- language- action model - Wikipedia https://en.wikipedia.org/wiki/Vision- language- action_model

3 4 27 SHRDLU - Wikipedia https://en.wikipedia.org/wiki/SHRDLU

5 6 7 32 33 34 35 90 Towards Helpful Robots: Grounding Language in Robotic Affordances https://research.google/blog/towards- helpful- robots- grounding- language- in- robotic- affordances/

Google DeepMind

https://deepmind.google/discover/blog/rt- 2- new- model- translates- vision- and- language- into- action/

18 36 37 38 39 40 41 76 PaLM- E: An embodied multimodal language model https://research.google/blog/palm- e- an- embodied- multimodal- language- model/

21 59 60 61 62 64 65 77 79 79 80 81 82 85 86 87 91 95 96 Helix: A Vision- Language- Action Model for Generalist Humanoid Control https://www.figure.ai/news/helix

26 106 [2405.14093] A Survey on Vision- Language- Action Models for Embodied AI https://arxiv.org/abs/2405.14093

28 29 30 31 89 A Generalist Agent - Google DeepMind https://deepmind.google/discover/blog/a- generalist- agent/

50 51 52 53 54 92 LINGO- 2: Driving with Natural Language - Wayve https://wayve.ai/thinking/lingo- 2- driving- with- language/

88 VLM- RL: A unified vision language models and reinforcement ... https://www.sciencedirect.com/science/article/abs/pii/S0968090X25003250

91 Personalized Autonomous Driving with LLMs and VLMs - Autoware https://autoware.org/driving- by- conversation- personalized- autonomous- driving- with- llms- and- vlms/ 93 97 98 105 [2505.04769] Vision- Language- Action Models: Concepts, Progress, Applications and Challenges https://arxiv.org/abs/2505.04769 99 100 101 102 103 Gemini Robotics - Google DeepMind https://deepmind.google/models/gemini- robotics/