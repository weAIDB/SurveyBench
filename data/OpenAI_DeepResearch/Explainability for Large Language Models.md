# Explainability for Large Language Models

## 1 Introduction

Large Language Models (LLMs) are a class of deep neural networks—often with hundreds of millions to billions of parameters—trained on massive text corpora to generate human- like language  $^{1}$ . These models underpin state- of- the- art systems in machine translation, question answering, dialogue (e.g. ChatGPT), summarization, and more. With their growing deployment in high- stakes domains (medicine, law, education, etc.), there is a pressing need to understand why an LLM produces a given output, not just what the output is. This capability, known broadly as explainability or interpretability, is crucial for building user trust, diagnosing errors, ensuring fairness, and aligning models with human values  $^{2}$ . However, explaining the behavior of such complex black- box models is exceedingly challenging. In this survey, we provide a comprehensive overview of explainability for LLMs, covering formal definitions of key concepts, foundational and recent methods, a historical timeline of milestones, applications across subfields, and open challenges. Our aim is to balance technical depth with clarity: newcomers will find an accessible entry point to the topic, while experts will find up- to- date coverage of the latest developments.

## 2 Key Concepts and Definitions

Before delving into methods, we clarify the terminology and scope of explainability in the context of LLMs:

Interpretability vs. Explainability: Interpretability generally refers to the intrinsic ability of a model to be understood by humans, whereas explainability often implies a post- hoc process of generating an explanation for an otherwise opaque model  $^{3}$ $^{4}$ . For example, a simple linear model is interpretable (its weights directly indicate feature importance), but a large transformer is not; instead, we apply explainability techniques to provide insight into its decisions. Doshi- Velez and Kim (2017) formally define interpretability as "the ability to explain or present [model decisions] in understandable terms to a human"  $^{5}$ . Explainability techniques strive to fulfill this by producing artifacts (such as feature attributions, visualizations, or natural language justifications) that make the model's behavior more comprehensible.

Transparency and Trust: Explainability is closely linked to the concept of transparency in AI. A transparent model reveals aspects of its internal workings or decision process to users, fostering appropriate trust  $^{6}$ . The goal is not blind trust, but calibrated trust – users should trust the model when it's correct and understand its limitations when it's not. An explanation serves as a sanity check, helping to validate whether an LLM's answer was arrived at for the "right reasons." This is critical given LLMs' tendency to occasionally produce confident- sounding yet incorrect or biased outputs.

Scope: In this survey, we focus on explainability methods for text- based LLMs (as opposed to multimodal models or vision networks). Many techniques are adapted from general Explainable AI (XAI) research, but we highlight unique challenges and innovations in the language domain. We cover both global explanations (understanding model behavior or knowledge as a whole) and local explanations (understanding individual outputs or decisions)  $^{7}$ . Additionally, we consider

both post- hoc explanations (analyzing a trained model after the fact) and intrinsic interpretability approaches (designing models or training procedures that make the model's workings more interpretable by design).

- Large Language Models (LLMs): While there is no strict parameter threshold, "large" typically refers to models on the order of billions of parameters, which emerged around 2018-2020 (e.g. BERT with 340M parameters, GPT-3 with 175B). LLMs are usually Transformer-based architectures  $\S$  characterized by self-attention mechanisms and deep layers. Their size and training on broad data endow them with surprising capabilities, but also make them difficult to interpret. Throughout this survey, we use "LLM" to denote such models, inclusive of both decoder-only (e.g. GPT series) and encoder-decoder (e.g. T5) Transformers.

## 3 Historical Timeline of Major Milestones

Explainability in machine learning has a rich history, but its intersection with large language models is relatively recent. Below, we outline a timeline of notable milestones and developments leading up to the current state of the field:

- 1980s-1990s: Early interpretability and expert systems. In the era of symbolic AI and expert systems, models were often inherently interpretable – they reasoned via human-readable rules and could provide logical audit trails for their conclusions. As machine learning shifted toward statistical models and neural networks, concern grew over the "black-box" nature of these approaches  $\S$  . Early research on explaining neural networks (e.g. extracting rules from trained networks) was explored in the 1990s, though in relatively simple networks by today's standards.

- 2015: Policy and program impetus. The need for explainable AI was formally recognized by institutions and governments. Notably, DARPA formulated the XAI (Explainable AI) research program in 2015, which began funding efforts to produce "glass box" models or explanation tools for black-box models  $\S$  . This pushed explainability into the mainstream ML research agenda. Around the same time, discussions of "Right to Explanation" in the EU's GDPR (2016) started highlighting the societal importance of model transparency.

- 2016: LIME and model-agnostic local explanations. Ribeiro et al. introduced LIME (Local Interpretable Model-agnostic Explanations)  $\S$  , a technique to explain any classifier's individual prediction. LIME generates perturbed versions of an input (e.g. removing words in a text) and learns a simple surrogate model (like a linear model) around the neighborhood of that input to estimate feature importance. This seminal work provided a widely used language-agnostic tool for local explainability, e.g. highlighting words in a sentence that influenced an LLM's classification decision.

- 2017: Shapley values and theoretical rigor. Lundberg and Lee unified several feature attribution methods under the framework of Shapley Additive Explanations (SHAP)  $\S$  . SHAP assigns each feature an importance value for a given prediction based on principles from cooperative game theory (the Shapley value). This method offered theoretical guarantees (satisfying properties like fairness, consistency) and became another popular tool for explaining model predictions, including those of text classifiers. Also in 2017, Vaswani et al. introduced the Transformer architecture  $\S$  , with its self-attention mechanism. While not designed for interpretability per se, attention distributions were soon viewed as a potential source of insight into model reasoning (sparking debate, as discussed below). The Transformer enabled the scale-up to modern LLMs, making explainability challenges more acute.

- 2018: Interpretability for NLP tasks. With the advent of large pre-trained models like ELMo and BERT, researchers began probing what these models learn. Techniques such as probing classifiers were used to interpret linguistic knowledge in embeddings (e.g. is part-of-speech or syntactic structure encoded in BERT's layers?). At the same time, rationale generation methods appeared in NLP; for example, e-SNLI (2018) provided a dataset of human-written natural language explanations for NLI (entailment) decisions, enabling models to be trained to output a human-like explanation alongside their prediction. While these explanations improved transparency, a key question emerged: do such generated explanations truly reflect the model's internal reasoning or just appease the user? This foreshadowed later work on explanation faithfulness.

- 2019: Attention is not (necessarily) explanation. Jain & Wallace (2019) published a provocative paper "Attention is not Explanation," demonstrating that the magnitudes of attention weights in NLP models often do not correlate with other feature-importance measures, and that one can sometimes redistribute attention weights without changing the model output 12. In short, just because a model "attends" to a word does not always mean that word was pivotal for the decision. This spurred a rebuttal "Attention is not not Explanation" (Wiegreffe & Pinter, 2019), arguing that attention can still be useful for interpretability if used carefully (for example, as a mechanism to extract a rationale by design, or when attention distributions sharply focus on specific tokens) 13. This debate highlighted the need for rigorous evaluation of explanation methods. Also in 2019, the concept of influence functions (a classic idea from robust statistics) was revisited for deep nets, enabling researchers to trace which training examples most impacted a given prediction – an idea later scaled to LLMs.

- 2020: Emergence of LLMs and new XAI challenges. OpenAI's GPT-3 (mid-2020) demonstrated an unprecedented jump in language modeling capability via a 175 billion-parameter transformer, capable of few-shot learning. This marked the start of the LLM era, raising the stakes for explainability. Concerns of hallucination (LLMs making up facts), bias, and unpredictable behavior became front-page issues. In explainability research, 2020 saw the introduction of the ERASER benchmark, a collection of datasets with human annotations for rationales, to systematically evaluate explanation methods and models' ability to provide faithful rationales. Methods like Integrated Gradients (a 2017 method) were now applied at scale: for instance, researchers used them to attribute a language model's next-word prediction to input tokens or even to intermediate neurons.

- 2021: Knowledge neurons and probing deeper. As LLM analysis deepened, Dai et al. (2021) identified "knowledge neurons" in transformers 14 – neurons whose activation correlates with the expression of specific factual knowledge. By intervening on such a neuron's activity, one could cause the model to forget or recall a particular fact, suggesting a degree of localization of knowledge. This finding hinted at modularity within the tangle of neural weights. Meanwhile, mechanistic interpretability research gained traction: rather than treating the model as a black-box to explain, this approach seeks to reverse-engineer the model's internal mechanisms, circuit by circuit. Early successes included identifying circuits in GPT-2 for linguistic tasks (e.g. a circuit for indirect object identification in a sentence) 15. These efforts were part of a broader trend, often driven by safety and alignment researchers, to open up the black box and understand how exactly specific concepts or computations are realized within large networks.

- 2022: Alignment and interpretability meet. The year saw large-scale efforts to align LLMs (e.g. via reinforcement learning from human feedback), which in turn underscored the importance of interpretability to verify alignment. OpenAI, Anthropic, and others invested in interpretability teams. Techniques for editing model knowledge (such as ROME, MEMIT, and others) were introduced, which depend on locating where a factual association is stored in the model's

weights - essentially a form of interpretability used for model debugging. There was also growing acknowledgment of evaluation challenges: how do we know if an explanation method is "faithful" (truly reflects the model's decision- making) as opposed to just "plausible" (merely convincing to a human)? This concern led to new metrics and tests, some of which revealed that plausibility and faithfulness can diverge significantly.

- 2023: Chain-of-thought and its discontents. By 2022, prompting LLMs to generate chain-of-thought (CoT) explanations 
- i.e. a step-by-step reasoning trace in natural language 
- became popular for improving performance on reasoning tasks. In 2023, researchers critically examined these explanations. Turpin et al. (2023) found that CoT explanations, while looking reasonable, often misrepresent the true reasoning behind an LLM's answer 16. For instance, they showed that by subtly biasing an input (e.g. reordering options so that the correct answer is always choice A), models would still produce fluent CoT justifications that omit mentioning the bias and instead rationalize the answer on other grounds 17. In cases where the bias tricked the model into a wrong answer, the CoT would confidently explain the incorrect reasoning. These results caused up to a 36% accuracy drop on certain tasks when a bias was introduced, all while the model's self-explanation appeared valid 18. This highlighted that LLMs can generate explanations that sound coherent but are not faithful to the actual decision process, a crucial pitfall for explainability. In response, efforts emerged to make CoT more faithful, or to develop alternative strategies for self-explanation.

- 2024: Scaling interpretability to larger models. Very recent work has demonstrated interpretability techniques on state-of-the-art LLMs. For example, Anthropic's interpretability team managed to extract meaningful monosemantic features from a 52-billion-parameter model (Claude 3) by training sparse autoencoders on its activations 19 20. They discovered neurons (or small sets of neurons) corresponding to high-level concepts like famous people, cities, or code vulnerabilities, which were consistent across languages and modalities 21. Intriguingly, they identified features potentially linked to bias, deception, or dangerous content, offering a tentative foothold for safety monitoring 22. Another milestone was scaling up influence functions to LLMs: researchers showed it's possible to trace an output of a 50B+ model back to specific training data influences 23, revealing that larger models rely more on abstract conceptual similarities to training data rather than surface overlaps 24 (Figure 1). These advances in 2024 illustrate both the progress and the remaining gaps in explainability: we can find intriguing patterns and interpretable components in large models, yet a full understanding or interpretability by default remains distant.

- 2025 and beyond: Explainability research is continuing to intensify. There is optimism that mechanistic interpretability (automated or semi-automated discovery of circuits and features) will scale to the largest models, potentially yielding a mapping of the "neurons to knowledge" within an LLM. At the same time, regulatory and societal demands for AI transparency (e.g. the forthcoming EU AI Act) may mandate practical explainability solutions in deployed systems. The field is likely to see hybrids of approaches 
- from human-in-the-loop interactive explanations, to training-time penalties for unexplainable behavior, to community-wide benchmarks for rigorous evaluation of explanation fidelity. We discuss open challenges and future directions at the end of this survey.

## 4 Foundations of Model Explainability

To understand explainability techniques for LLMs, it helps to first review the foundational approaches developed for machine learning models at large. Many of these techniques are model-agnostic

(applicable to any black- box, including LLMs), while others are model- specific adaptations. Here we summarize key categories of explainability methods:

- **Feature Attribution (Saliency Methods):** These methods aim to quantify how important each input feature was to a particular prediction. In the context of text, "features" are typically input tokens or phrases. A classic approach is to use gradient-based saliency, computing the gradient of the model's output (e.g. the probability of a certain answer) with respect to each input token embedding. The magnitude of this gradient can serve as an importance score for the token. An extension is **Integrated Gradients** (Sundararajan et al. 2017), which accumulates gradients as the input is interpolated from a baseline (e.g. empty text) to the actual input, yielding an attribution that satisfies certain axioms (e.g. completeness) 25. Feature attribution can also be done by occlusion or ablation 
- e.g. systematically remove or mask each word and see how much the model output probability changes (the intuition being that if removing a word greatly lowers confidence, that word was important). **Saliency maps or highlighting** are visualizations of these attributions on the input text 26 7. For example, in a sentiment analysis by an LLM, an explanation might highlight words like "excellent" or "disappointing" in green or red to show their positive/negative influence on the model's prediction. An important point is that while saliency highlights often look intuitive, they must be interpreted with care 
- they are correlational explanations and can miss context interactions (a word might only be important in presence of another word, etc.).

- **Local Surrogate Models:** Instead of directly attributing importance, one can train an interpretable model to locally approximate the LLM's mapping from inputs to outputs 7. LIME, mentioned earlier, is a prime example: for a given input, sample perturbed versions (e.g. drop some words) and see how the LLM's output changes; then fit a simple model (like linear regression) that uses binary features (word present or not) to predict the LLM's output on those perturbed inputs. The learned weights of this surrogate model act as explanation 
- essentially, a linear approximation of the LLM's decision boundary around the instance 27. Another approach in this vein is to use decision trees or rule-based surrogates that approximate the model on a region of the input space. Surrogates can provide either local explanations (fitting near one point, as LIME does) or global explanations (fitting on a broader set of inputs). The advantage is that the surrogate is inherently interpretable (e.g. a small tree or sparse linear model). The challenge is ensuring the surrogate is faithful 
- it must approximate the LLM well in the relevant region. If the model's decision boundary is very complex, a simple surrogate might be misleading. Despite this, surrogate methods remain popular for explaining classification decisions of LLMs (e.g. explaining a document classification by learning which keywords the model is essentially using) 28.

- **Example-Based Explanations:** Often, an intuitive explanation for a model's behavior is analogical: "The model answered this way because it has seen something similar before." Techniques in this category find influential training examples or prototypes that significantly influenced the model's output. **Influence functions** (Koh & Liang, 2017) approximate the effect of removing a given training example on the model's predictions 29. Initially practical only on smaller models, these have been scaled up to LLMs by algorithmic tricks, including efficient Hessian approximations 30. A recent study identified, for a 52B parameter LLM, which training documents most increased the probability of a specific model output 1. Strikingly, they found that larger models' outputs were influenced by more abstractly related examples rather than verbatim memorization: for a prompt about an AI not wanting to be shut down, a 52B model's top influences included texts about survival and sentience, whereas a 0.8B model latched onto unrelated snippets sharing just a key phrase 24. Figure 1 illustrates such differences in influential examples between a small and large model. Another line of example-based explanation is nearest-neighbor search in the embedding space: given an input or a generated output, retrieve

the closest training examples in representation space. This can highlight which past data the model might be “remembering” or extrapolating from. Example- based explanations are especially useful to detect potential training data leakage or biases (e.g. if the model’s toxic output was triggered by a specific offensive training example).

![](images/22cf1934ffb784e82a53a4b494817889e9f9dd5175026e5811324c8b51afa40a.jpg)  
Figure 1: Examples of training-data influence on model outputs, comparing a smaller 810M-parameter model vs. a larger 52B-parameter LLM 24 31. In a reasoning task (prompt omitted for brevity), the smaller model’s most influential training text (highlighted portion in top excerpt) is only superficially related – it contains the word “clips” repeatedly (in a legal context) matching a key token in the prompt, but is semantically irrelevant. The larger model’s top influence (bottom excerpt), while not sharing obvious keywords, discusses a similar conceptual structure (e. puzzle about friends and averages), indicating the big model learned a more abstract generalization 24. This illustrates that as models scale, their explanations for outputs (in terms of training influence) become less about surface overlap and more about high-level similarity, a desirable trend for true understanding. (Figure content adapted from Anthropic’s analysis of influence functions.)

- Visualization of Internal Mechanics: Opening the black box, we can visualize parts of the model itself to gain insights. In computer vision, visualization of neurons (e.g. heatmaps of what a CNN filter detects) is common. For LLMs, useful visualizations include attention maps and saliency heatmaps over text. Attention maps show how strongly each word attends to others in each self-attention layer. Tools like BERTviz allow users to pick a head and see lines connecting words with thickness proportional to attention weight. Such visualizations can sometimes reveal intuitive patterns (e.g. heads that attend mostly to the next word, or heads that align pronouns to their referents). However, given dozens of heads per layer in an LLM, it can be hard to interpret which attention is relevant for a task. Another approach is activation visualizations: for example, projecting the hidden states or neuron activations of words into 2D to see clustering of concepts, or plotting how a particular neuron’s activation varies across different inputs. One notable technique is the Activation Atlas (Carter et al. 2019) which provided a visual map of neurons for vision models; analogous efforts for text involve clustering neurons by the words or contexts that maximally activate them 21. Visualization also extends to model graphs: showing, say, how information flows from input to output, possibly highlighting a critical path or “circuit” inside the network responsible for a certain behavior. These methods often require expert analysis but

can yield surprising discoveries (e.g. finding a neuron that acts as a Boolean flag for whether the text is talking about the past or future).

- Intrinsic and Self-Explanatory Models: Instead of explaining a black-box after the fact, another strategy is to build models that explain themselves as they go. For LLMs, one form of this is self-rationalization: the model is trained or prompted to produce a justification along with its answer. For example, a model might output: "Answer: The defendant is liable because the contract clause explicitly states X, and prior cases set a precedent Y." These explanations can be in natural language or in a structured form like a logic tree. Some architectures have been proposed where the model first generates an explicit rationale (a subset of the input or some intermediate explanation), then bases its final answer on that rationale (ensuring consistency) 32. An early instance is the Rationalizing Neural Prediction approach (Lei et al. 2016) where the network's first component selects a few key text snippets from the input, and the second component uses only those to make a prediction, thereby guaranteeing those snippets form a faithful rationale. Such approaches imbue interpretability by design. In the LLM era, chain-of-thought prompting is a mild form of self-explanation: we simply ask the model to "think step-by-step" and show its reasoning. This has been very effective for solving math and logic problems, but as discussed, it doesn't guarantee faithfulness to the model's actual computation. Researchers are exploring ways to enforce a correspondence between the model's latent computations and the verbal reasoning it outputs - for instance, by training the model with a special loss to align its internal activations with the steps of a known reasoning procedure (a form of program synthesis or interpretability). Another direction is interactive explanation: the model can answer follow-up questions about its own output, e.g. "Why did you phrase it this way?" or "What evidence from the text supports your summary?" . By treating explanation as a dialogue, users can drill down until satisfied. This is an active area of HCI and NLP research, aiming to make explanations more accessible and tailored to the user' s needs.

- Concept Extraction and High-Level Features: Humans often think of explanations in terms of high-level concepts (like "the model thought the text was about sports due to words related to sports"). There are methods to bridge between neurons and human concepts: one is TCAV (Testing with Concept Activation Vectors), originally for vision but extended to NLP 33. TCAV defines a vector direction in model's latent space corresponding to a concept (e.g. a sentiment concept defined by examples of positive vs negative sentences) and measures how sensitive the model's output is to moving in that direction. This gives a conceptual attribution: e.g. "this review was classified positive mainly because of the presence of the concept 'positive sentiment' as captured by the model." In LLMs, defining concepts can be tricky, but work like the Knowledge Neurons approach effectively treats each factual attribute (say "Einstein's birthplace") as a concept and finds neurons that correspond to it 14. Another emerging idea is monosemantic neurons - ideally each neuron or a small subset could be aligned to a distinct interpretable concept (as attempted by Anthropic with sparse autoencoders) 19. If successful, one could explain outputs in terms of which concepts (neurons) were activated. For instance, an LLM answer might be explained as: "this response was influenced by features X and Y - X is a neuron associated with 'legal context', and Y is a neuron for 'apology sentiment' - together pushing the model to produce a polite legal tone."

## 5 Explainability Methods Specific to LLMs

Building on the foundations above, researchers have developed or adapted numerous methods with LLM- specific considerations. Large language models present some unique challenges for explainability: sequence generation (output is not a single label but a sequence of tokens), multi- step reasoning, huge

input contexts, and world knowledge embedded in billions of parameters. Here we outline major approaches and recent methods tailored for LLMs:

### 5.1 Feature Attribution for Text Generation

While attribution is straightforward for classification (attributing importance to input tokens for a single decision), for generation it becomes more nuanced. An LLM generating a long answer might require explaining each output token or the whole sequence. One approach is per- token attribution: e.g. use integrated gradients to attribute the probability of each generated token to the input tokens (which could include the prompt or context) 34 . This can highlight, for instance, which parts of a prompt or question were most influential for each part of the answer. If the model also has retrieved documents (as in retrieval- augmented LLMs), attribution can include those sources (indeed, some LLM- based QA systems now highlight which sentences from a reference text led to each answer sentence as a built- in feature). Another approach is causal attribution within the model: e.g. for a given generated token, trace backward to see which earlier layer activations or neurons had the strongest causal effect on that token' s probability. Techniques like layer- wise relevance propagation and DeepLIFT have been explored for this, though applying them to a 100- layer Transformer requires careful approximation to remain tractable.

A particularly interesting method is "Attention flow" attribution: since Transformers pass information via attention, one can back- propagate an output importance through the attention graph to distribute credit to input tokens. Some words have computed such attention rollout, aggregating attention weights across layers to estimate each input token' s influence on an output. However, the interpretation is complicated by nonlinearity and the question of whether attention weights truly signify influence (per earlier debate). Nonetheless, visualization of attention during generation can be insightful: for example, in translation tasks, the attention of the decoder to source words often aligns with expected word correspondences, making a decent explanation of which source phrase a target phrase is tied to - an idea that traces back to the attention mechanisms in seq2seq models which were partly motivated by interpretability of alignment.

Beyond input attribution, one may want to attribute outputs to the model' s internal knowledge. For example, if an LLM states a factual claim not explicitly in the prompt, we might ask: which training data or stored fact in the model contributed to this? Methods to do this include knowledge tracing (identify which training examples containing that fact had high influence) and factual neuron tracing (find neurons whose activation indicates that specific fact). Recent work on factuality metrics also try to measure how much an answer is supported by provided context vs. model' s own priors, which is a form of attributing the answer either to context or to the model' s latent knowledge.

### 5.2 Example: Explaining a GPT-3 Answer

To make this concrete, consider an LLM answering a question: "Who is the author of The Hobbit?" The model outputs: "The Hobbit was written by J.R.R. Tolkien." An explanation pipeline for this could be: (1) highlight the part of the question prompting the answer (probably "author" and "The Hobbit" are key, though it' s a short prompt); (2) since the answer presumably comes from the model' s internal knowledge (no additional context given), find the training sources - perhaps the model was trained on Wikipedia articles about The Hobbit and Tolkien, which we could identify via an influence function 29 ; (3) at the neuron level, identify a "knowledge neuron" associated with the fact "The Hobbit  $\rightarrow$  Tolkien" (if it exists) or the cluster of weights that store this; (4) optionally, have the model justify its answer: e.g. it might continue, "... which is a fantasy novel published in 1937" - while not exactly an explanation of why it answered, this provides context showing it has broader knowledge on the topic, increasing our confidence. We might also verify counterfactually: if we tweak the question (e.g. "Who is

the author of The Lord of the Rings?"), does the model correctly switch to Tolkien as well? If yes, it's consistent. If not, and it answers something incorrect, analyzing that divergence could be insightful (perhaps it confuses authors if phrased differently, meaning the explanation might be that it relies on a pattern linking book titles to authors in training data).

This simple Q&A example belies the complexity in real scenarios where questions are ambiguous or answers are long and derived from multiple pieces of knowledge. Nonetheless, it shows how multiple explanation methods can converge: highlight input cues, reference similar data, probe internal representations, and have the model articulate reasoning.

### 5.3 Mechanistic Interpretability and Model Internals

One of the most advanced fronts of explainability for LLMs is mechanistic interpretability - attempting to reverse- engineer the model's computations. In essence, it treats the trained model like a scientific phenomenon to be analyzed. Key techniques and findings include:

- Circuit Analysis: A circuit is a small sub-network (a subset of neurons and weights) within the model that accomplishes a specific function. Pioneered by work from Olah et al. (2020) on vision models, this approach for LLMs has found, for example, a circuit in GPT-2 Small that handles indirect object identification. In a sentence like "Alice told Bob's friend that he was funny," identifying that "he" refers to Bob's friend can be traced to an arrangement of attention heads across layers 15. Researchers identified certain heads that copy subject names, others that detect syntactic roles, and together they form a circuit solving the task. Understanding such circuits is powerful: it's a full explanation at the mechanistic level for that behavior, though limited to very specific tasks so far. As models scale, circuits multiply and become more intertwined, but there is hope that many functions can still be decomposed and interpreted.

- Automated Decomposition: Recent work uses techniques like sparse autoencoders to automatically find interpretable directions in activation space 19. By training an autoencoder on LLM activations with a sparsity penalty, the hidden dimensions of the autoencoder tend to align with meaningful features (because to minimize reconstruction error with a sparsity constraint, it's incentivized to capture salient patterns). In Anthropic's 2024 study, this revealed features corresponding to concepts like "contains a programming vulnerability" or "text involves a conversation with user" 21. These features were monosemantic, meaning each feature corresponded to one concept. If a model's entire state could be described as a combination of such features, we'd have a high-level explainable representation of its processing at each step.

- Neurons and Superposition: One difficulty discovered is that individual neurons in LLMs are often polysemantic – they fire for multiple unrelated reasons (due to an effect called superposition, where the network crams more features than it has neurons into linear combinations). This complicates naive neuron interpretation (unlike early findings like the "sentiment neuron" in an RNN, which encoded sentiment fairly purely). Research into superposition (e.g. Elhage et al. 2022) attempts to understand how and when features are entangled, and whether larger models reduce superposition (some evidence suggests larger models might learn more redundant or specialized features, making them ironically more interpretable in some cases). One implication: explanations focusing on single neurons might miss the full picture; we may need to consider combinations of neurons or latent dimensions post-rotation to truly find the semantic basis of decisions.

- Causal Interventions: A rigorous way to test understanding is to intervene in the model and see the effect. For instance, ablating a neuron or an attention head (setting it to zero) can show if it was important for a behavior – if removing a certain head consistently causes, say, translation quality to drop, that head likely was attending to important context like negation or gender. Causal methods also include resampling or patching activations: take the hidden state from one input, insert it into another model run, and see if the output changes, to isolate where a particular piece of information is located. Such techniques have been used to find where in the network various facts are represented during processing. For example, when an LLM is answering a question, one can swap out the residual stream after reading the question with that from a different question to pinpoint where the model encodes the query’s key aspects.

- Knowledge Editing: An unexpected angle on interpretability comes from methods that edit model knowledge. Tools like ROME (Rank-One Model Editing) locate a specific weight in a mid-layer feedforward network that can be adjusted to change a stored fact (e.g. “Paris is the capital of [MASK]” from France to Spain) without wrecking other performance. The ability to do this suggests that specific factual associations correspond to specific parameters or directions 35. By analyzing these, one can glean how the model organizes knowledge. It also intersects with explainability: one could explain a model’s output by saying “it relied on the connection between X and Y, which is stored in these weights.” Furthermore, if a model’s answer is wrong due to a specific misconception, identifying and editing the responsible weights is a form of explanatory intervention – it’s like saying “the model thought X, because its parameters encoded X, but we changed it to encode the correct Y.” This is of course very nascent and not something done in real-time for each query, but it shows the interplay between understanding and manipulating the model.

### 5.4 Human-Friendly Explanations vs. Formal interpretations

A distinction worth noting is between explanations meant for end- users and interpretations meant for researchers. An LLM can produce a natural language explanation or summary of its reasoning that a non- expert user or domain expert can read. For example, a medical chatbot might explain a diagnosis by saying: “I suspect diabetes because the patient shows symptoms A, B, and has risk factor C.” This is immediately useful to a doctor. On the other hand, an analyst or AI researcher might prefer a different level of explanation: “Neuron 453 in layer 20 heavily influenced this output; it appears to activate on symptom combinations indicative of diabetes.” The latter is not useful for the layperson but is valuable for debugging or improving the model. Our survey primarily focuses on methods that provide the latter (technical) type of insight, but aligning the two is a future goal. Ideally, one could translate the mechanistic explanation into a user explanation: e.g. “the model internally detected a pattern of symptoms matching diabetes, which is why it answered as it did.” There are some efforts on explanation translation – mapping from raw feature attributions or neuron firings to higher- level concepts that make sense to humans (using concept libraries or ontology mapping). Ensuring that user- facing explanations are faithful (i.e. they reflect the actual model reasoning and not a “sugar- coated” version) is a key challenge, as we will discuss.

## 6 Applications Across Subfields and Domains

Explainability for LLMs is not just an academic exercise; it has practical ramifications across various domains where these models are employed. We highlight a few areas and how explainable LLMs are being pursued in each:

Healthcare: LLMs are being explored for summarizing medical records, suggesting diagnoses, or patient Q&A. In such high- stakes settings, a right answer for the wrong reason can be dangerous. Clinicians demand transparency - for instance, if an LLM suggests a diagnosis, it should provide evidence or reasoning, such as pointing to relevant symptoms in the patient' s record or aligning with known medical knowledge 36 37 . Current research involves training LLMs on medical text with citation of sources, so the model can indicate which medical guideline or research paper supports its recommendation. Also, ensuring the model is not relying on biases (e.g. race or gender) in making predictions is critical; explainability techniques like saliency can be used to check if, say, certain demographic words in a patient description were unduly influential on an outcome. Regulatory bodies may even require explanations for automated medical decisions. Therefore, a lot of effort is going into faithful and simple explanations for LLM- based clinical decision support - often coupling a large model with a smaller expert system that double- checks or explains the reasoning.

Legal and Finance: These fields also require high reliability and often auditability. Imagine an LLM system assisting a lawyer by suggesting relevant precedents for a case. The lawyer will trust it more if it also explains why those precedents are relevant, perhaps by highlighting overlapping facts or legal principles. For generative models used in drafting documents, traceability is key: if asked to draft a contract clause, a good explainable system might flag which sections of standard templates it drew from and why. In finance, if an LLM flags a transaction as fraudulent, compliance officers need to know the basis - e.g. "This transaction is flagged because its pattern of amounts and timings is very similar to known laundering schemes X and Y". Achieving this may involve storing intermediate reasoning or using hybrid models (neural+symbolic) where the symbolic part tracks reasons. Notably, the right to an explanation for automated decisions (like loan approvals) is increasingly demanded; if LLMs are used to evaluate applications or generate credit risk assessments, they must produce explanations that regulators and customers can understand.

Education and Communication: LLM- based tutoring systems need explainability to gain trust from students and teachers. If a student asks a math question and the LLM gives an answer, providing a step- by- step solution is essentially a built- in explanation that also aids learning. Even beyond academic tasks, if an LLM corrects a student's essay, it should explain the corrections (much like a teacher's feedback) so the student learns, not just see a corrected sentence. In AI communication (like when ChatGPT explains something to a user), one could argue the explanation is the output - but there is another layer: explaining why the AI gave a certain response if challenged. Companies deploying chatbots have started providing disclaimers or context windows that let the model explain its limitations (e.g. "I am just a language model and might not always be right" - a meta- explanation). While not an explanation for each answer, it's a step to transparency about the system's nature.

Safety- Critical Systems: In domains like aerospace, defense, or autonomous driving, the tolerance for unexplained behavior is low. If an LLM is used within a larger system (say, analyzing logs or issuing high- level commands), any action needs an attached rationale. Explainable planning (where an AI agent explains why it chose a strategy) intersects with LLMs when these models are used for decision- making. For example, a drone might use an LLM- based system to

interpret a command and plan steps; one would want it to produce a justification for its plan: "Approach route A because it avoids no- fly zones and meets the time constraint". This helps a human supervisor verify it's not doing something unsafe. There's active research on combining LLMs with symbolic planners precisely to have a clear chain of reasoning that can be inspected - essentially using the LLM for its flexibility but grounding it in a framework that can produce logical explanations.

- Bias and Fairness Audits: LLMs, like other AI, can exhibit biases. Explainability is a tool for uncovering these. By examining which features the model uses, we can detect if protected attributes (gender, ethnicity, etc.) are influencing decisions inappropriately. In an NLP context, this could mean checking if changing "he" to "she" in a resume significantly alters the model's scoring of the applicant in a summarization or evaluation task – an input perturbation test that indicates bias. Saliency methods can also highlight if irrelevant demographic terms are being given weight. Moreover, there is a push for counterfactual explanations in fairness: e.g. "If this applicant had a slightly higher income, the loan would be approved" – which is an explanation that informs the applicant what they could change. LLMs can even be used to generate such counterfactuals. Ensuring these are faithful (the model actually would change its decision) is critical. In summary, explainability is both a microscope to find biases and a way to communicate decisions fairly.

- Knowledge Discovery and Science: Interestingly, explainable LLMs can assist scientists in interpreting data or models. For example, an LLM fine-tuned on genomic data might find patterns of genes and provide explanations like "Gene X is grouped with Gene Y because they co-occur in these contexts", hinting at a biological relationship. While this is still speculative, one could imagine using interpretable features from LLMs trained on scientific texts to generate hypotheses – effectively leveraging the model's internal "knowledge network". In this way, interpretability isn't just about explaining to lay users or oversight authorities; it can also accelerate discovery by highlighting connections learned from massive data that humans hadn't noticed, but in a form humans can grasp.

## 7 Open Challenges

Despite significant progress, truly explainable large language models remain an unsolved problem. We outline some of the key open challenges that researchers are grappling with:

- Faithfulness vs. Plausibility: As mentioned, there is often a trade-off between making an explanation faithful to the model's actual computations and making it plausible (i.e. understandable and convincing to humans). Many current explanation systems err on the side of plausibility – they produce something that looks right to a human, but it might not reflect the real internal reasoning (the CoT issue is a prime example) 16. Bridging this gap is hard. One approach is to quantitatively evaluate faithfulness: for instance, completeness tests (do the identified important features account for the change in output if we remove them?) or fidelity metrics (how well does a surrogate explanation model mimic the original model's behavior?) 7. Efforts like the ERASER benchmark encourage models to output rationales and then check those rationales by seeing if the model's prediction changes when the rationale is altered 16. Nonetheless, creating explanations that are both truthful and easily understood remains a core challenge.

- Scalability and Automating Interpretability: Manually analyzing neurons or circuits in a 100+ billion parameter model is not feasible; we need automated tools. Recent endeavors with autoencoders and influence functions show we can handle models up to ~50B 23, but beyond

that, methods may hit computational limits or the explanations become too complex to summarize. Moreover, as models incorporate more modalities (images, code, etc.), interpreting them becomes even more complex. Developing scalable interpretability algorithms (perhaps using AI to interpret AI, e.g. using one model to analyze another's neurons) is an ongoing direction. An intriguing idea is applying LLMs to their own explanations: e.g. asking an LLM to summarize or reason about the output of an interpretability analysis (like describing what a cluster of neurons represents) 38. Some preliminary work has done this, but then we have a recursion: we must trust the explaining LLM is telling the truth about the explained LLM! Methods to validate such second- order explanations are required.

- Integrating Symbolic and Neural Explanations: There's growing interest in neuro-symbolic approaches where the neural model's behavior is constrained or explained by symbolic structures (like logic rules or programs). For example, one might distill an LLM's behavior on a certain task into a decision tree or simple program for that task, which is then an explanation. Or during generation, an LLM might be forced to output a JSON of its reasoning steps according to a schema (e.g. a proof tree). The challenge is that forcing too much structure can reduce model performance, and not all tasks neatly decompose. However, if done successfully, it could provide verifiable explanations – a program that a user or another system can check for correctness. OpenAI's "scratchpads" (where models write intermediate calculations in a structured way) and similar ideas are steps in this direction. Balancing the free-form power of LLMs with the rigidity needed for clear explanation is a design challenge.

- User Understanding and Human Factors: An explanation is only useful if the recipient understands and trusts it. Different users (expert vs layperson, adult vs child, etc.) need different kinds of explanations. How to adapt LLM explanations to the audience is an open question. LLMs could be prompted to explain at varying levels of complexity (and indeed have shown some ability to do so), but ensuring consistency and accuracy across these levels is non-trivial. Moreover, there's the danger of information overload: an LLM could spew a very detailed explanation (it has no inherent sense of how much is enough), which might overwhelm or confuse users. Human-AI interaction studies are needed to find the sweet spot for explanation detail and format. Additionally, there's the concept of mental model alignment: the explanation should shape the human's mental model of the AI in a correct way. If the explanation is too simplistic, the user might think the AI is more limited or deterministic than it is; if too opaque, the user might misunderstand when the AI could err. Designing explanations that calibrate user expectations properly is an ongoing pursuit 39 40.

- Adversarial Robustness of Explanations: It has been shown that explanations themselves can be manipulated. For example, one can adversarially tweak an input in ways that do not change the model's prediction but dramatically change the saliency map (making the explanation meaningless) 28 41. This is a concern in domains like security – an AI system might be fooled not only in its decisions but also in the rationale it presents, potentially hiding malicious behavior. Ensuring that our explanation methods are robust to such adversarial manipulation is important for reliable deployment. It also raises interesting possibilities: one could imagine malicious uses of LLM explainability – e.g. if one can interpret how a content filter LLM decides a message is disallowed, an adversary could use that to craft inputs that avoid triggering the filter (effectively, understanding the "decision boundary" helps you evade it). So, there is a dual-use concern: interpretability research is mostly positive, but bad actors could use those insights too. This calls for careful consideration of how much and what kind of interpretability information is made public, especially for security-sensitive systems.

Quantifying and Comparing Explanations: With the growing array of explanation methods, how do we choose which is best for a given scenario? The field still lacks standardized metrics to directly compare explanation quality across methods (beyond task- specific evaluations). Some metrics like "sparsity" (fewer features highlighted is often more interpretable) or "consistency" (similar inputs yield similar explanations) have been proposed. However, an ideal metric would measure something like "how much did the explanation help a human predict or trust the model correctly," which often requires user studies. Large- scale evaluations (e.g. showing different explanations to users and testing their understanding) are costly but necessary to ground the science of explainability. Initiatives like explanation inference (forcing models to justify and then evaluating the justification quality separately) are one way to indirectly assess fidelity. But more work is needed in developing automated proxies for explanation quality - possibly using LLMs themselves to critique explanations by checking them against the model's behavior (another meta- LLM use case).

- Domain-Specific Nuances: As LLMs proliferate in diverse fields, domain-specific explainability challenges arise. For instance, explaining a code generation model might involve showing the relevant lines of documentation or analogous code, whereas explaining a poetry generation model might require conveying thematic influences (a far more abstract notion). Domain experts might want to see references to domain knowledge (e.g. a legal model's explanation citing statutes). Ensuring explanation methods can plug in domain knowledge (knowledge graphs, ontologies, etc.) is an open area. One size may not fit all: the techniques used to explain a math problem solver LLM could differ from those for a conversational agent. Thus, we foresee the development of customized explainability modules for different classes of LLM applications, while sharing common underlying principles.

## 8 Future Perspectives and Research Directions

The quest for explainable LLMs sits at the intersection of AI research, cognitive science, and even philosophy (what does it mean for a machine to "explain" its thinking?). Looking ahead, several promising research directions are emerging:

- Neuroscience Analogies: There's growing cross-pollination between neuroscience and interpretability of deep nets. Concepts like explanation via recording and stimulating neurons (common in brain science) are being mirrored with probes and interventions in networks. Some researchers are applying techniques like fMRI-style analysis to LLMs, identifying "activation regions" for concepts. The analogy might not be perfect, but interdisciplinary collaboration could spark new methods (for example, "neuro-symbolic language models" that emulate how humans combine symbolic reasoning with neural intuition, and can explain their steps akin to how humans articulate reasoning).

- Interactive Explainable AI: Instead of a static one-shot explanation, conversational explanations allow users to ask follow-up questions. LLMs being conversational by nature are well-suited for this. One could interact with an LLM: "Why did you give that answer?"  $\rightarrow$  (LLM explains)  $\rightarrow$  "Can you show me which part of the input led you to think that?"  $\rightarrow$  (LLM highlights text)  $\rightarrow$  "What if that detail was different?"  $\rightarrow$  (LLM answers hypothetically). This dialogue can significantly enhance understanding. Research needs to ensure that in each turn, the LLM's explanations remain truthful and don't become a just-so story to appease the user. Reinforcement learning or other control mechanisms might be needed to keep the model's explanations grounded.

- Personalized Explanations: If AI is to be pervasive, explanations might need tailoring to individual users' backgrounds. An engineer and a poet might require different explanations for the same LLM output. LLMs, with their vast knowledge, could potentially detect the user's level of expertise (perhaps from the conversation) and adjust explanation complexity. This raises questions of how to evaluate correctness across varied explanations and how to avoid the model making assumptions about the user that lead to biased or condescending explanations (e.g. not oversimplifying to an expert or overwhelming a novice). Machine learning techniques like meta-learning could be used to train models to explain in different "styles" and pick the appropriate one.

- Regulation and Standards: On the extrinsic side, as regulations come in, they might define what constitutes an acceptable explanation. This could spur research into formal properties of explanations – for instance, a law might require that for any automated decision affecting a person, an explanation must include the main factors and be understandable by an average person. This could push AI companies to integrate explanation generators that meet certain criteria (like a reading level or completeness measure). Standards bodies may even propose benchmarks or test cases for explainability (similar to how safety is tested). Researchers might collaborate with policymakers to define these in a technically sound way. In turn, this will likely accelerate progress in explanation techniques as compliance becomes a driver (similar to how privacy regulations boosted research in differential privacy).

- Explainability in Model Development: A future ideal is that explainability is not an add-on, but built into the lifecycle of LLM development. This means during training or architecture design, one actively optimizes for interpretability. There is some precedent: work on disentangled representations in earlier ML, or adding losses that encourage sparsity or modularity can make models easier to interpret. For LLMs, one could imagine constraints that encourage the model to route different concepts through different heads or layers (modularizing it), or using smaller explanation networks attached to the main network that learn to produce a faithful summary of what the main network is doing. If such techniques succeed, the byproduct would be models that are both high-performing and inherently easier to explain. It's a tough balance (simpler models are usually less accurate), but perhaps large models have enough capacity to allocate some for interpretability without sacrificing performance.

- Human-AI Collaboration through Explainability: Ultimately, the vision is that explainability will enable a better partnership between humans and AI. In fields like decision support, rather than AI replacing humans, a loop is envisioned: the AI makes a suggestion with explanation, the human evaluates and possibly corrects it, the AI takes that feedback and improves or clarifies further, and so on. For LLMs, which can fluidly interact, this synergy could be very natural. The explanation becomes a medium of teaching both ways: the AI teaching the human about the data or reasoning, and the human teaching the AI (with corrections) about their preferences or additional context. Closing this loop—making explanations not just one-way but a channel for continuous improvement—is a frontier that could fundamentally change how we integrate AI systems in society.

## 9 Conclusion

Explainability for large language models is a rapidly evolving domain at the heart of trustworthy AI. We have surveyed its foundations, from feature attribution and surrogate models to the cutting- edge work of interpreting neural circuits and neurons in massive transformers. We traced a historical arc illustrating how the rise of powerful LLMs has made explainability both more challenging and more urgent, fueling

innovations like influence tracing at scale and sparse autoencoders that peek into a model's soul. We discussed the breadth of methods now available - from highlighting a few salient words to mapping entire networks of reasoning - and how they are being applied in various fields to make AI's decisions more transparent and justifiable.

Yet, for all these advances, current explainability techniques only scratch the surface of answering "What is going on inside a giant language model's mind?" Complete interpretability may ultimately require new paradigms or theoretical breakthroughs. In the meantime, a combination of approaches can provide partial insights: Local attributions to explain individual decisions, global analyses to understand overall model behavior, human- aligned rationales to communicate with users, and mechanistic deep- dives to debug and refine models. Each approach contributes a piece to the puzzle.

Finally, we emphasize the importance of evaluation and humility in this field. An explanation from an LLM (or about an LLM) should not be taken at face value without verification. As researchers and practitioners, we must rigorously test whether explanations truly reflect model decisions and improve human- AI interaction outcomes. The goal is not explanations for their own sake, but to foster understanding: enabling users to predict when a model will succeed or fail, to uncover and fix its flaws, and to trust its outputs with reason and evidence. Achieving this will lead to AI systems that are not only powerful, but also reliable and aligned with human values. The journey to explainability for large language models is far from over, but with each year bringing new insights and tools, the once opaque box is steadily becoming more transparent. Each explanation, however small, is a step toward illuminating the reasoning of our most complex machines - and in doing so, ensuring they remain accountable to their human creators and users. 12

## References

1 23 24 29 30 31 34 Tracing Model Outputs to the Training Data \ Anthropic https://www.anthropic.com/research/influence- functions

2 6 10 36 37 39 40 Explainable artificial intelligence (XAl) users and development timeline |

Download Scientific Diagram https://www.researchgate.net/figure/Explainable- artificial- intelligence- XAl- users- and- development- timeline_fig3_356781652

3 2 Interpretability - Interpretable Machine Learning https://christophm.github.io/interpretable- ml- book/interpretability.html

4 [1705.07874] A Unified Approach to Interpreting Model Predictions https://arxiv.org/abs/1705.07874

5 [PDF] Interpretability and Explainability: A Machine Learning Zoo Mini- tour https://arxiv.org/pdf/2012.01805

7 8 11 15 27 28 41 Explainability Meets Text Summarization: A Survey https://aclanthology.org/2024. inlg- main.49. pdf

9 Explainable AI: A Brief History of the Concept - ERCIM News https://ercim- news.ercim.eu/en134/special/explainable- ai- a- brief- history- of- the- concept

12 [PDF] WHY IS ATTENTION NOT SO INTERPRETABLE? - OpenReview https://openreview.net/pdf?id=pQhnag- dlt

13 [1908.04626] Attention is not not Explanation - arXiv https://arxiv.org/abs/1908.04626

14 [2104.08696] Knowledge Neurons in Pretrained Transformers - arXiv https://arxiv.org/abs/2104.08696

16 17 18 42 [2305.04388] Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain- of- Thought Prompting https://arxiv.org/abs/2305.04388

19 20 21 22 35 Scaling Monosemanticity: Extracting Interpretable Features From Claude 3 Sonnet https://www.pelayoarbues.com/literature- notes/Articles/Scaling- Monosemanticity- Extracting- Interpretable- Features- From- Claude- 3- Sonnet

25 Attributing a deep network's prediction to its input features https://www.unofficialgoogledatascience.com/2017/03/attributing- deep- networks- prediction- to.html

26 32 Figuring where the attention is. Highlighted words got high attention... | Download Scientific Diagram

https://www.researchgate.net/figure/Figuring- where- the- attention- is- Highlighted- words- got- high- attention- scores- A- red_fig5_322886454

33 Captum - Model Interpretability for PyTorch https://captum.ai/tutorials/Image_and_Text_Classification_LIME

38 Interpretable Features in Large Language Models | by Jeremi Nuer https://medium.com/data- science/interpretable- features- in- large- language- models- 377fb25c72eb