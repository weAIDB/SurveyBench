# Reinforcement Learning for Large Language Models

## 1 Introduction

Reinforcement learning (RL) provides a formal framework for learning decision- making policies through trial- and- error interaction with an environment. An RL problem is often modeled as a Markov Decision Process (MDP), defined by a tuple  $\mathbb{S}(\{\mathrm{mathcal{S}}\} ,\{\mathrm{mathcal{A}}\} ,\mathrm{P},\mathrm{R}\} \mathbb{S}$ , where  $\mathbb{S}(\{\mathrm{mathcal{A}}\} \mathbb{S})$  is a (possibly huge) state space,  $\mathbb{S}(\{\mathrm{mathcal{A}}\} \mathbb{S})$  is an action space,  $\mathbb{S}(\mathbb{P}(\mathbb{S}_{- }[\mathfrak{t} + 1])\mathrm{mid}\mathbb{S}_{- }\mathfrak{t},\mathfrak{a}_{- }\mathfrak{t})\mathbb{S}$  is the transition probability, and  $\mathbb{S}(\mathbb{R}(\mathbb{S}_{- }\mathbb{t},\mathbb{a}_{- }\mathbb{t},\mathbb{S}_{- }[\mathfrak{t} + 1])\mathbb{S}$  is a reward function  $1\quad 2$ . An RL agent observes state  $\mathbb{S}\mathbb{S}_{- }\mathbb{t}|\mathfrak{m}|\mathfrak{m}\mathfrak{a}\mathfrak{h}\mathfrak{c}\mathfrak{a}\{\mathbb{S}\} \mathbb{S}$  at each time step, takes an action  $\mathbb{S}\mathbb{a}_{- }\mathbb{t}|\mathfrak{m}|\mathfrak{m}\mathfrak{a}\mathfrak{h}\mathfrak{c}\mathfrak{a}\{\mathbb{A}\} \mathbb{S}$  according to a policy  $\mathbb{S}\{\mathfrak{p}\mathfrak{i}(\mathfrak{a}|\mathfrak{m}\mathfrak{i}\mathfrak{d}\mathfrak{s})\} \mathfrak{S}$  (a probability distribution over actions given the state)  $3$ , and receives a scalar reward. The goal is to learn a policy that maximizes the expected cumulative reward (return) over time  $4\quad 3$ . In practice, policies and value functions are often represented by large neural networks, and algorithms such as policy gradient or actor- critic methods (e.g. PPO, TRPO, SAC) are used to optimize them.

In parallel, large language models (LLMs) are deep neural networks, typically based on the Transformer architecture, trained via self- supervised learning on massive text corpora  $5$ . Formally, an LLM is a parameterized model that assigns probabilities to text sequences, commonly trained to predict the next token in a sequence  $5$ . Examples include GPT (Generative Pretrained Transformers) and similar models with billions of parameters. While LLMs excel at language modeling and generate coherent text, their training objectives (usually next- word prediction) do not directly encode user goals or quality of outputs. Thus, RL is introduced as a fine- tuning mechanism to align LLM behavior with external objectives (such as user satisfaction or task- specific quality) that are hard to capture with supervised learning alone.

Integrating RL with LLMs treats the LLM as the policy: the state is the context or prompt (including conversation history), the action is generating the next token (or sequence of tokens), and the reward is a scalar measure of output quality (e.g. human preference, task- specific metrics, or auxiliary models)  $6$ . Because language generation involves extremely large discrete action spaces (vocabulary sizes) and long horizons (sequence length), specialized RL techniques and careful reward modeling are required. In practice, the most common framework is Reinforcement Learning from Human Feedback (RLHF), which uses human judgments to train a reward model guiding the LLM's fine- tuning  $8$ . Figure 1 illustrates the typical three- stage RLHF pipeline: supervised fine- tuning on demonstration data, reward- model training from human preference comparisons, and policy optimization (e.g. via PPO) on the learned reward.

Figure 1: The standard RLHF pipeline for language models (adapted from Ouyang et al., 2022  $6$ ). First, a base model is fine- tuned on human demonstrations (SET). Second, a reward model (RM) is trained to predict human preference labels on pairs of model outputs. Third, the policy (language model) is further fine- tuned via an RL algorithm (e.g. PPO) to maximize the reward model's score.

## 2 Historical Timeline

The combination of RL and language modeling has evolved rapidly, especially in the context of LLM alignment:

- 2017 – Deep RL from Human Preferences: Christiano et al. (2017) introduce the idea of using human preference comparisons to train a reward model for RL agents 10. This early work shows that RL agents can learn complex behaviors (in games and simulated tasks) by optimizing a reward model learned from human feedback, laying groundwork for RLHF.- 2019–2021 – Early RL in NLP: Researchers begin applying RL to sequence generation and dialogue. For example, self-critical sequence training and policy gradient were used for machine translation and image captioning, and preliminary work explores RL for dialogue agents. However, large-scale LLMs remain primarily trained via supervised learning.- 2020 – RLHF for Summarization: OpenAI applies RLHF to language summarization 7. They train a reward model on human rankings of summaries and fine-tune a GPT-based model with PPO. This yields summaries preferred by human judges over those from much larger baseline models (even  $10 \times$  larger) 7. This work demonstrates the benefit of human-aligned fine-tuning for NLP tasks.- 2022 – InstructGPT and Alignment: Ouyang et al. (2022) introduce the InstructGPT models, using RLHF to align GPT-3 with user instructions 6 11. Starting from supervised finetuning, they train a reward model on human-labeled preferences and apply PPO to fine-tune the model. Remarkably, their 1.3B-parameter InstructGPT produces outputs preferred over a 175B GPT-3 baseline 6. This establishes RLHF as a key method for improving helpfulness, truthfulness, and safety of LLMs.- Nov 2022 – ChatGPT: OpenAI releases ChatGPT, a conversational assistant trained with RLHF 12 13. The training pipeline mirrors InstructGPT: human trainers create dialogue data, provide preference comparisons of model outputs, and PPO is used to fine-tune the model's policy. The resulting chat agent can answer follow-up questions, admit mistakes, and reject inappropriate requests.- 2023 – Alternative Algorithms and Limitations: Research explores new algorithms and critiques RLHF. Rafailov et al. (2023) propose Direct Preference Optimization (DPO), a simpler method that directly optimizes a policy from preference data without separate reward-model fitting 14. They show DPO matches or exceeds PPO-based RLHF in aligning LLM outputs (e.g., controlling sentiment) while being more stable to train 15. At the same time, Casper et al. (2023) survey RLHF's weaknesses, noting that RLHF remains prone to issues like hallucinations, bias amplification, and robustness failures 16.- 2024 – Scaling with AI Feedback: With human feedback costly, new work uses AI to generate feedback. Williams (2024) introduces MORLAIF, a multi-objective RL from AI feedback approach: separate preference models (e.g. for toxicity or factuality) are trained using GPT-3.5-generated comparisons, and these are combined into a reward for PPO fine-tuning. MORLAIF outperforms standard single-model RLAF baselines 17. This reflects a trend of automating preference data collection.- 2025 – Emergent Reasoning via Pure RL: Zhou et al. (2025) demonstrate that LLMs can develop advanced reasoning abilities through purely RL-based training 18. Their DeepSeek-R1 model skips supervised finetuning, instead using only a reward on final answer correctness (via a novel GRPO algorithm) 19. The model outperforms supervised baselines on math and coding tasks, showing emergent chain-of-thought behaviors. Likewise, Feng et al. (2025) adapt the R1-Zero RL framework to machine translation (MT). Their MT-R1-Zero method uses mixed rule-metric rewards to fine-tune an LLM for translation, achieving competitive results (comparable to GPT-4-level

models) without any supervised MT data  $20$ . These works suggest RL alone can drive LLMs to solve complex tasks previously thought to require explicit supervision.

These milestones illustrate how RL has shifted from a niche tool to a central component in training advanced LLM- based systems.

## 3 Foundations of Reinforcement Learning

Markov Decision Processes. A Markov Decision Process provides the mathematical basis for RL. It is defined by the state space  $\mathfrak{S}\backslash \mathfrak{mathcal{L}}(\mathfrak{S})\mathfrak{S}$ , action space  $\mathfrak{S}\backslash \mathfrak{mathcal{L}}(\mathfrak{A})\mathfrak{S}$ , transition probabilities  $\mathfrak{S}\mathfrak{P}(\mathfrak{s}_{- }(\mathfrak{t} + 1)\backslash \mathfrak{m i d}\mathfrak{s}_{- }\mathfrak{t},\mathfrak{a}_{- }\mathfrak{t})\mathfrak{S}$ , and reward function  $\mathfrak{S}\mathfrak{R}(\mathfrak{s}_{- }\mathfrak{t},\mathfrak{a}_{- }\mathfrak{t})\mathfrak{S}$ $1$ . The Markov assumption implies that the next state depends only on the current state and action. In RL for LLMs, the "state" can encode the current dialogue or prompt context, and "action" is generating a token or selecting a response. Because text generation is partially observable (we may not track an external environment beyond text), one could model it as a Partially Observable MDP, but in practice the LLM's hidden activations encapsulate history, effectively providing a state embedding.

Agent, Policy, and Value. An RL agent follows a policy  $\mathfrak{S}\backslash \mathfrak{p}\mathfrak{i}(\mathfrak{a}\backslash \mathfrak{m i d}\mathfrak{s})\mathfrak{S}$  that maps states to a probability distribution over actions  $3$ . In LLMs, the policy is the probability distribution over the next token (or next message) given the prompt. A state- value function  $\mathfrak{S}\mathfrak{V}_{- }\mathfrak{p}\mathfrak{i}(\mathfrak{s}) = \backslash \mathfrak{m a t h b b}(\mathfrak{k})[\backslash \mathfrak{s u m}_{- }(\mathfrak{t}\backslash \mathfrak{g}\mathfrak{e}\mathfrak{0})]\backslash \mathfrak{g a m m a}\mathfrak{t}\mathfrak{r}_{- }\mathfrak{t}$ $\backslash \mathfrak{m i d}\mathfrak{s}_{- }0 = \mathfrak{s}\mathfrak{j}\mathfrak{s}$  estimates the expected discounted return from state  $\mathfrak{S}\mathfrak{s}$  under policy  $\mathfrak{S}\backslash \mathfrak{p}\mathfrak{i}\mathfrak{S}$ $21$ . In policy- gradient methods, we often directly optimize the policy to increase expected return, avoiding explicit value function learning. Common on- policy algorithms include REINFORCE and Proximal Policy Optimization (PPO)  $6$ ; off- policy methods like SAC or Q- learning are less common for LLMs due to the large discrete space.

RL in the Language Context. Unlike games or simulated robotics, language generation has an enormous action space (vocabulary of tens of thousands) and very long horizons (hundreds of tokens). Traditional RL must be adapted: one treats each next- token selection as an action, and an entire generated text as an episode. Exploration vs. exploitation trade- offs (e.g.  $\epsilon$ - greedy) are less straightforward, though some methods do add noise in sampling. Crucially, the reward function is usually not given a priori; it must be constructed from human feedback or automatic metrics. This is where reward modeling enters: we collect data (often human rankings of outputs) to train a differentiable reward model  $\mathfrak{S}\mathfrak{r}_{- }\backslash \mathfrak{p}\mathfrak{h}\mathfrak{i}(\backslash \mathfrak{t}\mathfrak{x}\mathfrak{t}(\mathfrak{o}\mathfrak{u}\mathfrak{p}\mathfrak{u}\mathfrak{t}))\mathfrak{S}$ , which the policy then tries to maximize  $6$ .

## 4 Key Methods: Reinforcement Learning for LLMs

### 4.1 Reinforcement Learning from Human Feedback (RLHF)

The dominant paradigm for aligning LLMs with human preferences is RL from Human Feedback (RLHF)  $6$ . In RLHF, one typically follows three stages:

1. Supervised Fine-Tuning (SFT): Collect demonstrations of desired behavior (e.g. human-written answers or dialogues). Fine-tune a pretrained LLM on this data with standard supervised learning  $6$ . This gives a reasonable base policy that follows instructions or conversation style. 
2. Reward Model (RM) Training: Generate multiple outputs from the SFT model for many prompts, and have humans rank or compare pairs of outputs in terms of quality. Train a reward model  $\mathfrak{S}\mathfrak{r}_{-}\backslash \mathfrak{p}\mathfrak{h}\mathfrak{i}\mathfrak{S}$  (often itself a neural network) to predict these human preference rankings  $6$ . Formally, if  $\mathfrak{S}\mathfrak{x}\_ \mathfrak{A}\mathfrak{S}$  and  $\mathfrak{S}\mathfrak{x}\_ \mathfrak{B}\mathfrak{S}$  are two model-generated completions, the model is trained so  $\mathfrak{S}\mathfrak{r}_{-}\backslash \mathfrak{p}\mathfrak{h}\mathfrak{i}(\mathfrak{x}\_ \mathfrak{A}) > \mathfrak{r}_{-}\backslash \mathfrak{p}\mathfrak{h}\mathfrak{i}(\mathfrak{x}\_ \mathfrak{B})\mathfrak{S}$  whenever the human prefers  $\mathfrak{S}\mathfrak{x}\_ \mathfrak{A}\mathfrak{S}$ $6$ .  $9$ .

3. Reinforcement Learning: Use an RL algorithm (commonly PPO 6 ) to update the LLM' s policy parameters to maximize the learned reward  $\S r_{-}\backslash \mathsf{phi}\S$  often with a penalty to stay close to the original language-model distribution (to prevent degenerate outputs) 6 . The RL agent "samples" responses to prompts, receives reward  $\S r_{-}\backslash \mathsf{phi}(\backslash \mathsf{text}\{\mathsf{response}\})\S$  , and adjusts its policy accordingly.

This pipeline is illustrated in Figure 1. Empirically, RLHF has proven effective at aligning LLMs to human intent. In InstructGPT, even a 1.3B- parameter model fine- tuned with RLHF outperformed the much larger 175B- parameter GPT- 3 in human preferences 6 . The ChatGPT system uses the same steps: human trainers write example dialogues (SFT), label model- generated responses (RM), and apply PPO- based RL on the reward model 12 15.

### 4.2 Alternative and Improved Algorithms

While PPO- based RLHF is common, recent work explores simpler or more stable alternatives:

- Direct Preference Optimization (DPO): Rafailov et al. (2023) propose DPO, which eliminates the separate reward-model stage. They show that by reparameterizing the policy, one can directly optimize a classification loss on preference data to find the optimal policy under those preferences 14 . DPO matches or exceeds PPO-based RLHF on alignment tasks, and is easier to train (no sampling or tunable KL penalty) 15 . For example, DPO achieves better control of output sentiment and at least as good summarization quality compared to PPO-based RLHF 15.

- Reinforcement Learning from AI Feedback (RLAIF): Due to the expense of human labeling, researchers use other models to provide feedback. Williams (2024) introduces Multi-Objective RLAIF (MORLAIF): multiple preference models (e.g. for toxicity, factuality, style) are trained on feedback generated by a smaller LLM (GPT-3.5) 17. These AI-based preference models are combined (e.g. by weighted sum) to produce a scalar reward for PPO fine-tuning. MORLAIF outperforms standard RLAIF baselines, suggesting that decomposing complex objectives can be beneficial 17 . This line of work demonstrates that "artificial labels", can partially replace human labels, trading some quality for much larger scale of feedback.

- Emergent Reasoning via Pure RL: Zhou et al. (2025) show that LLMs can learn complex reasoning without any supervised instruction data. Their DeepSeek-R1 approach skips SFT entirely: a pretrained model is fine-tuned by RL from scratch, with the only reward being task correctness 19 . Using Group Relative Policy Optimization (GRPO), the model discovers advanced reasoning behaviors (e.g. self-reflection, verification) through RL alone, outperforming supervised baselines on math and coding tasks 18 . Similarly, Feng et al. (2025) apply an RL-Zero style RL to machine translation (MT-R1-Zero) 20 . By combining rule-based and metric-based rewards, their 7B model matches state-of-the-art MT performance (on par with GPT-4-level systems) without any supervised MT data 10 . These results suggest that RL can induce emergent capabilities in LLMs previously thought to require explicit examples.

Other technical innovations include hierarchical or curriculum RL to break down long- generation tasks, and off- policy or batch RL techniques to better utilize logged data. For instance, StepCoder (2024) uses an RL curriculum of code- completion subtasks to improve code generation (see Applications below) 22 .

## 5 Applications Across Subfields

RL has been applied to many LLM- driven tasks where task quality is hard to encode via likelihood alone:

- Summarization: As mentioned, OpenAI's summarization models trained with RLHF produce higher-quality summaries than much larger models trained only on next-word likelihood 7.

Human evaluations show that optimizing a reward model of human preferences leads to more informative, fluent summaries.

- Dialogue and Chatbots: ChatGPT is an archetypal example. By fine-tuning with RLHF, it learns to engage in multi-turn conversation, follow instructions, and avoid harmful topics 12 13 . Similar approaches train assistants or question-answering bots; human ratings guide the bot to be helpful and harmless.

- Question-Answering and Reasoning: RL can improve factuality and reasoning. For example, InstructGPT and ChatGPT models show increased truthfulness (measured on benchmarks like TruthfulQA) after RLHF 6 16 . The DeepSeek-R1 work further uses pure RL to train models that excel on math and coding problem-solving, exhibiting chain-of-thought patterns 18 .

- Code Generation: Large models like Codex generate code, but RL can refine them using executable feedback. StepCoder (2024) uses RL with compiler feedback: unit-test results provide reward to explore correct code completions 22 . They break long code into subtasks and focus rewards on executed parts, improving test-pass rates. In general, integrating execution or static analysis as the environment enables RL to optimize code quality.

- Machine Translation: Traditional NMT has used RL (e.g. with BLEU scores). With LLMs, MT-R1- Zero applies an R1-Zero RL approach to translation: it mixes rule-based and learned metrics as rewards and fine-tunes an LLM for translation without supervised data 20 . This yields competitive results even on low-resource languages.

- Other Domains: Similar ideas appear in tasks like semantic parsing, content recommendation, information retrieval, and even multi-agent coordination where LLMs serve as agents. In each case, RL allows one to directly optimize task-specific rewards or human preferences that are difficult to capture in training data alone.

## 6 Challenges and Open Problems

Despite successes, RL for LLMs faces many challenges:

- Reward Specification and Misalignment: Learning an accurate reward model from limited human data is hard. Reward models can misgeneralize, leading to unexpected or undesired behaviors 16 23 . Even with high-quality labels, many policies can exploit the learned proxy, a phenomenon known as reward hacking 23 . For example, optimized models may find shortcuts (e.g. overly verbose or overly polite responses) that trick the reward model without truly satisfying human intent.- Sample Inefficiency and Scalability: RL typically requires many interactions. Gathering human feedback at scale is expensive, limiting RLHF datasets to tens of thousands of comparisons 6 24 . Off-policy or offline RL methods are still immature for LLMs, so most approaches rely on on-policy PPO with relatively small batches. This can lead to instability and requires careful tuning (e.g. mixing in the original pretraining loss to prevent divergence 25 ).- Safety and Robustness: Models fine-tuned with RLHF can still produce hallucinations or toxic content 16 . They may memorize and regurgitate private data 16 . Furthermore, RLHF-tuned models often remain vulnerable to adversarial prompts or "jailbreaks" that circumvent their alignment 16 . RL does not inherently solve long-term safety; it only encodes whatever objectives are given.- Human Feedback Quality: Humans may be inconsistent or biased in labeling preferences. The RLHF pipeline assumes that majority labeler preferences align with broader human values, which is not guaranteed 6 . Efforts to diversify and calibrate feedback are ongoing research topics.- Computational Cost: Fine-tuning multi-billion-parameter LMs with RL is expensive. PPO requires backpropagating through entire episodes (long sequences), and stabilizing training often means mixing supervised objectives. This limits iteration speed and experimentation compared to supervised finetuning.

Addressing these challenges is an active area. Proposed directions include multi- step or recursive reward modeling, integrating symbolic reasoning or world models into RL, and developing new algorithms (like DPO or GRPO) tailored to sequence generation. Some works also emphasize that RLHF is not a panacea, advocating for layered safety (e.g. additional inference- time filters, rule- based constraints, or adversarial training) alongside RL  $^{16}$ .

## 7 Conclusion

Reinforcement learning has become a crucial component in the toolkit for aligning and improving large language models. By framing LLM fine- tuning as an RL problem - with policy gradients or other methods driving models toward human- preferred outputs - researchers have achieved remarkable gains in helpfulness, truthfulness, and other desirable attributes  $^{6}$ $^{7}$ . The field continues to evolve rapidly: from the seminal RLHF pipelines of OpenAI to new approaches like direct preference optimization, AI- generated feedback, and emergent self- supervised RL for reasoning  $^{14}$ $^{18}$ .

Despite progress, many open questions remain. How to guarantee robustness and true alignment in arbitrary scenarios? How to scale human- aligned RL to ever more complex tasks without prohibitive cost? Recent milestones, such as RL- trained reasoning models and RL for translation  $^{18}$ $^{20}$ , hint at a future where LLMs self- improve through interaction. For newcomers and experts alike, mastering the intersection of RL and LLMs will be essential as AI systems grow more powerful.

Sources: Core concepts and survey facts are drawn from foundational RL literature  $^{1}$ $^{3}$  and from recent primary publications, including Christiano et al. (2017)  $^{10}$ , Ouyang et al. (2022)  $^{6}$ , OpenAI (2020, 2022) blogs  $^{7}$ $^{12}$ , Rafailov et al. (2023)  $^{14}$ , Casper et al. (2023)  $^{16}$ , Williams (2024)  $^{17}$ , Zhou et al. (2025)  $^{18}$ , and Feng et al. (2025)  $^{20}$ . These illustrate both established methods and cutting- edge developments in RL for LLMs.

## References

1 Markov decision process - Wikipedia https://en.wikipedia.org/wiki/Markov_decision_process

2 3 4 21 Reinforcement learning - Wikipedia https://en.wikipedia.org/wiki/Reinforcement_learning

5 Large language model - Wikipedia https://en.wikipedia.org/wiki/Large_language_model

6 8 11 25 [2203.02155] Training language models to follow instructions with human feedback https://ar5iv.labs.arxiv.org/html/2203.02155

7 Learning to summarize with human feedback | OpenAI https://openai.com/index/learning- to- summarize- with- human- feedback/

9 10 Deep Reinforcement Learning from Human Preferences http://papers.neurips.cc/paper/7017- deep- reinforcement- learning- from- human- preferences.pdf

12 13 24 Introducing ChatGPT | OpenAI https://openai.com/index/chatgpt/

14 15 [2305.18290] Direct Preference Optimization: Your Language Model is Secretly a Reward Model https://arxiv.org/abs/2305.18290

16 23 arxiv.org https://arxiv.org/pdf/2307.15217

17 [2406.07295] Multi- objective Reinforcement learning from AI Feedback https://arxiv.org/abs/2406.07295

18 19 DeepSeek- R1 incentivizes reasoning in LLMs through reinforcement learning | Nature https://www.nature.com/articles/s41586- 025- 09422- z?error=cookies_not_supported&code=5072c416- f324- 4bfa- 8eb3- 770fdf282c48

20 [2504.10160] MT- R1- Zero: Advancing LLM- based Machine Translation via R1- Zero- like Reinforcement Learning https://arxiv.org/abs/2504.10160

22 aclanthology.org https://aclanthology.org/2024. acl- long.251. pdf