# Retrieval-Augmented Generation for Large Language Models

## 1 Introduction

Large language models (LLMs) have transformed language understanding but exhibit limitations: they rely on fixed training data and can produce outdated or factually incorrect answers 1 . Retrieval- Augmented Generation (RAG) addresses these issues by augmenting the LLM' s input with externally retrieved information 2 . In a RAG system, a user' s query is first used to retrieve relevant documents from an external knowledge source. These retrieved passages are then provided alongside the original prompt to the LLM 2 3 . This gives the model "new facts" beyond its training data, allowing it to generate more accurate and contextually relevant responses 2 3 .

Figure 1: Conceptual pipeline of a retrieval- augmented generation system. A user query triggers a search over an external knowledge source, returning top- ranked passages. The LLM then consumes both the original query and the retrieved context to produce a grounded answer 3 .

## 2 Formal Definitions and Key Concepts

Formal Definitions and Key Concepts- Large Language Model (LLM): A deep neural network (typically transformer- based) pretrained on massive text corpora to predict or generate language. Such models implicitly store factual and linguistic knowledge in their parameters 4 . However, this parametric knowledge is static after training.- Parametric Memory: The internal knowledge stored in an LLM' s weights. Parametric memory is fixed once the model is trained and cannot be easily updated without retraining or fine- tuning the model.- Non- Parametric Memory: External sources of knowledge, such as text corpora or knowledge bases. In RAG systems, non- parametric memory is often implemented as a large document collection indexed for retrieval 5.- Retrieval- Augmented Generation (RAG): A paradigm that combines a pretrained generative model (parametric memory) with an external retrieval mechanism over non- parametric memory 5 6 . Formally, RAG models use a retriever to fetch relevant documents for each query, and then condition the LLM' s generation on these documents. This way, the model is effectively provided with "new facts" beyond its training data 2 5.- Knowledge- Intensive Tasks: NLP tasks that require up- to- date or specialized factual knowledge (e.g., open- domain question answering, fact verification, or domain- specific dialogue). RAG is particularly suited to these tasks because it allows the model to incorporate external evidence at inference time.

## 3 Historical Timeline

Historical Timeline- 2014- 2016: Early work on memory- augmented neural models. For example, Memory Networks and key- value memory networks allowed neural models to explicitly read from an external memory (such as a text corpus) for tasks like question answering. These ideas laid the groundwork for later RAG systems.- 2020 (REALM & RAG): Google introduced REALM, a retrieval- augmented model that learned a latent document retriever during pretraining 7 . In the same year, Lewis et al. proposed the RAG

framework, combining a pretrained seq2seq model with a dense index of Wikipedia documents 5 . Both models showed that augmenting LMs with explicit retrieval yields large gains on knowledge- intensive tasks.  - 2021 (Fusion- in- Decoder): Izacard & Grave (2021) proposed Fusion- in- Decoder (FiD), which retrieves multiple passages and processes each through the encoder of a large seq2seq model (e.g., T5), then fuses their representations in the decoder. FiD achieved state- of- the- art results on open- domain QA benchmarks, demonstrating that incorporating more retrieved evidence can significantly improve generation 8 .  - 2022 (Atlas & RETRO): Google' s Atlas (Izacard et al., 2022) is a retrieval- augmented transformer pretrained for knowledge tasks. It excels in few- shot settings, reaching over  $42\%$  accuracy on Natural Questions with just 64 examples 9 . DeepMind' s RETRO (Borgeaud et al., 2022) introduced a chunked cross- attention mechanism and a massive 2- trillion- token retrieval corpus. RETRO matched GPT3' s performance on language modeling while using  $25x$  fewer parameters 10 .  - 2023 (Deployment & Tooling): RAG has moved into production systems. For example, custom GPTs can automatically retrieve from user- uploaded documents: a GPT for customer support can fetch relevant internal FAQ or ticket content and answer product questions using that context 11 . These developments show that RAG is now widely deployed for customer service, enterprise search, and knowledge- based chatbots.

## 4 Foundations: Retrieval and Generation

Retrieval techniques: At RAG' s core is an information retrieval component. Traditional IR methods (TFIDF, BM25) have largely been surpassed by dense neural retrievers. For instance, Karpukhin et al. (2020) showed that a learned dual- encoder dense retriever vastly outperforms BM25 on open- domain QA, increasing top- 20 passage recall by  $9 - 19\%$  12 . Dense retrievers encode queries and documents into fixed- size vectors and perform nearest- neighbor search, typically via a vector store (e.g., FAISS, Milvus) 12 13 . The choice and tuning of the retrieval index (corpus size, chunking scheme, and embedding model) critically affect RAG performance.

Generative models: RAG builds on powerful pretrained LLMs. Common choices are encoder- decoder transformers (e.g. T5, BART) or decoder- only models (e.g. GPT). In RAG architectures, the LLM serves as the parametric memory: it is fine- tuned or prompted to generate an answer conditioned on both the query and retrieved context 5 . For example, Lewis et al. used a BART- based encoder- decoder as the generator in RAG 5 . During generation, the model attends over the content of retrieved passages (added to the input prompt) so that its output is grounded in those facts rather than solely its pretrained knowledge.

Retrieval pipeline: In practice, the RAG pipeline follows these steps. First, external documents are chunked (e.g. by paragraph) and each chunk is embedded via a text encoder; the embeddings are stored in a vector index 14 . At query time, the user' s question is embedded and used to retrieve the most similar document chunks. These top-  $\cdot \S \kappa \S$  passages are concatenated (or otherwise integrated) into the model' s prompt. The LLM then generates a response based on the augmented prompt 15 . This pipeline - chunking, embedding, indexing, querying, and conditioned generation - is now standard in deployed RAG systems.

## 5 Architectures and Methods

Architectures and Methods- REALM (2020): Augmented BERT pretraining with a neural retriever, training the model to retrieve relevant Wikipedia passages during masked language modeling  $^{16}$ . REALM demonstrated end- to- end training of retriever and encoder for retrieval- augmented objectives.- RAG (Lewis et al., 2020): Combines a pretrained seq2seq model (like BART) with a neural retriever over Wikipedia  $^{17}$ . The retrieved passages are prepended to the prompt and the model attends to all of them. In RAG (sequence) all passages are fixed before generation.- RAG- Token: A variant where the model can attend to different retrieved documents for each generated token  $^{18}$ . This allows the decoder to switch focus among multiple contexts dynamically.- Fusion- in- Decoder (FiD, 2021): Retrieves many passages and processes each through the encoder; the decoder then fuses these encoded representations. FiD has achieved strong performance on QA benchmarks by effectively combining evidence from multiple documents  $^{19}$ .- Atlas (2022): A large- scale pretrained RAG model using the Contriever retriever. Atlas retrieves relevant documents during inference and excels in few- shot knowledge tasks, outperforming much larger pure- LM baselines  $^{20}$ .- RETRO (2022): A decoder- only Transformer with chunked cross- attention to retrieved text. RETRO uses a frozen (non- parametric) retriever and was trained on a 2T- token corpus; it matched GPT- 3's performance with  $25 \times$  fewer parameters  $^{21}$ .- Knowledge- Graph RAG: Some systems integrate structured knowledge. For example, KG- FiD (Izacard et al., 2023) injects knowledge graph substructures into the FiD architecture to combine textual and graph information.- Tool- Augmented LMs: RAG can be seen as a special case of LLM tool use, where the tool is an information retrieval system. Modern LLM- based agents often interleave retrieval (e.g. web search or database queries) with generation to solve complex tasks.

## 6 Open Challenges

Open ChallengesWhile RAG improves factual grounding, it introduces new challenges. The relevance and quality of retrieved documents is critical: a poor retrieval can mislead the model. Even when sources are correct, an LLM may misinterpret context, potentially generating plausible but incorrect answers  $^{17}$ . Scaling RAG is also demanding: building and searching massive vector indexes (millions of billions of passages) requires substantial compute and efficient algorithms. Training remains challenging since retrieval is not easily differentiable; most systems train retrievers and generators separately or use weak supervision. Evaluating RAG- generated content is nontrivial; one must assess not only language fluency but factual consistency and faithfulness to sources. Finally, practical concerns include controlling copyrighted or sensitive information in the knowledge base, and mitigating bias in retrieved data. On the positive side, RAG allows including explicit citations, which can increase user trust by letting people verify sources  $^{18}$ . However, false or misleading citations remain a risk if not properly managed  $^{17}$ .

## 7 Applications

ApplicationsRAG is applicable wherever up- to- date or specific knowledge is needed. Key applications include: - Open- Domain QA: Systems that answer general knowledge questions by retrieving relevant encyclopedia or web passages. - Customer Support: RAG- powered chatbots retrieve company manuals, FAQs, or ticket logs to answer user questions. For example, a support bot can answer "How can I reset my password?" by finding and using the relevant documentation  $^{11}$ . - Enterprise Search and Chat: Internal knowledge assistants query proprietary documents or wikis and generate answers based on those sources, enabling natural- language search of corporate knowledge. - Code Generation and Documentation: Programming

assistants use RAG to incorporate up- to- date API documentation or code examples into generated code. - Summarization: By retrieving specific articles or reports, RAG can ground summaries in actual source texts, improving factual accuracy. - Specialized Domains: In medicine or law, RAG ensures that LLMs cite the latest guidelines, research papers, or statutes rather than relying on outdated training knowledge.

These examples illustrate how RAG extends LLMs to new domains by seamlessly integrating external knowledge sources into generation 6 11.