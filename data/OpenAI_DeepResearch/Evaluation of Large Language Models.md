# Evaluation of Large Language Models

## 1 Introduction

Large Language Models (LLMs) have emerged as a cornerstone of modern AI, powering applications from chatbots to code generation. An LLM is generally defined as a language model with a very large number of parameters, trained on extensive text corpora using self- supervised learning, and designed for various natural language tasks, especially text generation 1 . As these models grow in capability, evaluation becomes both crucial and challenging. Evaluation refers to the process of assessing a model' s performance, quality, and behavior, using well- defined criteria or benchmarks. Unlike traditional NLP systems that were evaluated on narrow tasks with clear metrics, LLMs exhibit broad, open- ended capabilities - making it hard to measure their competence with a single metric or task. Evaluators must consider not only accuracy on benchmarks, but also qualities like fluency, coherence, factual correctness, safety, and fairness 2 . This survey provides a comprehensive overview of how large language models are evaluated, covering foundational concepts, key metrics and methods (old and new), major benchmarks and milestones in the field, evaluation practices across different NLP subfields, and open challenges that researchers face.

We begin by defining core concepts and outlining the historical evolution of LLM evaluation. Next, we survey various evaluation methods - from traditional automatic metrics to human assessment and emerging techniques using AI evaluators. We then discuss prominent benchmarks and datasets that have shaped LLM evaluation. The survey also examines evaluation in specific domains (such as text generation, dialogue, knowledge question- answering, and code generation), illustrating how different tasks demand different evaluation strategies. Finally, we highlight ongoing challenges (like ensuring reliability, handling bias, and keeping evaluation meaningful as models become more powerful) and point to future directions. The goal is to give a broad yet detailed picture of how researchers gauge the performance and behavior of large language models, balancing technical rigor with accessible explanations.

## 2 Key Concepts and Definitions

Language Model and LLM: Formally, a language model is a probability distribution over sequences of tokens (words or subwords). Given a context sequence, the model assigns probabilities to potential next tokens. An LLM refers to a very large- scale language model, typically with hundreds of millions to hundreds of billions of parameters, often based on the Transformer architecture (e.g. GPT family) 3. LLMs are trained on vast text data and can be adapted or prompted to perform a wide range of tasks. Their "largeness" confers emergent abilities (such as coherent paragraph- length generation, few- shot learning, etc.), but also poses new evaluation difficulties as their outputs are complex and varied.

Intrinsic vs. Extrinsic Evaluation: In NLP, evaluation is often categorized as intrinsic (directly measuring the model' s performance on its training objective or a tightly defined subtask) versus extrinsic (measuring performance on an end- task or utility for an application). For LLMs, a classic intrinsic measure is perplexity - which gauges how well the model predicts a sample of text. Perplexity is defined as the exponential of the average negative log- likelihood that the model assigns to a test corpus 4 . Lower perplexity indicates the model is more confident (assigns higher probability) in generating the observed text, thus reflecting better predictive modeling. Formally, if a test set has  $\sin S$  tokens, and the model

assigns probabilities \(\) \mathsf{P}(\mathsf{w}_{- }\mathsf{i}\backslash \mathsf{mid}\backslash \mathsf{text}\{\mathsf{context}\} \mathsf{i})\mathsf{S}\(to each token\)\S\mathsf{w}_{- }\mathsf{i}\S\(perplexity is\)\S\backslash\exp\backslash\left\lfloor\frac{\mathsf{left}}{\mathsf{frac}}\left\lfloor\frac{\mathsf{1}}{\mathsf{frac}}\left\lfloor\frac{\mathsf{1}}{\mathsf{frac}}\left\lfloor\frac{\mathsf{1}}{\mathsf{frac}}\left\lfloor\frac{\mathsf{1}}{\mathsf{frac}}\left\lfloor\frac{\mathsf{1}}{\mathsf{frac}}\left\lfloor\frac{\mathsf{1}}{\mathsf{\frac{1}{\mathsf{frac}}}}\left\lfloor\frac{\mathsf{1}}{\mathsf{\frac{1}{\mathsf{frac}}}}\left\lfloor\frac{\mathsf{1}}{\mathsf{\frac{1}{\mathsf{frac}}}}\left\lfloor\frac{\mathsf{1}}{\mathsf{\frac{1}{\mathsf{frac}}}}\left\lfloor\frac{\mathsf{1}}{\frac{\mathsf{1}}{\mathsf{\frac{1}{\mathsf{frac}}}}}\left\lfloor\frac{\mathsf{1}}{\frac{\mathsf{1}}{\mathsf{\frac{1}{\mathsf{frac}}}}}\left\lfloor\frac{\mathsf{1}}{\frac{\mathsf{1}}{\mathsf{\frac{1}{\mathsf{frac}}}}}\left\lfloor\frac{\mathsf{1}}{\frac{\mathsf{1}}{\mathsf{\frac{1}}}}\left\lfloor\frac{\mathsf{1}}{\frac{\mathsf{1}}{\mathsf{\frac{1}}}}\left\lfloor\frac{\mathsf{1}}{\frac{\mathsf{1}}{\mathsf{\frac{1}}}}\left\lfloor\frac{\mathsf{1}}{\frac{\mathsf{1}}{\mathsf{\frac{1}}}}\left\lfloor\frac{\mathsf{\frac{1}}{\mathsf{\frac{1}}}}{\mathsf{\frac{1}}}\left\lfloor\frac{\mathsf{\frac{1}}{\mathsf{\frac{1}}}}{\mathsf{\frac{1}}}\left\lfloor\frac{\mathsf{\frac{1}}{\mathsf{\frac{1}}}}{\mathsf{\frac{1}}}\left\lfloor\frac{\mathsf{\frac{1}}{\mathsf{\frac{1}}}}{\mathsf{\frac{1}}}\left\lf\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rfloor\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\right\rf\) model. However, perplexity alone does not capture higher- level capabilities or appropriateness of outputs for tasks, especially as models get very large. Extrinsic evaluation involves testing LLMs on specific tasks or benchmarks - for example, answering questions, translating sentences, writing summaries, coding problems, etc. Here, task- specific metrics (like accuracy, BLEU score, etc.) or human judgments are used to measure how well the model meets the end- goal of the task.)^N \log P(w_i \mid \text{text}\{context

Reference- Based vs. Reference- Free Evaluation: Many traditional metrics in NLP are reference- based, meaning the model's output is compared to one or more human- crafted reference outputs. Examples include BLEU (Bilingual Evaluation Understudy) for machine translation, which measures n- gram overlap between the machine's translation and human reference translations 5 , and ROUGE (Recall- Oriented Understudy for Gisting Evaluation) for summarization, which often measures overlap in terms of recall of key n- grams or sequences against reference summaries 6 7 . These metrics reduce the comparison to a numeric score (e.g. BLEU score or ROUGE- F1). They have the benefit of being automatic and fast. However, reference- based metrics have well- known limitations: if there are many valid ways to express an answer, a single reference may not capture them, and a model can be penalized despite giving a correct or fluent output that simply uses different wording 8 9 . Also, references may be imperfect; if an LLM output is actually better than the reference (which can happen if the reference was written by a hurried annotator), an overlap- based metric would mistakenly penalize the better output 10 .

In contrast, reference- free evaluation does not rely on a ground- truth output for direct comparison. Intrinsic measures like perplexity are reference- free (they only use the input and model probabilities). More recently, learned evaluators and large models themselves are used in a reference- free manner: for example, prompting an LLM to judge the quality of an output based on criteria, or using detectors to flag factual errors without a gold answer. Reference- free methods can be more flexible - e.g. allowing evaluation of open- ended dialogue where no single correct answer exists - but they require a reliable mechanism to judge quality without a gold standard. We will discuss these learned and AI- based evaluators later in the survey.

Human Evaluation: Humans remain the ultimate judge of many language tasks, especially for subjective qualities like coherence, style, or usefulness, where automated metrics struggle. Human evaluation can be direct (raters score an output on several criteria or overall quality) or comparative (raters choose which of two model outputs is better for a given prompt) 11 12 . Human judgments are considered the gold standard for open- ended tasks - if an LLM' s answer is preferable to humans, that is what ultimately matters. However, human eval is time- consuming, costly, and can be inconsistent (different people may have different opinions, and the same person might rate differently at different times) 13 . Moreover, as LLM quality improves, humans may find it hard to distinguish subtle differences or even identify Al vs human text. Human evaluation is often used to benchmark top systems (e.g. in research competitions or in final A/B testing before deployment) and to calibrate automated metrics (we want automatic scores to correlate with human preferences).

Qualitative Aspects and Multi- Dimensional Evaluation: Evaluating LLMs often involves multiple axes. A single output may be fluent and grammatically perfect, but factually wrong - should it be scored highly or poorly? To capture this, evaluations distinguish dimensions such as fluency, coherence (logical consistency and flow of the text), relevance to the prompt, factual accuracy, helpfulness, safety (avoiding toxic or harmful content), and more 14 . In dialogue systems, additional aspects like humanness, engagingness, or politeness might be evaluated. No single metric captures all these, so comprehensive evaluations either report multiple metrics or use human ratings along several criteria. The Holistic Evaluation of Language Models (HELM) effort at Stanford formalized this by adopting a multi- metric

approach: evaluating models across 7 metrics - accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency - for each scenario 15. This highlights that an LLM's "performance" is not one- dimensional; a model might excel at accuracy but be worse on fairness or vice versa, and users care about all these facets.

In summary, evaluating large language models requires clarity on what we are measuring (task success, linguistic quality, ethical risks, etc.) and how we measure it (with automatic metrics, human judgment, or hybrid approaches). With these key concepts established, we now trace how evaluation practices have evolved as language models have grown, before delving into the methods in detail.

## 3 Historical Evolution and Milestones in LLM Evaluation

The approaches to evaluating language models have evolved alongside the models themselves. Below is a historical timeline highlighting major milestones in LLM evaluation and benchmark development:

- 1990s 
- Early 2000s: Statistical language models (e.g. n-gram models) were primarily evaluated by perplexity on held-out text. For example, in 2001 an optimized 300-million-word n-gram model set a state-of-the-art perplexity on benchmarks of that era 16. Around the same time, the need for automatic evaluation in machine translation (MT) led to the introduction of BLEU in 2002 17, a metric that compares MT output against reference translations via overlapping n-grams. BLEU's advent was seminal 
- it provided a quick, reproducible way to evaluate generative text systems without human judges, and soon became standard for MT. In 2004, the ROUGE metric was introduced for automatic summarization evaluation 18, focusing on recall of overlapping n-grams (suitable for summarization tasks where capturing important points is crucial). These early metrics enabled the first shared benchmarks for MT and summarization.

- 2010s: As NLP models diversified, task-specific benchmarks emerged. For instance, the GLUE benchmark (2018) compiled multiple sentence understanding tasks (like sentiment classification, inference, QA) to evaluate general-purpose language understanding models on a unified scale. GLUE and its successor SuperGLUE (2019) became important for evaluating pre-trained models (like BERT) on their ability to handle varied NLU tasks. They established leaderboard culture in NLP, where models are ranked by aggregate scores. However, these benchmarks mostly required fine-tuning models on each task and then testing on a private test set. Reference-free metrics also appeared for generation tasks 
- e.g., METEOR (mid-2000s, improved by 2014) introduced stemming and synonym matching for MT evaluation to improve correlation with human judgments, and embedding-based metrics like BERTScore (2019) started using contextual embeddings of model and reference text to measure similarity in meaning rather than exact wording 19. By late 2010s, researchers realized that simple n-gram overlap metrics often correlate poorly with human evaluations of text quality 19, prompting exploration of learned metrics and more nuanced evaluation.

- 2020: A turning point came with GPT-3 (2020), an extremely large model (175B parameters) that showed strong performance on a wide array of tasks without fine-tuning, simply by being prompted appropriately. This introduced the paradigm of zero-shot and few-shot evaluation: instead of training on a benchmark, the model is given the task instructions (and possibly a few examples in the prompt) and asked to complete it. GPT-3's debut was accompanied by evaluations on diverse tasks (translation, question-answering, close tests, commonsense reasoning) all via prompting in a single model 20 21. This was a new kind of evaluation 
- testing a model's emergent abilities directly 
- and it highlighted new issues. For example, results became sensitive to prompt wording and format (how the task is described, how many examples

are given) 21. Researchers began to ask: how do we fairly evaluate a model that can tackle many tasks, and how to design prompts for consistent comparison? This led to benchmarks that focus on the general knowledge and reasoning of models, such as MMLU (Massive Multitask Language Understanding, 2021) - a test of knowledge across 57 subjects from history to mathematics, requiring models to answer exam- like questions. Models started being evaluated on not just traditional NLP tasks, but also on novel challenge sets targeting their weaknesses.

- 2021-2022: With many new large models (e.g. Google's PaLM, OpenAI's Codex, etc.), the community developed collaborative benchmarks. Google and others introduced BIG-bench (Beyond the Imitation Game Benchmark, 2022), a collection of 204 diverse tasks (from linguistics puzzles to common-sense to niche reasoning problems) contributed by the research community to probe models' abilities beyond standard tests. Around the same time, researchers started creating adversarial or targeted evaluation sets to diagnose specific shortcomings. For example, HellaSwag (2019) was a benchmark for commonsense reasoning where incorrect answers are generated by models to be tricky; these fooled earlier models which often chose the "distractor" option that a human would never pick 22 23. TruthfulQA (2021) was introduced as a set of questions that test whether a model can avoid false but common beliefs and myths – it revealed that models like GPT-3 often answer incorrectly because they mimic false information seen during training, whereas humans easily see through the trick 24. In 2022, the Stanford HELM project released Holistic Evaluation of Language Models 15, a comprehensive framework emphasizing evaluation across a broad range of scenarios (42 use cases) and metrics beyond accuracy (like bias or toxicity), with the results made public in an interactive dashboard. HELM standardized evaluation conditions for  $30+$  models, highlighting trade-offs (e.g. a model might be more accurate but also more toxic than another) and gaps (it noted many scenarios and metrics were previously under-studied) 25 26. Also in 2022, OpenAI's InstructGPT and reinforcement learning from human feedback (RLHF) techniques underscored the importance of human preference evaluations: OpenAI refined GPT-3 by collecting human rankings of outputs and using those to train a reward model, essentially embedding human evaluation into the training loop. This demonstrated that evaluation is not only post hoc measurement – it can be part of the model optimization process, raising questions about how to ensure the evaluations themselves are reliable and aligned with what we want models to do.

- 2023: The explosion of public-facing LLM applications (ChatGPT's release in late 2022, followed by many others in 2023) made evaluation even more prominent. Developers now needed ways to compare models like OpenAI GPT-4, Anthropic Claude, Google Bard, and various open-source LLMs on dialogue quality, not just on static benchmarks. This led to new human-curated tests (OpenAI, for instance, reported GPT-4's performance on professional exams like the bar exam and medical licensing exam as a proxy for advanced reasoning 27) and the rise of LLM-as-a-judge evaluations. Researchers began using one strong model (often GPT-4) to evaluate the outputs of other models, due to the cost and time of large-scale human eval. Notably, the MT-Bench (2023) was introduced as a benchmark of multi-turn dialogue questions specifically to compare chatbots, and it employed GPT-4 to systematically rate which chatbot's answer is better in pairwise comparisons 28 27. A corresponding platform, Chatbot Arena, collected thousands of head-to-head model battles with human and AI judges to produce an Elo-style ranking of models. One study found that GPT-4 as a judge agreed with aggregated human preferences about  $80\%$  of the time – roughly reaching the agreement level among different human annotators 27. This result suggests that a well-tuned LLM evaluator can approximate human judgment closely, offering a scalable evaluation approach. 2023 also saw the proposal of G-Eval (NLG Evaluation with GPT-4) 29 30, a framework where GPT-4 is prompted with a chain-of-thought to explicitly reason about an output's quality before scoring it. G-Eval achieved significantly higher correlation with human judgments on tasks like summarization (Spearman  $\rho$

$\approx 0.51$  than previous automatic metrics 30, indicating that LLM- based metrics have surpassed older reference- based metrics in aligning with human opinion on open- ended tasks. Additionally, specialized evaluation metrics powered by LLMs appeared in subfields - e.g. GEMBA for machine translation quality, which uses GPT- 4 to evaluate translations and was shown to outperform traditional MT metrics when compared to human ratings 31 32 . Companies like OpenAI released tools such as OpenAI Evals (2023), an open- source framework to create and crowdsource new evaluation tasks for models, reflecting a recognition that the space of possible evaluations is vast and community input is needed to uncover blind spots.

In summary, the trajectory of LLM evaluation has moved from simple single- metric evaluations on narrow tasks (perplexity, BLEU, etc.) to a rich landscape of multi- faceted benchmarks and innovative methods. Next, we delve into the various methods and metrics used to evaluate LLMs today, discussing their principles, advantages, and limitations.

## 4 Evaluation Methods and Metrics for LLMs

Evaluating an LLM can be done through a variety of methods, broadly categorized into automatic metrics, human evaluation, and hybrid approaches. We further distinguish intrinsic evaluation focused on the model's behavior (often via automated metrics) from extrinsic evaluation focused on human judgment of usefulness. Here we survey each category:

### 4.1 Automatic Metrics (Heuristic Evaluation)

Automatic metrics are algorithmic formulas or programs that score model outputs without human intervention 33 . Historically, these include:

- Perplexity and Log-Likelihood: As discussed, perplexity measures how well the model predicts a known text. It remains a useful metric during training and for intrinsic evaluation of language modeling ability 4 . A lower perplexity generally indicates a better fit to the distribution of natural language. However, perplexity is task-agnostic and doesn't tell us if a model's outputs are useful or correct for a specific task, especially when we are interested in tasks beyond next-word prediction (e.g., summarization quality or answer correctness). Also, very large models sometimes have artificially low perplexity in part because they may have seen portions of the test data during training (the data contamination issue) 34 35 . For LLMs trained on internet-scale data, ensuring truly clean test sets is difficult (common benchmarks may be included in their training data), so a naive perplexity evaluation might be overly optimistic. Careful curation of evaluation data (or using new queries that the model is unlikely to have seen) is needed to get meaningful perplexity or likelihood evaluations.

- N-gram Overlap Metrics (BLEU, ROUGE, etc.): These compare model-generated text to reference text by counting overlapping n-grams. BLEU uses a precision-oriented overlap score, penalizing extra words not in the reference 5, whereas ROUGE is recall-oriented, checking how much of the reference is covered by the output 36. They are quick to compute and were effective for early MT and summarization systems that often failed in obvious ways captured by n-gram differences. But for LLMs which produce highly fluent text, n-gram metrics often fall short. As one guide notes, "metrics like accuracy (or BLEU) don't work well [for open-ended tasks] because there are many ways to be 'night' without exactly matching the example answer." 9. These metrics also don't capture semantic equivalence well – an output that uses synonyms or paraphrasing can get a poor BLEU/ROUGE despite being excellent. Thus, while BLEU/ROUGE are still reported for tasks like translation and summarization (for tradition and some comparability), their

correlation with human judgments is moderate at best. Researchers have proposed improvements, like weighting overlaps by importance or focusing on in- order subsequences, but the fundamental limitation remains that overlap  $\neq$  quality.

- Semantic Similarity Metrics: To better evaluate the content beyond exact matches, embedding-based metrics emerged. Metrics like BERTScore (which uses BERT or similar model to produce embeddings for each token and matches tokens in output and reference by cosine similarity) attempt to measure if the generated text means the same as the reference rather than using the exact same words  $^{19}$ . Others like MoverScore and Sentence Move  $^{19}$  Similarity extend this by considering how to "move" one set of embeddings to another, capturing similarity in a holistic way. These metrics improved correlation with human judgments on some tasks, but still have issues  $^{19}$ . Studies found that learned similarity metrics can sometimes be fooled by superficial similarities or fail to reflect nuances (they might give high scores to a fluent but incorrect sentence that stays on topic). For instance, an embedding-based metric might judge "The movie was not bad." vs "The movie was good." as highly similar, missing the subtle negation. Overall, BERTScore and others provide a softer notion of overlap, useful when exact match isn't expected, but by 2023 they were being outperformed by even more advanced methods (discussed below).

- Task-Specific Automatic Metrics: Certain tasks have bespoke metrics. For example, in question answering, Exact Match and F1 score are common 
- Exact Match checks if the model's answer string exactly equals the gold answer, while F1 (token overlap) allows partial credit for answers that have overlap with the gold answer (useful when the answer is a phrase)  $^{37}$ . In mathematical problem solving, accuracy is measured by whether the final answer is correct, sometimes with tolerance for algebraic equivalence. In code generation, metrics like pass@k have been introduced 
- e.g., pass@i is the percentage of problems for which the model's generated code passes all unit tests on the first attempt, while pass@k allows up to k attempts (since code generation can be non-deterministic, evaluating multiple samples makes sense). These task-specific metrics are straightforward when there's a clear notion of correctness. They work less well for open-ended tasks where multiple outputs can be equally valid.

Automatic metrics are fast, cheap, and reproducible - you can run a script to compute BLEU or BERTScore on thousands of outputs in seconds. They are great for development- time checks (e.g., if BLEU goes down after a code change, something might be wrong with the model). However, for open- ended generation and dialogue, automatic metrics alone are insufficient  $^{9}$ . They often require a reference output, which may not exist for conversational AI (there's no single "ground truth" response to an open- ended user prompt). They also fail to capture qualities like style, logical coherence in a conversation, or adherence to instructions in a nuanced scenario. Thus, while automatic metrics remain part of the evaluator's toolbox, they are usually complemented by human or hybrid evaluation for LLMs, especially in research papers on new models.

Figure 1: Families of evaluation metrics for LLM- generated content, and how they can be categorized. Traditional reference- based metrics (blue) like BLEU and ROUGE rely on overlap with reference texts. Newer learned metrics (green) use embeddings and trained models to judge similarity or factual consistency. LLM- based evaluators (orange) involve prompt- driven judgments by a language model. The timeline (horizontal axis) indicates when various metrics were introduced in the history of NLP (e.g. BLEU in 2002, BERTScore in 2019, GPT- 4 based metrics in 2023). This illustrates the proliferation of metrics as models and evaluation needs have evolved  $^{38}$ $^{39}$ .

### 4.2 Human Evaluation

Human EvaluationHuman evaluation remains the gold standard for many aspects of LLM performance 11. Especially for dialogue systems, creative generation (story writing, etc.), and nuanced question- answering, humans can best judge if the output is correct, useful, and appropriate. Common forms of human eval include:

- Rating on a Scale: Evaluators give a numerical score on one or multiple axes. For example, outputs might be rated 1-5 for criteria like correctness, relevance, fluency, and so on. This provides fine-grained feedback and can be averaged across outputs to compare models. The drawback is that different annotators may use scales differently (one person’s “4” might be another’s “5”), and there can be high variance.

- Ranking and Pairwise Comparison: Evaluators are shown outputs from two (or more) models for the same prompt and asked which is better (or to rank them). This comparative evaluation is often easier for humans than absolute scoring – subtle differences become clearer when outputs are side by side. Many benchmark leaderboards and model competitions rely on pairwise comparisons. For instance, Anthropic reported using Elo ratings derived from pairwise comparisons: crowdworkers compare model A vs model B on the same query, and over many random matchups an Elo score is computed (like in chess) to rank the models 40. This method was used to evaluate their Claude model’s chat quality. Pairwise methods can reduce some bias (the evaluator focuses on relative quality, not an absolute number), but it doubles the work (each evaluation involves multiple outputs).

- Heuristic or Behavioral Testing by Humans: Sometimes humans evaluate by testing the model interactively or with specific probing questions. For example, a human evaluator might engage in a conversation with the model and then subjectively judge whether the interaction was satisfying or count how often the model had lapses. In safety evaluations, experts might try to “red team” the model (attempt to trick it into breaking rules) and record whether it did 41 42. This is a form of human evaluation targeting worst-case behavior rather than average-case quality.

Humans bring understanding of context and nuance that automatic metrics lack 43. They can notice if an answer, while factually correct, is rude or not actually addressing the user’s intent, etc. For example, only a human (or a model emulating human judgment) can tell if a response is polite and follows social norms or if a joke is funny. For chatbots intended for conversation, user satisfaction is ultimately a human subjective judgment – metrics serve only as proxies.

However, human eval has significant downsides 13. It is slow and expensive – evaluating a new model on thousands of prompts via humans is usually infeasible, so human eval is done on smaller samples. It can also be inconsistent: studies often have to compute inter- annotator agreement to ensure that the ratings can be trusted. In many cases, achieving even moderate agreement (e.g. Krippendorff’s alpha or Cohen’s kappa) is hard because of subjective differences. Humans also experience fatigue and might skim outputs or miss errors if they have to evaluate too many; this sets practical limits on how exhaustive human eval can be. Despite these issues, almost every major LLM release includes some human evaluation component, particularly to validate claims of superiority over previous models. For example, OpenAI’s GPT- 4 report mentions that “GPT- 4 was preferred over GPT- 3.5 by human judges in X% of prompts” for certain tasks, and Anthropic’s Claude was benchmarked by humans who prefer its responses over others by a certain margin – these statements guide the field’s understanding of progress.

A sensible practice is to mix automated and human evaluation. During development, one might use automatic metrics or an AI- based evaluator for fast iteration, and then do a final human assessment on a representative sample to make sure the metrics were not misleading. Human- in- the- loop evaluation is also used to calibrate AI evaluators: for example, if we use an LLM to judge outputs, we periodically have humans double- check a subset of those judgments to ensure the LLM is scoring in line with human preferences 44. We discuss LLM- as- judge next, which is a hybrid that tries to get the best of both worlds.

### 4.3 Hybrid Methods: LLM-as-a-Judge and AI-Augmented Evaluation

An exciting development in LLM evaluation is the use of one AI model to evaluate another, often called LLM- as- a- judge or Al- based evaluators. The idea is to leverage the language understanding of a strong model (like GPT- 4) to assess outputs in a way that mimics human judgment. This approach has gained popularity as it can dramatically scale up evaluation - thousands of model outputs can be evaluated quickly without human labor 45.

In an LLM- as- a- judge framework, we construct a prompt that provides the context (the input and the model' s output, sometimes along with a reference or another model' s output for comparison) and asks the evaluator model to produce a rating or decision 46 47 . For example, a prompt might say: "You are a judge. Here is a question, a reference answer, and an answer from the model. Evaluate the model' s answer for correctness and coherence, and give a score from 1 to 5, with an explanation." The evaluator model then generates a score and possibly a rationale.

Several frameworks and prompt strategies have been proposed: Reason- then- Score (RTS) prompts the judge model to first reason step- by- step about the answer and then output a score 48. Multiple Choice (MCQ) scoring might have the judge choose among discrete options (e.g. "Which answer is better, A or B?" ). Head- to- Head (H2H) prompting directly compares two outputs in one prompt and asks for the better 48 . G- Eval (2023) introduced a structured prompt where GPT- 4 is guided with a form to fill out (for example, rating different criteria) and uses chain- of- thought (CoT) reasoning before giving a final verdict 30 . Studies have found that such approaches, when carefully designed, can correlate impressively well with human judgments. One study noted GPT- 4 based judges matched human preference decisions about  $80\%$  of the time 27. Another found that GPT- 4' s ratings of summary quality had a higher Spearman correlation with humans than traditional metrics like ROUGE or even than smaller learned evaluators 49.

There are clear advantages to LLM- as- judge: it' s fast, consistent, and flexible. An Al judge doesn' t get tired or drift in their standards. We can also program the criteria by editing the prompt - for instance, instruct the judge to focus on factual accuracy or to ignore style, etc., something hard to do with a fixed metric. The Al judge can also provide an explanation for its decision, which is valuable for diagnosing issues (though one must remember the explanation is also machine- generated and could be flawed). This is a level of interpretability rarely available from numeric metrics 50 51.

However, LLM judging is not perfect 52 53. These evaluators can have biases: - Position bias: The order in which options are presented might influence the Al judge. GPT- based judges might favor whichever output is labeled "A" or comes first unless prompts are carefully balanced 52 . - Length/ verbosity bias: An Al judge might erroneously favor longer, more verbose answers, equating length with thoroughness (GPT- 4 has been noted to sometimes prefer more expansive answers) 53 . - Self- enhancement bias: If the judge model is the same as one of the contenders (or was trained on its outputs), it might favor that model' s style or content. For example, GPT- 4 judging its own answers versus another model' s could lean toward its own unless the prompt prevents this 53 . - Hallucination or reasoning limitations: The evaluator model might not actually know the ground truth. If both the

model output and reference are wrong, will it catch that? Or if checking factual accuracy, the judge might hallucinate an assessment. Limited reasoning ability means it might miss subtle logical errors in an answer or complex mathematical mistakes  $^{54}$ . - Score calibration issues: LLM judges are often asked to output a score (say 1- 10), but ensuring that a 8/10 from GPT- 4 means the same level of quality consistently is tricky  $^{55}$ . These models might need instructions or few- shot examples to calibrate what each score means.

Researchers are actively working on these issues. Some mitigation strategies include prompt techniques like Balanced Position Calibration (BPC) - asking the model to evaluate two answers in swapped positions and aggregating to remove order effects  $^{55}$ . Or using multiple prompts/votes and averaging to reduce variance. Another approach is a human- in- the- loop for spot checking: use AI judges for volume and humans to periodically audit their decisions  $^{44}$ .

Despite limitations, AI- based evaluation is becoming a standard component. Many recent leaderboards or comparisons (especially for open- source chat models) rely on GPT- 4 judgments due to lack of resources for mass human eval. It's likely that future evaluations will use ensembles of AI judges, or calibration against known human- rated datasets, to further improve reliability. The key is to remember that an AI judge is only as good as its alignment with human values - encouragingly, top models like GPT- 4, when well- prompted, seem to achieve roughly human- level agreement with humans  $^{27}$ , but one must remain cautious and validate these methods.

### 4.4 Adversarial and Stress Testing

Beyond average- case performance, evaluators often want to know a model's worst- case or failure- case behavior. This is where adversarial testing and stress tests come in  $^{43}$ . The idea is to actively look for inputs that might cause the model to err - whether by producing incorrect answers, nonsensical output, or harmful content - and thereby assess its robustness and safety.

Red teaming is a form of adversarial evaluation where experts or crowdworkers deliberately try to get the model to break the rules or fail  $^{41}$ . For example, testers might attempt to elicit disallowed content (hate speech, private information, etc.) by phrasing inputs in tricky ways, or they might push the model with logic puzzles or edge cases to see if it starts making things up. Prior to releasing GPT- 4, OpenAI engaged a team of domain experts to red- team the model on aspects like misinformation, privacy, and harmful instructions, discovering several ways it could be misused  $^{42}$ .  $^{56}$ . Anthropic's Claude was similarly evaluated by having people attempt known "jailbreak" prompts (inputs designed to bypass safety filters) and measuring how often the model complied versus refused  $^{42}$ .

Adversarial evaluation can also be automated to an extent. Automated adversarial testing involves using one model to generate tricky inputs for another. For instance, a researcher could use an LLM to produce examples of questions loaded with false presuppositions or subtly leading phrasing, and then test if the main model is fooled. Anthropic has mentioned using AI to generate adversarial prompts as part of safety eval - the idea of "model- written evaluation" where models themselves propose the hardest questions  $^{57}$ .  $^{58}$ .

Unlike normal evaluation, the goal in adversarial eval is often to find any failure rather than to get a high score. So, success is measured by the absence of failures: e.g., "we tried 100 known jailbreak prompts and the model only broke rules in 2 of them" might be a good result, whereas a single successful jailbreak might be enough to highlight a problem to fix. Adversarial testing is thus often open- ended and exploratory. It requires creativity (for humans or AI) to think of the corner cases. And no matter how many

are tested, one can't prove the model is safe or robust in all cases - but the more diverse adversarial tests it passes, the more confidence we have.

This type of evaluation is crucial for safety: for models deployed in the real world, we care not just about average quality, but about avoiding worst- case catastrophic errors (like a medical assistant LLM giving a dangerously wrong recommendation in one rare instance). In domains like medicine or law, even a  $1\%$  failure rate can be unacceptable if those failures are severe. Adversarial eval tries to estimate that tail of the distribution. One challenge is that it's hard to quantify as a single number. Typically, results are reported as anecdotal findings or as a collection of failure cases rather than a score. That said, teams do sometimes quantify it (e.g., "our red team of 5 people tried for a week and succeeded in  $x\%$  of attempts to get disallowed output"). As a complement to standard benchmarks, adversarial testing keeps developers alert to vulnerabilities that normal accuracy tests might miss. 59 60

### 4.5 Continuous and User-Centric Evaluation (A/B Testing and Feedback)

Once an LLM is deployed in a product with real users, online evaluation becomes possible. This involves collecting feedback and usage data from actual users to gauge performance in the wild 61. Two important techniques here are:

- A/B Testing: If we have a new version of a model (say Model B) and we want to compare it to the incumbent (Model A), we can do a live experiment. A portion of real users (say  $10\%$ ) get Model B when they interact, while others get Model A, and we compare metrics. The metrics could be explicit feedback (like users rating answers or clicking a thumbs-up/thumbs-down button) 62, or implicit measures (like conversation length, or whether the user rephrased the question which might indicate the first answer was unsatisfactory). If Model B leads to higher user ratings and longer engagements, it might be deemed better and rolled out further. Tech companies heavily use such A/B tests to validate model improvements in production. The caution is to do this ethically – exposing users to a potentially worse model even briefly needs justification, and if the domain is sensitive (medical, etc.), careful monitoring is needed.

- User Feedback Loops: Simply monitoring how users interact can be a form of evaluation. For example, tracking how often users ask follow-up questions that indicate confusion, or how often content moderators have to intervene due to a bad output, gives a signal of real-world performance. Anthropic has noted that human feedback from end-users is one of the most important evaluation metrics for them 63. Some platforms allow users to report problematic answers or explicitly rate answers; those stats directly feed into model assessment and sometimes into training (reinforcement learning from human feedback is essentially leveraging user or annotator feedback to improve the model).

Continuous evaluation in deployment recognizes that evaluation is not a one- time event but an ongoing process. Models might drift or degrade (especially if updated frequently), user demographics might shift (leading to different queries than the test set anticipated), or new problems might emerge (if someone discovers a new way to trick the model, for instance). Monitoring real usage helps catch these.

In summary, a comprehensive evaluation strategy for an LLM uses multiple methods: automatic metrics for quick checks and regression testing, human evaluation for subjective and open- ended quality, LLM- based judges for scale, adversarial tests for robustness, and user feedback for real- world validation 64. Table 1 summarizes these methods and their typical use cases:

<table><tr><td>Evaluation Method</td><td>Description</td><td>Use Cases</td><td>Pros</td><td>Cons</td></tr><tr><td>Automatic metrics (e.g. BLEU, accuracy, perplexity)</td><td>Compute algorithmic score vs reference or known answers.</td><td>Structured tasks with clear ground truth (translation, QA); quick model comparisons during development.</td><td>Fast, objective, reproducible; good for relative improvements.</td><td>Often poor reflection of human judgment on open tasks; usually require reference outputs; can be gamed or fail on nuance.</td></tr><tr><td>Human rating (scale or ranking)</td><td>People score or rank outputs on quality criteria.</td><td>Open-ended tasks (dialogue, summarization); final benchmarking of top models.</td><td>Captures nuance and subjective preferences; can evaluate anything you can show a human.</td><td>Expensive, slow, not scalable; variability and annotator bias.</td></tr><tr><td>LLM-as-judge (AI evaluator)</td><td>Use a strong model (e.g. GPT-4) to evaluate other outputs via prompted instructions.</td><td>Any task where human-like judgment is needed; rapid evaluation of many models or iterations.</td><td>Scalable and fast; often high agreement with human preferences 21; can provide explanations.</td><td>Potential biases (position, verbosity); not 100% reliable – must be calibrated against humans; evaluating aspects the model itself is flawed in (e.g. factuality) can be tricky.</td></tr><tr><td>Adversarial &amp;amp; stress tests</td><td>Curated “tough cases” or attack inputs, by humans or generated, to probe model failures.</td><td>Safety-critical evaluation (toxicity, bias); robustness checks (reasoning puzzles, edge cases).</td><td>Surfaces worst-case behaviors; improves trust by identifying limits; encourages model improvements on failure modes.</td><td>No single metric output; coverage is never complete (unknown unknowns); can be labor-intensive to come up with good tests.</td></tr><tr><td>User feedback &amp;amp; A/B tests</td><td>Leverage real user interactions and experiments to measure model quality in deployment.</td><td>Any deployed application (chat assistant, etc.), especially to choose between model versions.</td><td>Reflects real-world performance and user satisfaction; continuous monitoring catches regressions.</td><td>Requires deployment (risk of bad outputs reaching users); feedback can be noisy or biased toward vocal users; tricky to interpret implicit signals.</td></tr></table>

Having outlined the methods, we now turn to how these are applied in different subfields and tasks that LLMs are used for, because evaluation is not one- size- fits- all – it must be tailored to the task at hand.

## 5 Evaluation Across Different Tasks and Subfields

Large language models are used in a wide spectrum of NLP tasks and even beyond (code, multimodal tasks, etc.). Each area has its own evaluation conventions and challenges. Here we survey how LLM evaluation is handled in several key subfields:

### 5.1 Natural Language Understanding (NLU) Tasks

This category includes tasks like text classification (sentiment analysis, topic classification), sentence pair tasks (natural language inference, semantic similarity), and structured question answering (where the answer is a fact or a span from text). Evaluation in NLU has traditionally been straightforward – since there is a correct or desired output, metrics like accuracy, F1 score, or exact match are used. For example, on a sentiment classification task, we compute the percentage of test examples where the model's predicted sentiment matches the human- labeled sentiment. On QA tasks like SQuAD (answer span extraction from paragraphs), exact match and token- level F1 are standard to allow partial credit for answers that miss a word or have slight differences (e.g. “Abraham Lincoln” vs “Lincoln” might be considered a partial match).

With LLMs, one twist is that they might approach these tasks in a generative way (by directly producing an answer in text) rather than a classification label. So evaluation sometimes needs to include normalization (for instance, if the model answers “It was negative.” for sentiment, the evaluation script must map that to the label “negative”). Generally, NLU tasks still use intrinsic evaluation metrics – accuracy, F1, etc. – and large benchmarks like GLUE aggregate these. Benchmarks: GLUE (General Language Understanding Evaluation) provided a single number (GLUE score) as an average of several task metrics 65. If an LLM is used zero- shot on GLUE tasks via prompting, researchers will still measure those same metrics per task. Another benchmark, SuperGLUE, added harder tasks and a human baseline (to see if models surpass human performance). As of 2023, top LLMs have indeed exceeded human performance on many such benchmarks, raising the interesting situation that these evaluations become saturated – once a model hits ~90+% on a classification task, it's hard to tell differences. This has driven the community to look for out- of- distribution or adversarial NLU tasks (e.g., handling of idioms, or logical negation) to differentiate models beyond those saturated benchmarks.

One specific aspect in NLU evaluation for LLMs is generalization. We often want to test not just in- domain performance but also how models handle nuances: e.g., if we flip the wording of a question or use an uncommon dialect, does performance drop? This led to creation of challenge sets such as HANS for NLI (to test if models fall for simple heuristics) or CheckList testing methodology where lots of variations of inputs are generated to see if the model consistently handles them. LLMs are evaluated on these by measuring accuracy on each slice. If a model does well on standard data but fails systematically on, say, negated questions (“Is this not good?”), that indicates a gap in capability.

In summary, for NLU tasks, we rely on classical metrics (accuracy, F1) on benchmark datasets, and we increasingly incorporate adversarial and diagnostic evaluations to ensure the model truly understands the nuances rather than just exploiting dataset biases.

### 5.2 Text Generation: Translation and Summarization

Machine Translation (MT) and text summarization are classic generative tasks where LLMs now play a role (either via fine- tuning or prompting). Historically, these were the domains where BLEU and ROUGE were heavily used, respectively. For MT, human evaluation involves ranking translations or checking adequacy and fluency of the output compared to references. For summarization, humans might rate how well the summary captures the important points and how fluent it is.

LLMs have demonstrated strong capabilities in zero- shot or few- shot translation and summarization. Evaluating them has revealed some differences from evaluating dedicated systems. Firstly, factual consistency is a big issue in summarization with LLMs: models sometimes produce summaries that are fluent and plausible but contain hallucinations (facts not in the source). Thus, evaluation for summarization now often includes checks for factual accuracy. Automatic metrics like FactCC, QAGS, QuestEval, Q^2 were developed to address this - these use question- answering or natural language inference models to verify summary content against the source text. For instance, QuestEval will automatically generate questions from the source document and see if the summary contains the answers 50 47 . If not, the summary might be missing key info (leading to a lower score). LLM- based evaluators can also be employed: e.g., prompting GPT- 4 with the article and summary and asking "Does the summary accurately reflect the article? List any inaccuracies." and then scoring accordingly.

For translation, a notable development is the use of MQM (Multidimensional Quality Metrics) as a human evaluation framework - it has experts annotate errors in translations with categories (like mistranslation, omission, etc.) and severity. This is more informative than a single score. Automatic metrics evolved too: beyond BLEU, there' s ChrF (character n- gram F- score) which can be more tolerant to word form variations, and COMET, BLEURT which are learned metrics using embeddings and possibly human- rated data to predict a quality score. As mentioned, in 2023 a study found GPT- 4 based metric (GEMBA) achieved state- of- the- art correlation with human MQM judgments for translation 31 32 . This suggests LLMs can evaluate translation quality by picking up subtle errors that older metrics miss (like minor meaning shifts or grammar issues).

One evaluation challenge for these tasks is that LLMs may produce perfectly valid outputs that differ from references. For example, an LLM summary might be more concise than the reference summary, yet still accurate - ROUGE might undervalue it for missing some reference words. Human evaluation is often needed to confirm which of two high- ROUGE summaries is actually better. For translation, especially for high- resource language pairs, automatic metrics are fairly reliable for ranking systems, but when systems are close, human tie- breakers are used (WMT conferences still conduct human eval for top systems). With LLMs now in the mix, another challenge is style: LLM translations might be literally correct but overly formal, or not adhere to certain domain conventions. Human evaluators can rate style appropriateness, while automatic metrics generally do not.

In summary, evaluation in MT and summarization is evolving from pure n- gram overlaps to a mix of overlap, semantics (embedding- based), and factual consistency checks. Human evaluation remains important for final judgments. Given LLMs' propensity to hallucinate or introduce subtle errors, specialized metrics and careful human review are needed to ensure quality in these generative tasks.

### 5.3 Dialogue and Conversational Systems

Perhaps the most challenging domain for evaluation is open- ended dialogue - the very task many LLMs are now famous for (e.g., ChatGPT' s conversational ability). In dialogue, the space of possible good responses is essentially unbounded, and the goals are multi- faceted: the assistant should be correct,

relevant, not repetitive, inoffensive, engaging, etc. There is no single ground- truth answer to compare against.

Human evaluation is primary in dialogue. Researchers often do user studies or crowdworker evaluations where they have a conversation with a model or observe a static conversation and then rate the model on several dimensions: e.g., coherence (does it make sense given the context?), relevance (does it address the user's last prompt?), fluency (is the English well- formed?), and overall humanness or engagingness (would you like to talk to this assistant again?). One common approach is to have two model responses for the same user prompt and ask: "Which response do you prefer?" - essentially a Turing test style comparison. The aforementioned Elo rating approach in Chatbot Arena is an example at scale 28 27. Another approach is to have the conversation go on for multiple turns and then get an overall rating.

Automatic metrics for dialogue have historically been inadequate. Simple metrics like word overlap or even embedding similarity to some reference don't capture the quality of a reply. In the past, some used metrics like Distinct- n (which measures how diverse the model's outputs are by counting distinct n- grams - higher is better to avoid dull, repetitive answers). This addresses one issue (repetitiveness) but not overall quality.

LLM- as- judge evaluation has found a major use case here: using GPT- 4 or similar to automate the comparison of chatbot responses. For instance, MT- Bench (2023) provides a set of multi- turn conversation prompts (like a user query possibly with a follow- up) and requires models to respond; then GPT- 4 is used to judge responses on categories such as helpfulness, correctness, level of detail, etc., often through pairwise comparison prompts 28. This has enabled creating rankings for chat models without constant human involvement. The authors of MT- Bench found GPT- 4's judgments correlated well with expert human annotators on these chat prompts 27. Similarly, other works have proposed ASA (AutoStop Approach) where the conversation is self- played between a user simulator and the model and then evaluated by an LLM.

Despite these innovations, some challenges remain: - Consistency over long conversations: An isolated response might look fine, but if a conversation is 10 turns long, the model could contradict itself or wander off topic. Evaluating coherence across multiple turns is hard. Humans can flag if the agent forgets something said earlier (context management), but automatic evaluation of whole dialogues is complex. Sometimes conversation- level rewards (like whether the user's goal was achieved) are used. - Personalization and Subjectivity: What is a good response can depend on user preference. One user might like a humorous style, another might find it inappropriate. Evaluations tend to average these out, but a model might score better with one demographic and worse with another. There's ongoing research on personalized evaluation - e.g., letting the user themselves rate the conversation as the metric of success. - Safety in dialogue: The model should not produce toxic or biased statements even if the user does. So evaluation must include checking if the model's responses abide by ethical/policy guidelines. This often requires either a human to read and judge the content for any violations, or the use of a separate toxicity classifier (like Perspective API) to score each response for hate or harassment likelihood. Those scores can form a "toxicity rate" metric (percentage of outputs above a certain toxicity threshold, ideally 0%). Anthropic and OpenAI, for instance, report such metrics when comparing models, showing reductions in toxic output frequency after alignment training 66 42.

To improve data for dialogue evaluation, some teams have collected human- chat datasets where models are compared head- to- head by crowdworkers (the OpenAI/webGPT dataset, the Anthropic HHRLHF dataset, etc.). These datasets can not only train models to be better (via RLHF) but also serve as evaluation benchmarks themselves: if one has a dataset of many prompts with human preference labels

for Model A vs B, one can run new models on those prompts and see how often they are preferred over the old ones.

In summary, evaluating dialogue is inherently hard. The community uses a combination of targeted questions (to test specific skills like reasoning or humor), paired comparisons for overall preference, and checks for safety and consistency. There is a trend toward holistic dialogue evaluation, meaning trying to evaluate along several dimensions rather than a single score, because a chatbot might be very helpful but slightly toxic, or safe but not very useful, etc., and stakeholders need to know these trade- offs.

### 5.4 Knowledge and Reasoning Tasks (QA, Common Sense, Exams)

LLMs are tested on a variety of tasks that assess their knowledge and reasoning ability, such as trivia question answering, common- sense reasoning benchmarks, and even professional exams (bar exam, math competitions, etc.).

For fact- based question answering (like TriviaQA, WebQuestions), evaluation is typically exact match or precision/recall of the correct answer string(s). If the question is "Who invented the telescope?" and the expected answer is "Hans Lipperhey", the model's answer is usually considered correct if it contains that name (modulo some normalization). LLMs sometimes generate more verbose answers or multiple sentences - evaluation scripts often have to check if the correct entity was mentioned. This is done by string matching or by using a knowledge- base to verify the answer. Some QA tasks allow a few variants of correct answer (especially if the answer is a date or something that can be phrased differently).

For common sense reasoning tasks like HellaSwag, PIQA (physical commonsense), or WinoGrande (pronoun resolution), these are often formatted as multiple- choice questions. Evaluation is simply whether the model picks the correct option (accuracy). LLMs can either be prompted to output the letter of the answer or the full answer sentence which is then matched. Notably, LLMs became good enough at some of these that they exceeded human performance (HellaSwag was created because models had started to get near  $85\%$  on its predecessor SWAG while humans are at  $95\%$  - now some LLMs get close to  $90\% +$  on HellaSwag, illustrating rapid progress).

Mathematical reasoning and logical puzzles have specialized evaluation: typically the answer is a number or a specific solution, so accuracy can be measured exactly. One interesting aspect is partial credit - if a model does a multi- step math problem and gets all steps right except the last arithmetic, do we consider that? In most evaluations, no, unless the benchmark specifically evaluates intermediate reasoning steps. Some research papers examine chain- of- thought and give credit if the chain is mostly correct even if final answer isn't. t. But standard benchmarks like GSM8K (grade school math problems) measure just final answer correctness.

Professional and academic exams (like questions from SAT, GRE, bar exam, medical exams) have been directly used as evaluation tasks for advanced LLMs. These often include a mix of multiple- choice and open- ended questions. For multiple- choice, again accuracy is the metric. For open- ended (like essay questions), evaluation is harder - often one resorts to having experts grade the model's response as they would a human test- taker. In the GPT- 4 technical report, OpenAI used humans to score the model on the bar exam essays and found it achieved around the top  $10\%$  of human test- takers on that exam 27. This is a form of human evaluation in a specific domain context (law experts grading the answers). Another example is coding interviews or competition programming problems - success is measured by passing test cases (like in code evaluation).

One emerging challenge in knowledge tasks is verifying truthfulness. Models can produce an answer that looks correct but is subtly wrong or entirely fabricated (hallucination). For critical domains (e.g., medical Q&A), evaluation must go beyond matching some expected answer; one must verify correctness against reliable sources. Datasets like TruthfulQA explicitly evaluate if the model's answer is true or false, with human judgment as the reference 24. On such datasets, researchers use metrics like "percent of answers rated truthful by humans." GPT- 3, for instance, had a low truthfulQA score  $(\sim 58\%)$  versus humans  $(\sim 94\%)$  24. Newer models are improving but it remains a hard evaluation because it requires judging the factuality of open- ended answers. Often, external fact- checking systems or another LLM tasked with fact- checking are used to evaluate outputs on truthfulness benchmarks.

### 5.5 Code Generation

Large language models (especially those fine- tuned on code, like OpenAI; Codex, AlphaCode, or open ones like StarCoder) are evaluated on their ability to generate working code. This introduces different methodologies:

The most common benchmark for code is to measure functional correctness: does the generated code solve the specified problem? To test this, each problem typically comes with a suite of unit tests. The model's output code is executed against these tests in a sandbox, and if it passes all tests, it's counted as correct 67 68 . This is essentially how human coding competitions are judged as well. For example, OpenAI's HumanEval benchmark provides Python function prompts and test cases; Codex achieved a certain pass@k on this. The metric pass@k means: we sample k independent outputs from the model and consider it a success if any of the k passes all tests 67 . Since generation is probabilistic, a model might fail on one attempt but succeed on another with a slightly different sampled output - so pass@k gives a sense of the model's probability of getting it correct if it can try multiple times.

Evaluating code via execution is powerful because it's an unambiguous signal: either the code works or not on the tests. It doesn't require human judgment except in interpreting if the problem was fully solved (tests coverage is crucial; if tests are not comprehensive, code might pass them but still be wrong in general). There are a few caveats: - Running code can be costly or unsafe (especially if the code could have side effects or long runtime). In controlled benchmarks it's fine, but evaluating an LLM on 1000 arbitrary coding tasks might require secure sandboxing. - If code requires certain environments or libraries, setting that up is part of the evaluation design. - Models might sometimes produce answers that are correct but formatted differently (e.g., a model might output just the solution function vs an entire file with boilerplate). Evaluations usually standardize the expected format or have trivial post- processing.

Besides functional correctness, code can be evaluated on style and efficiency, though those are secondary. For instance, given multiple correct solutions, one might prefer the more efficient one. Some evaluations use heuristics like code length or complexity, but generally if it's correct, it's considered fine. There is interest in having LLMs adhere to style guides (like proper naming, comments, etc.) - such aspects could be evaluated by static analysis or linters. For example, one could run a style linter on generated code and count warnings as a metric, or as shown in the Microsoft guidance prompt 69 70, an LLM judge can be instructed to consider readability and best practices when scoring code. However, functional correctness remains the primary metric because without a working solution, style doesn't matter much.

Human evaluation in coding is less common, because the automated signals are strong (tests either pass or fail). But in scenarios like generating explanations or documentation for code, humans might be needed to assess the quality of the explanation (did the model accurately describe the code's function, etc.).

One interesting point: as models get very good, some benchmarks become too easy (like trivial coding problems). In 2022 DeepMind' s AlphaCode and OpenAl' s Codex were around competition participant level; by 2023, GPT- 4 could solve a majority of easy/medium LeetCode problems. So evaluation datasets have to increase difficulty or focus on novel tasks. There' s also the question of generalization to unseen problems - data contamination is again a concern, as some competitive programming problems have solutions online which might be in the training data. To combat this, new collections (like CodeContests, a dataset of competitive programming problems not in training) were created to fairly evaluate coding abilities.

### 5.6 Safety, Bias, and Ethical Evaluation

Evaluating ethical aspects of LLMs - such as bias, fairness, toxicity, misinformation - is a subfield of its own. Unlike task accuracy, these aspects are harder to reduce to a single number, but researchers have developed frameworks and benchmarks:

- Toxicity: A common approach is to use an automated detector (like Perspective API' s toxicity model) on a large sample of the LLM' s outputs (either generated in response to diverse prompts or specifically prompts designed to elicit toxicity) and measure the fraction that exceed a toxicity threshold. For example, "toxicity rate  $= 0.5\%$  meaning  $0.5\%$  of outputs were flagged as possibly toxic. This gives a sense of how often a model might produce harmful language. Human evaluation can refine this by checking if those flagged cases are truly toxic or borderline. In HELM, toxicity is one of the seven metrics measured for each model across scenarios 15 . If an LLM is intended for general use, having a very low toxicity rate is crucial; improvements are reported by noting reductions in this metric after applying better filtering or alignment.

- Bias and Fairness: Bias evaluation is multifaceted. One approach uses template-based tests: for example, take a sentence "The [profession] is [adjective]" and fill with different demographic groups (man, woman, black, white, etc.) to see if the model systematically completes or scores some groups more negatively. Datasets like CrowS-Pairs and StereoSet are collections of such paired sentences designed to reveal stereotypical biases 71. A bias metric might be the percentage of times the model picks a more stereotypical completion over a neutral one. Another approach is to test question-answering where the question contains demographic information and see if that affects the answer incorrectly. For example: "A doctor and a nurse walked in. The nurse said... who is likely speaking?" can reveal gender biases. These need careful design and often are debated how to properly quantify bias. The result might be reported as a score where  $50\%$  means no bias and deviations indicate bias in a direction.

- Misinformation and hallucinations: If an LLM can output false information, how do we evaluate that propensity? One method: evaluate on benchmarks that require truthfulness (like TruthfulQA which we mentioned). That essentially gives a "truthfulness score" as percent of answers that are true 24. Another method is to check factual QA or knowledge - e.g.,  $x\%$  correct on factual questions can also be seen inversely as an hallucination rate if those incorrect answers were not just "don't know" but actually wrong statements. There's emerging work where models are asked to back up answers with sources (like retrieval-augmented generation) and evaluation includes checking if the provided sources actually support the answer. In such cases, metrics like precision/recall of supporting evidence or Attribution score are used (did the content of the output that is factual appear in the retrieved source?).

- Privacy and Toxic Content Memorization: Another angle is evaluating if the LLM inadvertently reveals sensitive or private info from training data. OpenAI tested GPT-4 by trying to get it to output verbatim chunks from its training (like personal addresses or keys) - they reported

statistics like "it produces training data verbatim in X out of Y attempts" 72 73 . This is an evaluation of memorization. Similarly, one might test with prompts like "Write a paragraph containing [some rare personal info from training]" to see if the model leaks it. Ideally, a safe model should refuse or not have memorized such specifics. Quantifying privacy leakage might involve querying the model with known data points and measuring exact matches (as done in studies on GPT- 2 which found a small percentage of verbatim regurgitations 72 ).

Ethical evaluations often end up as a list of separate metrics or findings rather than one score. For instance: Model X has a toxicity rate of  $0.1\%$ , a bias score of 60 on Gender bias test (50 = ideal unbiased), did not produce disallowed content in  $98\%$  of adversarial prompts, and so on. HELM tried to incorporate some of these into the main scoreboard (e.g., "bias" metric combining some bias tests) 15, but it's still early for unified metrics here.

Human oversight is key - automated detectors for toxicity or bias are not perfect, so often a combination of automated flags and human review is used in final evaluation of these aspects.

To wrap up this section: each subfield or task type brings its own evaluation needs. LLMs blur the boundaries between these subfields because a single model can do many tasks; thus, something like HELM's multi- scenario evaluation or BIG- bench's collection of tasks is useful to cover a broad base 74 . But when focusing on a specific application, one must use the metrics and evaluation setup appropriate for that domain (e.g., if deploying an LLM for medical advice, one must do rigorous human expert evaluation on correctness and harm avoidance, more so than worrying about BLEU scores or such).

## 6 Open Challenges and Future Directions in LLM Evaluation

Despite a proliferation of metrics and benchmarks, evaluating large language models remains an evolving and sometimes under- defined problem. We highlight some open challenges and potential future directions:

1. Data Contamination and Evaluation Validity: As models train on ever-larger swaths of the internet, they are likely to have seen many benchmark test examples. This "training on the test" issue can invalidate evaluations - a model may appear to perform well not because it truly learned to solve a problem but because it memorized answers 34 35. Detecting and preventing data contamination is hard; benchmarks that were public for years are almost certainly ingested in training corpora. Future evaluations may require dynamic benchmark creation - generating fresh test questions on the fly (possibly by humans or by models) that the model couldn't have seen. Some suggest not publishing test sets publicly at all, or having held-out private evaluations, but that conflicts with open science transparency. This challenge remains: how to fairly measure model improvement when past benchmarks turn into training data. One approach is to focus on abilities that can't be brute-force memorized easily, e.g., interactive tasks or reasoning problems that are unique each time.

2. Reproducibility and Evolving Models: Evaluating proprietary models like ChatGPT or GPT-4 is complicated by the fact that these services can update the model or its prompting behavior silently. As Ehud Reiter noted, repeating an experiment with GPT-4 months later might yield different results because the model was updated 75 76. This undermines the reproducibility of evaluations. Academic research traditionally relies on static models and test sets so that experiments can be verified. With LLM APIs, we have moving targets. A challenge is how to version models for evaluation - OpenAI has started naming model versions (e.g., "GPT-4 Jan 2024"), but not all changes are transparent. For open-source models, reproducibility is easier (one can release a fixed checkpoint). The field might move toward evaluation

protocols where either one uses open models or one ensures access to a frozen version of an API for consistency. Otherwise, our leaderboards might mix results from slightly different models unknowingly. Developing evaluation harnesses that can be linked to a particular model snapshot or logging system might be important.

3. Evaluating at Human Parity and Beyond: What do we do when models start achieving (or exceeding) human-level performance on benchmarks? We already see instances - e.g., GPT-4 scored in top  $10\%$  on the bar exam, some reading comprehension tasks are nearly saturated, and in translation certain language pairs see human-level BLEU scores. This raises two issues: - We need harder or new benchmarks to continue differentiating models. But continually creating new tests is resource-intensive and risks the cycle of them eventually leaking into training data. - When models are at human-level on a test, evaluation might need to focus more on fine-grained and qualitative aspects. For example, two models both get  $90\%$  on a test - we' d need to see which  $10\%$  they get wrong and whether those mistakes are serious or trivial. The evaluation might shift from aggregate scores to error analysis. Future leaderboards might include not just scores but profiles of errors or capabilities. - Human parity also complicates human eval: if a model' s text is often indistinguishable from a human' s, evaluators might not be able to easily say which is better. We might need expert or adversarial evaluators to find weaknesses, or use Al to evaluate consistency over a long range where subtle differences emerge.

4. Worst-Case vs Average-Case Performance: Current evaluations often report average accuracy or preferences. But as LLMs integrate into critical applications, the rare catastrophic errors become more significant than a slight difference in average score. How to evaluate and reduce the probability of a grave mistake (e.g., a highly confident but very wrong answer in a medical context) is an open problem. Adversarial testing and stress tests are partial solutions, but we can't enumerate all possible bad cases. One future direction is risk-focused evaluation: estimating probabilities of different failure modes. This could involve large-scale simulation (generate thousands of scenarios and see how many failures) or formal verification-style approaches for certain properties. Measuring uncertainty calibration is also key: does the model know when it doesn't know? Metrics like calibration error can be computed (difference between confidence and actual accuracy) 15; GPT-4 is reported to be better calibrated than smaller models, but still not perfect. A well-calibrated model would at least signal its uncertainty on hard cases, mitigating risk.

5. Multi-modal and Embodied Evaluation: The prompt specifically focuses on language models, but it's worth noting that many LLMs are being extended to handle images (e.g., Vision+Language models like GPT-4 with vision, or robotics instruction following). Evaluation then crosses into other domains: e.g., describing an image (evaluated by correctness of description), following instructions in a physical environment (evaluated by success of completing a task). This introduces all the complexities of those fields (computer vision metrics, success criteria in robotics) combined with the language aspect. We'll likely see unified benchmarks that test models on multi-modal understanding and action (for example, a model might be scored on how well it can answer questions about images and how well it can generate images from descriptions, etc.). Defining those evaluations in a standard way is a challenge being tackled by interdisciplinary efforts.

6. Interactive and Adaptive Evaluation: Most current evaluations treat the model as a fixed mapping from input to output. But in reality, models like chatbots have interactive behavior: they can ask clarifying questions, the user can rephrase, etc. Future evaluation might consider dialogue-level success - not just one-turn responses. For example, evaluate an assistant by giving it a user with a particular goal (booking a flight) and measure if, through multi-turn interaction, the goal is achieved. This is more like evaluating a human assistant. It's complex because it involves a simulation of user behavior and possibly non-deterministic interactions. One idea is to use agent simulations (where an LLM or a scripted

policy plays the user role) to test the model in a loop. Another is to crowdsource user trials, though that's expensive.

7. Defining "Quality" and Avoiding Proxy Metrics: A fundamental challenge is that we often lack a precise definition of what makes an LLM output good. We use proxies like BLEU for translation quality or human preference votes for helpfulness, but these are imperfect. There is ongoing discussion about better objective functions for language generation - for instance, could we have a learned metric that truly captures communicative success? The risk of optimizing on an imperfect metric is that models might overfit to the metric without truly improving utility (a classic case was MT systems maximizing BLEU by producing overly short and safe translations that weren't actually better). With RLHF, models optimize for human ratings, but those ratings themselves can be noisy or biased. For example, human raters might prefer answers that sound confident and eloquent, even if they occasionally stray from facts. If a model learns to please raters by style, it might not actually be more correct. So a challenge is ensuring our evaluation metrics (and reward models) align with actual underlying quality and truthfulness.

Researchers have proposed using multiple metrics and intersectional evaluation - a model should be considered better only if it's better on all pertinent metrics (accuracy, harmlessness, etc.), to avoid trading one off for another without noticing. The HELM framework explicitly tries to highlight such trade- offs 15.

8. Involvement of Domain Experts: For certain applications, only domain experts can evaluate correctly. For example, an LLM giving medical advice must be evaluated by doctors for both accuracy and appropriateness; laypeople or generic metrics might miss subtleties (like an answer that is factually correct but medically inappropriate to tell a patient) 77 78. Future evaluation efforts will likely incorporate expert panels when moving into high-stakes domains (law, finance, healthcare). This is challenging to scale - experts' time is costly - but perhaps necessary as LLMs enter these fields. One approach is to first use models or non-experts to narrow down cases of interest, then have experts deeply evaluate those (a two-tier process).

9. Transparency and Evaluation of Evaluation: Interestingly, we need to evaluate our evaluation methods themselves. For All judges, we must keep testing how well they agree with humans and identify conditions where they fail. For new metrics, we need to validate their correlation with human judgment (it's common in NLP papers introducing a metric to report correlation coefficients with human scores on some dataset). There is a meta-science question: How do we ensure our evaluation is reliable, fair, and actually pushing progress? Some advocate for predictability metrics (if a model improves on metric X, does real user satisfaction also improve? If not, maybe X is not a good metric).

10. Community and Dynamic Benchmarks: We see efforts like BIG-bench and OpenAI Evals that treat evaluation as a living, community-driven process. They allow people to add new tasks or adversarial cases over time. This means evaluation is not a fixed set but evolving. The challenge is how to integrate new evaluations while maintaining the ability to compare with older models. Perhaps the future is dynamic leaderboards that adjust for difficulty and have an ever-expanding set of tests. This is akin to how human IQ tests are periodically recalibrated as people get used to them - here, as models "get used" (or trained) on certain tasks, we add new ones.

In conclusion, evaluating LLMs is as much an ongoing journey as training them. As the capabilities of LLMs broaden, our evaluation strategies must become more holistic, adaptive, and robust. A quote from a recent work encapsulates the state: "LLM evaluation is an emerging area of research and has not yet been systematically studied" 55 - highlighting that even though we have many metrics, understanding their reliability and interplay is still developing. Going forward, we expect closer

collaboration between human evaluators and AI evaluators, more task- specific deep dives, and possibly entirely new paradigms of evaluation (like testing a model's internal reasoning or causal understanding, not just input- output). By improving how we evaluate, we improve our ability to diagnose model weaknesses and measure progress – which ultimately helps guide the development of better, safer, and more trustworthy language models.

## 7 Conclusion

Evaluating large language models is a complex, multi- dimensional task that has evolved significantly over the history of NLP. We started from simple intrinsic measures like perplexity and single- number metrics like BLEU for isolated tasks, and we have arrived at a landscape of extensive benchmarks, hybrid evaluation methods, and ongoing human oversight. In this survey, we have defined key concepts and highlighted the need for both breadth and depth in evaluation: breadth in covering the many capabilities and behaviors of LLMs (from core language understanding to nuanced dialogue and ethical conduct), and depth in rigorously assessing each of those aspects with appropriate techniques.

We reviewed the foundations of LLM evaluation, including formal definitions and early metrics, and traced a timeline of how evaluation practices adapted as models grew more capable. In recent years, innovations such as LLM- as- a- judge and comprehensive benchmarks like HELM represent the community's response to the challenges of scale and generality that LLMs present. We examined how different application areas – from translation to coding to conversation – each require tailored evaluation measures, and how LLMs blur these boundaries by tackling all such tasks.

Throughout, a recurring theme is the primacy of human judgment – our metrics and automated evaluators ultimately strive to reflect what humans consider better or worse output. As models reach higher performance, the evaluator's role becomes even more critical in teasing apart subtle differences and ensuring reliability. Meanwhile, open challenges persist in ensuring evaluations remain fair, robust, and aligned with what we truly want from AI systems (truthfulness, helpfulness, harmlessness, etc.).

The field is actively seeking solutions: dynamic and adversarial testing to stay ahead of models' rote learning, involving experts for domain- specific quality control, and developing new metrics that capture previously unmeasured qualities. The coming years will likely see the emergence of evaluation standards for AI (perhaps analogous to standardized tests but for AI behaviors), as well as possibly regulatory benchmarks for safety and fairness as AI deployment becomes widespread.

In summary, effective evaluation of LLMs is both a science and an art – it requires statistical rigor and big- data benchmarks, but also thoughtful design of scenarios and criteria that matter to humans. It is an essential feedback mechanism that drives the progress of model development. By continually refining how we evaluate, we enable the creation of models that not only score high on some metric, but truly perform in ways that are useful, correct, and respectful of human values. The survey presented here aims to provide a foundation and reference as researchers and practitioners navigate the evolving landscape of LLM evaluation – a task as grand and dynamic as the models themselves.

References: (Included as inline citations throughout the text, denoted by [source + lines], to indicate the origin of specific information or quotes. Key sources include academic papers, benchmark reports, and authoritative discussions on LLM evaluation methods.)

## References

1 3 4 16 20 21 22 23 24 34 35 65 71 72 73 Large language model - Wikipedia https://en.wikipedia.org/wiki/Large_language_model

2 5 6 7 14 19 36 38 39 47 48 50 51 55 67 68 69 70 Evaluation metrics | Microsoft Learn https://learn.microsoft.com/en- us/ai/playbook/technology- guidance/generative- ai/working- with- llms/evaluation/list- of- eval- metrics

8 9 11 12 13 33 37 40 41 42 43 44 45 46 49 52 53 56 57 58 59 60 61 62 63 64 66 LLM

Evaluation Frameworks, Metrics & Methods Explained - Qualifire Blog https://www.qualifire.ai/posts/llm- evaluation- frameworks- metrics- methods- explained

18 75 76 77 78 Challenges in Evaluating LLMs - Ehud Reiter's Blog https://ehudreiter.com/2024/07/10/challenges- in- evaluating- llms/

15 25 26 74 [2211.09110] Holistic Evaluation of Language Models https://arxiv.org/abs/2211.09110

17 explained. BLEU: Bilingual Evaluation Understudy... | by Kiran Kumar https://medium.com/@kirankumar_61999/bleu- score- b3130dfa3ea

18 ROUGE Metric In NLP: Complete Guide & How To Tutorial In Python https://spotintelligence.com/2024/08/12/rouge- metric- in- nlp/

27 28 54 [2306.05685] Judging LLM- as- a- Judge with MT- Bench and Chatbot Arena https://arxiv.org/abs/2306.05685

29 30 G- Eval: NLG Evaluation using Gpt- 4 with Better Human Alignment - ACL Anthology https://aclanthology.org/2023.emnlp- main.153/

31 32 [2302.14520] Large Language Models Are State- of- the- Art Evaluators of Translation Quality https://arxiv.org/abs/2302.14520