[
    {
        "topic": "Multimodal Large Language Models",
        "outline": "1 Introduction\n2 Architecture\n    2.1 Modality encoder\n    2.2 Pre-trained LLM\n    2.3 Modality interface\n3 Training Strategy and Data\n    3.1 Pre-training\n        3.1.1 Training Detail\n        3.1.2 Data\n    3.2 Instruction-tuning\n        3.2.1 Introduction\n        3.2.2 Training Detail\n        3.2.3 Data Collection\n        3.2.4 Data Quality\n    3.3 Alignment tuning\n        3.3.1 Introduction\n        3.3.2 Training Detail\n        3.3.3 Data\n4 Evaluation\n    4.1 Closed-set\n    4.2 Open-set\n5 Extensions\n6 Multimodal Hallucination\n    6.1 Preliminaries\n    6.2 Evaluation Methods\n    6.3 Mitigation Methods\n7 Extended Techniques\n    7.1 Multimodal In-Context Learning\n        7.1.1 Improvement on ICL capabilities\n        7.1.2 Applications\n    7.2 Multimodal Chain of Thought\n        7.2.1 Learning Paradigms\n        7.2.2 Chain Configuration\n        7.2.3 Generation Patterns\n    7.3 LLM-Aided Visual Reasoning\n        7.3.1 Introduction\n        7.3.2 Training Paradigms\n        7.3.3 Functions\n8 Challenges and Future Directions\n9 Conclusion"
    },
    {
        "topic": "Evaluation of Large Language Models",
        "outline": "1 Introduction\n2 Background\n    2.1 Large Language Models\n    2.2 AI Model Evaluation\n3 What to Evaluate\n    3.1 Natural Language Processing Tasks\n    3.2 Robustness, Ethic, Bias, and Trustworthiness\n    3.3 Social Science\n    3.4 Natural Science and Engineering\n    3.5 Medical Applications\n    3.6 Agent Applications\n    3.7 Other Applications\n4 Where to Evaluate: Datasets and Benchmarks\n    4.1 Benchmarks for General Tasks\n    4.2 Benchmarks for Specific Downstream Tasks\n    4.3 Benchmarks for Multi-modal task\n5 How to Evaluate\n    5.1 Automatic Evaluation\n    5.2 Human Evaluation\n6 Summary\n    6.1 Task: Success and Failure Cases of LLMs\n    6.2 Benchmark and Evaluation Protocol\n7 Grand Challenges and Opportunities for Future Research\n    7.1 Designing AGI Benchmarks\n    7.2 Complete Behavioral Evaluation\n    7.3 Robustness Evaluation\n    7.4 Dynamic and Evolving Evaluation\n    7.5 Principled and Trustworthy Evaluation\n    7.6 Unified Evaluation that Supports All LLMs Tasks\n    7.7 Beyond Evaluation: LLMs Enhancement\n8 Conclusion"
    },
    {
        "topic": "3D Object Detection in Autonomous Driving",
        "outline": "1 Introduction\n2 Background\n    2.1 What is 3D Object Detection?\n    2.2 Datasets\n    2.3 Evaluation Metrics\n3 LiDAR-based 3D Object Detection\n    3.1 Data Representations for 3D Object Detection\n        3.1.1 Point-based 3D Object Detection\n        3.1.2 Grid-based 3D Object Detection\n        3.1.3 Point-voxel Based 3D Object Detection\n        3.1.4 Range-based 3D Object Detection\n    3.2 Learning Objectives for 3D Object Detection\n3.2.1 Anchor-based 3D Object Detection\n3.2.2 Anchor-free 3D Object Detection\n3.2.3 3D Object Detection with Auxiliary Tasks\n4 Camera-based 3D Object Detection\n    4.1 Monocular 3D Object Detection\n4.1.1 Image-only Monocular Detection\n4.1.2 Depth-assisted Monocular Detection\n4.1.3 Prior-guided Monocular Detection\n    4.2 Stereo-based 3D Object Detection\n    4.3 Multi-view 3D Object Detection\n5 Multi-modal 3D Object Detection\n    5.1  Multi-modal detection with LiDAR-camera fusion\n5.1.1 Early-fusion based 3D object detection\n5.1.2 Intermediate-fusion based 3D object detection\n5.1.3 Late-fusion based 3D object detection\n    5.2 Multi-modal detection with radar signals\n    5.3 Multi-modal detection with high-definition maps\n6 Transformer-based 3D Object Detection\n    6.1 Transformer architectures for 3D object detection\n    6.2 Transformer applications in 3D object detection\n7 Temporal 3D Object Detection\n    7.1 3D object detection from LiDAR sequences\n    7.2 3D object detection from streaming data\n    7.3  3D object detection from videos\n8 Label-Efficient 3D Object Detection\n    8.1 Domain adaptation for 3D object detection\n    8.2 Weakly-supervised 3D object detection\n    8.3 Semi-supervised 3D object detection\n    8.4 Self-supervised 3D object detection\n9 3D Object Detection in Driving Systems\n    9.1 End-to-end Learning for autonomous driving\n    9.2 Simulation for 3D object detection\n    9.3 Robustness for 3D object detection\n    9.4 Collaborative 3D object detection\n10 Analysis and Outlooks\n    10.1 Research trends\n        10.1.1 Trends of dataset selection\n        10.1.2 Trends of inference time\n        10.1.3 Trends of the LiDAR-based methods\n        10.1.4 Trends of the camera-based methods\n        10.1.5 Trends of the multi-modal methods\n        10.1.6 Systematic comparisons\n    10.2 Future outlooks\n        10.2.1 Open-set 3D object detection\n        10.2.2 Detection with stronger interpretability\n        10.2.3 Efficient hardware design for 3D object detection\n        10.2.4 Detection in end-to-end self-driving systems\n11 Conclusion"
    },
    {
        "topic": "Vision Transformers",
        "outline": "I Introduction\nII Original Transformers\n    II-A (Multi-Head) Attention Mechanism\n    II-B Position-wise Feed-Forward Networks\n    II-C Positional Encoding\n    II-D Transformer Model\nIII Transformer for Classification\n    III-A Original Visual Transformer\n    III-B Transformer-enhanced CNNs\n    III-C CNN-enhanced Transformer\n    III-D Local Attention Enhanced Transformer\n    III-E Hierarchical Transformer\n    III-F Deep Transformer\n    III-G Transformers with Self-Supervised Learning\n    III-H Discussion\n        III-H1 Algorithm Evaluation and Comparative Analysis\n        III-H2 Brief Discussion on Alternatives\nIV Transformer for Detection\n    IV-A Transformer Neck\n        IV-A1 The Original Detectors\n        IV-A2 Transformer with Sparse Attention\n        IV-A3 Transformer with Spatial Prior\n        IV-A4 Transformer with Redesigned Structure\n        IV-A5 Transformer with Bipartite Matched Optimization\n        IV-A6 Transformer Detector with Pre-Training\n    IV-B Transformer Backbone\n    IV-C Discussion\nV Transformer for Segmentation\n    V-A Patch-Based Transformer\n    V-B Query-Based Transformer\n        V-B1 Object Queries\n        V-B2 Mask Embeddings\n    V-C Discussion\nVI Transformer for 3D Visual Recognition\n    VI-A Representation Learning\n    VI-B Cognition Mapping\n    VI-C Specific Processing\nVII Transformer for Multi-Sensory Data Stream\n    VII-A Homologous Stream\n        VII-A1 Interactive Fusion\n        VII-A2 Transfer Fusion\n    VII-B Heterologous Stream\n        VII-B1 Visual-Linguistic Pre-Training\n        VII-B2 Visual Grounding\nVIII Discussion and Conclusion\n    VIII-A Summary of Recent Improvements\n    VIII-B Discussion on Visual Transformers\n        VIII-B1 How Transformers Bridge the Gap Between Language and Vision\n        VIII-B2 The Relationship between Transformers, Self-Attention and CNNs\n        VIII-B3 Double Edges of Visual Transformers\n        VIII-B4 Learnable Embeddings for Different Visual Tasks\n    VIII-C Future Research Directions\n        VIII-C1 Set Prediction\n        VIII-C2 Self-Supervised Learning\n    VIII-D Conclusion"
    },
    {
        "topic": "Hallucination in Large Language Models",
        "outline": "1 Introduction\n2 Hallucination in the Era of LLM\n    2.1 Large Language Models\n    2.2 What is LLM Hallucination\n    2.3 Unique Challenge in the Era of LLM\n    2.4 Other Problems in LLMs\n3 Evaluation of LLM Hallucination\n    3.1 Evaluation Benchmarks\n    3.2 Evaluation Metrics\n4 Sources of LLM Hallucination\n5 Mitigation of LLM Hallucination\n    5.1 Mitigation during Pre-training\n    5.2 Mitigation during SFT\n    5.3 Mitigation during RLHF\n    5.4 Mitigation during Inference\n        5.4.1 Designing Decoding Strategies\n        5.4.2 Resorting to External Knowledge\n        5.4.3 Exploiting Uncertainty\n    5.5 Other Methods\n6 Outlooks\n7 Conclusion"
    },
    {
        "topic": "Generative Diffusion Models",
        "outline": "1 Introduction\n2 Preliminaries\n    2.1 Notions and Definitions\n        2.1.1 Time and States\n        2.1.2 Forward / Reverse Process, and Transition Kernel\n        2.1.3 From discrete to continuous\n    2.2 Background\n        2.2.1 Denoised Diffusion Probabilistic Models (DDPM)\n        2.2.2 Score SDE Formulation\n    2.3 Conditional Diffusion Probabilistic Models\n3 Algorithm Improvement\n    3.1 Sampling Acceleration\n        3.1.1 Knowledge Distillation\n        3.1.2 Training Schedule\n        3.1.3 Training-Free Sampling\n        3.1.4 Merging Diffusion and Other Generative Models\n    3.2 Diffusion Process Design\n        3.2.1 Latent Space\n        3.2.2 Emerging Forward Processes\n        3.2.3 Diffusion Models on non-Euclidean space\n    3.3 Likelihood Optimization\n        3.3.1 MLE Training\n        3.3.2 Hybrid Loss\n    3.4 Bridging Distributions\n4 Application\n    4.1 Image Generation\n        4.1.1 Text condition\n        4.1.2 Image condition\n    4.2 3D Generation\n        4.2.1 3D Data Condition\n        4.2.2 2D Diffusion prior\n    4.3 Video Generation\n    4.4 Medical Analysis\n        4.4.1 In-distribution Analysis\n        4.4.2 Cross-distribution Generation\n    4.5 Text Generation\n        4.5.1 Discrete Generation\n        4.5.2 Latent Generation\n    4.6 Time Series Generation\n    4.7 Audio Generation\n    4.8 Molecule Design\n        4.8.1 Unconditional Generation\n        4.8.2 Cross-modal Generation\n    4.9 Graph Generation\n5 Conclusions & Discussions\n    5.1 Conclusions\n    5.2 Comparison to Existing Surveys\n    5.3 Limitations and Future Directions\n        5.3.1 Challenges Under Data Limitations\n        5.3.2 Controllable Distribution-based Generation\n        5.3.3 Advanced Multi-modal Generation Leveraging LLMs\n        5.3.4 Integration with Machine Learning Fields\n6 Appendix A: Sampling Algorithms\n    6.1 Unconditional Sampling\n        6.1.1 Ancestral Sampling\n        6.1.2 Annealed Langevin Dynamics Sampling\n        6.1.3 Predictor-Corrector Sampling\n    6.2 Conditional Sampling\n        6.2.1 Labeled Condition\n    6.3 Unlabeled Condition\n7 Appendix B: Evaluation Metric\n    7.1 Inception Score (IS)\n    7.2 Frechet Inception Distance (FID)\n    7.3 Negative Log Likelihood (NLL)\n8 Appendix C: Benchmarks\n    8.1 Benchmarks on CelebA-64\n    8.2 Benchmarks on ImageNet-64\n    8.3 Benchmarks on CIFAR-10 Dataset\n9 Appendix D: Details for Improvement Algorithms\n10 Appendix E: Table of Notation"
    },
    {
        "topic": "3D Gaussian Splatting",
        "outline": "1 Introduction\n2 Background\n    2.1 Radiance Field\n    2.2 Context and Terminology\n3 Principles\n    3.1 Rendering with Learned 3D Gaussians\n    3.2 Optimization of 3D GS\n4 Directions\n    4.1 3D GS for Sparse Input\n    4.2 Memory-efficient 3D GS\n    4.3 Photorealistic 3D GS\n    4.4 Improved Optimization Algorithms\n    4.5 3D Gaussian with More Properties\n    4.6 Hybrid Representation\n    4.7 New Rendering Algorithm\n5 Application Areas & Tasks\n    5.1 Robotics\n    5.2 Dynamic Scene Reconstruction\n    5.3 Generation and Editing\n    5.4 Avatar\n    5.5 Endoscopic Scene Reconstruction\n    5.6 Large-scale Scene Reconstruction\n    5.7 Physics\n6 Performance Comparison\n    6.1 Localization\n    6.2 Static Scenes\n    6.3 Dynamic Scenes\n    6.4 Human Avatar\n    6.5 Surgical Scenes\n7 Research Directions\n8 Conclusions"
    },
    {
        "topic": "LLM-based Multi-Agent",
        "outline": "1 Introduction\n2 LLM-based Autonomous Agent Construction\n    2.1 Agent Architecture Design\n        2.1.1 Profiling Module\n        2.1.2 Memory Module\n        2.1.3 Planning Module\n        2.1.4 Action Module\n    2.2 Agent Capability Acquisition\n3 LLM-based Autonomous Agent Application\n    3.1 Social Science\n    3.2 Natural Science\n    3.3 Engineering\n4 LLM-based Autonomous Agent Evaluation\n    4.1 Subjective Evaluation\n    4.2 Objective Evaluation\n5 Related Surveys\n6 Challenges\n    6.1 Role-playing Capability\n    6.2 Generalized Human Alignment\n    6.3 Prompt Robustness\n    6.4 Hallucination\n    6.5 Knowledge Boundary\n    6.6 Efficiency\n7 Conclusion"
    },
    {
        "topic": "Graph Neural Networks",
        "outline": "1 Introduction\n2 Preliminaries\n3 Architectures\n    3.1 Graph Convolutional Neural Networks (GCNNs)\n    3.2 Graph Pooling Operators\n    3.3 Graph Attention Mechanisms\n    3.4 Graph Recurrent Neural Networks\n4 Extensions and Applications\n    4.1 GCNNs on Special Graphs\n    4.2 Capability and Interpretability\n    4.3 Deep Graph Representation Learning\n    4.4 Deep Graph Generative Models\n    4.5 Combinations of the PI and GNNs\n    4.6 Adversarial Attacks for the GNNs\n    4.7 Graph Neural Architecture Search\n    4.8 Graph Reinforcement Learning\n    4.9 Applications\n5 Benchmarks and Evaluation Pitfalls\n6 Future Research Directions\n7 Conclusions"
    },
    {
        "topic": "Retrieval-Augmented Generation for Large Language Models",
        "outline": "1 Introduction\n2 Overview of RAG\n    2.1 Naive RAG\n    2.2 Advanced RAG\n    2.3 Modular RAG\n        2.3.1 New Modules\n        2.3.2 New Patterns\n    2.4 RAG vs Fine-tuning\n3 Retrieval\n    3.1 Retrieval Source\n        3.1.1 Data Structure\n        3.1.2 Retrieval Granularity\n    3.2 Indexing Optimization\n        3.2.1 Chunking Strategy\n        3.2.2 Metadata Attachments\n        3.2.3 Structural Index\n    3.3 Query Optimization\n        3.3.1 Query Expansion\n        3.3.2 Query Transformation\n        3.3.3 Query Routing\n    3.4 Embedding\n        3.4.1 Mix/hybrid Retrieval\n        3.4.2 Fine-tuning Embedding Model\n    3.5 Adapter\n4 Generation\n    4.1 Context Curation\n        4.1.1 Reranking\n        4.1.2 Context Selection/Compression\n    4.2 LLM Fine-tuning\n5 Augmentation process in RAG\n    5.1 Iterative Retrieval\n    5.2 Recursive Retrieval\n    5.3 Adaptive Retrieval\n6 Task and Evaluation\n    6.1 Downstream Task\n    6.2 Evaluation Target\n    6.3 Evaluation Aspects\n        6.3.1 Quality Scores\n        6.3.2 Required Abilities\n    6.4 Evaluation Benchmarks and Tools\n7 Discussion and Future Prospects\n    7.1 RAG vs Long Context\n    7.2 RAG Robustness\n    7.3 Hybrid Approaches\n    7.4 Scaling laws of RAG\n    7.5 Production-Ready RAG\n    7.6 Multi-modal RAG\n8 Conclusion"
    },
    {
        "topic": "Agentic Reinforcement Learning",
        "outline": "1 Introduction\n2 Preliminary: From LLM RL to Agentic RL\n    2.1 Markov Decision Processes\n    2.2 Environment State\n    2.3 Action Space\n    2.4 Transition Dynamics\n    2.5 Reward Function\n    2.6 Learning Objective\n    2.7 RL Algorithms\n3 Agentic RL: The model capability perspective\n    3.1 Planning\n    3.2 Tool Using\n    3.3 Memory\n    3.4 Self-Improvement\n    3.5 Reasoning\n    3.6 Perception\n    3.7 Others\n4 Agentic RL: The Task Perspective\n    4.1 Search & Research Agent\n        4.1.1 Open Source RL Methods\n        4.1.2 Closed Source RL Methods\n    4.2 Code Agent\n        4.2.1 RL for Code Generation\n        4.2.2 RL for Iterative Code Refinement\n        4.2.3 RL for Automated Software Engineering\n    4.3 Mathematical Agent\n        4.3.1 RL for Informal Mathematical Reasoning\n        4.3.2 RL for Formal Mathematical Reasoning\n    4.4 GUI Agent\n        4.4.1 RL-free Methods\n        4.4.2 RL in Static GUI Environments\n        4.4.3 RL in Interactive GUI Environments\n    4.5 RL in Vision Agents\n    4.6 RL in Embodied Agents\n    4.7 RL in Multi-Agent Systems\n    4.8 Other Tasks\n5 Enviroment and Frameworks\n    5.1 Environment Simulator\n        5.1.1 Web Environments\n        5.1.2 GUI Environments\n        5.1.3 Coding & Software Engineering Environments\n        5.1.4 Domain-specific Environments\n        5.1.5 Simulated & Game Environments\n        5.1.6 General-Purpose Environments\n    5.2 RL Framework\n6 Open Challenges and Future Directions\n    6.1 Trustworthiness\n    6.2 Scaling up Agentic Training\n    6.3 Scaling up Agentic Environment.\n7 Conclusion"
    },
    {
        "topic": "Alignment of Large Language Models",
        "outline": "1 Introduction\n2 Alignment Data Collection\n    2.1  Instructions from Human\n        2.1.1 NLP Benchmarks\n        2.1.2  Hand-crafted Instructions\n    2.2 Instructions From Strong LLMs\n        2.2.1  Self-Instruction\n        2.2.2 Multi-turn Instructions\n        2.2.3 Multilingual Instructions\n    2.3 Instruction Data Management\n3 Alignment Training\n    3.1 Online Human Preference Training\n    3.2 Offline Human Preference Training\n        3.2.1 Ranking-based Approach\n        3.2.2 Language-based Approach\n    3.3 Parameter-Effective Training\n4 Alignment Evaluation\n    4.1 Evaluation Benchmarks\n        4.1.1 Closed-set Benchmarks\n        4.1.2 Open-ended Benchmarks\n    4.2 Evaluation Paradigm\n        4.2.1 Human-based Evaluation\n        4.2.2 LLMs-based Evaluation\n5 Challenges and Future Directions\n6 Conclusion"
    },
    {
        "topic": "Efficient Inference for Large Language Models",
        "outline": "1 Introduction\n2 Preliminaries\n    2.1 Transformer-Style LLMs\n    2.2 Inference Process of LLMs\n    2.3 Efficiency Analysis\n3 Taxonomy\n4 Data-level Optimization\n    4.1 Input Compression\n        4.1.1 Prompt Pruning\n        4.1.2 Prompt Summary\n        4.1.3 Soft Prompt-based Compression\n        4.1.4 Retrieval-Augmented Generation\n    4.2 Output Organization\n    4.3 Knowledge, Suggestions and Future Direction\n5 Model-level Optimization\n    5.1 Efficient Structure Design\n        5.1.1 Efficient FFN Design\n        5.1.2 Efficient Attention Design\n        5.1.3 Transformer Alternates\n    5.2 Model Compression\n        5.2.1 Quantization\n        5.2.2 Sparsification\n        5.2.3 Structure Optimization\n        5.2.4 Knowledge Distillation\n        5.2.5 Dynamic Inference\n    5.3 Knowledge, Suggestions and Future Direction\n6 System-level Optimization\n    6.1 Inference Engine\n        6.1.1 Graph and Operator Optimization\n        6.1.2 Speculative Decoding\n        6.1.3 Offloading\n    6.2 Serving System\n        6.2.1 Memory Management\n        6.2.2 Continuous Batching\n        6.2.3 Scheduling Strategy\n        6.2.4 Distributed Systems\n    6.3 Hardware Accelerator Design\n    6.4 Comparison of LLM Frameworks\n    6.5 Knowledge, Suggestions and Future Direction\n7 Discussions of Key Application Scenarios\n8 Conclusion"
    },
    {
        "topic": "Vision-Language-Action Models",
        "outline": "1 Introduction\n2 Concepts of Vision-Language-Action Models\n    2.1 Evolution and Timeline\n    2.2 Multimodal Integration: From Isolated Pipelines to Unified Agents\n    2.3 Tokenization and Representation: How VLAs Encode the World\n    2.4 Learning Paradigms: Data Sources and Training Strategies\n    2.5 Adaptive Control and Real-Time Execution\n3 Progress in Vision-Language-Action Models\n    3.1 Architectural Innovations in VLA Models\n    3.2 Training and Efficiency Advancements in Vision–Language–Action Models\n    3.3 Parameter‐Efficient Methods and Acceleration Techniques in VLA Models\n    3.4 Applications of Vision-Language-Action Models\n        3.4.1 Humanoid Robotics\n        3.4.2 Autonomous Vehicle Systems\n        3.4.3 Industrial Robotics\n        3.4.4 Healthcare and Medical Robotics\n        3.4.5 Precision and Automated Agriculture\n        3.4.6 Interactive AR Navigation with Vision-Language-Action Models\n4 Challenges and Limitations of Vision-Language-Action Models\n    4.1 Real-Time Inference Constraints\n    4.2 Multimodal Action Representation and Safety Assurance\n    4.3 Dataset Bias, Grounding, and Generalization to Unseen Tasks\n    4.4 System Integration Complexity and Computational Demands\n    4.5 Robustness and Ethical Challenges in VLA Deployment\n5 Discussion\n    5.1 Potential Solutions\n    5.2 Future Roadmap\n6 Conclusion"
    },
    {
        "topic": "Explainability for Large Language Models",
        "outline": "1 Introduction\n2 Training Paradigms of LLMs\n    2.1 Traditional Fine-Tuning Paradigm\n    2.2 Prompting Paradigm\n3 Explanation for Traditional Fine-Tuning Paradigm\n    3.1 Local Explanation\n        3.1.1 Feature Attribution-Based Explanation\n        3.1.2 Attention-Based Explanation\n        3.1.3 Example-Based Explanations\n        3.1.4 Natural Language Explanation\n    3.2 Global Explanation\n        3.2.1 Probing-Based Explanations\n        3.2.2 Neuron Activation Explanation\n        3.2.3 Concept-Based Explanation\n        3.2.4 Mechanistic Interpretability\n    3.3 Making Use of Explanations\n        3.3.1 Debugging Models\n        3.3.2 Improving Models\n4 Explanation for Prompting Paradigm\n    4.1 Base Model Explanation\n        4.1.1 Explaining In-context Learning\n        4.1.2 Explaining CoT Prompting\n        4.1.3 Representation Engineering\n    4.2 Assistant Model Explanation\n        4.2.1 Explaining the Role of Fine-tuning\n        4.2.2 Explaining Hallucination\n        4.2.3 Uncertainty Quantification\n    4.3 Making Use of Explanations\n        4.3.1 Improving LLMs\n        4.3.2 Downstream Applications\n5 Explanation Evaluation\n    5.1 Explanation Evaluations in Traditional Fine-tuning Paradigms\n    5.2 Evaluation of Explanations in Prompting Paradigms\n6 Research Challenges\n    6.1 Explanation without Ground Truths\n    6.2 Sources of Emergent Abilities\n    6.3 Comparing Two Paradigms\n    6.4 Shortcut Learning of LLMs\n    6.5 Attention Redundancy\n    6.6 Shifting from Snapshot Explainability to Temporal Analysis\n    6.7 Safety and Ethics\n7 Conclusions"
    },
    {
        "topic": "Scientific Large Language Models",
        "outline": "1 Introduction\n2 Background\n    2.1 Taxonomy of Scientific Data\n        2.1.1 Textual Formats\n        2.1.2 Visual Data\n        2.1.3 Symbolic Representations\n        2.1.4 Structured Data\n        2.1.5 Time-Series Data\n        2.1.6 Multi-omics Integration\n    2.2 Hierarchical Structure of Scientific Knowledge\n        2.2.1 Factual Level\n        2.2.2 Theoretical Level\n        2.2.3 Methodological and Technological Level\n        2.2.4 Modeling and Simulation Level\n        2.2.5 Insight Level\n        2.2.6 Dynamic Interactions and Evolution\n        2.2.7 Implications for Sci-LLMs\n    2.3 Key Challenges in Scientific AI\n        2.3.1 Interpretability in Scientific AI\n        2.3.2 Cross‑scale and Multimodal Integration\n        2.3.3 Dynamic Knowledge Evolvement\n    2.4 Quality Standards for Scientific Datasets\n        2.4.1 Accuracy\n        2.4.2 Completeness\n        2.4.3 Timeliness\n        2.4.4 Traceability\n    2.5 Dimensions for Evaluating Scientific AI\n        2.5.1 Expert-Level Scientific Knowledge Comprehension and Retrieval\n        2.5.2 Scientific Reasoning and Problem Solving\n        2.5.3 Multimodal Scientific Data\n3 Scientific Large Language Models\n    3.1 Introduction of Large Language Models\n    3.2 General-purpose Sci-LLMs\n    3.3 Domain-specific Sci-LLMs\n        3.3.1 Physics\n        3.3.2 Chemistry\n        3.3.3 Materials Science\n        3.3.4 Life Sciences\n        3.3.5 Astronomy\n        3.3.6 Earth Science\n    3.4 Sci-LLMs Analysis\n4 Scientific Data for Pre-training\n    4.1 Physics, Chemistry and Material Sciences: the Foundation for Understanding the Material World\n        4.1.1 Physics\n        4.1.2 Chemistry\n        4.1.3 Materials Science\n    4.2 Life Sciences: Complexity from Molecules to Systems\n        4.2.1 Molecular and Cell Biology\n        4.2.2 Multi-Omics\n        4.2.3 Neuroscience\n        4.2.4 Healthcare and Medical Science\n        4.2.5 Agriculture\n    4.3 Astronomy and Earth Science: Understanding Our Planet\n        4.3.1 Astronomy\n        4.3.2 Earth Science\n    4.4 Pre-training Data Analysis\n5 Scientific Data for Post-training\n    5.1 Current Landscape Across Scientific Domains\n        5.1.1 Physics\n        5.1.2 Chemistry\n        5.1.3 Materials Science\n        5.1.4 Life Sciences\n        5.1.5 Astronomy\n        5.1.6 Earth Science\n    5.2 Post-training Data Analysis\n6 Evaluation of Sci-LLMs\n    6.1 Current Landscape Across Scientific Domains\n        6.1.1 Physics\n        6.1.2 Chemistry\n        6.1.3 Materials Science\n        6.1.4 Life Sciences\n        6.1.5 Astronomy\n        6.1.6 Earth Science\n        6.1.7 General Science\n    6.2 Evaluation Data Analysis\n        6.2.1 Tiered Regime in Data Generation and Annotation\n        6.2.2 Skewed Knowledge Level with Increasing Difficulty\n        6.2.3 Shift towards Domain-Specific Metrics\n    6.3 LLM / Agent as a Judge\n    6.4 Inspiration from Test-Time Learning\n7 Scientific Data Development\n    7.1 Data Collection and Labeling\n        7.1.1 Data Source Heterogeneity and Acquisition Strategies\n        7.1.2 Annotation Methodologies and Quality Control\n        7.1.3 Cross-Domain Patterns and Domain-Specific Considerations\n    7.2 Limitations of Current Scientific Datasets\n        7.2.1 Scarcity of Experimental Data\n        7.2.2 Over-reliance on Text Modality Data\n        7.2.3 Representation Gap between Static Knowledge and Dynamic Processes\n        7.2.4 Multi-level Biases in Scientific Datasets\n    7.3 Systematic Issues in Data Quality\n        7.3.1 Data Traceability Crisis\n        7.3.2 Scientific Data Latency\n        7.3.3 The Lack of AI-readiness\n8 New Paradigms for Data-Driven Sci-LLMs\n    8.1 Scientific Agent\n        8.1.1 LLMs as Scientific Agents\n        8.1.2 Multi-Agent Collaboration\n        8.1.3 Tool Use\n        8.1.4 Self-evolving Agents\n        8.1.5 Evaluation Frameworks and Benchmarking\n        8.1.6 Autonomous Scientific Discovery\n    8.2 Data Ecosystems for Sci-LLMs\n        8.2.1 The Data Bottleneck Behind the Rise of Scientific Agents\n        8.2.2 Building an Operating System-level Interaction Protocol\n        8.2.3 Design Principles for Next-Generation Scientific Data Architecture\n        8.2.4 Sustainable Data Sharing Mechanism\n        8.2.5 Data Safety and Privacy\n9 Challenges and Outlook\n    9.1 Challenges\n        9.1.1 Scientific Data Selection for Efficient Pretraining\n        9.1.2 Optimizing Data Processing Pipelines\n        9.1.3 Representing Non-Sequential and Non-Textual Data\n        9.1.4 LLM Knowledge Update and Version Control\n    9.2 Future Work\n        9.2.1 Integrated Scientific Data Ecosystems\n        9.2.2 Automated Scientific Data Standardization Pipeline\n        9.2.3 Comprehensive Evaluation System\n        9.2.4 Advanced Scientific Reasoning\n        9.2.5 Autonomous Scientific Agents\n        9.2.6 From Sci-LLMs to Scientific Discovery\n        9.2.7 Ethical Governance for Responsible Scientific AI Innovation\n10 Conclusion"
    },
    {
        "topic": "Safety in Large Language Models",
        "outline": "1 Introduction\n2 Data Safety\n    2.1 Pretraining Data Safety\n    2.2 Fine-tuning Data Safety\n    2.3 Alignment Data Safety\n    2.4 Safety in Data Generation\n    2.5 Roadmap & Perspective\n        2.5.1 Reliable Data Distillation\n        2.5.2 Novel Data Generation Paradigms\n        2.5.3 Advanced Data Poisoning & Depoisoning\n3 Pre-training Safety\n    3.1 Data Filtering for Pretrain Safety\n        3.1.1 Heuristic based Filtering\n        3.1.2 Model based Filtering\n        3.1.3 Blackbox Filtering\n    3.2 Augmenting Training Data for Pre-training Safety\n    3.3 Roadmap & Perspective\n4 Post-training Safety\n    4.1 Attacks in Post-training\n        4.1.1 Toxic Data Construction Phase\n        4.1.2 Fine-tuning Phase\n    4.2 Defenses in Post-training\n        4.2.1 Alignment\n        4.2.2 Downstream Fine-tuning\n        4.2.3 Safety Recovery\n        4.2.4 Safety Location\n        4.2.5 Open-Weight LLMs Safeguard\n    4.3 Evaluation\n        4.3.1 Evaluation Metrics\n        4.3.2 Evaluation Benchmarks\n    4.4 Roadmap & Perspective\n        4.4.1 From Low-Level to High-Level Safety\n        4.4.2 Provably Safe AI System\n        4.4.3 Beyond Fine-tuning, Systematic Safety\n5 Safety in Model Editing & Unlearning\n    5.1 Safety in Model Editing\n    5.2 Safety in Unlearning\n    5.3 Roadmap & Perspective\n        5.3.1 Model Editing\n        5.3.2 Unlearning\n6 LLM(-Agent) Deployment Safety\n    6.1 Deployment Safety\n        6.1.1 Attack in Deployment\n        6.1.2 Defensive Mechanisms in Deployment\n        6.1.3 Evaluation and Benchmarks in Deployment\n    6.2 Single-agent Safety\n        6.2.1 Definition of Agent\n        6.2.2 Tool Safety\n        6.2.3 Memory Safety\n        6.2.4 Attack\n        6.2.5 Defense\n        6.2.6 Environment Safety\n    6.3 Multi-agent Safety\n        6.3.1 Attack\n        6.3.2 Defense\n    6.4 Agent Communication Safety\n        6.4.1 Attack\n        6.4.2 Defense\n    6.5 Agent Safety Evaluation\n        6.5.1 Attack-Specific Benchmarks\n        6.5.2 Module-Specific Benchmarks\n        6.5.3 General Benchmarks\n        6.5.4 LLM Deployment Roadmap\n        6.5.5 LLM Deployment Perspective\n        6.5.6 Agent Roadmap\n        6.5.7 Perspective\n7 Safety in LLM-Based Application\n8 Potential Research Directions\n9 Conclusion"
    },
    {
        "topic": "Large Language Models for Time Series",
        "outline": "1 Introduction\n2 Background and Problem Formulation\n3 Taxonomy\n    3.1 Prompting\n    3.2 Quantization\n    3.3 Aligning\n    3.4 Vision as Bridge\n    3.5 Tool\n4 Comparison within the Taxonomy\n5 Multimodal Datasets\n6 Challenges and Future Directions\n    6.1 Theoretical Understanding\n    6.2 Multimodal and Multitask Analysis\n    6.3 Efficient Algorithms\n    6.4 Combining Domain Knowledge\n    6.5 Customization and Privacy\n7 Conclusion"
    },
    {
        "topic": "Large Language Models for Recommendation",
        "outline": "1 Introduction\n2 Modeling Paradigms and Taxonomy\n3 Discriminative LLMs for Recommendation\n    3.1 Fine-tuning\n    3.2 Prompt Tuning\n4 Generative LLMs for Recommendation\n    4.1 Non-tuning Paradigm\n        4.1.1 Prompting\n        4.1.2 In-context Learning\n    4.2 Tuning Paradigm\n        4.2.1 Fine-tuning\n        4.2.2 Prompt Tuning\n        4.2.3 Instruction Tuning\n5 Findings\n    5.1 Model Bias\n    5.2 Recommendation Prompt Designing\n    5.3 Promising Ability\n    5.4 Evaluation Issues\n6 Conclusion"
    },
    {
        "topic": "Reinforcement Learning for Large Language Models",
        "outline": "1 Introduction\n2 Preliminaries\n    2.1 Background\n    2.2 Frontier Models\n    2.3 Related Surveys\n3 Foundational Components\n    3.1 Reward Design\n        3.1.1 Verifiable Rewards\n        3.1.2 Generative Rewards\n        3.1.3 Dense Rewards\n        3.1.4 Unsupervised Rewards\n        3.1.5 Rewards Shaping\n    3.2 Policy Optimization\n        3.2.1 Policy Gradient Objective\n        3.2.2 Critic-based Algorithms\n        3.2.3 Critic-Free Algorithms\n        3.2.4 Off-policy Optimization\n        3.2.5 Regularization Objectives\n    3.3 Sampling Strategy\n        3.3.1 Dynamic and Structured Sampling\n        3.3.2 Sampling Hyper-parameters\n4 Foundational Problems\n    4.1 RL's Role: Sharpening or Discovery\n    4.2 RL vs. SFT: Generalize or Memorize\n    4.3 Model Prior: Weak and Strong\n    4.4 Training Recipes: Tricks or Traps\n    4.5 Reward Type: Process or Outcome\n5 Training Resources\n    5.1 Static Corpus\n    5.2 Dynamic Environment\n    5.3 RL Infrastructure\n6 Applications\n    6.1 Coding Tasks\n    6.2 Agentic Tasks\n    6.3 Multimodal Tasks\n    6.4 Multi-Agent Systems\n    6.5 Robotics Tasks\n    6.6 Medical Tasks\n7 Future Directions\n    7.1 Continual RL for LLMs\n    7.2 Memory-based RL for LLMs\n    7.3 Model-based RL for LLMs\n    7.4 Teaching LRMs Efficient Reasoning\n    7.5 Teaching LLMs Latent Space Reasoning\n    7.6 RL for LLMs Pre-training\n    7.7 RL for Diffusion-based LLMs\n    7.8 RL for LLMs in Scientific Discovery\n    7.9 RL for Architecture-Algorithm Co-Design\n8 Conclusion"
    }
]