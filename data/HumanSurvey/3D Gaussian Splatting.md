# A Survey on 3D Gaussian Splatting

Guikun Chen, and Wenguan Wang, Senior Member, IEEE

Abstract- 3D Gaussian splating (GS) has emerged as a transformative technique in explicit radiance field and computer graphics. This innovative approach, characterized by the use of millions of learnable 3D Gaussians, represents a significant departure from mainstream neural radiance field approaches, which predominantly use implicit, coordinate- based models to map spatial coordinates to pixel values. 3D GS, with its explicit scene representation and differentiable rendering algorithm, not only promises real- time rendering capability but also introduces unprecedented levels of editability. This positions 3D GS as a potential game- changer for the next generation of 3D reconstruction and representation. In the present paper, we provide the first systematic overview of the recent developments and critical contributions in the domain of 3D GS. We begin with a detailed exploration of the underlying principles and the driving forces behind the emergence of 3D GS, laying the groundwork for understanding its significance. A focal point of our discussion is the practical applicability of 3D GS. By enabling unprecedented rendering speed, 3D GS opens up a plethora of applications, ranging from virtual reality to interactive media and beyond. This is complemented by a comparative analysis of leading 3D GS models, evaluated across various benchmark tasks to highlight their performance and practical utility. The survey concludes by identifying current challenges and suggesting potential avenues for future research. Through this survey, we aim to provide a valuable resource for both newcomers and seasoned researchers, fostering further exploration and advancement in explicit radiance field.

Index Terms- 3D Gaussian Splatting, Explicit Radiance Field, Real- time Rendering, Scene Understanding

## 1 INTRODUCTION

The objective of image based 3D scene reconstruction is to convert a collection of views or videos capturing a scene into a digital 3D model that can be computationally processed, analyzed, and manipulated. This hard and long- standing problem is fundamental for machines to comprehend the complexity of real- world environments, facilitating a wide array of applications such as 3D modeling and animation, robot navigation, historical preservation, augmented/virtual reality, and autonomous driving.

The journey of 3D scene reconstruction began long before the surge of deep learning, with early endeavors focusing on light fields and basic scene reconstruction methods [1]- [3]. These early attempts, however, were limited by their reliance on dense sampling and structured capture, leading to significant challenges in handling complex scenes and lighting conditions. The emergence of structure- frommotion [4] and subsequent advancements in multi- view stereo [5] algorithms provided a more robust framework for 3D scene reconstruction. Despite these advancements, such methods struggled with novel- view synthesis and texture loss. NeRF represents a quantum leap in this progression. By leveraging deep neural networks, NeRF enabled the direct mapping of spatial coordinates to color and density. The success of NeRF hinged on its ability to create continuous, volumetric scene functions, producing results with unprecedented fidelity. However, as with any burgeoning technology, this implementation came at a cost: i) Computational Intensity. NeRF based methods are computationally intensive [6]- [9], often requiring extensive training times and substantial resources for rendering, especially for highresolution outputs. ii) Editability. Manipulating scenes represented implicitly is challenging, since direct modifications to the neural network's weights are not intuitively related to changes in geometric or appearance properties of the scene.

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-10/314abcd8-9d57-4d71-9a3d-4a5ac206b069/329b4458844cda9a6409c09c5669eb86903fb91085b0b0ec8ad7f0953a73c852.jpg)  
Fig. 1. The number of published papers and official GitHub stars on 3D GS. The set of statistics is sourced from # Papers and # GitHub Stars.

It is in this context that 3D Gaussian splatting (GS) [10] emerges, not merely as an incremental improvement but as a paradigm- shifting approach that redefines the boundaries of scene representation and rendering. While NeRF excelled in creating photorealistic images, the need for faster, more efficient rendering methods was becoming increasingly apparent, especially for applications (e.g., virtual reality and autonomous driving) that are highly sensitive to latency. 3D GS addressed this need by introducing an advanced, explicit scene representation that models a scene using millions of learnable 3D Gaussians in space. Unlike the implicit, coordinate- based models [11], [12], 3D GS employs an explicit representation and highly parallelized workflows, facilitating more efficient computation and rendering. The innovation of 3D GS lies in its unique blend of the benefits of differentiable pipelines and point- based rendering techniques [13]- [17]. By representing scenes with learnable 3D Gaussians, it preserves the strong fitting capability of continuous volumetric radiance fields, essential for high- quality image synthesis, while simultaneously avoiding the computational overhead associated with NeRF based methods (e.g., computationally expensive ray- marching, and unnecessary calculations in empty space).

The introduction of 3D GS is not just a technical advancement; it represents a fundamental shift in how we approach scene representation and rendering in computer vision and graphics. By enabling real- time rendering capabilities without compromising on visual quality, 3D GS opens up a plethora of possibilities for applications ranging from virtual reality and augmented reality to real- time cinematic rendering and beyond [18]- [21]. This technology holds the promise of not only enhancing existing applications but also enabling new ones that were previously unfeasible due to computational constraints. Furthermore, 3D GS's explicit scene representation offers unprecedented flexibility to control the objects and scene dynamics, a crucial factor in complex scenarios involving intricate geometries and varying lighting conditions [22]- [24]. This level of editability, combined with the efficiency of the training and rendering process, positions 3D GS as a transformative force in shaping future developments in relevant fields.

In an effort to assist readers in keeping pace with the swift evolution of 3D GS, we provide the first survey on 3D GS, which presents a systematic and timely collection of the most significant literature on the topic. Given that 3D GS is a very recent innovation (Fig. 1), this survey focuses in particular on its principles, and the diverse developments and contributions that have emerged since its introduction. The selected follow- up works are primarily sourced from top- tier conferences, to provide a thorough and up- to- date (Dec. 2024) analysis of the theoretical foundations, remarkable developments, and burgeoning applications of 3D GS. Acknowledging the nascent yet rapidly evolving nature of 3D GS, this survey is inevitably a biased view, but we strive to offer a balanced perspective that reflects both the current state and the future potential of this field. Our aim is to encapsulate the primary research trends and serve as a valuable resource for both researchers and practitioners eager to understand and contribute to this rapidly evolving domain. The distinctions of this survey from existing literature [25]- [28] are evident in the following aspects:

We provide the first systematic and comprehensive review that examines 3D GS from a macro- level perspective by establishing clear taxonomies and frameworks. This high- level systematization helps researchers identify trends and potential directions that might not be apparent from paper- specific reviews. Our organizational structure serves as a roadmap for understanding how different approaches relate to and build upon each other within the 3D GS ecosystem.

This paper is the first and only survey to thoroughly delve into the theoretical background and fundamental principles of 3D GS. The comprehensive coverage makes the field more approachable for newcomers while providing valuable insights for experienced researchers.

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-10/314abcd8-9d57-4d71-9a3d-4a5ac206b069/2818316ddb277b582bd295837372e9af4388d0be8bb593a4e1d999305b46733b.jpg)  
Fig. 2. Structure of the overall review.

To ensure our survey remains relevant and offer long- term value in this rapidly evolving field, we maintain two dynamic GitHub repositories: one that follows our survey's organizational structure and another that includes comprehensive performance comparisons with analysis data.

A summary of the structure of this article can be found in Fig. 2, which is presented as follows: Sec. 2 provides a brief background on problem formulation, terminology, and related research domains. Sec. 3 introduces the essential insights of 3D GS, encompassing the rendering process with learned 3D Gaussians and the optimization details (i.e., how to learn 3D Gaussians) of 3D GS. Sec. 4 presents several fruitful directions that aim to improve the capabilities of the original 3D GS. Sec. 5 unveils the diverse application areas and tasks where 3D GS has made significant impacts, showcasing its versatility. Sec. 6 conducts performance comparison and analysis. Finally, Sec. 7 and 8 highlight the open questions for further research and conclude the survey.

## 2 BACKGROUND

In this section, we first provide a brief formulation of radiance fields (Sec. 2.1), including both implicit and explicit ones. Sec. 2.2 further establishes linkages with relevant rendering algorithms and terminologies. For a comprehensive overview of radiance fields, scene reconstruction and representation, and rendering methods, please see the excellent surveys [29]- [33] for more insights.

### 2.1 Radiance Field

Implicit Radiance Field. An implicit radiance field represents light distribution in a scene without explicitly defining the geometry of the scene. In the deep learning era, neural networks are often used to learn a continuous volumetric scene representation [34]- [35]. The most prominent example is NeRF [12]. In NeRF (Fig. 3a), one or more MLPs are used to map a set of spatial coordinates  $(x,y,z)$  and viewing directions  $(\theta ,\phi)$  to color  $c$  and volume density  $\sigma$  ..

$$
(c,\sigma)\leftarrow \mathrm{MLP}(x,y,z,\theta ,\phi). \tag{1}
$$

This format allows for a differentiable and compact representation of complex scenes, albeit often at the cost of high computational load due to volumetric ray marching. Note that typically, the color  $c$  is direction- dependent, whereas the volume density  $\sigma$  is not [12].

Explicit Radiance Field. An explicit radiance field directly represents the distribution of light in a discrete spatial structure, such as a voxel grid or a set of points [36], [37]. Each element in this structure stores the radiance information for its respective location. This allows for direct and often faster access to radiance data but at the cost of higher memory usage and potentially lower resolution. Similar to the implicit radiance field, the explicit one is written as:

$$
(c,\sigma)\leftarrow \mathrm{DataStructure}(x,y,z,\theta ,\phi), \tag{2}
$$

where DataStructure could be in the format of volumes, point clouds, etc. DataStructure encodes directional color in two main ways. One is encoding high- dimensional features that are subsequently decoded by a lightweight MLP. Another one is directly storing coefficients of directional basis functions, such as spherical harmonics or spherical Gaussians, where the final color is computed as a function of these coefficients and the viewing direction.

3D Gaussian Splatting: Best- of- Both Worlds. 3D GS [10] is an explicit radiance field with the advantages of implicit radiance fields. Concretely, it leverages the strengths of both paradigms by utilizing learnable 3D Gaussians as the basis elements of DataStructure. Note that 3D GS encodes the opacity  $\alpha$  directly for each Gaussian, as opposed to approaches of first establishing density  $\sigma$  and then computing opacity based on that density. As in previous reconstruction work, 3D Gaussians are optimized under the supervision of multi- view images to represent the scene. Such a 3D Gaussian based differentiable pipeline combines the benefits of neural network based optimization and explicit, structured data storage. This hybrid approach aims to achieve realtime, high- quality rendering and requires less training time, particularly for complex scenes and high- resolution outputs.

### 2.2 Context and Terminology

Volumetric rendering aims to transform a 3D volumetric representation into an image by integrating radiance along camera rays. A camera ray  $\boldsymbol {r}(t)$  can be parameterized as:  $\boldsymbol {r}(t) = \boldsymbol {o} + t\boldsymbol {d},t\in [t_{\mathrm{near}},t_{\mathrm{far}}]$  , where  $\pmb{O}$  represents the ray origin (camera center),  $\pmb{d}$  is the ray direction, and  $t$  indicates the distance along the ray between near and far clipping planes. The pixel color  $C(r)$  is computed through a line integral along the ray  $\boldsymbol {r}(t)$  , mathematically expressed as [12]:

$$
C(\pmb {r}) = \int_{t_{\mathrm{near}}}^{t_{\mathrm{far}}}T(t)\sigma (\pmb {r}(t))c(\pmb {r}(t),\pmb {d})dt, \tag{3}
$$

where  $\sigma (\boldsymbol {r}(t))$  is the volume density at point  $\boldsymbol {r}(t),c(\boldsymbol {r}(t),\boldsymbol {d})$  is the color at that point, and  $T(t)$  is the transmittance. Raymarching directly approximates the volumetric rendering integral by systematically "stepping" along a ray and sampling the scene's properties at discrete intervals. NeRF [12] shares the same spirit of ray- marching and introduces importance sampling and positional encoding to improve the quality of synthesized images. While providing highquality results, ray- marching is computationally expensive, especially for high- resolution images.

Point- based rendering represents another class of rendering algorithms, of which 3D GS introduces a notable implementation. Its simplest form [38] rasterizes point clouds with a fixed size, which introduces drawbacks such as holes and rendering artifacts. Seminal works addressed these limitations through various methods, including: i) splatting point primitives with a spatial extent [14], [15], [39], [40], and ii) more recently, embedding neural features directly into points for subsequent network- based rendering [41], [42]. 3D GS uses 3D Gaussian as the point primitive that contains explicit attributes (e.g., color and opacity) instead of implicit neural features. The rendering approach, i.e., pointbased  $\alpha$  - blending (exemplified in Eq. 5), shares the same image formation model as NeRF- style volumetric rendering (Eq. 3) [10], but demonstrates substantial speed advantages. This advantage originates from fundamental algorithmic differences. NeRFs approximate a line integral along a ray for each pixel, requiring expensive sampling. Point- based methods render point clouds using rasterization, which inherently benefits from parallel computational strategies [43].

## 3 3D GAUSSIAN SPLATTING:PRINCIPLES

3D GS offers a breakthrough in real- time, high- resolution image rendering, without relying on deep neural networks. This section aims to provide essential insights of 3D GS. We first elaborate on how 3D GS synthesizes an image given well- constructed 3D Gaussians in Sec. 3.1, i.e., the forward process of 3D GS. Then, we introduce how to obtain well- constructed 3D Gaussians for a given scene in Sec. 3.2, i.e., the optimization process of 3D GS.

### 3.1 Rendering with Learned 3D Gaussians

Consider a scene represented by (millions of) optimized 3D Gaussians. The objective is to generate an image from a specified camera pose. Recall that NeRFs approach this task through computationally demanding volumetric ray- marching, sampling 3D space points per pixel. Such a

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-10/314abcd8-9d57-4d71-9a3d-4a5ac206b069/c6b9a21996389484e1e77fd793411e0eeb53c17555fbc943407f14637854b017.jpg)  
Fig. 3. NeRFs vs. 3D GS. (a) NeRF samples along the ray and then queries the MLP to obtain corresponding colors and densities, which can be seen as a backward mapping (ray tracing). (b) In contrast, 3D GS projects all 3D Gaussians into the image space (i.e., splatting) and then performs parallel rendering, which can be viewed as a forward mapping (rasterization). Best viewed in color.

paradigm struggles with high- resolution image synthesis, failing to achieve real- time rendering, especially for platforms with limited computing resources [10]. By contrast, 3D GS begins by projecting these 3D Gaussians onto a pixel- based image plane, a process termed "splatting" [39], [40] (see Fig. 3b). Afterwards, 3D GS sorts these Gaussians and computes the value for each pixel. As shown in Fig. 3, the rendering of NeRFs and 3D GS can be viewed as an inverse process of each other. In what follows, we begin with the definition of a 3D Gaussian, which is the minimal element of the scene representation in 3D GS. Next, we describe how these 3D Gaussians can be used for differentiable rendering. Finally, we introduce the acceleration technique used in 3D GS, which is the key to fast rendering.

- Properties of 3D Gaussian. A 3D Gaussian is characterized by its center (position)  $\mu$ , opacity  $\alpha$ , 3D covariance matrix  $\Sigma$ , and color  $c$ .  $c$  is represented by spherical harmonics for view-dependent appearance. All the properties are learnable and optimized through back-propagation.

- Frustum Culling. Given a specified camera pose, this step determines which 3D Gaussians are outside the camera's frustum. By doing so, 3D Gaussians outside the given view will not be involved in the subsequent computation.

- Splatting. In this step, 3D Gaussians (ellipsoids) in 3D space are projected into 2D image space (ellipses). The projection proceeds through two transformations: first, transforming 3D Gaussians from world coordinates to camera coordinates using the viewing transformation, and subsequently splatting these Gaussians into 2D image space via an approximation of the projective transformation. Mathematically, given the 3D covariance matrix  $\Sigma$  describing a 3D Gaussian's spatial distribution, and the viewing transformation matrix  $W$ , the 2D covariance matrix  $\Sigma '$  characterizing the projected 2D Gaussian is computed through:

$$
\Sigma^{\prime} = JW\Sigma W^{\top}J^{\top}, \tag{4}
$$

where  $J$  is the Jacobian of the affine approximation of the projective transformation [10], [39]. One might wonder why the standard camera intrinsics based projective transformation is not used here. This is because its mappings are not affine and therefore cannot directly project  $\Sigma$ . 3D GS adopts an affine one proposed in [39] which approximates the projective transformation using the first two terms (including  $J$ ) of the Taylor expansion (see Sec. 4.4 in [39]).

- Rendering by Pixels. Before delving into the final version of 3D GS which utilizes several techniques to boost parallel computation, we first elaborate on its simpler form to offer insights into its basic working mechanism. Given the position of a pixel  $\mathcal{X}$ , its distance to all overlapping Gaussians, i.e., the depths of these Gaussians, can be computed through the viewing transformation matrix  $W$ , forming a sorted list of Gaussians  $\mathcal{N}$ . Then,  $\alpha$ -blending is adopted to compute the final color of this pixel:

$$
C = \sum_{n = 1}^{|\mathcal{N}|}c_n\alpha_n'\prod_{j = 1}^{n - 1}\left(1 - \alpha_j'\right), \tag{5}
$$

where  $c_{n}$  is the learned color. The final opacity  $\alpha_{n}^{\prime}$  is the multiplication result of the learned opacity  $\alpha_{n}$  and the Gaussian, defined as follows:

$$
\alpha_{n}^{\prime} = \alpha_{n}\times \exp \big(-\frac{1}{2} (\pmb{x}^{\prime} - \pmb{\mu}_{n}^{\prime})^{\top}\pmb{\Sigma}_{n}^{\prime -1}(\pmb{x}^{\prime} - \pmb{\mu}_{n}^{\prime})\big), \tag{6}
$$

where  $\mathcal{X}^{\prime}$  and  $\mu_n^{\prime}$  are coordinates in the projected space. It is a reasonable concern that the rendering process described could be slower compared to NeRFs, given that generating the required sorted list is hard to parallelize. Indeed, this concern is justified; rendering speeds can be significantly impacted when utilizing such a simplistic, pixel- by- pixel approach. To achieve real- time rendering, 3D GS makes several concessions to accommodate parallel computation.

- Tiles (Patches). To avoid the cost computation of deriving Gaussians for each pixel, 3D GS shifts the precision from pixel-level to patch-level detail, which is inspired by tile-based rasterization [43]. Concretely, 3D GS initially divides the image into multiple non-overlapping patches (tiles). Fig. 4b provides an illustration of tiles. Each tile comprises  $16 \times 16$  pixels as suggested in [10]. 3D GS further determines which tiles intersect with these projected Gaussians. Given that a projected Gaussian may cover several tiles, a logical method involves replicating the Gaussian, assigning each copy an identifier (i.e., a tile ID) for the relevant tile.

- Parallel Rendering. After replication, 3D GS combines the respective tile ID with the depth value obtained from the view transformation for each Gaussian. This results in an unsorted list of bytes where the upper bits represent the tile ID and the lower bits signify depth. By doing so, the sorted list can be directly utilized for rendering (i.e., alpha compositing). Fig. 4c and Fig. 4d provide the visual demonstration of such concepts. It's worth highlighting that rendering each tile and pixel occurs independently, making this process highly suitable for parallel computations. An additional benefit is that each tile's pixels can access a common shared memory and maintain an uniform read sequence (Fig. 5), enabling parallel execution of alpha compositing with increased efficiency. In the official implementation of the original paper [10], the framework regards the processing of tiles and pixels as analogous to the blocks and threads, respectively, in CUDA programming architecture.

In a nutshell, 3D GS introduces several approximations during rendering to enhance computational efficiency while maintaining a high standard of image synthesis quality.

### 3.2 Optimization of 3D Gaussian Splatting

At the heart of 3D GS lies an optimization procedure devised to construct a copious collection of 3D Gaussians that

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-10/314abcd8-9d57-4d71-9a3d-4a5ac206b069/7d0dd0707151a1ff4240e7ec77198f3a6861853d32862743fc2ae3361a0b4127.jpg)  
Fig. 4. An illustration of the forward process of 3D GS (see Sec. 3.1). (a) The splatting step projects 3D Gaussians into image space. (b) 3D GS divides the image into multiple non-overlapping patches, i.e., tiles. (c) 3D GS replicates the Gaussians which cover several tiles, assigning each copy an identifier, i.e., a tile ID. (d) By rendering the sorted Gaussians, we can obtain all pixels within the tile. Note that the computational workflows for pixels and tiles are independent and can be done in parallel. Best viewed in color.

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-10/314abcd8-9d57-4d71-9a3d-4a5ac206b069/f899f030ef6e95eb4b6647a905bee6f4ab6583a9de4e2c33c6a3d319e659dc41.jpg)  
Fig. 5. An illustration of the tile based parallel (at the pixel-level) rendering. All the pixels within a tile (Tile1 here) access the same ordered Gaussian list stored in a shared memory for rendering. As the system processes each Gaussian sequentially, every pixel in the tile evaluates the Gaussian's contribution according to the distance (i.e., the exp term in Eq. 6). Therefore, the rendering for a tile can be completed by iterating through the list of Gaussians just once. The computation for the red Gaussian follows a similar way and is omitted here for simplicity.

accurately captures the scene's essence, thereby facilitating free- viewpoint rendering. On the one hand, the properties of 3D Gaussians should be optimized via differentiable rasterization to fit the textures of a given scene. On the other hand, the number of 3D Gaussians that can represent a given scene well is unknown in advance. We will introduce how to optimize the properties of each Gaussian in Sec. 3.2.1 and how to adaptively control the density of the Gaussians in Sec. 3.2.2. The two procedures are interleaved within the optimization workflow. Since there are many manually set hyperparameters in the optimization process, we omit the notations of most hyperparameters for clarity.

#### 3.2.1 Parameter Optimization

- Loss Function. Once the synthesis of the image is completed, the difference between the rendered image and ground truth can be measured. All the learnable parameters are optimized by stochastic gradient descent using the  $\ell_1$  and D-SSIM loss functions:

$$
\mathcal{L} = (1 - \lambda)\mathcal{L}_1 + \lambda \mathcal{L}_{\mathrm{D - SSIM}}, \tag{7}
$$

where  $\lambda \in [0,1]$  is a weighting factor.

- Parameter Update. Most properties of a 3D Gaussian can be optimized directly through back-propagation. It is essential to note that directly optimizing the covariance matrix  $\Sigma$  can result in a non-positive semi-definite matrix, which would not adhere to the physical interpretation typically associated with covariance matrices. To circumvent this issue, 3D GS chooses to optimize a quaternion  $q$  and a 3D vector  $s$ . Here  $q$  and  $s$  represent rotation and scale, respectively. This approach allows the covariance matrix  $\Sigma$  to be reconstructed as follows:

$$
\pmb {\Sigma} = \pmb {R}\pmb {S}\pmb{S}^{\top}\pmb{R}^{\top}, \tag{8}
$$

where  $\pmb{R}$  is the rotation matrix derived from the quaternion  $q$  and  $S$  is the scaling matrix given by  $\mathrm{diag}(s)$ . As seen, there is a complex computational graph to obtain the opacity  $\alpha$ , i.e.,  $q$  and  $s \mapsto \Sigma$ ,  $\Sigma \mapsto \Sigma '$ , and  $\Sigma '\mapsto \alpha$ . To avoid the cost of automatic differentiation, 3D GS derives the gradients for  $q$  and  $s$  so as to compute them directly during optimization.

#### 3.2.2 Density Control

- Initialization. 3D GS starts with the initial set of sparse points from SfM or random initialization. Note that a good initialization is essential to convergence and reconstruction quality [44]. Afterwards, point densification and pruning are adopted to control the density of 3D Gaussians.

- Point Densification. In the point densification phase, 3D GS adaptively increases the density of Gaussians to better capture the details of a scene. This process focuses on areas with missing geometric features or regions where Gaussians are too spread out. The densification procedure will be performed at regular intervals (i.e., after a certain number of training iterations), focusing on those Gaussians with large view-space positional gradients (i.e., above a specific threshold). It involves either cloning small Gaussians in under-reconstructed areas or splitting large Gaussians in over-reconstructed regions. For cloning, a copy of the Gaussian is created and moved towards the positional gradient. For splitting, a large Gaussian is replaced with two smaller ones, reducing their scale by a specific factor. This step seeks an optimal distribution and representation of Gaussians in 3D space, enhancing the overall quality of the reconstruction.

- Point Pruning. The point pruning stage involves the removal of superfluous or less impactful Gaussians, which can be viewed as a regularization process. It is executed by eliminating Gaussians that are virtually transparent (with  $\alpha$  below a specified threshold) and those that are excessively large in either world-space or view-space. In addition, to prevent unjustified increases in Gaussian density near input cameras, the alpha value of the Gaussians is set close to zero after a certain number of iterations. This allows for a controlled increase in the density of necessary Gaussians while enabling the culling of redundant ones. The process not only helps in conserving computational resources but also ensures that the Gaussians in the model remain precise and effective for the representation of the scene.

## 4 3D GAUSSIAN SPLATTING: DIRECTIONS

Though 3D GS has achieved impressive milestones, significant room for improvement remains, e.g., data and hardware requirement, rendering and optimization algorithm, and applications in downstream tasks. In the subsequent sections, we seek to elaborate on select extended versions. These are: i) 3D GS for Sparse Input [45]–[55] (Sec. 4.1), ii) Memory- efficient 3D GS [56]–[64] (Sec. 4.2), iii) Photorealistic 3D GS [65]–[80] (Sec. 4.3), iv) Improved Optimization Algorithms [22], [77], [81]–[86] (Sec. 4.4), v) 3D Gaussian with More Properties [87]–[93] (Sec. 4.5), vi) Hybrid Representation [94]–[96] (Sec. 4.6), and vii) New Rendering Algorithm (Sec. 4.7). While we have carefully selected several key directions, we acknowledge that it is inevitably a biased view. A more comprehensive collection is given in Github.

### 4.1 3D GS for Sparse Input

A notable issue of 3D GS is the emergence of artifacts in areas with insufficient observational data. This challenge is a prevalent limitation in radiance field rendering, where sparse data often leads to inaccuracies in reconstruction. From a practical perspective, reconstructing scenes from limited viewpoints is of significant interest, particularly for the potential to enhance functionality with minimal input.

Existing methods can be categorized into two primary groups. i) Regularization based methods introduce additional constraints such as depth information to enhance the detail and global consistency [46], [49], [51], [55]. For example, DNGaussian [49] introduced a depth- regularized approach to address the challenge of geometry degradation in sparse input. FSGS [46] devised a Gaussian Unpooling process for initialization and also introduced depth regularization. MVSplat [51] proposed a cost volume representation so as to provide geometry cues. Unfortunately, when dealing with a limited number of views, or even just one, the efficacy of regularization techniques tends to diminish, which leads to ii) generalizability based methods that use learned priors [47], [48], [53], [97]. One approach involves synthesizing additional views through generative models, which can be seamlessly integrated into existing reconstruction pipelines [98]. However, this augmentation strategy is computationally intensive and inherently bounded by the capabilities of the used generative model. Another well- known paradigm employs feed- forward Gaussian model to directly generates the properties of a set of 3D Gaussians. This paradigm typically requires multiple views for training but can reconstruct 3D scenes with only one input image. For instance, PixelSplat [47] proposed to sample Gaussians from dense probability distributions. Splatter Image [48] introduced a 2D image- to- image network that maps an input image to a 3D Gaussian per pixel. However, as the generated pixel- aligned Gaussians are distributed nearly evenly in the space, they struggle to represent high- frequency details and smoother regions with an appropriate number of Gaussians.

The challenge of 3D GS for sparse inputs centers on the modeling of priors, whether through depth information, generative models, or feed- forward Gaussian models. The fundamental trade- off lies between overfitting to available views and using learned priors for generalization. Future research could explore adaptive mechanisms for controlling this trade- off, potentially through learned confidence measures, context- aware prior selection, user preferences, etc. In addition, while current methods focus on static scenes, extending these approaches to dynamic scenarios presents an exciting frontier for investigation, particularly in handling temporal consistency and motion- induced artifacts.

### 4.2 Memory-efficient 3D GS

While 3D GS demonstrates remarkable capabilities, its scalability poses significant challenges, particularly when juxtaposed with NeRF- based methods. The latter benefits from the simplicity of storing merely the parameters of a learned MLP. This scalability issue becomes increasingly acute in the context of large- scale scene management, where the computational and memory demands escalate substantially. Consequently, there is an urgent need to optimize memory usage in both model training and storage.

Recent research has pursued two primary directions to address memory efficiency. First, several approaches focus on reducing the number of 3D Gaussians [58], [62], [63]. These methods either employ strategic pruning of low- impact Gaussians, such as the volume- based masking [58], or represent neighboring Gaussians using the same properties stored within a "local anchor" obtained by clustering [22], hash- grid [62], etc. Second, researchers have developed methods for compressing Gaussian's properties [58], [61], [62]. For instance, Niedermayr et al. [61] compressed color and Gaussian parameters into compact codebooks, using sensitivity measures for effective quantization and fine- tuning. HAC [62] predicted the probability of each quantized attribute using Gaussian distributions and then devise an adaptive quantization module. These directions are not mutually exclusive; instead, one framework might use a hybrid approach combining multiple strategies.

While current compression techniques have achieved significant storage reduction ratios (often by factors of 10-  $20\times$ ), several challenges remain. The field particularly needs advances in memory efficiency during the training phase, potentially through quantization- aware training protocols, the development of scene- agnostic, reusable codebooks, etc. Furthermore, optimizing the trade- off between compression efficiency and visual fidelity remains an open problem.

### 4.3 Photorealistic 3D GS

The current rendering pipeline of 3D GS (Sec. 3.1) is straightforward and involves several drawbacks. For instance, the simple visibility algorithm may lead to a drastic switch in the depth/blending order of Gaussians [10]. The visual fidelity of rendered images, including aspects such as aliasing, reflections, and artifacts, can be further optimized.

Recent research has focused on addressing three main aspects of visual quality, with aliasing being specific to 3D GS's rendering algorithm, while reflection and blur handling represent broader challenges in 3D reconstruction. i) Aliasing. Due to the discrete sampling paradigm (viewing each pixel as a single point instead of an area), 3D GS is susceptible to aliasing when dealing with varying resolutions, which leads to blurring or jagged edges. Solutions emerged at both training and inference stages. Researchers developed training- time improvements from the sampling rate perspective and introduced schemes such as multi- scale Gaussians [67], 2D Mip filter [65], and conditioned logistic function [78]. Inference- time solutions, such as 2D scale- adaptive filtering [80], offer enhanced fidelity that can be integrated into any existing 3D GS frameworks. ii) Reflection. Achieving realistic rendering of reflective materials is a hard, long- standing problem in 3D scene reconstruction. Recent works have introduced various approaches to model reflective materials [68], [73], [99] and enable relightable Gaussian representation [23], though achieving physically accurate specular effects remains challenging. iii) Blur. While 3D GS excels on carefully curated datasets, real- world captures often suffer from blurs such as motion blur and defocus blur. Recent approaches explicitly incorporated blur modeling during training, employing techniques such as coarse- to- fine kernel optimization [74] and photometric bundle adjustment [75] to address this challenge.

While the approximations made in 3D GS (Sec. 3.1) contribute to its computational efficiency, they also lead to aliasing, difficulties in illumination estimation, etc. Current solutions, though impressive, typically address individual problems rather than providing a universal solution. A practical intermediate approach involves first detecting specific issues (e.g., aliasing, blur) and then applying targeted optimization strategies. The ultimate goal remains developing an advanced reconstruction system that overcomes these limitations, either through fundamental improvements to 3D GS or through brand- new architectures.

### 4.4 Improved Optimization Algorithms

The optimization of 3D GS presents several challenges that affect the quality of reconstruction. These include issues with convergence speed, visual artifacts from improper Gaussians, and the need for better regularization during optimization. The raw optimization method (Sec. 3.2) might lead to overreconstruction in some regions while underrepresenting others, resulting in blur and visual inconsistencies.

Three main directions stand out for improving the optimization of 3D GS. i) Additional Regularization (e.g., frequency [84] and geometry [22], [77]). Geometry- aware approaches have been particularly successful, preserving scene structure through the incorporation of local anchor points [22], depth and surface constraints [100]- [102], Gaussian volumes [103], etc. ii) Optimization Procedure Enhancement [44], [101], [104]. While the original strategy of density control (Sec. 3.2.2) has proven valuable, considerable room for improvement remains. For example, GaussianPro [44] addresses the challenge of dense initialization in texture- less surfaces and large- scale scenes through an advanced Gaussian densification strategy. iii) Constraint

Relaxation. Reliance on external tools/algorithms can introduce errors and cap the system's performance potential. For instance, SfM, commonly used in the initialization process, is error- prone and struggle with complex scenes. Recent works have begun exploring COLMAP- free approaches utilizing stream continuity [81], [105], potentially enabling learning from internet- scale unposed video datasets.

Though impressive, existing methods primarily concentrate on optimizing Gaussians to accurately reconstruct scenes from scratch, neglecting a challenging yet promising solution which reconstructs scenes in a few- shot manner through established "meta representations". Such solution could enable adaptive meta- learning strategies that combine scene- specific and general knowledge. See "learning physical priors from large- scale data" in Sec. 7 for further insights.

### 4.5 3D Gaussian with More Properties

Despite impressive, the properties of 3D Gaussian (Sec. 3.1) are designed to be used for novel- view synthesis only. By augmenting 3D Gaussian with additional properties, such as linguistic [87]- [89], semantic/instance [90]- [92], and spatial- temporal [93] properties, 3D GS demonstrates its considerable potential to revolutionize various domains.

Here we list several interesting applications using 3D Gaussians with specially designed properties. i) Language Embedded Scene Representation [87]- [89]. Due to the high computational and memory demands of current language- embedded scene representations, Shi et al. [87] proposed a quantization scheme that augments 3D Gaussian with streamlined language embeddings instead of the original high- dimensional embeddings. This method also mitigated semantic ambiguity and enhanced the precision of open- vocabulary querying by smoothing out semantic features across different views, guided by uncertainty values. ii) Scene Understanding and Editing [90]- [92]. Feature 3DGS [90] integrated 3D GS with feature field distillation from 2D foundation models. By learning a lower- dimensional feature field and applying a lightweight convolutional decoder for upsampling, Feature 3DGS achieved faster training and rendering speeds while enabling high- quality feature field distillation, supporting applications like semantic segmentation and language- guided editing. iii) Spatiotemporal Modeling [93], [106]. To capture the complex spatial and temporal dynamics of 3D scenes, Yang et al. [93] conceptualized spacetime as a unified entity and approximates the spatiotemporal volume of dynamic scenes using a collection of 4D Gaussians. The proposed 4D Gaussian representation and corresponding rendering pipeline are capable of modeling arbitrary rotations in space and time and allow for end- to- end training.

### 4.6 Hybrid Representation

Rather than augmenting 3D Gaussian with additional properties, another promising avenue of adapting to downstream tasks is to introduce structured information (e.g., spatial MLPs and grids) tailored for specific applications.

Next we showcase various fascinating uses of 3D GS with specially devised structured information. i) Facial Expression Modeling. Considering the challenge of creating

high- fidelity 3D head avatars under sparse view conditions, Gaussian Head Avatar [96] introduced controllable 3D Gaussians and an MLP- based deformation field. Concretely, it captured detailed facial expressions and dynamics by optimizing neutral 3D Gaussians alongside the deformation field, thus ensuring both detail fidelity and expression accuracy. ii) Spatiotemporal Modeling. Yang et al. [94] proposed to reconstruct dynamic scenes with deformable 3D Gaussians. The deformable 3D Gaussians are learned in a canonical space, coupled with a deformation field (i.e., a spatial MLP) that models the spatial- temporal dynamics. The proposed method also incorporated an annealing smoothing training mechanism to enhance temporal smoothness without additional computational costs. iii) Style Transfer. Saroha et al. [107] proposed GS in style, an advanced approach for real- time neural scene stylization. To maintain a cohesive stylized appearance across multiple views without compromising on rendering speed, they used pre- trained 3D Gaussians coupled with a multi- resolution hash grid and a small MLP to produce stylized views. In a nutshell, incorporating structured information can serve as a complementary part for adapting to tasks that are incompatible with the sparsity and disorder of 3D Gaussians.

### 4.7 New Rendering Algorithm for 3D Gaussians

While the rasterization- based pipeline of 3D GS offers impressive real- time performance, it still suffers from the inherent limitations, including inefficient handling of highlydistorted cameras (crucial for robotics), secondary rays (for optical effects like reflections and shadows), and stochastic ray sampling (needed in various existing pipelines). In addition, the assumptions that Gaussians do not overlap and can be sorted accurately using only centers are often violated in practice, leading to temporal artifacts when camera movement changes sorting order.

Recent works [108]- [110] explored ray tracing based rendering algorithms as an alternative. For instance, GaussianTracer [108] introduced a new ray tracing implementation for Gaussian primitives, and devised several accelerating strategies according to the uneven density and interleaved nature of Gaussians. EVER [109] devised a physically accurate, constant density ellipsoid representation that allows for the exact computation of the volume rendering integral, rather than relying on somewhat satisfactory approximations. This advancement eliminates popping artifacts.

Thanks to the fundamental paradigm shift, several exciting possibilities might emerge, including advanced optical effects (reflection, refraction, shadows, global illumination, etc.), support for complex camera models (highly- distorted lenses, rolling shutter effects, etc.), physically accurate rendering with true directional appearance evaluation (vs. tile based approximation), and more. While these capabilities currently come with additional computational costs, they provide essential building blocks for future research in inverse rendering, physical material modeling, relighting, and complex scene reconstruction.

## 5 APPLICATION AREAS AND TASKS

Building on the rapid advancements in 3D GS, a wide range of innovative applications has emerged across multiple do

Building on the rapid advancements in 3D GS, a wide range of innovative applications has emerged across multiple domains (Fig. 6) such as robotics (Sec. 5.1), dynamic scene reconstruction and representation (Sec. 5.2), generation and editing (Sec. 5.3), avatar (Sec. 5.4), medical systems (Sec. 5.5), large- scale scene reconstruction (Sec. 5.6), physics (Sec. 5.7), and even other scientific disciplines [24], [174]- [176]. Here, we highlight key examples that underscore the transformative impact and potential of 3D GS and offer a more comprehensive collection in Github.

### 5.1 Robotics

The evolution of scene representation in robotics has been profoundly shaped by the emergence of NeRF, which revolutionized dense mapping and environmental interaction through implicit neural models. However, NeRF's computational cost poses a critical bottleneck for real- time robotic applications. The shift from implicit to explicit representation not only accelerates optimization but also unlocks direct access to spatial and structural scene data, making 3D GS a transformative tool for robotics. Its ability to balance high- fidelity reconstruction with computational efficiency positions 3D GS as a cornerstone for advancing robotic perception, manipulation, and navigation in dynamic, real- world environments.

The integration of GS into robotic systems has yielded significant advancements across three core domains. In SLAM, GS- based methods [111]- [117], [123], [124], [177]- [182] excel in real- time dense mapping but face inherent trade- offs. Visual SLAM frameworks, particularly RGB- D variants [112], [114], [178], leverage depth supervision for geometric fidelity but falter in low- texture or motion- degraded environments. RGB- only approaches [113], [115], [183] circumvent depth sensors but grapple with scale ambiguity and drift. Multi- sensor fusion strategies, such as LiDAR integration [159], [177], [182], enhance robustness in unstructured settings at the cost of calibration complexity. Semantic SLAM [116], [117], [123] extends scene understanding through object- level semantics but struggles with scalability due to lighting sensitivity in color- based methods or computational overhead in feature- based methods. 3D GS based manipulation [118]- [122] bypasses the need for auxiliary pose estimation in NeRF- based methods, enabling rapid single- stage tasks like grasping in static environments via geometric and semantic attributes encoded in Gaussian properties. Multi- stage manipulation [118], [120], where environmental dynamics demand real- time map updates, requires explicit modeling of dynamic adjustments (e.g., object motions and interactions), material compliance, etc.

The advancement of 3D GS in robotics faces three pivotal challenges. First, adaptability in dynamic and unstructured environments remains critical: real- world scenes are rarely static, requiring systems to continuously update representations amid motion, occlusions, and sensor noise without sacrificing accuracy. Second, current semantic mapping methods rely on costly, scene- specific optimization processes, limiting generalizability and scalability for real- world deployment. Third, unlike NeRF based systems which can use MLP parameters as input features for downstream decision- making, 3D Gaussians' inherent lack of spatial order complicates feature aggregation, with no standardized framework yet established. Bridging the gap between high-

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-10/314abcd8-9d57-4d71-9a3d-4a5ac206b069/c89eeda5a8184ffc4cc666127300e42c1bd788e14c47553f335ed1c3b438d6cc.jpg)  
Fig. 6. Typical applications benefited from GS (Sec. 5). Some images are borrowed from [132], [135], [146], [154], [160], [166] and redrawn.

fidelity reconstruction and actionable semantic/physical understanding will define the next frontier for 3D GS, moving beyond passive mapping towards embodied intelligence.

### 5.2 Dynamic Scene Reconstruction

Dynamic scene reconstruction refers to the process of capturing and representing the three- dimensional structure and appearance of a scene that changes over time [184]- [187]. This involves creating a digital model that accurately reflects the geometry, motion, and visual aspects of the objects in the scene as they evolve. Dynamic scene reconstruction is crucial in various applications, e.g., VR/AR, 3D animation, and autonomous driving [188]- [190].

The key to adapt 3D GS to dynamic scenes is the modeling of temporal dimension which allows for the representation of scenes that change over time. 3D GS based methods [93], [95], [106], [125], [130], [191], [199] for dynamic scene reconstruction can generally be divided into two main categories as discussed in Sec. 4.5 and Sec. 4.6. The first category utilizes additional fields like spatial MLPs or grids to model deformation (Sec. 4.6). For example, Yang et al. [94] first proposed deformable 3D Gaussians tailored for dynamic scenes. These 3D Gaussians are learned in a canonical space and can be used to model spatial- temporal deformation with an implicit deformation field (implemented as an MLP). GaGS [132] devised the voxelization of a set of Gaussian distributions, followed by the use of sparse convolutions to extract geometry- aware features, which are then utilized for deformation learning. On the other hand, the second category is based on the idea that scene changes can be encoded into the 3D Gaussian representation with a specially designed rendering process (Sec. 4.5). For instance, Luiten et al. [125] introduced dynamic 3D Gaussians to model dynamic scenes by keeping the properties of 3D Gaussians unchanged over time while allowing their positions and orientations to change. Yang et al. [93] designed a 4D Gaussian representation, where additional properties are used to represent 4D rotations and spherindrical harmonics, to approximate the spatial- temporal volume of scenes.

While 3D GS advances dynamic scene reconstruction by modeling per- Gaussian deformations, its reliance on fine- grained primitives limits scalability and robustness. Current methods struggle to balance computational efficiency and precision: small- scale reconstructions unify dynamic and static elements but become intractable in large environments, often requiring manual priors to segment regions—a barrier in unstructured settings. Furthermore, the absence of object- level motion reasoning leads to artifacts and poor generalization over long sequences. Future work might prioritize object- centric frameworks that hierarchically group Gaussians into persistent entities, enabling efficient large- scale reconstruction through inherent motion disentanglement (dynamic vs. static).

### 5.3 Generation and Editing

Content generation and editing represent two fundamental and inherently interconnected capabilities in modern AI systems. While generation enables the synthesis of novel digital content from scratch or conditional inputs [200]- [202], editing provides the crucial ability to refine, adapt,

and manipulate existing content with precise control [203]. Together, these capabilities revolutionize creative workflows by combining initial content creation with iterative refinement, enabling applications from professional content production to interactive consumer tools.

Recent advances in generation [133]- [138], [204]- [227] have led to the emergence of three main approaches. Optimization based methods [133], [134], [204] distill diffusion priors (gradients) to guide 3D model updates with the score functions. While these methods demonstrate impressive fidelity, they face significant computational overhead due to the necessity of comparing multiple viewpoints during the optimization process. Reconstruction based methods [135], [225], [227] reframe the generation problem as a multiview reconstruction task utilizing pre- trained multi- view diffusion models. Although this approach offers an intuitive and straightforward solution, it grapples with fundamental limitations in maintaining view consistency. The lack of strict geometric constraints across different viewpoints often results in inconsistent surface geometry and degraded texture quality, particularly in regions with complex visual features. Direct 3D generation methods train diffusion models on 3D representations [138], [220], [226]. While the learned 3D diffusion models facilitate multi- view consistency, the demanding computational costs impede the expansion of training scales necessary for improved generative diversity.

Current editing works [90]- [92], [126]- [128], [140]- [143], [228]- [239] fall into two primary classes. The first class leverages 2D image- editing models (e.g., diffusion- based editors) to iteratively refine 3D Gaussians. Early efforts [141], [142], [233] adopt optimization- or reconstruction- based strategies akin to methods in generation, but introduce task- specific control signals. However, naively applying 2D edits independently across views often introduces multi- view inconsistencies. Subsequent works [140], [238]- [240] mitigate this through iterative refinement or cross- view attention, albeit at increased computational costs for alignment. A notable challenge is unintended object deformations, attributed to the weak 3D geometric priors in 2D editing models and the difficulty of reconciling 2D edits with underlying 3D structures. The second class exploits the explicit nature of 3D GS to enable direct manipulation based on embedded properties such as semantics [91], [92], [143], [232] and key points [128]. However, this class remains underexplored due to essential challenges: the lack of inherent ordering of Gaussians complicates the design of efficient indexing schemes, while editing attributes (e.g., texture and geometry) requires careful regularization and alignment to preserve plausibility.

### 5.4 Avatar

Avatars, the digital representations of users in virtual spaces, bridge physical and digital realms, enabling immersive interaction, identity expression, and remote collaboration. Spanning entertainment (gaming, virtual influencers), enterprise (AI agents, virtual meetings), healthcare, and education, they underpin metaverse economies. Advances in AR and VR amplify their role in redefining social, industrial, and creative landscapes.

3D GS has emerged as a powerful tool for human avatar reconstruction, primarily advancing along two directions:

full- body modeling and head- centric modeling. For full- body avatars [139], [144]- [147], [241]- [252], the current methods typically anchor 3D Gaussians in a canonical space and deform them via parametric body models (e.g., SMPL) or cage- based rigging to model dynamic motions. These approaches adopt a hybrid deformation strategy: linear blend skinning handles rigid skeletal transformations such as joint rotations, while pose- conditioned deformation fields account for secondary non- rigid effects like muscle jiggles. For head avatars [23], [148]- [151], [252]- [256], the emphasis shifts to modeling intricate facial expressions, fine- grained geometry (e.g., wrinkles, hair [257]), and dynamic speech- driven animations. Techniques mainly combine parametric morphable face models (e.g., FLAME) with deformable 3D Gaussians, employing diffusion strategies and expression- aware deformation fields to disentangle rigid head poses from non- rigid facial movements. Both directions exploit the speed advantage and editability of 3D GS to enable efficient training, real- time rendering, and precise control over deformations, while addressing challenges in cross- frame correspondence, topology flexibility, and multi- view consistency.

Reconstruction in challenging scenes (e.g., occlusions, sparse single- view inputs, or loose clothing) and enhancing avatar interactivity represent critical challenges and opportunities. Parametric model- free methods, which bypass predefined priors by learning skinning weights directly from data, show promise for such scenarios. Complementary to this, generative models can mitigate ambiguities inherent in underconstrained settings. Further integrating physics- based constraints might bridge the gap between static reconstructions and responsive, lifelike interactions, unlocking applications in AR, embodied AI, etc.

### 5.5 Endoscopic Scene Reconstruction

Surgical 3D reconstruction represents a fundamental task in robot- assisted minimally invasive surgery, aimed at enhancing intraoperative navigation, preoperative planning, and educational simulations through precise modeling of dynamic surgical scenes. Pioneering the integration of dynamic radiance fields into this domain, recent advancements have focused on surmounting the inherent challenges of single- viewpoint video reconstructions such as occlusions by surgical instruments and sparse viewpoint diversity within the confined spaces of endoscopic exploration [258]- [260]. Despite the progress, the call for high fidelity in tissue deformability and topological variation remains, coupled with the pressing demand for faster rendering to bridge the utility in applications sensitive to latency [152]- [154]. This synthesis of immediacy and precision in reconstructing deformable tissues from endoscopic videos is essential in propelling robotic surgery towards reduced patient trauma and AR/VR applications, ultimately fostering a more intuitive surgical environment and nurturing the future of surgical automation and robotic proficiency.

Endoscopic scene reconstruction introduces distinct challenges compared to general dynamic scenes, including sparse training data from limited camera mobility in narrow cavities, frequent tool occlusions obscuring critical regions, and single- view geometry ambiguities. Existing approaches

mainly used additional depth guidance to infer the geometry of tissues [152]- [154]. For instance, EndoGS [154] integrated depth- guided supervision with spatial- temporal weight masks and surface- aligned regularization terms to enhance the quality and speed of 3D tissue rendering while addressing tool occlusion. EndoGaussian [153] introduced two new strategies: holistic Gaussian initialization for dense initialization and spatiotemporal Gaussian tracking for modeling surface dynamics. Zhao et al. [155] argued that these methods suffer from under- reconstruction and proposed to alleviate this problem from frequency perspectives. In addition, EndoGSLAM [156] and Gaussian Pancake [157] devised SLAM systems for endoscopic scenes and showed significant speed advantages.

Advancing endoscopic 3D reconstruction requires targeted efforts in both data and dynamics modeling. Data limitations arise from single- viewpoint videos, which produce ill- posed reconstruction problems due to instrument occlusions and constrained camera mobility, leaving critical tissue regions unobserved. While depth estimators provide temporary workarounds, integrating multi- view camera systems addresses the root cause. In addition, existing datasets often feature truncated sequences (e.g.,  $4\sim 8s$  in EndoNeRF [258]), which fail to capture prolonged tissue deformation dynamics or complex surgical workflows. Extending temporal coverage to include longer, clinically representative sequences would benefit downstream applications as aforementioned. Modeling limitations persist in current methods, which often represent tissue dynamics at the Gaussian level rather than object- or 3D region- level. This reduces their capacity to encode semantically meaningful anatomical interactions and deserves further explorations.

### 5.6 Large-scale Scene Reconstruction

Large- scale scene reconstruction is a critical component in fields such as autonomous driving, aerial surveying, and AR/VR, demanding both photorealistic visual quality and real- time rendering capabilities. Before the emergence of 3D GS, the task has been approached using NeRF based methods, which, while effective for smaller scenes, often fall short in detail and rendering speed when scaled to larger areas (e.g., over  $1.5km^2$ ). Though 3D GS has demonstrated considerable advantages over NeRFs, the direct application of 3D GS to large- scale environments introduces significant challenges. 3D GS requires an immense number of Gaussians to maintain visual quality over extensive areas, leading to prohibitive GPU memory demands and considerable computational burdens during rendering. For instance, a scene spanning  $2.7km^2$  may require over 20 million Gaussians, pushing the limits of even the most advanced hardware (e.g., NVIDIA A100 with 40GB memory) [163].

To address the highlighted challenges, researchers have made significant strides in two key areas: i) For training, a divide- and- conquer strategy [162]- [165] has been adopted, which segments a large scene into multiple, independent cells. This facilitates parallel optimization for expansive environments. With the same spirit, Zhao et al. [161] proposed a distributed implementation of 3D GS training. An additional challenge lies in maintaining visual quality, as large- scale scenes often feature texture- less surfaces that can hamper the effectiveness of optimization such as Gaussian initialization and density control (Sec. 3.2). Enhancing the optimization algorithm presents a viable solution to mitigate this issue [44], [164]. ii) Regarding rendering, the adoption of the Level of Details (LoD) technique from computer graphics has proven instrumental. LoD adjusts the complexity of 3D scenes to balance visual quality with computational efficiency. Current implementations involve feeding only the essential Gaussians to the rasterizer [164], or designing explicit LoD structures like the Octree [165] and hierarchy [162]. Furthermore, integrating extra input modalities like LiDAR can further enhanced the reconstruction process [158]- [160].

One prominent challenge in large- scale scene reconstruction lies in handling sparse or incomplete capture data, which can be mitigated through few- shot adaptation schemes (see Sec. 4.1) or generalizable priors (see "learning physical priors from large- scale data" in Sec. 7). Meanwhile, memory and computational bottlenecks can be addressed via distributed learning strategies [161], such as parameter partitioning across GPU clusters and parallel batched multiview optimization.

### 5.7 Physics

The simulation of complex real- world dynamics, such as seed dispersal or fluid motion, is pivotal for applications spanning virtual reality, animation, and scientific modeling, where realism hinges on accurate physical behavior. Advances in video diffusion models have driven progress in 4D content generation, yet these methods might produce visually plausible results that violate fundamental physical laws. 3D GS emerges as a promising solution by embedding physical constraints and properties into scene representations, enabling both visually convincing and physically coherent simulations.

Existing methods differ in how they formulate and integrate physics- based priors into their frameworks. The most common approach is employing physics simulation engines (e.g., MLS- MPM [268]) to guide the dynamics generation. The material point method [268] and position based dynamics [269] — numerical methods used in computer graphics for simulating deformations in materials like fluids, granular media, and fracturing solids — have been extensively explored by the community through various customizations [21], [143], [166]- [171]. Analytical material models, such as mass- spring systems, have also demonstrated success in approximating deformations by explicitly encoding material properties into 3D Gaussians [172]. Across these methods, 3D Gaussians are treated as discrete particles (with one exception [173] using a continuous representation) and serve as computational units within the chosen simulator. Unknown material properties or physical parameters are typically learned through video- based supervision from conditional generative models.

Despite advancements in physics based 3D GS frameworks, critical limitations persist. Current systems struggle to unify diverse physical behaviors (e.g., rigid, elastic, or soft- body dynamics) into cohesive simulations, handle complex multi- object interactions without manual intervention, and model scene- level interactions such as environmental

TABLE 1 Comparison of localization methods (6.1) on Replica [261] (static scenes), in terms of absolute trajectory error (ATE, cm). (The three best scores are marked in red, blue, and green, respectively. These notes also apply to the other tables.)  

<table><tr><td>Method</td><td>GS</td><td>Room0</td><td>Room1</td><td>Room2</td><td>Office0</td><td>Office1</td><td>Office2</td><td>Office3</td><td>Office4</td><td>Avarage</td><td></td></tr><tr><td>iMAP [262]</td><td>[ICCV21]</td><td></td><td>3.12</td><td>2.54</td><td>2.51</td><td>1.69</td><td>1.03</td><td>3.99</td><td>4.05</td><td>1.93</td><td>2.58</td></tr><tr><td>Vox-Fusion [263]</td><td>[ISMAR22]</td><td></td><td>1.37</td><td>4.70</td><td>1.47</td><td>8.48</td><td>2.04</td><td>2.58</td><td>1.11</td><td>2.94</td><td>3.09</td></tr><tr><td>NICE-SLAM [264]</td><td>[ICVPR22]</td><td></td><td>0.97</td><td>1.31</td><td>1.07</td><td>0.88</td><td>1.00</td><td>1.06</td><td>1.10</td><td>1.13</td><td>1.06</td></tr><tr><td>ESLAM [265]</td><td>[ICVPR23]</td><td></td><td>0.71</td><td>0.70</td><td>0.52</td><td>0.57</td><td>0.55</td><td>0.58</td><td>0.72</td><td>0.63</td><td>0.63</td></tr><tr><td>Point-SLAM [266]</td><td>[ICCV23]</td><td></td><td>0.61</td><td>0.41</td><td>0.37</td><td>0.38</td><td>0.48</td><td>0.54</td><td>0.69</td><td>0.72</td><td>0.52</td></tr><tr><td>Co-SLAM [267]</td><td>[ICVPR23]</td><td></td><td>0.70</td><td>0.95</td><td>1.35</td><td>0.59</td><td>0.55</td><td>2.03</td><td>1.56</td><td>0.72</td><td>1.00</td></tr><tr><td>Gaussian-SLAM [114]</td><td>[arrot]</td><td>✓</td><td>3.35</td><td>8.74</td><td>3.13</td><td>1.11</td><td>0.81</td><td>0.78</td><td>1.08</td><td>7.21</td><td>3.27</td></tr><tr><td>GSSLAM [113]</td><td>[ICVPR24]</td><td>✓</td><td>0.47</td><td>0.43</td><td>0.31</td><td>0.70</td><td>0.57</td><td>0.31</td><td>0.31</td><td>3.20</td><td>0.79</td></tr><tr><td>GS-SLAM [111]</td><td>[ICVPR24]</td><td>✓</td><td>0.48</td><td>0.53</td><td>0.33</td><td>0.52</td><td>0.41</td><td>0.59</td><td>0.46</td><td>0.70</td><td>0.50</td></tr><tr><td>SplaTAM [112]</td><td>[ICVPR24]</td><td>✓</td><td>0.31</td><td>0.40</td><td>0.29</td><td>0.47</td><td>0.27</td><td>0.29</td><td>0.32</td><td>0.55</td><td>0.36</td></tr></table>

TABLE2 Collection of representative datasets for 3D GS.Here PC represents point clouds.  

<table><tr><td>Name</td><td>Type</td><td># Sample</td><td>Task</td></tr><tr><td>Tanks&amp;amp;Temples [270]</td><td>RGB</td><td>14</td><td rowspan="10">Novel View Synthesis</td></tr><tr><td>RealEstate10K [271]</td><td>RGB</td><td>1,000</td></tr><tr><td>DeepBlending [272]</td><td>RGB</td><td>19</td></tr><tr><td>LLFF [273]</td><td>RGB</td><td>8</td></tr><tr><td>NeRF [12]</td><td>RGB</td><td>8</td></tr><tr><td>ACID [274]</td><td>RGB</td><td>700+</td></tr><tr><td>Mip-NeRF 360 [8]</td><td>RGB</td><td>9</td></tr><tr><td>TUM RGB-D [275]</td><td>RGB-D</td><td>39</td></tr><tr><td>KITTI [276]</td><td>RGB-D&amp;amp;PC</td><td>11</td></tr><tr><td>ScanNet [277]</td><td>RGB-D</td><td>1,513</td></tr><tr><td>Replica [261]</td><td>RGB-D</td><td>18</td><td rowspan="5">Robotics</td></tr><tr><td>Waymo [278]</td><td>RGB-D&amp;amp;PC</td><td>1,150</td></tr><tr><td>nuScenes [279]</td><td>RGB-D&amp;amp;PC</td><td>1,000</td></tr><tr><td>RLBench [280]</td><td>RGB</td><td>100</td></tr><tr><td>Robomimic [281]</td><td>RGB</td><td>800</td></tr><tr><td>D-NeRF [184]</td><td>RGB</td><td>8</td><td rowspan="3">Dynamic Scene Reconstruction</td></tr><tr><td>HyperNeRF [185]</td><td>RGB</td><td>6</td></tr><tr><td>NeRF-DS [282]</td><td>RGB</td><td>8</td></tr><tr><td>CoNeRF [283]</td><td>RGB</td><td>7</td><td rowspan="5">Generation and Editing</td></tr><tr><td>SPIn-NeRF [284]</td><td>RGB</td><td>10</td></tr><tr><td>Tensor4D [285]</td><td>RGB</td><td>4</td></tr><tr><td>OmniObject3D [286]</td><td>3D Object</td><td>6,000</td></tr><tr><td>Objaverse [287]</td><td>3D Object</td><td>800K+</td></tr><tr><td>People-Snapshot [288]</td><td>RGB</td><td>24</td><td rowspan="8">Avatar</td></tr><tr><td>VOCASET [289]</td><td>RGB</td><td>12</td></tr><tr><td>THuman [290]</td><td>RGB</td><td>200</td></tr><tr><td>THUman2.0 [291]</td><td>RGB-D</td><td>50</td></tr><tr><td>ZJU-Mocap [292]</td><td>RGB</td><td>9</td></tr><tr><td>H3DS [293]</td><td>RGB</td><td>23</td></tr><tr><td>THUman3.0 [294]</td><td>3D Scan</td><td>20</td></tr><tr><td>SCARED [295]</td><td>RGB-D</td><td>9</td></tr><tr><td>EndoNeRF [258]</td><td>RGB</td><td>2</td><td rowspan="2">Medical</td></tr><tr><td>X3D [296]</td><td>X-ray</td><td>15</td></tr><tr><td>CityNeRF [297]</td><td>RGB</td><td>12</td><td rowspan="4">Large-scale Reconstruction</td></tr><tr><td>Waymo Block-NeRF [298]</td><td>RGB&amp;amp;PC</td><td>1</td></tr><tr><td>UrbanBIS [299]</td><td>RGB&amp;amp;PC</td><td>6</td></tr><tr><td>GauU-Scene [160]</td><td>RGB&amp;amp;PC</td><td>1</td></tr></table>

feedback and dynamic lighting changes. Integrating adaptive physics engines capable of multi- object and multimaterial interactions, developing new simulation architectures that are compatible with priors learned from large- scale data, and expanding datasets to encompass diverse materials and dynamic scenarios are equally vital.

## 6 PERFORMANCE COMPARISON

In this section, we provide more empirical evidence by presenting the performance of several 3D GS algorithms that we previously discussed. The diverse applications of 3D GS across numerous tasks, coupled with the custom- tailored algorithmic designs for each task, render a uniform comparison of all 3D GS algorithms across a single task or dataset impracticable. For comprehensiveness, we provide a collection of representative datasets in Table 2 according to our analysis in Sec. 5. Due to the limited space, we have chosen several representative tasks for an in- depth performance evaluation. The performance scores are primarily sourced from the original papers, except where indicated otherwise. We also maintain a Github repository for this section.

### 6.1 Performance Benchmarking: Localization

The localization task in SLAM involves determining the precise position and orientation of a robot or device within an environment, typically using sensor data.

Dataset: Replica [261] dataset is a collection of 18 highly detailed 3D indoor scenes. These scenes are not only visually realistic but also offer comprehensive data including dense meshes, high- quality HDR textures, and detailed semantic information for each element. Following [262], three sequences about rooms and five sequences about offices are used for the evaluation.

Benchmarking Algorithms: For performance comparison, we involve four recent 3D GS based algorithms [111]- [114] and six typical SLAM methods [262]- [267].

Evaluation Metric: The root mean square error (RMSE) of the absolute trajectory error (ATE) is a commonly used metric in evaluating SLAM systems [275], which measures the root mean square of the Euclidean distances between the estimated and true positions over the entire trajectory.

Result: As shown in Table 1, the recent 3D Gaussians based localization algorithms have a clear advantage over existing NeRF based dense visual SLAM. For example, SplaTAM [112] achieves a trajectory error improvement of  $\sim 50\%$ , decreasing it from  $0.52\mathrm{cm}$  to  $0.36\mathrm{cm}$  compared to the previous state- of- the- art (SOTA) [266]. We attribute this to the dense and accurate 3D Gaussians reconstructed for scenes, which can handle the noise of real sensors. This reveals that effective scene representations can improve the accuracy of localization tasks.

### 6.2 Performance Benchmarking: Static Scenes

Rendering focuses on transforming computer- readable information (e.g., 3D objects in the scene) to pixel- based images. This section focuses on evaluating the quality of rendering results in static scenes.

Dataset: The same dataset as in Sec. 6.1, i.e., Replica [261], is used for comparison. The testing views are the same as those collected by [262].

TABLE 3 Comparison of mapping methods (6.2) on Replica [261] (static scenes), in terms of PSNR, SSIM, and LPIPS. The results for FPS are taken from [113] using one 4090 GPU.  

<table><tr><td>Method</td><td>GS</td><td>Metric</td><td>Room0</td><td>Room1</td><td>Room2</td><td>Office0</td><td>Office1</td><td>Office2</td><td>Office3</td><td>Office4</td><td>Avarage</td><td>FPS</td></tr><tr><td rowspan="3">NICE-SLAM [264] [CVR22]</td><td rowspan="3"></td><td>PSNR↑</td><td>22.12</td><td>22.47</td><td>24.52</td><td>29.07</td><td>30.34</td><td>19.66</td><td>22.23</td><td>24.94</td><td>24.42</td><td></td></tr><tr><td>SSIM↑</td><td>0.69</td><td>0.76</td><td>0.81</td><td>0.87</td><td>0.89</td><td>0.80</td><td>0.80</td><td>0.86</td><td>0.81</td><td>0.54</td></tr><tr><td>LPIPS↓</td><td>0.33</td><td>0.27</td><td>0.21</td><td>0.23</td><td>0.18</td><td>0.23</td><td>0.21</td><td>0.20</td><td>0.23</td><td></td></tr><tr><td rowspan="3">Vox-Fusion [263] [ISMAR22]</td><td rowspan="3"></td><td>PSNR↑</td><td>22.39</td><td>22.36</td><td>23.92</td><td>27.79</td><td>29.83</td><td>20.33</td><td>23.47</td><td>25.21</td><td>24.41</td><td></td></tr><tr><td>SSIM↑</td><td>0.68</td><td>0.75</td><td>0.80</td><td>0.86</td><td>0.88</td><td>0.79</td><td>0.80</td><td>0.85</td><td>0.80</td><td>2.17</td></tr><tr><td>LPIPS↓</td><td>0.30</td><td>0.27</td><td>0.23</td><td>0.24</td><td>0.18</td><td>0.24</td><td>0.21</td><td>0.20</td><td>0.24</td><td></td></tr><tr><td rowspan="3">Point-SLAM [266] [ICV23]</td><td rowspan="3"></td><td>PSNR↑</td><td>32.40</td><td>34.08</td><td>35.50</td><td>38.26</td><td>39.16</td><td>33.99</td><td>33.48</td><td>33.49</td><td>35.17</td><td></td></tr><tr><td>SSIM↑</td><td>0.97</td><td>0.98</td><td>0.98</td><td>0.98</td><td>0.99</td><td>0.96</td><td>0.96</td><td>0.98</td><td>0.97</td><td>1.33</td></tr><tr><td>LPIPS↓</td><td>0.11</td><td>0.12</td><td>0.11</td><td>0.10</td><td>0.12</td><td>0.16</td><td>0.13</td><td>0.14</td><td>0.12</td><td></td></tr><tr><td rowspan="3">SplaTAM [112] [CVR24]</td><td rowspan="3">✓</td><td>PSNR↑</td><td>32.86</td><td>33.89</td><td>35.25</td><td>38.26</td><td>39.17</td><td>31.97</td><td>29.70</td><td>31.81</td><td>34.11</td><td></td></tr><tr><td>SSIM↑</td><td>0.97</td><td>0.97</td><td>0.98</td><td>0.98</td><td>0.98</td><td>0.97</td><td>0.95</td><td>0.95</td><td>0.97</td><td>-</td></tr><tr><td>LPIPS↓</td><td>0.08</td><td>0.10</td><td>0.08</td><td>0.09</td><td>0.09</td><td>0.10</td><td>0.12</td><td>0.15</td><td>0.10</td><td></td></tr><tr><td rowspan="3">GS-SLAM [111] [CVR24]</td><td rowspan="3">✓</td><td>PSNR↑</td><td>31.56</td><td>32.86</td><td>32.59</td><td>38.70</td><td>41.17</td><td>32.36</td><td>32.03</td><td>32.92</td><td>34.27</td><td>-</td></tr><tr><td>SSIM↑</td><td>0.97</td><td>0.97</td><td>0.97</td><td>0.99</td><td>0.99</td><td>0.98</td><td>0.97</td><td>0.97</td><td>0.97</td><td></td></tr><tr><td>LPIPS↓</td><td>0.09</td><td>0.07</td><td>0.09</td><td>0.05</td><td>0.03</td><td>0.09</td><td>0.11</td><td>0.11</td><td>0.08</td><td></td></tr><tr><td rowspan="3">GSSLAM [113] [CVR24]</td><td rowspan="3">✓</td><td>PSNR↑</td><td>34.83</td><td>36.43</td><td>32.49</td><td>39.95</td><td>42.09</td><td>36.24</td><td>36.70</td><td>36.07</td><td>37.50</td><td></td></tr><tr><td>SSIM↑</td><td>0.95</td><td>0.96</td><td>0.96</td><td>0.97</td><td>0.98</td><td>0.96</td><td>0.96</td><td>0.96</td><td>0.96</td><td>769</td></tr><tr><td>LPIPS↓</td><td>0.07</td><td>0.08</td><td>0.07</td><td>0.07</td><td>0.06</td><td>0.08</td><td>0.07</td><td>0.10</td><td>0.07</td><td></td></tr><tr><td rowspan="3">Gaussian-SLAM [114] [arxiv]</td><td rowspan="3">✓</td><td>PSNR↑</td><td>34.31</td><td>37.28</td><td>38.18</td><td>43.97</td><td>43.56</td><td>37.39</td><td>36.48</td><td>40.19</td><td>38.90</td><td></td></tr><tr><td>SSIM↑</td><td>0.99</td><td>0.99</td><td>0.99</td><td>1.00</td><td>0.99</td><td>0.99</td><td>0.99</td><td>1.00</td><td>0.99</td><td>-</td></tr><tr><td>LPIPS↓</td><td>0.08</td><td>0.07</td><td>0.07</td><td>0.04</td><td>0.07</td><td>0.08</td><td>0.08</td><td>0.07</td><td>0.07</td><td></td></tr></table>

Benchmarking Algorithms: For performance comparison, we involve four recent papers which introduce 3D Gaussians into their systems [111]- [114], as well as three dense SLAM methods [263], [264], [266].

Evaluation Metric: Peak signal- to- noise ratio (PSNR), structural similarity (SSIM) [300], and learned perceptual image patch similarity (LPIPS) [301] are used for measuring RGB rendering performance.

Result: Table 3 shows that 3D Gaussians based systems generally outperform the three dense SLAM competitors. For example, Gaussian- SLAM [114] establishes new SOTA and outperforms previous methods by a large margin. Compared to Point- SLAM [266], GSSLAM [113] is about 578 times faster in achieving very competitive accuracy. In contrast to previous method [266] that relies on depth information, such as depth- guided ray sampling, for synthesizing novel views, 3D GS based system eliminates this need, allowing for high- fidelity rendering for any views.

### 6.3 Performance Benchmarking: Dynamic Scenes

This section focuses on evaluating the rendering quality in dynamic scenes.

Dataset: D- NeRF [184] dataset includes videos with 50 to 200 frames each, captured from unique viewpoints. It features synthetic, animated objects in complex scenes, with non- Lambertian materials. The dataset provides 50 to 200 training images and 20 test images per scene, designed for evaluating models in the monocular setting. The testing views are the same as the original paper [184].

Benchmarking Algorithms: For performance comparison, we involve five recent papers that model dynamic scenes with 3D GS [93]- [95], [126], [132], as well as six NeRF based approaches [37], [184], [187], [302]- [304].

Evaluation Metric: The same metrics as in Sec. 6.2, i.e., PSNR, SSIM [300], and LPIPS [301], are used for evaluation. Result: From Table 4 we can observe that 3D GS based methods outperform existing SOTAs by a clear margin. The static version of 3D GS [10] fails to reconstruct dynamic scenes, resulting in a sharp drop in performance. By modeling the dynamics, D- 3DGS [94] outperforms the SOTA method, FFDNeRF [187], by 6.83dS in terms of PSNR. These results indicate the effectiveness of introducing additional properties or structured information to model the deformation of Gaussians so as to model the scene dynamics.

TABLE4 Comparison of reconstruction methods (6.3) on D-NeRF [184] (dynamic scenes), in terms of PSNR, SSIM, and LPIPS. \* denotes results reported in [95].  

<table><tr><td>Method</td><td>GS</td><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS↓</td></tr><tr><td>D-NeRF [184]</td><td>[CVR21]</td><td></td><td>30.50</td><td>0.95</td></tr><tr><td>TiNeuVox-B [302]</td><td>[SGA21]</td><td></td><td>32.67</td><td>0.97</td></tr><tr><td>KPlanes [37]</td><td>[CVR23]</td><td></td><td>31.61</td><td>0.97</td></tr><tr><td>HexPlane-Slim [303]</td><td>[CVR23]</td><td></td><td>32.68</td><td>0.97</td></tr><tr><td>FFDNeRF [187]</td><td>[CVR23]</td><td></td><td>32.68</td><td>0.97</td></tr><tr><td>MSTH [304]</td><td>[NeurIPS23]</td><td></td><td>31.34</td><td>0.98</td></tr><tr><td>3D GS* [10]</td><td>[TOG23]</td><td>✓</td><td>23.19</td><td>0.93</td></tr><tr><td>4DGS [93]</td><td>[ICLR24]</td><td>✓</td><td>34.09</td><td>0.98</td></tr><tr><td>4D-GS [95]</td><td>[CVR24]</td><td>✓</td><td>34.05</td><td>0.98</td></tr><tr><td>GaGS [132]</td><td>[CVR24]</td><td>✓</td><td>37.36</td><td>0.99</td></tr><tr><td>CoGS [126]</td><td>[CVR24]</td><td>✓</td><td>37.90</td><td>0.98</td></tr><tr><td>D-3DGS [94]</td><td>[CVR24]</td><td>✓</td><td>39.51</td><td>0.99</td></tr></table>

### 6.4 Performance Benchmarking: Human Avatar

Human avatar modeling aims to create the model of human avatars from a given multi- view video.

Dataset: ZJU- MoCap [292] is a prevalent benchmark in human modeling from videos, captured with 23 synchronized cameras at a  $1024 \times 1024$  resolution. Six subjects (i.e., 377, 386, 387, 392, 393, and 394) are used for evaluation [305]. The same testing views following [306] are adopted.

Benchmarking Algorithms: For performance comparison, we involve three recent papers which model human avatar with 3D GS [145], [146], [249], as well as six human rendering approaches [292], [305]- [309].

Evaluation Metric: PSNR, SSIM [300], and LPIPS* [301] are used for measuring RGB rendering performance. Here LPIPS* equals to LPIPS × 1000.

TABLE 5 Comparison of reconstruction methods (s6.4) on ZJU-MoCap [292] (avatar), in terms of PSNR, SSIM, and LPIPS\*. The results for non-GS methods are taken from [146].  

<table><tr><td>Method</td><td>GS</td><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS*↓</td></tr><tr><td>NeuralBody [292]</td><td>[CVPR21]</td><td></td><td>29.03</td><td>0.96</td></tr><tr><td>AnimNeRF [307]</td><td>[ICCV21]</td><td></td><td>29.77</td><td>0.96</td></tr><tr><td>PixelNeRF [308]</td><td>[ICCV21]</td><td></td><td>24.71</td><td>0.89</td></tr><tr><td>NHP [309]</td><td>[NeurIPS21]</td><td></td><td>28.25</td><td>0.95</td></tr><tr><td>HumanNeRF [305]</td><td>[CVPR22]</td><td></td><td>30.66</td><td>0.97</td></tr><tr><td>Instant-NVR [306]</td><td>[CVPR23]</td><td></td><td>31.01</td><td>0.97</td></tr><tr><td>GauHuman [145]</td><td>[CVPR24]</td><td>✓</td><td>31.34</td><td>0.97</td></tr><tr><td>3DGS-Avatar [249]</td><td>[CVPR24]</td><td>✓</td><td>30.61</td><td>0.97</td></tr><tr><td>GART [146]</td><td>[CVPR24]</td><td>✓</td><td>32.22</td><td>0.98</td></tr></table>

TABLE 6 Comparison of reconstruction methods (s6.5) on EndoNeRF [258] (surgical scenes), in terms of PSNR, SSIM, and LPIPS. The results for non-GS methods are taken from [153]. FPS and GPU usage for training (Mem.) are measured using one 4090 GPU [153].  

<table><tr><td>Method</td><td>GS</td><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS↓</td><td>FPS↑</td><td>Mem↓</td></tr><tr><td>EndoNeRF [258]</td><td>[MCCA122]</td><td></td><td>36.06</td><td>0.93</td><td>0.09</td><td>0.04</td></tr><tr><td>EndoSurf [260]</td><td>[MCCA123]</td><td></td><td>36.53</td><td>0.95</td><td>0.07</td><td>0.04</td></tr><tr><td>LerPlane-9k [259]</td><td>[MCCA123]</td><td></td><td>35.00</td><td>0.93</td><td>0.08</td><td>0.91</td></tr><tr><td>LerPlane-32k [259]</td><td>[MCCA123]</td><td></td><td>37.38</td><td>0.95</td><td>0.05</td><td>0.87</td></tr><tr><td>Endo-4DGS [152]</td><td>[MCCA124]</td><td>✓</td><td>37.00</td><td>0.96</td><td>0.05</td><td>-</td></tr><tr><td>EndoGaussian [153]</td><td>[arXiv]</td><td>✓</td><td>37.85</td><td>0.96</td><td>0.05</td><td>195.09</td></tr><tr><td>HFGS [155]</td><td>[BMVC24]</td><td>✓</td><td>38.14</td><td>0.97</td><td>0.03</td><td>-</td></tr></table>

Result: Table 5 presents the numerical results of top- leading solutions in human avatar modeling. We observe that introducing 3D GS into the framework leads to consistent performance improvements in both rendering quality and speed. For instance, GART [146] outperforms current SOTA, Instant- NVR [306], by 1.21dB in terms of PSNR. Considering the enhanced fidelity, inference speed and editability, 3D GS based avatar modeling may revolutionize the field of 3D animation, interactive gaming, etc.

### 6.5 Performance Benchmarking: Surgical Scenes

3D reconstruction from endoscopic video is critical to robotic- assisted minimally invasive surgery, enabling preoperative planning, training through AR/VR simulations, and intraoperative guidance.

Dataset: EndoNeRF [258] dataset presents a specialized collection of stereo camera captures, comprising two samples of in- vivo prostatectomy. It is tailored to represent real- world surgical complexities, including challenging scenes with tool occlusion and pronounced non- rigid deformation. The same testing views as in [260] are used.

Benchmarking Algorithms: For performance comparison, we involve three recent papers which reconstruct dynamic 3D endoscopic scenes with GS [152], [153], [155], as well as three NeRF- based surgical reconstruction approaches [258] [260].

Evaluation Metric: PSNR, SSIM [300], and LPIPS [301] are adopted for evaluation. In addition, the requirement for GPU memory is also reported.

Result: Table 6 shows that introducing the explicit representation of 3D Gaussians leads to several significant improvements. For instance, EndoGaussian [153] outperforms a strong baseline, LerPlane- 32k [259], among all metrics. In particular, EndoGaussian demonstrates an approximate 224- fold increase in speed while consumes just  $10\%$  of the GPU

resources. These impressive results attest to the efficiency of GS- based methods, which not only expedite processing but also minimize GPU load, thus easing the demands on hardware. Such attributes are vitally significant for real- world surgical application deployment, where optimized resource usage can be a key determinant of practical utility.

## 7 FUTURE RESEARCH DIRECTIONS

As impressive as those follow- up works on 3D GS are, and as much as those fields have been or might be revolutionized by 3D GS, there is a general agreement that 3D GS still has considerable room for improvement.

- Physics- and Semantics-aware scene Representation. As a new, explicit scene representation technique, 3D Gaussian offers transformative potential beyond merely enhancing novel-view synthesis. It has the potential to pave the way for simultaneous advancements in scene reconstruction and understanding by devising physics- and semantics-aware 3D GS systems. While significant progress has been made in physics (Sec. 5.7) and semantics [310]-[315] individually, there remains considerable untapped potential in their synergistic integration. This is poised to revolutionize a range of fields and downstream applications. For instance, incorporating prior knowledge such as the general shape of objects can reduce the need for extensive training viewpoints [47], [48] while improving geometry/surface reconstruction [77], [316]. A critical metric for assessing scene representation is the quality of its generated scenes, which encompasses challenges in geometry, texture, and lighting fidelity [66], [128], [141]. By merging physical principles and semantic information within the 3D GS framework, one can expect that the quality will be enhanced, thereby facilitating dynamics modeling [21], [166], editing [90], [92], generation [133], [134], and beyond. In a nutshell, pursuing this advanced and versatile scene representation opens up new possibilities for innovation in computational creativity and practical applications across diverse domains.

- Learning Physical Priors from Large-scale Data. As we explore the potential of physics- and semantics-aware scene representations, leveraging large-scale datasets to learn generalizable, physical priors emerges as a promising direction. The goal is to model the inherent physical properties and dynamics embedded within real-world data, transforming them into actionable insights that can be applied across various domains such as robotics and visual effects. Establishing a learning framework for extracting these generalizable priors enables the application of these insights to specify tasks in a few-shot manner. For instance, it allows for rapid adaptation to new objects and environments with minimal data input. Furthermore, integrating physical priors can enhance not only the accuracy and quality of generated scenes but also their interactive and dynamic qualities. This is particularly valuable in AR/VR environments, where users interact with virtual objects that behave in ways consistent with their real-world counterparts. However, the existing body of work on capturing and distilling physics-based knowledge from extensive 2D and 3D datasets remains sparse. Notable efforts in related area include the continuum mechanics based GS systems (Sec. 5.7), and the generalizable Gaussian representation based on multi-view stereo [317].

Further exploration on real2sim and sim2real might offer viable routes for advancements in this field.

- Modeling Internal Structures of Objects with 3D GS. Despite the ability of 3D GS to produce highly photorealistic renderings, modeling internal structures of objects (e.g., for a scanned object in computed tomography) within the current GS framework presents a notable challenge. Due to the splatting and density control process, the current representation of 3D Gaussian is unorganized and cannot align well with the object's actual internal structures. Moreover, there is a strong preference in various applications to depict objects as volumes (e.g., computed tomography). However, the disordered nature of 3D GS makes volume modeling particularly difficult. Li et al. [318] used 3D Gaussians with density control as the basis for the volumetric representation and did not involve the splatting process. X-Gaussian [319] involves the splatting process for fast training and inference but cannot generate volumetric representation. Using 3D GS to model the internal structures of objects remains unanswered and deserves further exploration.

- 3D GS for Simulation in Autonomous Driving and beyond. Collecting real-world datasets for autonomous driving is both expensive and logistically challenging, yet crucial for training effective image-based perception systems. To mitigate these issues, simulation emerges as a cost-effective alternative, enabling the generation of synthetic datasets across diverse environments. However, the development of simulators capable of producing photorealistic and diverse synthetic data is fraught with challenges. These include achieving a high level of quality, accommodating various control methods, and accurately simulating a range of lighting conditions. While early efforts [188]-[190] in reconstructing urban/street scenes with 3D GS have been encouraging, they are just the tip of the iceberg in terms of the full capabilities. There remain numerous critical aspects to be explored, such as the integration of user-defined object models, the modeling of physics-aware scene changes (e.g., the rotation of vehicle wheels), and the enhancement of controllability and quality (e.g., in varying lighting conditions). Mastery of these capabilities would not only advance autonomous systems but also redefine computational understanding of physical spaces — a leap with implications for world models, spatial intelligence, embodied AI, and beyond.

- Empowering 3D GS with More Possibilities. Despite the significant potential of 3D GS, the full scope of applications for 3D GS remains largely untapped. A promising avenue for exploration involves augmenting 3D Gaussians with additional attributes (e.g., linguistic and spatiotemporal properties as mentioned in Sec. 4.5) and introducing structured information (e.g., spatial MLPs and grids as mentioned in Sec. 4.6), tailored for specific applications. Moreover, recent studies have begun to unveil the capability of 3D GS in several domains, e.g., point cloud registration [320], image representation and compression [60], and fluid synthesis [171]. These findings highlight a significant opportunity for interdisciplinary scholars to explore 3D GS further.

## 8 CONCLUSIONS

To the best of our knowledge, this survey presents the first comprehensive overview of 3D GS, a groundbreaking technique revolutionizing explicit radiance fields, computer graphics, and computer vision. It delineates the paradigm shift from traditional NeRF based methods, spotlighting the advantages of 3D GS in real- time rendering and enhanced editability. Our in- depth analysis and extensive quantitative studies demonstrate the superiority of 3D GS in practical applications, particularly those highly sensitive to latency. We offer insights into principles, prospective research directions, and the unresolved challenges within this domain. Overall, 3D GS stands as a transformative technology, poised to significantly influence future advancements in 3D reconstruction and representation. This survey is intended to serve as a foundational resource, propelling further exploration and progress in this rapidly evolving field.

## REFERENCES

[1] S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen, "The lumigraph," in Seminal Graphics Papers: Pushing the Boundaries, Volume 2, 2023, pp. 453- 464. [2] M. Levoy and P. Hanrahan, "Light field rendering," in Seminal Graphics Papers: Pushing the Boundaries, Volume 2, 2023, pp. 441- 452. [3] C. Buehler, M. Bosse, L. McMillan, S. Gortler, and M. Cohen, "Unstructured lumigraph rendering," in Seminal Graphics Papers: Pushing the Boundaries, Volume 2, 2023, pp. 597- 504. [4] N. Snavely, S. M. Seitz, and R. Szeliski, "Photo tourism: exploring photo collections in 3d," in ACM Trans. Graph., 2006, pp. 835- 846. [5] M. Goesele, N. Snavely, B. Curless, H. Hoppe, and S. M. Seitz, "Multi- view stereo for community photo collections," in Proc. IEEE Int. Conf. Comput. Vis., 2007, pp. 1- 8. [6] S. J. Garbin, M. Kowalski, M. Johnson, J. Shotton, and J. Valentin, "Fastner: High- fidelity neural rendering at 200fps," in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 14346- 14355. [7] C. Reiser, S. Peng, Y. Liao, and A. Geiger, "Kilnerf: Speeding up neural radiance fields with thousandths of tiny mlpss," in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 14335- 14345. [8] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman, "Mip- nerf 360: Unbounded, anti- aliased neural radiance fields," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 5470- 5479. [9] T. Müller, A. Evans, C. Schied, and A. Keller, "Instant neural graphics primitives with a multiresolution hash encoding," ACM Trans. Graph., vol. 41, no. 4, pp. 1- 15, 2022. [10] B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis, "3d gaussian splatting for real- time radiance field rendering," ACM Trans. Graph., vol. 42, no. 4, 2023. [11] V. Sitzmann, J. Thies, F. Heide, M. Nießner, G. Wetzstein, and M. Zollhoffer, "Deepvoxels: Learning persistent 3d feature embeddings," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 2437- 2446. [12] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, "Nerf: Representing scenes as neural radiance fields for view synthesis," in Proc. Eur. Conf. Comput. Vis., 2020, pp. 405- 421. [13] H. Pfister, M. Zwicker, J. Van Baar, and M. Gross, "Surfels: Surface elements as rendering primitives," in Proceedings of the 27th annual conference on Computer graphics and interactive techniques, 2000, pp. 335- 342. [14] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, "Surface splatting," in Proceedings of the 28th annual conference on Computer graphics and interactive techniques, 2001, pp. 371- 378. [15] L. Ren, H. Pfister, and M. Zwicker, "Object space ewa surface splatting: A hardware accelerated approach to high quality point rendering," in Comput. Graph. Forum, no. 3, 2002, pp. 461- 470. [16] W. Yifan, F. Serena, S. Wu, C. Öztireli, and O. Sorkine- Hornung, "Differentiable surface splatting for point- based geometry processing," ACM Trans. Graph., vol. 38, no. 6, pp. 1- 14, 2019. [17] O. Wiles, G. Gkioxari, R. Szeliski, and J. Johnson, "Synsin: End- to- end view synthesis from a single image," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2020, pp. 7467- 7477. [18] D. Kalkofen, E. Mendez, and D. Schmalstieg, "Comprehensible visualization for augmented reality," IEEE Trans. Vis. Comput. Graph., vol. 15, no. 2, pp. 193- 204, 2008.

[19] A. Patney, M. Salvi, J. Kim, A. Kaplanyan, C. Wyman, N. Benty, D. Luebke, and A. Lefohn, "Towards foveated rendering for gazetracked virtual reality," ACM Trans. Graph., vol. 35, no. 6, pp. 1- 12, 2016. [20] R. Albert, A. Patney, D. Luebke, and J. Kim, "Latency requirements for foveated rendering in virtual reality," ACM Transactions on Applied Perception, vol. 14, no. 4, pp. 1- 13, 2017. [21] Y. Jiang, C. Yu, T. Xie, X. Li, Y. Feng, H. Wang, M. Li, H. Lau, F. Gao, Y. Yang et al., "Vr- gs: A physical dynamics- aware interactive gaussian splatting system in virtual reality," arXiv preprint arXiv:2401.16663, 2024. [22] T. Lu, M. Yu, L. Xu, Y. Xiangli, L. Wang, D. Lin, and B. Dai, "Scaffold- gs: Structured 3d gaussians for view- adaptive rendering," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [23] S. Saito, G. Schwartz, T. Simon, J. Li, and G. Nam, "Relightable gaussian codec avatars," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [24] T. Zhang, K. Huang, W. Zhi, and M. Johnson- Roberson, "Darkgs: Learning neural illumination and 3d gaussians relighting for robotic exploration in the dark," in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2024. [25] B. Fei, J. Xu, R. Zhang, Q. Zhou, W. Yang, and Y. He, "3d gaussian splatting as new era: A survey," IEEE Trans. Vis. Comput. Graph., 2024. [26] A. Dalal, D. Hagen, K. G. Robbersmyr, and K. M. Knausgard, "Gaussian splatting: 3d reconstruction and novel view synthesis, a review," IEEE Access, 2024. [27] Y. Bao, T. Ding, J. Hug, Y. Liu, Y. Li, W. Li, Y. Gao, and J. Luo, "3d gaussian splatting: Survey, technologies, challenges, and opportunities," arXiv preprint arXiv:2407.17418, 2024. [28] T. Wu, Y.- J. Yuan, L.- X. Zhang, J. Yang, Y.- P. Cao, L.- Q. Yan, and L. Gao, "Recent advances in 3d gaussian splatting," Comput. Vis. Media, pp. 1- 30, 2024. [29] L. Kobbelt and M. Botsch, "A survey of point- based techniques in computer graphics," Comput. Graph., vol. 28, no. 6, pp. 801- 814, 2004. [30] Y. Xie, T. Takikawa, S. Saito, O. Litany, S. Yan, N. Khan, F. Tombari, J. Tompkin, V. Sitzmann, and S. Sridhar, "Neural fields in visual computing and beyond," in Comput. Graph. Forum, no. 2, 2022, pp. 641- 676. [31] W. Wang, Y. Yang, and Y. Pan, "Visual knowledge in the big model era: Retrospect and prospect," arXiv preprint arXiv:2404.04308, 2024. [32] A. Tewari, J. Thies, B. Mildenhall, P. Srinivasan, E. Tretschk, W. Yifan, C. Lassner, V. Sitzmann, R. Martin- Brualla, S. Lombardi et al., "Advances in neural rendering," in Comput. Graph. Forum, no. 2, 2022, pp. 703- 735. [33] X.- F. Han, H. Laga, and M. Bennamoun, "Image- based 3d object reconstruction: State- and- the- art and trends in the deep learning era," IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 5, pp. 1578- 1604, 2019. [34] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger, "Occupancy networks: Learning 3d reconstruction in function space," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 4460- 4470. [35] J. J. Park, P. Florence, J. Strath, R. Newcombe, and S. Lovegrove, "Deepsdf: Learning continuous signed distance functions for shape representation," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 165- 174. [36] C. Sun, M. Sun, and H.- T. Chen, "Direct voxel grid optimization: Super- fast convergence for radiance fields reconstruction," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 5459- 5469. [37] S. Fridovich- Keil, G. Meanti, F. R. Warburg, B. Recht, and A. Kanazawa, "K- planes: Explicit radiance fields in space, time, and appearance," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 12479- 12488. [38] J. P. Grossman and W. J. Dally, "Point sample rendering," in Render. Tech., 1998, pp. 181- 192. [39] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, "Ewa volume splatting," in Proceedings Visualization, 2001. VIS'01., 2001, pp. 29- 538. [40] ——, "Ewa splatting," IEEE Trans. Vis. Comput. Graph., vol. 8, no. 3, pp. 223- 238, 2002. [41] K.- A. Aliev, A. Sevastopolsky, M. Kolos, D. Ulyanov, and V. Lempitsky, "Neural point- based graphics," in Proc. Eur. Conf. Comput. Vis., 2020, pp. 696- 712.

[42] D. Rückert, L. Franke, and M. Stamminger, "Adop: Approximate differentiable one- pixel point rendering," ACM Trans. Graph., vol. 41, no. 4, pp. 1- 14, 2022. [43] C. Lassner and M. Zollhofer, "Pulsar: Efficient sphere- based neural rendering," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 1440- 1449. [44] K. Cheng, X. Long, K. Yang, Y. Yao, W. Yin, Y. Ma, W. Wang, and X. Chen, "Gaussianpro: 3d gaussian splatting with progressive propagation," in Proc. ACM Int. Conf. Mach. Learn., 2024. [45] H. Xiong, S. Muttukuru, R. Upadhyay, P. Chari, and A. Kadambi, "Sparsegs: Real- time 360  $\{\mathrm{\backslash\deg}\}$  sparse view synthesis using gaussian splatting," arXiv preprint arXiv:2312.00206, 2023. [46] Z. Zhu, Z. Fan, Y. Jiang, and Z. Wang, "Fsgs: Real- time few- shot view synthesis using gaussian splatting," in Proc. Eur. Conf. Comput. Vis., 2024. [47] D. Charatan, S. Li, A. Tagliasacchi, and V. Sitzmann, "pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [48] S. Szymanowicz, C. Rupprecht, and A. Vedaldi, "Splatter image: Ultra- fast single- view 3d reconstruction," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [49] J. Li, J. Zhang, X. Bai, J. Zheng, X. Ning, J. Zhou, and L. Gu, "Dngaussian: Optimizing sparse- view 3d gaussian radiance fields with global- local depth normalization," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [50] A. Swann, M. Strong, W. K. Do, G. S. Camps, M. Schwager, and M. Kennedy III, "Touch- gs: Visual- tactile supervised 3d gaussian splatting," arXiv preprint arXiv:2403.09875, 2024. [51] Y. Chen, H. Xu, C. Zheng, B. Zhuang, M. Pollefeys, A. Geiger, T.- J. Cham, and J. Cai, "Mvsplat: Efficient 3d gaussian splatting from sparse multi- view images," in Proc. Eur. Conf. Comput. Vis., 2024. [52] C. Wewer, K. Raj, E. Ilg, B. Schiele, and J. E. Lenssen, "latentsplat: Autoencoding variational gaussians for fast generalizable 3d reconstruction," arXiv preprint arXiv:2403.16292, 2024. [53] Y. Xu, Z. Shi, W. Yifan, H. Chen, C. Yang, S. Peng, Y. Shen, and G. Wezstein, "Gnm: Large gaussian reconstruction model for efficient 3d reconstruction and generation," arXiv preprint arXiv:2403.14621, 2024. [54] Q. Shen, X. Yi, Z. Wu, P. Zhou, H. Zhang, S. Yan, and X. Wang, "Gamba: Marry gaussian splatting with mamba for single view 3d reconstruction," arXiv preprint arXiv:2403.18795, 2024. [55] J. Zhang, J. Li, X. Yu, L. Huang, L. Gu, J. Zheng, and X. Bai, "Corgs: Sparse- view 3d gaussian splatting via co- regularization," in Proc. Eur. Conf. Comput. Vis., 2024. [56] Z. Fan, K. Wang, K. Wen, Z. Zhu, D. Xu, and Z. Wang, "Light- gaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps," arXiv preprint arXiv:2311.17245, 2023. [57] K. Navaneet, K. P. Meibodi, S. A. Koohpayegani, and H. Pirsiavash, "Compact3d: Compressing gaussian splat radiance field models with vector quantization," arXiv preprint arXiv:2311.18159, 2023. [58] J. C. Lee, D. Rho, X. Sun, J. H. Ko, and E. Park, "Compact 3d gaussian representation for radiance field," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [59] W. Morgenstern, F. Barthel, A. Hilsmann, and P. Eisert, "Compact 3d scene representation via self- organizing gaussian grids," arXiv preprint arXiv:2312.13399, 2023. [60] X. Zhang, X. Ge, T. Xu, D. He, Y. Wang, H. Qin, G. Lu, J. Geng, and J. Zhang, "Gaussianimage: 1000 fps image representation and compression by 2d gaussian splatting," in Proc. Eur. Conf. Comput. Vis., 2024. [61] S. Niedermayr, J. Stumpfegger, and R. Westermann, "Compressed 3d gaussian splatting for accelerated novel view synthesis," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [62] Y. Chen, Q. Wu, J. Cai, M. Harandi, and W. Lin, "Hac: Hash- grid assisted context for 3d gaussian splatting compression," in Proc. Eur. Conf. Comput. Vis., 2024. [63] P. Papantonakis, G. Kopanas, B. Kerbl, A. Lanvin, and G. Drettakis, "Reducing the memory footprint of 3d gaussian splatting," in 13D, 2024, pp. 1- 17. [64] G. Fang and B. Wang, "Mini- splatting: Representing scenes with a constrained number of gaussians," arXiv preprint arXiv:2403.14166, 2024.

[65] Z. Yu, A. Chen, B. Huang, T. Sattler, and A. Geiger, "Mipsplatting: Alias- free 3d gaussian splatting," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024, pp. 19447- 19456. [66] J. Gao, C. Gu, Y. Lin, H. Zhu, X. Cao, L. Zhang, and Y. Yao, "Relightable 3d gaussian: Real- time point cloud relighting with brdf decomposition and ray tracing," arXiv preprint arXiv:2311.16043, 2023. [67] Z. Yan, W. F. Low, Y. Chen, and G. H. Lee, "Multi- scale 3d gaussian splatting for anti- aliased rendering," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [68] Y. Jiang, J. Tu, Y. Liu, X. Gao, X. Long, W. Wang, and Y. Ma, "Gaussianshader: 3d gaussian splatting with shading functions for reflective surfaces," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [69] B. Lee, H. Lee, X. Sun, U. Ali, and E. Park, "Deblurring 3d gaussian splatting," arXiv preprint arXiv:2401.00834, 2024. [70] D. Mahar, W. Shtolak, J. labor, S. Tadeja, and P. Sparek, "Gaussian splitting algorithm with color and opacity depended on viewing direction," arXiv preprint arXiv:2312.13729, 2023. [71] L. Bolanos, S.- Y. Su, and H. Rhodin, "Gaussian shadow casting for neural characters," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [72] L. Radl, M. Steiner, M. Parger, A. Weinrauch, B. Kerbl, and M. Steinberger, "Stopthepop: Sorted gaussian splatting for view- consistent real- time rendering," ACM Trans. Graph., 2024. [73] Z. Yang, X. Gao, Y. Sun, Y. Huang, X. Lyu, W. Zhou, S. Jiao, X. Qi, and X. Jin, "Spec- gaussian: Anisotropic view- dependent appearance for 3d gaussian splatting," arXiv preprint arXiv:2402.15870, 2024. [74] C. Peng, Y. Tang, Y. Zhou, N. Wang, X. Liu, D. Li, and R. Chellappa, "Bags: Blur agnostic gaussian splatting through multiscale kernel modeling," arXiv preprint arXiv:2403.04926, 2024. [75] L. Zhao, P. Wang, and P. Liu, "Bad- gaussians: Bundle adjusted deblur gaussian splatting," arXiv preprint arXiv:2403.11831, 2024. [76] H. Dahmani, M. Bennehar, N. Piasco, L. Roldao, and D. Tsishkou, "Swag: Splatting in the wild images with appearance- conditioned gaussians," arXiv preprint arXiv:2403.10427, 2024. [77] Y. Li, C. Lyu, Y. Di, G. Zhai, G. H. Lee, and F. Tombari, "Geogaussian: Geometry- aware gaussian splatting for scene rendering," arXiv preprint arXiv:2403.11324, 2024. [78] Z. Liang, Q. Zhang, W. Hu, Y. Feng, L. Zhu, and K. Jia, "Analytic- splatting: Anti- aliased 3d gaussian splatting via analytic integration," arXiv preprint arXiv:2403.11056, 2024. [79] O. Seiskari, J. Yilammii, V. Kaatrasalo, P. Rantalankila, M. Turku- lainen, J. Kannala, E. Rahtu, and A. Solin, "Gaussian splatting on the move: Blur and rolling shutter compensation for natural camera motion," arXiv preprint arXiv:2403.13327, 2024. [80] X. Song, J. Zheng, S. Yuan, H.- a. Gao, J. Zhao, X. He, W. Gu, and H. Zhao, "Sa- gs: Scale- adaptive gaussian splatting for training- free anti- aliasing," arXiv preprint arXiv:2403.19615, 2024. [81] Y. Fu, S. Liu, A. Kulkarni, J. Kautz, A. A. Efros, and X. Wang, "Colmap- free 3d gaussian splatting," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [82] J. Jung, J. Han, H. An, J. Kang, S. Park, and S. Kim, "Relaxing accurate initialization constraint for 3d gaussian splatting," arXiv preprint arXiv:2403.09413, 2024. [83] M. Yu, T. Lu, L. Xu, L. Jiang, Y. Xiangli, and B. Dai, "Gsdf: 3dgs meets sdf for improved rendering and reconstruction," arXiv preprint arXiv:2403.16944, 2024. [84] J. Zhang, F. Zhan, M. Xu, S. Lu, and E. Xing, "Fregs: 3d gaussian splatting with progressive frequency regularization," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [85] L. Huang, J. Bai, J. Guo, and Y. Guo, "Gs++: Error analyzing and optimal gaussian splatting," arXiv preprint arXiv:2402.00752, 2024. [86] J. Li, L. Cheng, Z. Wang, T. Mu, and J. He, "Loopgaussian: Creating 3d cinemagraph with multi- view images via eulerian motion field," arXiv preprint arXiv:2404.08966, 2024. [87] J.- C. Shi, M. Wang, H.- B. Duan, and S.- H. Guan, "Language embedded 3d gaussians for open- vocabulary scene understanding," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [88] M. Qin, W. Li, J. Zhou, H. Wang, and H. Pfister, "Langsplat: 3d language gaussian splatting," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [89] X. Zuo, P. Samangouei, Y. Zhou, Y. Di, and M. Li, "Fmgs: Foundation model embedded 3d gaussian splatting for holistic 3d scene understanding," arXiv preprint arXiv:2401.01970, 2024.

[90] S. Zhou, H. Chang, S. Jiang, Z. Fan, Z. Zhu, D. Xu, P. Chari, S. You, Z. Wang, and A. Kadambi, "Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [91] M. Ye, M. Danelljan, F. Yu, and L. Ke, "Gaussian grouping: Segment and edit anything in 3d scenes," in Proc. Eur. Conf. Comput. Vis., 2024. [92] J. Cen, J. Fang, C. Yang, L. Xie, X. Zhang, W. Shen, and Q. Tian, "Segment any 3d gaussians," arXiv preprint arXiv:2312.00860, 2023. [93] Z. Yang, H. Yang, Z. Pan, X. Zhu, and L. Zhang, "Real- time photorealistic dynamic scene representation and rendering with 4d gaussian splatting," in Proc. Int. Conf. Learn. Represent., 2024. [94] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, "Deformable 3d gaussians for high- fidelity monocular dynamic scene reconstruction," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [95] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, and X. Wang, "4d gaussian splatting for real- time dynamic scene rendering," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [96] Y. Xu, B. Chen, Z. Li, H. Zhang, L. Wang, Z. Zheng, and Y. Liu, "Gaussian head avatar: Ultra high- fidelity head avatar via dynamic gaussians," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [97] S. Szymanowicz, E. Insafutdinov, C. Zheng, D. Campbell, J. F. Henriques, C. Rupprecht, and A. Vedaldi, "Flash3d: Feedforward generalisable 3d scene reconstruction from a single image," arXiv preprint arXiv:2406.04343, 2024. [98] K. Sargent, Z. Li, T. Shah, C. Herrmann, H.- X. Yu, Y. Zhang, E. R. Chan, D. Lagun, L. Fei- Fei, D. Sun et al., "Zeronvs: Zero- shot 360- degree view synthesis from a single image," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024, pp. 9420- 9429. [99] J. Meng, H. Li, Y. Wu, Q. Gao, S. Yang, J. Zhang, and S. Ma, "Mirror- 3ds: Incorporating mirror reflections into 3d gaussian splatting," arXiv preprint arXiv:2404.01168, 2024. [100] H. Chen, C. Li, and G. H. Lee, "Neusg: Neural implicit surface reconstruction with 3d gaussian splatting guidance," arXiv preprint arXiv:2312.00846, 2023. [101] Z. Yu, T. Sattler, and A. Geiger, "Gaussian opacity fields: Efficient and compact surface reconstruction in unbounded scenes," arXiv preprint arXiv:2404.10772, 2024. [102] B. Zhang, C. Fang, R. Shrestha, Y. Liang, X. Long, and P. Tan, "Rade- gs: Rasterizing depth in gaussian splatting," arXiv preprint arXiv:2406.01467, 2024. [103] A. Chen, H. Xu, S. Esposito, S. Tang, and A. Geiger, "Lara: Efficient large- baseline radiance fields," arXiv preprint arXiv:2407.04699, 2024. [104] E. Ververas, R. A. Potamias, J. Song, J. Deng, and S. Zafeiriou, "Sags: Structure- aware 3d gaussian splatting," in Proc. Eur. Conf. Comput. Vis., 2024, pp. 221- 238. [105] C. Smith, D. Charatan, A. Tewari, and V. Sitzmann, "Flowmap: High- quality camera poses, intrinsics, and depth via gradient descent," arXiv preprint arXiv:2404.15259, 2024. [106] Y. Lin, Z. Dai, S. Zhu, and Y. Yao, "Gaussian- flow: 4d reconstruction with dynamic 3d gaussian particle," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [107] A. Saroha, M. Gladkova, C. Gurreli, T. Yenamandra, and D. Cremers, "Gaussian splatting in style," arXiv preprint arXiv:2403.08498, 2024. [108] N. Moenne- Loccoz, A. Mirzaei, O. Perel, R. de Lutio, J. Martinez Esturo, G. State, S. Fidler, N. Sharp, and Z. Gojicic, "3d gaussian ray tracing: Fast tracing of particle scenes," ACM Trans. Graph., vol. 43, no. 6, pp. 1- 19, 2024. [109] A. Mai, P. Hedman, G. Kopanas, D. Verbin, D. Futschik, Q. Xu, F. Kuester, J. T. Barron, and Y. Zhang, "Ever: Exact volumetric ellipsoid rendering for real- time view synthesis," arXiv preprint arXiv:2410.01804, 2024. [110] J. Condor, S. Speierer, L. Bode, A. Bozic, S. Green, P. Didyk, and A. Jarabo, "Don't splat your gaussians: Volumetric ray- traced primitives for modeling and rendering scattering and emissive media," ACM Trans. Graph., 2025. [111] C. Yan, D. Qu, D. Wang, D. Xu, Z. Wang, B. Zhao, and X. Li, "Gs- slam: Dense visual slam with 3d gaussian splatting," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [112] N. Keetha, J. Karhade, K. M. Jatavallabhula, G. Yang, S. Scherer, D. Ramanan, and J. Luiten, "Splatant: Splat, track & map 3d

gaussians for dense rgb- d slam," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [113] H. Matsuki, R. Murai, P. H. Kelly, and A. J. Davison, "Gaussian splating slam," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [114] V. Yugay, Y. Li, T. Gevers, and M. R. Oswald, "Gaussian- slam: Photo- realistic dense slam with gaussian splating," arXiv preprint arXiv:2312.10070, 2023. [115] H. Huang, L. Li, H. Cheng, and S.- K. Yeung, "Photo- slam: Real- time simultaneous localization and photorealistic mapping for monocular, stereo, and rgb- d cameras," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [116] M. Li, S. Liu, and H. Zhou, "Sgs- slam: Semantic gaussian splat- . ting for neural dense slam," in Proc. Eur. Conf. Comput. Vis., 2024. [117] Y. Ji, Y. Liu, G. Xie, B. Ma, and Z. Xie, "Neds- slam: A novel neural explicit dense semantic slam framework using 3d gaussian splatting," arXiv preprint arXiv:2103.11679, 2021. [118] G. Lu, S. Zhang, Z. Wang, C. Liu, J. Lu, and Y. Tang, "Manigaussian: Dynamic gaussian splatting for multi- task robotic manipulation," arXiv preprint arXiv:2403.08321, 2024. [119] J. Abou- Chakra, K. Rana, F. Dayoub, and N. Sunderhauf, "Physically embodied gaussian splatting: A realtime correctable world model for robotics," in Proc. Annu. Conf. Robot Learn., 2024. [120] O. Shorinwa, J. Tucker, A. Smith, A. Swann, T. Chen, R. Firoozi, M. D. Kennedy, and M. Schwager, "Splat- mover: Multi- stage, open- vocabulary robotic manipulation via editable gaussian splatting," in Proc. Annu. Conf. Robot Learn., 2024. [121] M. Ji, R.- Z. Qiu, X. Zou, and X. Wang, "Graspsplats: Efficient manipulation with 3d feature splatting," arXiv preprint arXiv:2409.02084, 2024. [122] Y. Zheng, X. Chen, Y. Zheng, S. Gu, R. Yang, B. Jin, P. Li, C. Zhong, Z. Wang, L. Liu et al., "Gaussiangrasper: 3d language gaussian splatting for open- vocabulary robotic grasping," arXiv preprint arXiv:2403.09637, 2024. [123] S. Zhu, R. Qin, G. Wang, J. Liu, and H. Wang, "Semgauss- slam: Dense semantic gaussian splatting slam," arXiv preprint arXiv:2403.07494, 2024. [124] Z. Peng, T. Shao, Y. Liu, J. Zhou, Y. Yang, J. Wang, and K. Zhou, "Rtg- slam: Real- time 3d reconstruction at scale using gaussian splatting," ACM Trans. Graph., 2024. [125] J. Luiten, G. Kopanas, B. Leibe, and D. Ramanan, "Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis," in Proc. Int. Conf. 3D Vis., 2024. [126] H. Yu, J. Julin, Z. A. Milacski, K. Niinuma, and L. A. Jeni, "Cogs: Controllable gaussian splatting," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [127] R. Shao, J. Sun, C. Peng, Z. Zheng, B. Zhou, H. Zhang, and Y. Liu, "Control4d: Efficient 4d portrait editing with text," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [128] Y.- H. Huang, Y.- T. Sun, Z. Yang, X. Lyu, Y.- P. Cao, and X. Qi, "Sc- gs: Sparse- controlled gaussian splatting for editable dynamic scenes," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [129] D. Das, C. Wewer, R. Yunus, E. Ilg, and J. E. Lenssen, "Neural parametric gaussians for monocular non- rigid object reconstruction," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [130] Z. Li, Z. Chen, Z. Li, and Y. Xu, "Spacetime gaussian feature splatting for real- time dynamic view synthesis," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [131] J. Sun, H. Jiao, G. Li, Z. Zhang, L. Zhao, and W. Xing, "3dgstream: On- the- fly training of 3d gaussians for efficient streaming of photo- realistic free- view point videos," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [132] Z. Lu, X. Guo, L. Hui, T. Chen, M. Yang, X. Tang, F. Zhu, and Y. Dai, "3d geometry- aware deformable gaussian splatting for dynamic view synthesis," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [133] J. Tang, J. Ren, H. Zhou, Z. Liu, and G. Zeng, "Dreamgaussian: Generative gaussian splatting for efficient 3d content creation," in Proc. Int. Conf. Learn. Represent., 2024. [134] T. Yi, J. Fang, J. Wang, G. Wu, L. Xie, X. Zhang, W. Liu, Q. Tian, and X. Wang, "Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [135] J. Tang, Z. Chen, X. Chen, T. Wang, G. Zeng, and Z. Liu, "Lgm: Large multi- view gaussian model for high- resolution 3d content creation," in Proc. Eur. Conf. Comput. Vis., 2024.

[136] S. Zhou, Z. Fan, D. Xu, H. Chang, P. Chari, T. Bharadwaj, S. You, Z. Wang, and A. Kadambi, "Dreamscene360: Unconstrained text- to- 3d scene generation with panoramic gaussian splatting," in Proc. Eur. Conf. Comput. Vis., 2024. [137] Z. Li, Y. Chen, L. Zhao, and P. Liu, "Controllable text- to- 3d generation via surface- aligned gaussian splatting," arXiv preprint arXiv:2403.09981, 2024. [138] Y. Mu, X. Zuo, C. Guo, Y. Wang, J. Lu, X. Wu, S. Xu, P. Dai, Y. Yan, and L. Cheng, "Gsd: View- guided gaussian splatting diffusion for 3d reconstruction," in Proc. Eur. Conf. Comput. Vis., 2024. [139] Y. Jiang, Z. Shen, P. Wang, Z. Su, Y. Hong, Y. Zhang, J. Yu, and L. Xu, "HiFi4g: High- fidelity human performance rendering via compact gaussian splatting," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [140] Y. Wang, Q. Wu, G. Zhang, and D. Xu, "Gscream: Learning 3d geometry and feature consistent gaussian splatting for object removal," in Proc. Eur. Conf. Comput. Vis., 2024. [141] Y. Chen, Z. Chen, C. Zhang, F. Wang, X. Yang, Y. Wang, Z. Cai, L. Yang, H. Liu, and G. Lin, "Gaussianeditor: Swift and scrollable 3d editing with gaussian splatting," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [142] J. Fang, J. Wang, X. Zhang, L. Xie, and Q. Tian, "Gaussianeditor: Editing 3d gaussians delicately with text instructions," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [143] R.- Z. Qiu, G. Yang, W. Zeng, and X. Wang, "Feature splatting: Language- driven physics- based scene synthesis and editing," arXiv preprint arXiv:2404.01223, 2024. [144] Z. Li, Z. Zheng, L. Wang, and Y. Liu, "Animate gaussians: Learning pose- dependent gaussian maps for high- fidelity human avatar modeling," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [145] S. Hu and Z. Liu, "Gauhuman: Articulated gaussian splatting from monocular human videos," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [146] J. Lei, Y. Wang, G. Pavlakos, L. Liu, and K. Daniilidis, "Gart: Gaussian articulated template models," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [147] Y. Yuan, X. Li, Y. Huang, S. De Mello, K. Nagano, I. Kautz, and U. Iqbal, "Gavatar: Animatable 3d gaussian avatars with implicit mesh learning," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [148] Z. Zhou, F. Ma, H. Fan, and Y. Yang, "Headstudio: Text to animatable head avatars with 3d gaussian splatting," in Proc. Eur. Conf. Comput. Vis., 2024. [149] S. Qian, T. Kirschstein, L. Schoneveld, D. Davoli, S. Giebenhain, and M. Nießner, "Gaussianavatars: Photorealistic head avatars with rigged 3d gaussians," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [150] H. Dhamo, Y. Nie, A. Moreau, J. Song, R. Shaw, Y. Zhou, and E. Pérez- Pellitero, "Headgas: Real- time animatable head avatars via 3d gaussian splatting," arXiv preprint arXiv:2312.02902, 2023. [151] J. Li, J. Zhang, X. Bai, J. Zheng, X. Ning, J. Zhou, and L. Gu, "Talkinggaussian: Structure- persistent 3d talking head synthesis via gaussian splatting," in Proc. Eur. Conf. Comput. Vis., 2024. [152] Y. Huang, B. Cui, L. Bai, Z. Guo, M. Xu, and H. Ren, "Endo4dgs: Distilling depth ranking for endoscopic monocular scene reconstruction with 4d gaussian splatting," in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2024. [153] Y. Liu, C. Li, C. Yang, and Y. Yuan, "Endogaussian: Gaussian splatting for deformable surgical scene reconstruction," arXiv preprint arXiv:2401.12561, 2024. [154] L. Zhu, Z. Wang, Z. Jin, G. Lin, and L. Yu, "Deformable endoscopic tissues reconstruction with gaussian splatting," arXiv preprint arXiv:2401.11535, 2024. [155] H. Zhao, X. Zhao, L. Zhu, W. Zheng, and Y. Xu, "Hfgs: 4d gaussian splatting with emphasis on spatial and temporal high- frequency components for endoscopic scene reconstruction," arXiv preprint arXiv:2405.17872, 2024. [156] K. Wang, C. Yang, Y. Wang, S. Li, Y. Wang, Q. Dou, X. Yang, and W. Shen, "Endogslam: Real- time dense reconstruction and tracking in endoscopic surgeries using gaussian splatting," arXiv preprint arXiv:2403.15124, 2024. [157] S. Bonilla, S. Zhang, D. Psychogyios, D. Stoyanov, F. Vasconcelos, and S. Bano, "Gaussian pancakes: Geometrically- regularized 3d gaussian splatting for realistic endoscopic reconstruction," arXiv preprint arXiv:2404.06128, 2024.

[158] K. Wu, K. Zhang, Z. Zhang, S. Yuan, M. Tie, J. Wei, Z. Xu, J. Zhao, Z. Gan, and W. Ding, "Hgs- mapping: Online dense mapping using hybrid gaussian representation in urban scenes," arXiv preprint arXiv:2403.20159, 2024. [159] C. Wu, Y. Duan, X. Zhang, Y. Sheng, J. Ji, and Y. Zhang, "Mmgaussian: 3d gaussian- based multi- modal fusion for localization and reconstruction in unbounded scenes," in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2024. [160] B. Xiong, Z. Li, and Z. Li, "Gauu- scene: A scene reconstruction benchmark on large scale 3d reconstruction dataset using gaussian splattin," arXiv preprint arXiv:2401.14032, 2024. [161] H. Zhao, H. Weng, D. Lu, A. Li, J. Li, A. Panda, and S. Xie, "On scaling up 3d gaussian splattin training," arXiv preprint arXiv:2406.18533, 2024. [162] B. Kerbl, A. Meuleman, G. Kopanas, M. Wimmer, A. Lanvin, and G. Drettakis, "A hierarchical 3d gaussian representation for real- time rendering of very large datasets," ACM Trans. Graph., vol. 44, no. 3, 2024. [163] Y. Liu, H. Guan, C. Luo, L. Fan, J. Peng, and Z. Zhang, "Citygaussian: Real- time high- quality large- scale scene rendering with gaussians," in Proc. Eur. Conf. Comput. Vis., 2024. [164] J. Lin, Z. Li, X. Tang, J. Liu, S. Liu, J. Liu, Y. Lu, X. Wu, S. Xu, Y. Yan et al., "Vastgaussian: Vast 3d gaussians for large scene reconstruction," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [165] K. Ren, L. Jiang, T. Lu, M. Yu, L. Xu, Z. Ni, and B. Dai, "Octreegs: Towards consistent real- time rendering with lod- structured 3d gaussians," arXiv preprint arXiv:2403.17898, 2024. [166] T. Xie, Z. Zong, Y. Qiu, X. Li, Y. Feng, Y. Yang, and C. Jiang, "Physgaussian: Physics- integrated 3d gaussians for generative dynamics," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [167] F. Liu, H. Wang, S. Yao, S. Zhang, J. Zhou, and Y. Duan, "Physics3d: Learning physical properties of 3d gaussians via video diffusion," arXiv preprint arXiv:2406.04338, 2024. [168] P. Borycki, W. Smolak, J. Waczyńska, M. Mazur, S. Tadeja, and P. Spurek, "Gasp: Gaussian splatting for physic- based simulations," arXiv preprint arXiv:2409.05819, 2024. [169] Y. Huang, Y. Zeng, H. Lin, W. Zuo, and R. W. Lau, "Dreamphysics: Learning physical properties of dynamic 3d gaussians with video diffusion priors," in Proc. AAAI Conf. Artif. Intell., 2025. [170] T. Zhang, H.- X. Yu, R. Wu, B. Y. Feng, C. Zheng, N. Snavely, J. Wu, and W. T. Freeman, "Physdreamer: Physics- based interaction with 3d objects via video generation," in Proc. Eur. Conf. Comput. Vis., 2024, pp. 388- 406. [171] Y. Feng, X. Feng, Y. Shang, Y. Jiang, C. Yu, Z. Zong, T. Shao, H. Wu, K. Zhou, C. Jiang et al., "Gaussian splashing: Dynamic fluid synthesis with gaussian splattin," arXiv preprint arXiv:2401.15318, 2024. [172] L. Zhong, H.- X. Yu, J. Wu, and Y. Li, "Reconstruction and simulation of elastic objects with spring- mass 3d gaussians," in Proc. Eur. Conf. Comput. Vis., 2024. [173] Y. Shao, M. Huang, C. C. Loy, and B. Dai, "Gausim: Registering elastic objects into digital world by gaussian simulator," arXiv preprint arXiv:2412.17804, 2024. [174] S. Zhang, H. Zhao, Z. Zhou, G. Wu, C. Zheng, X. Wang, and W. Liu, "Togs: Gaussian splatting with temporal opacity offset for real- time 4d dsa rendering," arXiv preprint arXiv:2403.19586, 2024. [175] R. Wu, Z. Zhang, Y. Yang, and W. Zuo, "Dual- camera smooth zoom on mobile phones," arXiv preprint arXiv:2404.04908, 2024. [176] H. Li, Y. Gao, D. Zhang, C. Wu, Y. Dai, C. Zhao, H. Feng, E. Ding, J. Wang, and J. Han, "Ggrt: Towards generalizable 3d gaussians without pose priors in real- time," arXiv preprint arXiv:2403.10147, 2024. [177] S. Hong, J. He, X. Zheng, H. Wang, H. Fang, K. Liu, C. Zheng, and S. Shen, "Liv- gaussmap: Lidar- inertial- visual fusion for real- time 3d radiance field map rendering," arXiv preprint arXiv:2401.14857, 2024. [178] S. Sun, M. Mielle, A. J. Lilienthal, and M. Magnusson, "High- fidelity slam using gaussian splattin with rendering- guided densification and regularized optimization," in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2024. [179] F. Tosi, Y. Zhang, Z. Gong, E. Sandstrom, S. Mattoccia, M. R. Oswald, and M. Poggi, "How nerfs and 3d gaussian splattin are reshaping slam: a survey," arXiv preprint arXiv:2402.13255, 2024.

[180] T. Deng, Y. Chen, L. Zhang, J. Yang, S. Yuan, D. Wang, and W. Chen, "Compact 3d gaussian splatting for dense visual slam," arXiv preprint arXiv:2403.11247, 2024. [181] J. Hu, X. Chen, B. Feng, G. Li, L. Yang, H. Bao, G. Zhang, and Z. Cui, "Cg- slam: Efficient dense rgb- d slam in a consistent uncertainty- aware 3d gaussian field," arXiv preprint arXiv:2403.16095, 2024. [182] X. Lang, L. Li, H. Zhang, F. Xiong, M. Xu, Y. Liu, X. Zuo, and J. Lv, "Gaussian- lic: Photo- realistic lidar- inertial- camera slam with 3d gaussian splattin," arXiv preprint arXiv:2404.06926, 2024. [183] E. Sandström, K. Tateno, M. Oechsle, M. Niemeyer, L. Van Gool, M. R. Oswald, and F. Tombart, "Splat- slam: Globally optimized rgb- only slam with 3d gaussians," arXiv preprint arXiv:2405.16544, 2024. [184] A. Pumarola, E. Corona, G. Pons- Moll, and F. Moreno- Noguer, "D- nerf: Neural radiance fields for dynamic scenes," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 10318- 10327. [185] K. Park, U. Sinha, P. Hedman, J. T. Barron, S. Bouaziz, D. B. Goldman, R. Martin- Brualla, and S. M. Seitz, "Hypernerf: a higher- dimensional representation for topologically varying neural radiance fields," ACM Trans. Graph., vol. 40, no. 6, pp. 1- 12, 2021. [186] K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman, S. M. Seitz, and R. Martin- Brualla, "Nerfies: Deformable neural radiance fields," in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 5865- 5874. [187] X. Guo, J. Sun, Y. Dai, G. Chen, X. Ye, X. Tan, E. Ding, Y. Zhang, and J. Wang, "Forward flow for novel view synthesis of dynamic scenes," in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 16022- 16033. [188] X. Zhou, Z. Lin, X. Shan, Y. Wang, D. Sun, and M.- H. Yang, "Drivinggaussian: Composite gaussian splattin for surrounding dynamic autonomous driving scenes," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [189] Y. Yan, H. Lin, C. Zhou, W. Wang, H. Sun, K. Zhan, X. Lang, X. Zhou, and S. Peng, "Street gaussians for modeling dynamic urban scenes," in Proc. Eur. Conf. Comput. Vis., 2024. [190] H. Zhou, J. Shao, L. Xu, D. Bai, W. Qiu, B. Liu, Y. Wang, A. Geger, and Y. Liao, "Hugs: Holistic urban 3d scene understanding via gaussian splattin," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024, pp. 21336- 21345. [191] A. Kratimenos, J. Lei, and K. Daniilidis, "Dynmf: Neural motion factorization for real- time dynamic view synthesis with 3d gaussian splattin," arXiv preprint arXiv:2312.00112, 2023. [192] R. Shaw, J. Song, A. Moreau, M. Nazarczuk, S. Catley- Chandar, H. Dhamo, and E. Perez- Pellitero, "Swags: Sampling windows adaptively for dynamic 3d gaussian splattin," arXiv preprint arXiv:2312.13308, 2023. [193] Y. Liang, N. Khan, Z. Li, T. Nguyen- Phuoc, D. Lanman, J. Tompkin, and L. Xiao, "Gaufree: Gaussian deformation fields for real- time dynamic novel view synthesis," arXiv preprint arXiv:2312.11458, 2023. [194] K. Katsumata, D. M. Vo, and H. Nakayama, "An efficient 3d gaussian representation for monocular multi- view dynamic scenes," arXiv preprint arXiv:2311.12897, 2023. [195] Z. Guo, W. Zhou, L. Li, M. Wang, and H. Li, "Motion- aware 3d gaussian splattin for efficient dynamic scene reconstruction," arXiv preprint arXiv:2403.11447, 2024. [196] J. Bae, S. Kim, Y. Yau, H. Lee, G. Bang, and Y. Uh, "Per- gaussian embedding- based deformation for deformable 3d gaussian splattin," arXiv preprint arXiv:2404.03613, 2024. [197] J. Lei, Y. Weng, A. Harley, L. Guibas, and K. Daniilidis, "Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds," arXiv preprint arXiv:2405.17421, 2024. [198] Q. Wang, V. Ye, H. Gao, J. Austin, Z. Li, and A. Kanazawa, "Shape of motion: 4d reconstruction from a single video," arXiv preprint arXiv:2407.13764, 2024. [199] Y. Duan, F. Wei, Q. Dai, Y. He, W. Chen, and B. Chen, "4d- rotor gaussian splattin: towards efficient novel view synthesis for dynamic scenes," in Proc. ACM Spec. Interest Group Comput. Graph. Interact. Tech., 2024, pp. 1- 11. [200] I. Goodfellow, J. Pouget- Abadie, M. Mirza, B. Xu, D. Warde- Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial networks," Communications of the ACM, vol. 63, no. 11, pp. 139- 144, 2020.

[201] J. Ho, A. Jain, and P. Abbeel, "Denoising diffusion probabilistic models," in Proc. Adv. Neural Inf. Process. Syst., 2020, pp. 6840- 6851. [202] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "High- resolution image synthesis with latent diffusion models," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 10684- 10695. [203] L. Zhang, A. Rao, and M. Agrawala, "Adding conditional control to text- to- image diffusion models," in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 3836- 3847. [204] Z. Chen, F. Wang, and H. Liu, "Text- to- 3d using gaussian splatting," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [205] Y. Liang, X. Yang, J. Lin, H. Li, X. Xu, and Y. Chen, "Luciddreamer: Towards high- fidelity text- to- 3d generation via interval score matching," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [206] X. Liu, X. Zhan, J. Tang, Y. Shan, G. Zeng, D. Lin, X. Liu, and Z. Liu, "Humanguassian: Text- driven 3d human generation with gaussian splatting," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [207] X. Yang, Y. Chen, C. Chen, C. Zhang, Y. Xu, X. Yang, F. Liu, and G. Lin, "Learn to optimize denoising scores for 3d generation: A unified and improved diffusion prior on nerf and 3d gaussian splatting," arXiv preprint arXiv:2312.04820, 2023. [208] Z.- X. Zou, Z. Yu, Y.- C. Guo, Y. Li, D. Liang, Y.- P. Cao, and S.- H. Zhang, "Triplane meets gaussian splatting: Fast and generalizable single- view 3d reconstruction with transformers," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [209] H. Ling, S. W. Kim, A. Torralba, S. Fidler, and K. Kreis, "Align your gaussians: Text- to- 4d with dynamic 3d gaussians and composed diffusion models," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [210] J. Ren, L. Pan, J. Tang, C. Zhang, A. Cao, G. Zeng, and Z. Liu, "Dreamgaussian4d: Generative 4d gaussian splatting," arXiv preprint arXiv:2312.17142, 2023. [211] Y. Yin, D. Xu, Z. Wang, Y. Zhao, and Y. Wei, "4dgen: Grounded 4d content generation with spatial- temporal consistency," arXiv preprint arXiv:2312.17225, 2023. [212] J. Zhang, Z. Tang, Y. Pang, X. Cheng, P. Jin, Y. Wei, W. Yu, M. Ning, and L. Yuan, "Repaint123: Fast and high- quality one image to 3d generation with progressive controllable 2d repainting," arXiv preprint arXiv:2312.13271, 2023. [213] Z. Pan, Z. Yang, X. Zhu, and L. Zhang, "Fast dynamic 3d object generation from a single- view video," arXiv preprint arXiv:2401.08742, 2024. [214] D. Xu, Y. Yuan, M. Mardani, S. Liu, J. Song, Z. Wang, and A. Vahdat, "Agg: Amortized generative 3d gaussians for single image to 3d," arXiv preprint arXiv:2401.04099, 2024. [215] C. Yang, S. Li, J. Fang, R. Liang, L. Xie, X. Zhang, W. Shen, and Q. Tian, "Gaussianobject: Just taking four images to get a high- quality 3d object with gaussian splatting," arXiv preprint arXiv:2402.10259, 2024. [216] F. Barthel, A. Beckmann, W. Morgenstern, A. Hilsmann, and P. Eisert, "Gaussian splatting decoder for 3d- aware generative adversarial networks," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Worksh., 2024. [217] L. Jiang and L. Wang, "Brightdreamer: Generic 3d gaussian generative framework for fast text- to- 3d synthesis," arXiv preprint arXiv:2403.11273, 2024. [218] W. Zhuo, F. Ma, H. Fan, and Y. Yang, "Vividdreamer: Invariant score distillation for hyper- realistic text- to- 3d generation," in Proc. Eur. Conf. Comput. Vis., 2024. [219] Z. Wu, C. Yu, Y. Jiang, C. Cao, F. Wang, and X. Bai, "Sc4d: Sparse- controlled video- to- 4d generation and motion transfer," arXiv preprint arXiv:2404.03736, 2024. [220] X. He, J. Chen, S. Peng, D. Huang, Y. Li, X. Huang, C. Yuan, W. Ouyang, and T. He, "Gvgen: Text- to- 3d generation with volumetric representation," in Proc. Eur. Conf. Comput. Vis., 2024. [221] X. Yang and X. Wang, "Hash3d: Training- free acceleration for 3d generation," arXiv preprint arXiv:2404.06091, 2024. [222] J. Kim, J. Koo, K. Yeo, and M. Sung, "Synctweedies: A general generative framework based on synchronized diffusions," arXiv preprint arXiv:2403.14370, 2024. [223] Q. Feng, Z. Xing, Z. Wu, and Y.- G. Jiang, "Fdgaussian: Fast gaussian splatting from single image via geometric- aware diffusion model," arXiv preprint arXiv:2403.10242, 2024.

[224] H. Li, H. Shi, W. Zhang, W. Wu, Y. Liao, L. Wang, L.- h. Lee, and P. Zhou, "Dreamscene: 3d gaussian- based text- to- 3d scene generation via formation pattern sampling," arXiv preprint arXiv:2404.03575, 2024. [225] L. Melas- Kyriazi, I. Laina, C. Rupprecht, N. Neverova, A. Vedaldi, O. Gafni, and F. Kokkinos, "Im- 3d: Iterative multiview diffusion and reconstruction for high- quality 3d generation," in Proc. ACM Int. Conf. Mach. Learn., 2024. [226] B. Zhang, Y. Cheng, J. Yang, C. Wang, F. Zhao, Y. Tang, D. Chen, and B. Guo, "Gaussiancube: Structuring gaussian splatting using optimal transport for 3d generative modeling," arXiv preprint arXiv:2403.19655, 2024. [227] Y.- C. Lee, Y.- T. Chen, A. Wang, T.- H. Liao, B. Y. Feng, and J.- B. Huang, "Vividdreamer: Generating 3d scene with ambient dynamics," arXiv preprint arXiv:2405.20334, 2024. [228] J. Huang and H. Yu, "Point'n move: Interactive scene object manipulation on gaussian splatting radiance fields," arXiv preprint arXiv:2311.16737, 2023. [229] K. Lan, H. Li, H. Shi, W. Wu, Y. Liao, L. Wang, and P. Zhou, "2d- guided 3d gaussian segmentation," arXiv preprint arXiv:2312.16047, 2023. [230] J. Zhuang, D. Kang, Y.- P. Cao, G. Li, L. Lin, and Y. Shan, "Tip- editor: An accurate 3d editor following both text- prompts and image- prompts," arXiv preprint arXiv:2401.14828, 2024. [231] B. Dou, T. Zhang, Y. Ma, Z. Wang, and Z. Yuan, "Cosseggaussians: Compact and swift scene segmenting 3d gaussians," arXiv preprint arXiv:2401.05925, 2024. [232] X. Hu, Y. Wang, L. Fan, J. Fan, J. Peng, Z. Lei, Q. Li, and Z. Zhang, "Semantic anything in 3d gaussians," arXiv preprint arXiv:2401.17857, 2024. [233] F. Palandra, A. Sanchietti, D. Baleri, and E. Rodola, "Gsedit: Efficient text- guided editing of 3d objects via gaussian splatting," arXiv preprint arXiv:2403.05154, 2024. [234] Q. Gu, Z. Lv, D. Frost, S. Green, J. Strasse, and C. Sweeney, "Ego- lifter: Open- world 3d segmentation for egocentric perception," arXiv preprint arXiv:2403.18118, 2024. [235] W. Lyu, X. Li, A. Kundu, Y.- H. Tsai, and M.- H. Yang, "Gaga: Group any gaussians via 3d- aware memory bank," arXiv preprint arXiv:2404.07977, 2024. [236] Z. Liu, H. Ouyang, Q. Wang, K. L. Cheng, J. Xiao, K. Zhu, N. Xue, Y. Liu, Y. Shen, and Y. Cao, "Infusion: Inpainting 3d gaussians via learning depth completion from diffusion prior," arXiv preprint arXiv:2404.11613, 2024. [237] D. Zhang, Z. Chen, Y.- J. Yuan, Y.- L. Zhang, Z. He, S. Shan, and L. Gao, "Stylizedgts: Controllable stylization for 3d gaussian splatting," arXiv preprint arXiv:2404.05220, 2024. [238] Q. Zhang, Y. Xu, C. Wang, H.- Y. Lee, G. Wetzstein, B. Zhou, and C. Yang, "3ditscene: Editing any scene via language- guided disentangled gaussian splatting," arXiv preprint arXiv:2405.18424, 2024. [239] J. Wu, J.- W. Bian, X. Li, G. Wang, I. Reid, P. Torr, and V. A. Prisacariu, "Gaussctrl: Multi- view consistent text- driven 3d gaussian splatting editing," in Proc. Eur. Conf. Comput. Vis., 2024, pp. 55- 71. [240] Y. Wang, X. Yi, Z. Wu, N. Zhao, L. Chen, and H. Zhang, "View- consistent 3d editing with gaussian splatting," in Proc. Eur. Conf. Comput. Vis., 2024, pp. 404- 420. [241] R. Jena, G. S. Iyer, S. Choudhary, B. Smith, P. Chaudhari, and J. Gee, "Splatarmor: Articulated gaussian splatting for animatable humans from monocular rgb videos," arXiv preprint arXiv:2311.10812, 2023. [242] K. Ye, T. Shao, and K. Zhou, "Animatable 3d gaussians for high- fidelity synthesis of human motions," arXiv preprint arXiv:2311.13404, 2023. [243] A. Moreau, J. Song, H. Dhamo, R. Shaw, Y. Zhou, and E. Perez- Pellitero, "Human gaussian splatting: Real- time rendering of animatable avatars," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [244] M. Kocabas, J.- H. R. Chang, J. Gabriel, O. Tuzel, and A. Ranjan, "Hugs: Human gaussian splats," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [245] R. Abdal, W. Yifan, Z. Shi, Y. Xu, R. Po, Z. Kuang, Q. Chen, D.- Y. Yeung, and G. Wetzstein, "Gaussian shell maps for efficient 3d human generation," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [246] S. Zheng, B. Zhou, R. Shao, B. Liu, S. Zhang, L. Nie, and Y. Liu, "Gps- gaussian: Generalizable pixel- wise 3d gaussian splatting

for real- time human novel view synthesis," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [247] L. Hu, H. Zhang, Y. Zhang, B. Zhou, B. Liu, S. Zhang, and L. Nie, "Gaussian avatar: Towards realistic human avatar modeling from a single video via animatable 3d gaussians," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [248] H. Pang, H. Zhu, A. Kortylewski, C. Theobalt, and M. Habermann, "Ash: Animatable gaussian splats for efficient and photoreal human rendering," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [249] Z. Qian, S. Wang, M. Mihailovic, A. Geiger, and S. Tang, "3dgs- avatar: Animatable avatars via deformable 3d gaussian splatting," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [250] H. Jung, N. Brasch, J. Song, E. Perez- Pellitero, Y. Zhou, Z. Li, N. Navab, and B. Busam, "Deformable 3d gaussian splatting for animatable human avatars," arXiv preprint arXiv:2312.15059, 2023. [251] M. Li, J. Tao, Z. Yang, and Y. Yang, "Human101: Training 100+ fps human gaussians in 100s from 1 view," arXiv preprint arXiv:2312.15258, 2023. [252] M. Li, S. Yao, Z. Xie, K. Chen, and Y.- G. Jiang, "Gaussianbody: Clothed human reconstruction via 3d gaussian splatting," arXiv preprint arXiv:2401.09730, 2024. [253] J. Xiang, X. Gao, Y. Guo, and J. Zhang, "Flashavatar: High- fidelity digital avatar rendering at 300fps," arXiv preprint arXiv:2312.02214, 2023. [254] Y. Chen, L. Wang, Q. Li, H. Xiao, S. Zhang, H. Yao, and Y. Liu, "Monogaussian avatar: Monocular gaussian point- based head avatar," arXiv preprint arXiv:2312.04558, 2023. [255] Z. Zhao, Z. Bao, Q. Li, G. Qiu, and K. Liu, "Psavatar: A point- based morphable shape model for real- time head avatar creation with 3d gaussian splatting," arXiv preprint arXiv:2401.12900, 2024. [256] A. Rivero, S. Athar, Z. Shen, and D. Samaras, "Rig3dgs: Creating controllable portraits from casual monocular videos," arXiv preprint arXiv:2402.03723, 2024. [257] H. Luo, M. Ouyang, Z. Zhao, S. Jiang, L. Zhang, Q. Zhang, W. Yang, L. Xu, and J. Yu, "Gaussianhair: Hair modeling and rendering with light- aware gaussians," arXiv preprint arXiv:2402.10483, 2024. [258] Y. Wang, Y. Long, S. H. Fan, and Q. Dou, "Neural rendering for stereo 3d reconstruction of deformable tissues in robotic surgery," in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2022, pp. 431- 441. [259] C. Yang, K. Wang, Y. Wang, X. Yang, and W. Shen, "Neural lerplane representations for fast 4d reconstruction of deformable tissues," in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2023, pp. 46- 56. [260] R. Zha, X. Cheng, H. Li, M. Harandi, and Z. Ge, "Endosurf: Neural surface reconstruction of deformable tissues with stereo endoscope videos," in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv., 2023, pp. 13- 23. [261] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur- Artal, C. Ren, S. Verma et al., "The replica dataset: A digital replica of indoor spaces," arXiv preprint arXiv:1906.05797, 2019. [262] E. Sucar, S. Liu, J. Ortiz, and A. J. Davison, "imap: Implicit mapping and positioning in real- time," in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 6229- 6238. [263] X. Yang, H. Li, H. Zhai, Y. Ming, Y. Liu, and G. Zhang, "Voxfusion: Dense tracking and mapping with voxel- based neural implicit representation," in IEEE International Symposium on Mixed and Augmented Reality, 2022, pp. 499- 507. [264] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald, and M. Pollefeys, "Nice- slam: Neural implicit scalable encoding for slam," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 12786- 12796. [265] M. M. Johari, C. Carta, and F. Fleuret, "Eslam: Efficient dense slam system based on hybrid representation of signed distance fields," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 17408- 17419. [266] E. Sandström, Y. Li, L. Van Gool, and M. R. Oswald, "Point- slam: Dense neural point cloud- based slam," in Proc. IEEE Int. Conf. Comput. Vis., 2023, pp. 18433- 18444. [267] H. Wang, J. Wang, and L. Agapito, "Co- slam: Joint coordinate and sparse parametric encodings for neural real- time slam," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 13293- 13302.

[268] Y. Hu, Y. Fang, Z. Ge, Z. Qu, Y. Zhu, A. Pradhana, and C. Jiang, "A moving least squares material point method with displacement discontinuity and two- way rigid body coupling," ACM Trans. Graph., vol. 37, no. 4, pp. 1- 14, 2018. [269] M. Müller, B. Heidelberger, M. Hennix, and J. Ratcliff, "Position based dynamics," Journal of Visual Communication and Image Representation, vol. 18, no. 2, pp. 109- 118, 2007. [270] A. Knapitsch, J. Park, Q.- Y. Zhou, and V. Koltun, "Tanks and temples: Benchmarking large- scale scene reconstruction," ACM Trans. Graph., vol. 36, no. 4, pp. 1- 13, 2017. [271] T. Zhou, R. Tucker, J. Flynn, G. Fyffe, and N. Snavely, "Stereo magnification: learning view synthesis using multiplane images," ACM Trans. Graph., vol. 37, no. 4, pp. 1- 12, 2018. [272] P. Hedman, J. Philip, T. Price, J.- M. Frahm, G. Drettakis, and G. Brostow, "Deep blending for free- viewpoint image- based rendering," ACM Trans. Graph., vol. 37, no. 6, pp. 1- 15, 2018. [273] B. Middenhall, Y. P. Srinivasan, R. Ortiz- Cayon, N. K. Kalantari, R. Ramamoorthi, R. Ng, and A. Kar, "Local light field fusion: Practical view synthesis with prescriptive sampling guidelines," ACM Trans. Graph., vol. 38, no. 4, pp. 1- 14, 2019. [274] A. Liu, R. Tucker, V. Jampani, A. Makadia, N. Snavely, and A. Kanazawa, "Infinite nature: Perpetual view generation of natural scenes from a single image," in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 14458- 14467. [275] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers, "A benchmark for the evaluation of rgb- d slam systems," in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2012, pp. 573- 580. [276] A. Geiger, P. Lenz, and R. Urtasun, "Are we ready for autonomous driving? the kitti vision benchmark suite," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 3354- 3361. [277] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner, "Scannet: Richly- annotated 3d reconstructions of indoor scenes," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 5828- 5839. [278] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine et al., "Scalability in perception for autonomous driving: Waymo open dataset," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2020, pp. 2446- 2454. [279] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, "nuscenes: A multimodal dataset for autonomous driving," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2020, pp. 11621- 11631. [280] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, "Rlbench: The robot learning benchmark & learning environment," IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 3019- 3026, 2020. [281] A. Mandlekar, D. Xu, J. Wong, S. Nusiriany, C. Wang, R. Kulkarni, L. Fei- Fei, S. Savarese, Y. Zhu, and R. Martin- Martin, "What matters in learning from offline human demonstrations for robot manipulation," in Proc. Annu. Conf. Robot Learn., 2022, pp. 1678- 1690. [282] Z. Yan, C. Li, and G. H. Lee, "Nerf- ds: Neural radiance fields for dynamic specular objects," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 8285- 8295. [283] K. Kania, K. M. Yi, M. Kowalski, T. Trzumski, and A. Tagliasacchi, "Conerf: Controllable neural radiance fields," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 18623- 18632. [284] A. Mirzaei, T. Aumentado- Armstrong, K. G. Derpanis, J. Kelly, M. A. Brubaker, I. Gilitschenski, and A. Levinhtein, "Spin- nerf: Multiview segmentation and perceptual inpainting with neural radiance fields," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 20669- 20679. [285] R. Shao, Z. Zheng, H. Tu, B. Liu, H. Zhang, and Y. Liu, "Tensor4d: Efficient neural 4d decomposition for high- fidelity dynamic reconstruction and rendering," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 16632- 16642. [286] T. Wu, J. Zhang, X. Fu, Y. Wang, J. Ren, L. Pan, W. Wu, L. Yang, J. Wang, C. Qian et al., "Omnioject3d: Large- vocabulary 3d object dataset for realistic perception, reconstruction and generation," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 803- 814. [287] M. Deitke, D. Schwenk, J. Salvado, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi, "Objaverse: A universe of annotated 3d objects," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 13142- 13153. [288] T. Alldieck, M. Magnor, W. Xu, C. Thorobalt, and G. Pons- Moll,

"Video based reconstruction of 3d people models," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 8387- 8397. [289] D. Cudeiro, T. Bolkart, C. Laidlaw, A. Ranjan, and M. J. Black, "Capture, learning, and synthesis of 3d speaking styles," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 10101- 10111. [290] Z. Zheng, T. Yu, Y. Wei, Q. Dai, and Y. Liu, "Deephuman: 3d human reconstruction from a single image," in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 7739- 7749. [291] T. Yu, Z. Zheng, K. Guo, P. Liu, Q. Dai, and Y. Liu, "Function4d: Real- time human volumetric capture from very sparse consumer rgbd sensors," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 5746- 5756. [292] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, "Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 9054- 9063. [293] E. Ramon, G. Triginer, J. Escur, A. Pumarola, J. Garcia, X. Giro- i Nieto, and F. Moreno- Noguer, "H3d- net: Few- shot high- fidelity 3d head reconstruction," in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 5620- 5629. [294] Z. Su, T. Yu, Y. Wang, and Y. Liu, "Deepcloth: Neural garment representation for shape and style editing," IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 2, pp. 1581- 1593, 2022. [295] M. Allan, J. Mcleod, C. Wang, J. C. Rosenthal, Z. Hu, N. Gard, P. Eisert, K. X. Fu, T. Zeffiro, W. Xia et al., "Stereo- correspondence and reconstruction of endoscopic data challenge," arXiv preprint arXiv:2101.01133, 2021. [296] Y. Cai, J. Wang, A. Yuille, Z. Zhou, and A. Wang, "Structure- aware sparse- view x- ray 3d reconstruction," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024, pp. 11174- 11183. [297] Y. Xiangli, L. Xu, X. Pan, N. Zhao, A. Rao, C. Theobalt, B. Dai, and D. Lin, "Bungeeener: Progressive neural radiance field for extreme multi- scale scene rendering," in Proc. Eur. Conf. Comput. Vis. Springer, 2022, pp. 106- 122. [298] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. P. Srinivasan, J. T. Barron, and H. Kretzschmar, "Block- nerf: Scalable large scene neural view synthesis," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 8248- 8258. [299] G. Yang, F. Xue, Q. Zhang, K. Xie, C.- W. Fu, and H. Huang, "Urbanbis: a large- scale benchmark for fine- grained urban building instance segmentation," in Proc. ACM Spec. Interest Group Comput. Graph. Interact. Tech., 2023, pp. 1- 11. [300] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, "Image quality assessment: from error visibility to structural similarity," IEEE Trans. Image Process., vol. 13, no. 4, pp. 600- 612, 2004. [301] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, "The unreasonable effectiveness of deep features as a perceptual metric," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 586- 595. [302] J. Fang, T. Yi, X. Wang, L. Xie, X. Zhang, W. Liu, M. Nießner, and Q. Tian, "Fast dynamic radiance fields with time- aware neural voxels," in SIGGRAPH Asia, 2022, pp. 1- 9. [303] A. Cao and J. Johnson, "Hexplane: A fast representation for dynamic scenes," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 130- 141. [304] F. Wang, Z. Chen, G. Wang, Y. Song, and H. Liu, "Masked space- time hash encoding for efficient dynamic scene reconstruction," in Proc. Adv. Neural Inf. Process. Syst., 2023. [305] C.- Y. Weng, B. Curfiers, F. P. Srinivasan, J. T. Barron, and I. Kemelmacher- Shlizerman, "Humanner: Free- viewpoint rendering of moving people from monocular video," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 16210- 16220. [306] C. Geng, S. Peng, Z. Xu, H. Bao, and X. Zhou, "Learning neural volumetric representations of dynamic humans in minutes," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 8759- 8770. [307] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, "Animatable neural radiance fields for modeling dynamic human bodies," in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 14314- 14323. [308] A. Yu, V. Ye, M. Tancik, and A. Kanazawa, "pixelnerf: Neural radiance fields from one or few images," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 4578- 4587. [309] Y. Kwon, D. Kim, D. Ceylan, and H. Fuchs, "Neural human performer: Learning generalizable radiance fields for human performance rendering," in Proc. Adv. Neural Inf. Process. Syst., 2021, pp. 24741- 24752.

[310] J. Wang, Z. Zhang, Q. Zhang, J. Li, J. Sun, M. Sun, J. He, and R. Xu, "Query- based semantic gaussian field for scene representation in reinforcement learning," arXiv preprint arXiv:2406.02370, 2024. [311] Y. Qu, S. Dai, X. Li, J. Lin, L. Cao, S. Zhang, and R. Ji, "Goi: Find 3d gaussians of interest with an optimizable open- vocabulary semantic- space hyperplane," arXiv preprint arXiv:2405.17596, 2024. [312] Y. Ji, H. Zhu, J. Tang, W. Liu, Z. Zhang, Y. Xie, L. Ma, and X. Tan, "Fastligs: Speeding up language embedded gaussians with feature grid mapping," arXiv preprint arXiv:2406.01916, 2024. [313] G. Liao, J. Li, Z. Bao, X. Ye, J. Wang, Q. Li, and K. Liu, "Clip- gs: Clip- informed gaussian splatting for real- time and view- consistent 3d semantic understanding," arXiv preprint arXiv:2404.14249, 2024. [314] S. Choi, H. Song, J. Kim, T. Kim, and H. Do, "Click- gaussian: Interactive segmentation to any 3d gaussians," arXiv preprint arXiv:2407.11793, 2024. [315] S. Ji, G. Wu, J. Fang, J. Cen, T. Yi, W. Liu, Q. Tian, and X. Wang, "Segment any 4d gaussians," arXiv preprint arXiv:2407.04504, 2024. [316] A. Guedon and V. Lepetit, "Sugar: Surface- aligned gaussian splatting for efficient 3d mesh reconstruction and high- quality mesh rendering," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2024. [317] T. Liu, G. Wang, S. Hu, L. Shen, X. Ye, Y. Zang, Z. Cao, W. Li, and Z. Liu, "Fast generalizable gaussian splatting reconstruction from multi- view stereo," in Proc. Eur. Conf. Comput. Vis., 2024. [318] Y. Li, X. Fu, S. Zhao, R. Jin, and S. K. Zhou, "Sparse- view ct reconstruction with 3d gaussian volumetric representation," arXiv preprint arXiv:2312.15676, 2023. [319] Y. Cai, Y. Liang, J. Wang, A. Wang, Y. Zhang, X. Yang, Z. Zhou, and A. Yuille, "Radiative gaussian splatting for efficient x- ray novel view synthesis," in Proc. Eur. Conf. Comput. Vis., 2024. [320] J. Chang, Y. Xu, Y. Li, Y. Chen, and X. Han, "Gaussreg: Fast 3d registration with gaussian splatting," in Proc. Eur. Conf. Comput. Vis., 2024.