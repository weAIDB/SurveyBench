# A Survey of Visual Transformers

Yang Liu, Yao Zhang, Yixin Wang, Feng Hou, Jin Yuan, Jiang Tian, Yang Zhang, Zhongchao Shi, Jianping Fan, Zhiqiang He

Abstract—Transformer, an attention- based encoder- decoder model, has already revolutionized the field of natural language processing (NLP). Inspired by such significant achievements, some pioneering works have recently been done on employing Transformer- liked architectures in the computer vision (CV) field, which have demonstrated their effectiveness on three fundamental CV tasks (classification, detection, and segmentation) as well as multiple sensory data stream (images, point clouds, and vision- language data). Because of their competitive modeling capabilities, the visual Transformers have achieved impressive performance improvements over multiple benchmarks as compared with modern Convolution Neural Networks (CNNs). In this survey, we have reviewed over one hundred of different visual Transformers comprehensively according to three fundamental CV tasks and different data stream types, where a taxonomy is proposed to organize the representative methods according to their motivations, structures, and application scenarios. Because of their differences on training settings and dedicated vision tasks, we have also evaluated and compared all these existing visual Transformers under different configurations. Furthermore, we have revealed a series of essential but unexploited aspects that may empower such visual Transformers to stand out from numerous architectures, e.g., slack high- level semantic embeddings to bridge the gap between the visual Transformers and the sequential ones. Finally, two promising research directions are suggested for future investment. We will continue to update the latest articles and their released source codes at https://github.com/liuyang- ici/awesome- visual- transformers.

Index Terms—Visual Transformer, attention, high- level vision, 3D point clouds, multi- sensory data stream, visual- linguistic pretraining, self- supervision, neural networks, computer vision.

## 1 INTRODUCTION

T RANSFORMER [1], which adopts an attention- based structure, has first demonstrated its tremendous effects on the tasks of sequence modeling and machine translation. As illustrated in Fig. 1, Transformers have gradually emerged as the predominant deep learning models for many NLP tasks. The most recent dominant models are the selfsupervised Transformers, which are pre- trained over sufficient datasets and then fine- tuned over a small sample set for a given downstream task [2]- [9]. The Generative Pre- trained Transformer (GPT) families [2]- [4] leverage the Transformer decoders to enable auto- regressive language modeling, while the Bidirectional Encoder Representations from Transformers

(BERT) [5] and its variants [6], [7] serve as auto- encoder language models built on the Transformer encoders.

In the CV field, prior to the visual Transformers, Convolution Neural Networks (CNNs) have emerged as a dominant paradigm [10]- [12]. Inspired by the great success of such self- attention mechanisms for the NLP tasks [1], [13], some CNN- based models attempted to capture the long- range dependencies through adding a self- attention layer at either spatial level [14]- [16] or channel level [17]- [19], while others try to replace the traditional convolutions entirely with the global [20] or local self- attention blocks [21]- [27]. Although Ramachandr et al. have demonstrated the efficiency of selfattention block [24] without the help from CNNs, such pure attention model is still inferior to the State- of- The- Art (SoTA) CNN models on the prevailing benchmarks.

With the grateful achievements of linguistic Transformers and the rapid development of visual attention- based models, numerous recent works have migrated the Transformers to the CV tasks, and some comparable results have been achieved. Cordonnier et al. [28] theoretically demonstrated the equivalence between multi- head self- attention and CNNs, and they designed a pure Transformer by using patch downsampling and quadratic position encoding to verify their theoretical conclusion. Dosovitskiy et al. [29] further extended such a pure Transformer for large- scale pre- training, which has achieved SoTA performance over many benchmarks. Additionally, the visual Transformers have also obtained great performances for other CV tasks, such as detection [30], segmentation [31], [32], tracking [33], generation [34], and enhancement [35].

As illustrated in Fig. 1, following the pioneer works [29], [30], hundreds of Transformer- based models have been proposed for various vision applications within the last year. Thus, a systematic literature survey is strongly desired to identify, categorize, and evaluate these existing visual Transformers. Considering that the readers may come from different areas, we review all these visual Transformers according to three fundamental CV tasks (i.e., classification, detection, and segmentation) and different types of data streams (i.e., images, point clouds, multi- stream data). As illustrated in Fig. 3, this survey categorizes all these existing methods into multiple groups according to their dedicated vision tasks, data stream types, motivations, and structural characteristics.

Before us, several reviews on the Transformers have been published, where Tay et al. [46] reviewed the efficiency of the linguistic Transformers, Khan et al. [47] and Han et al. [48] summarized the early visual Transformers and attention- based models. The most recent review of the Transformers is introduced by Lin et al., which provides a systematic review of various Transformers, but they only mention vision applications sketchily [49]. Distinctively, this paper aims to provide

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-11/e729979e-6782-440e-aa44-610b9edd5863/b16b959ec06bce4ec28118cbcad7154da5732becb89df21006297789a9359f72.jpg)  
Fig. 1. Odyssey of Transformer application & Growth of both Transformer [1] and VIT [29] citations according to Google Scholar. (Upper Left) Growth of Transformer citations in multiple conference publication including: NIPS, ACL, ICML, IJCAI, ICLR, and ICASSP. (Upper Right) Growth of VIT citations in Arxiv publications. (Bottom Left) Odyssey of language model [1]–[8]. (Bottom Right) Odyssey of visual Transformer backbone where the black [29], [36]–[40] is the SoTA with external data and the blue [41]–[45] refers to the SoTA without external data (best viewed in color).

more comprehensive review of the most- recently visual Transformers and categorize them systematically:

(1) Comprehensiveness & Readability. This paper comprehensively reviews over a hundred visual Transformers according to their applications on three fundamental CV tasks (i.e., classification, detection, and segmentation) and different types of data streams (i.e., image, point clouds, multi-stream data). We select more representative methods with detailed descriptions and analyses, but introduce other related works briefly. In addition to analysing each model independently, we also build their internal connections from certain senses such as progressive, contrastive, and multi-view analysis.

(2) Intuitive Comparison. As these existing visual Transformers follow different training schemes and hyper-parameter settings for various vision tasks, this survey presents multiple lateral comparisons over different datasets and restrictions. More importantly, we summarize a series of promising components designed for each task, including: (a) shallow local convolution with hierarchical structure for backbone; (b) spatial prior acceleration with sparse attention for neck detector; and (c) general-purpose mask prediction scheme for segmentation.

(3) In-depth Analysis. We further provide well-thought insights from the following aspects: (a) How visual Transformers bridge the traditional sequential tasks to the visual ones (Why does Transformer work effectively in CV); (b) the correspondence between the visual Transformers and other neural networks; (c) the double edges of the visual Transformers; and (d) the correlation of the learnable embeddings (i.e., class token, object query, mask embedding) adopted in different tasks and data stream types. Finally, we outline some future research directions. For example, the encoder-decoder Transformer backbone can unify multiple visual tasks and data stream types through query embeddings.

The rest of this paper is organized as follows. An overview of the architectures and the critical components for the vanilla sequential Transformers are introduced in Sec. II. A comprehensive taxonomy for the Transformer backbones is summarized in Sec. III with a brief discussion of their applications for image classification. We then review contemporary Trans former detectors, including Transformer necks and backbones in Sec. IV. Sec. V clarifies the mainstream and its variants for the visual Transformers in the segmentation field according to their embedding forms (i.e., patch embedding and query embedding). Sec. III- V also briefly analyzes a specific aspect of their corresponding fields with performance evaluation. In addition to 2D visual recognition, Sec. VI briefly introduces the recently- developed 3D visual recognition from the perspective of point clouds. Sec. VII further overviews the fusion approaches within the visual Transformers for multiple data stream types (e.g., multi- view, multi- modality, visual- linguistic pre- training, and visual grounding). Finally, Sec. VIII provides three aspects for further discussion and points out some promising research directions for future investment.

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-11/e729979e-6782-440e-aa44-610b9edd5863/52860cfcc71485b8e2f21ea12941036f5441feb3a28c64cf84e639bb892b5458.jpg)  
Fig. 2. The structure of the attention layer. Left: Scaled Dot-Product Attention. Right: Multi-Head Attention Mechanism.

## 2 ORIGINAL TRANSFORMERS

The original Transformer [1] is first applied to the task for sequence- to- sequence auto- regression. Compared with previous sequence transduction models [50], [51], such original Transformer inherits the encoder- decoder structure but discards the recurrence and convolutions entirely by using multi- head attention mechanisms and point- wise feed- forward networks. In the following sub- sections, we will provide an architectural overview of the original Transformers and describe their four key components.

### 2.1 (Multi-Head) Attention Mechanism

The mechanism with one single head attention can be grouped into two parts: 1) A transformation layer maps the input sequences  $X \in \mathbb{R}^{n_x \times d_x}$ ,  $Y \in \mathbb{R}^{n_y \times d_y}$  into three different vectors (query  $Q$ , key  $K$ , and value  $V$ ), where  $n$  and

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-11/e729979e-6782-440e-aa44-610b9edd5863/621d5234843fe9705a3aa16d7be0adeaf817accca6e956e4ca5ba96d2dfe46c4.jpg)  
Fig. 4. The overall architecture of Transformer [1]. The 2D lattice represents the each states of queries during training (best viewed in color).

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-11/e729979e-6782-440e-aa44-610b9edd5863/f8f8563f792ef1976fc93597e880997cc51aa83ec380738d1a9a1639d60af6b4.jpg)  
Fig. 3. Taxonomy of Visual Transformers

$d$  are the length and the dimension of the inputs, respectively. 2) An attention layer, as shown in Fig. 2, explicitly aggregates the query with the corresponding key, assigns them to the value and updates the output vector.

The formula for the transformation layer is defined as

$$
Q = XW^{Q},\quad K = YW^{K},\quad V = YW^{V}, \tag{1}
$$

where  $W^{Q}\in \mathbb{R}^{d_{x}\times d^{k}}$ $W^{K}\in \mathbb{R}^{d_{y}\times d^{k}}$  , and  $W^{V}\in \mathbb{R}^{d_{y}\times d^{v}}$  are linear matrices.  $d^{k}$  and  $d^{v}$  are the dimension of the querykey pair and the value which are projected from  $Y$  and  $X$  respectively. Such two sequence inputs are referred as the cross- attention mechanism. It can also be regarded as a selfattention when  $Y = X$  .In form, self- attention is applied to both Transformer encoder and decoder, while the crossattention severs as a junction within the decoder.

Then, the scale- dot attention mechanism is formulated as

$$
\mathrm{Attention}(Q,K,V) = \mathrm{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V, \tag{2}
$$

where the attention weights are generated by a dot- product operation between  $Q$  and the  $K$  a scaling factor  $\sqrt{d_k}$  and a softmax operation are supplied to translate the attention weights into a normalized distribution. The resulting weights are assigned to the corresponding value elements, thereby yielding the final output vector.

Due to the restricted feature subspace, the modeling capability of the single- head attention block is quite coarse. To tackle this issue, as shown in Fig. 2, a Multi- Head Self- Attention mechanism (MHSA) is proposed to linearly project the input into multiple feature sub- spaces and process them by using several independent attention heads (layers) parallelly. The resulting vectors are concatenated and mapped to the final outputs. The process of MHSA can be formulated as

$$
\begin{array}{rl} & Q_{i} = XW^{Q_{i}},K_{i} = XW^{K_{i}},V_{i} = XW^{V_{i}},\\ & Z_{i} = \mathrm{Attention}(Q_{i},K_{i},V_{i}),i = 1\ldots h,\\ & \mathrm{MultiHead}(Q,K,V) = \mathrm{Concat}(Z_{1},Z_{2},\ldots,Z_{h})W^{O}, \end{array} \tag{3}
$$

where  $h$  is the head number,  $W^{O}\in \mathbb{R}^{d_{v}\times d_{model}}$  denotes the

output projected matrix,  $Z_{i}$  denotes the output vector of each head,  $W^{Q_{i}} \in \mathbb{R}^{d_{model} \times d_{k}}$ ,  $W^{K_{i}} \in \mathbb{R}^{d_{model} \times d_{k}}$ , and  $W^{V_{i}} \in \mathbb{R}^{d_{model} \times d_{v}}$  are three different groups of matrices. Multi- head attention separates the inputs into  $h$  independent attention heads with  $d_{model} / h$ - dimensional vectors, and integrates each head features dependently. Without extra costs, multi- head attention enriches the diversity of the feature subspaces.

### 2.2 Position-wise Feed-Forward Networks

The output of MHSA is then fed into two successive feedforward networks (FFN) with a ReLU activation as

$$
\mathrm{FFN}(x) = \mathrm{ReLU}(W_1x + b_1)W_2 + b_2. \tag{4}
$$

This position- wise feed- forward layer can be viewed as a point- wise convolution, which treats each position equally but uses different parameters between each layer.

### 2.3 Positional Encoding

Since the Transformer/Attention operates on the input embedding simultaneously and identically, the order of the sequence is neglected. To make use of the sequential information, a common solution is to append an extra positional vector to the inputs, hence term the "positional encoding". There are many choices for positional encoding. For example, a typical choice is cosine functions with different frequencies as

$$
\begin{array}{c}{PE_{(pos,i)} = \left\{ \begin{array}{ll}\sin (pos\cdot \omega_k) & \mathrm{if} i = 2k\\ \cos (pos\cdot \omega_k) & \mathrm{if} i = 2k + 1, \end{array} \right.}\\ {\omega_k = \frac{1}{10000^{2k / d}},\quad k = 1,\dots ,d / 2,} \end{array} \tag{5}
$$

where pos and  $d$  are the position and the length of the vector, respectively, and  $i$  is the index of each element within vector.

### 2.4 Transformer Model

Fig. 4 shows the overall Transformer models with the encoder- decoder architecture. Specifically, it consists of  $N$  successive encoder blocks, each of which is composed of two sub- layers. 1) An MHSA layer aggregates the relationship within the encoder embeddings. 2) A position- wise FFN layer extracts feature representations. For the decoder, it also involves  $N$  consecutive blocks that follow a stack of the encoders. Compared with the encoder, each decoder block appends to a multi- head cross- attention layer to aggregate both decoder embeddings and encoder outputs, where  $Y$  corresponds to the former, and  $X$  is the latter as shown in Eq. (1). Moreover, all of the sub- layers in both encoder and decoder employ a residual connection [11] and a Layer Normalization [163] to enhance the scalability of the Transformer. In order to record the sequential information, each input embedding is attached with a positional encoding at the beginning of the encoder stack and the decoder stack. Finally, a softmax operation are used for predicting the next word.

In an auto- regressive language model, the Transformer is originated from the machine translation tasks. Given a sequence of words, the Transformer vectorizes the input sequence into the word embeddings, adds the positional encodings, and feeds the resulting sequence of the vectors into an encoder. During training, as illustrated in Fig. 4, Vaswani et al. design a masking operation according to the rule for the autoregressive task [1], where the current position only depends on the outputs of the previous positions. Based on this masking, the Transformer decoder is able to process the sequence of the input labels parallelly. During the inference time, the sequence of the previously- predicted words is processed by the same operation to predict the next word.

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-11/e729979e-6782-440e-aa44-610b9edd5863/2e64f5147519b5d03355879d852425f3a2659975bd0f69d272474476189a61f8.jpg)  
Fig. 5. Taxonomy of Visual Transformer Backbone (best viewed in color).

## 3 TRANSFORMER FOR CLASSIFICATION

Following the prominent developments of the Transformers in NLP [2]–[5], recent works attempt to introduce visual Transformers for image classification. This section comprehensively reviews over 40 visual Transformers and groups them into six categories, as shown in Fig. 5. We start with introducing the Fully- Attentional Network [24], [28] and the Vision Transformer (ViT) [29], such Original Visual Transformer first demonstrates its efficacy on multiple classification benchmarks. Then we discuss Transformer Enhanced CNN methods that utilize Transformer to enhance the representation learning in CNNs. Due to the negligence of local information in the original ViT, the CNN Enhanced Transformer employs an appropriate convolutional inductive bias to augment the visual Transformer, while the Local Attention Enhanced Transformer redesigns patch partition and attention blocks to improve their locality. Following the hierarchical and deep structures in CNNs [164], the Hierarchical Transformer replaces the fixed- resolution columnar structure with a pyramid stem, while the Deep Transformer prevents the attention map from oversmooth and increases its diversity in the deep layer. Moreover, we also review the existing visual Transformers with Self- Supervised Learning. Finally, we make a brief discussion based on intuitive comparisons for further investigation. More visual Transformers' milestones are introduced in App. A.

### 3.1 Original Visual Transformer

Inspired by the tremendous achievements of the Transformers in the NLP field [2]–[5], the previous technology

trends for the vision tasks [14]- [17], [165] incorporate the attention mechanisms with the convolution models to augment the models' receptive field and global dependency.

Beyond such hybrid models, Ramachandran et al. contemplate whether the attention can completely replace the convolution, and then present a Stand- Alone self- attention network (SANet) [24], which has achieved superior performance on the vision tasks as compared with the original baseline. Given a ResNet [11] architecture, the authors straightforwardly replace the spatial convolution layer  $3\times 3$  kernel) in each bottleneck block with a locally spatial self- attention layer and keep other structures the same as the original setting in ResNet. Moreover, lots of ablations have shown that the positional encodings and convolutional stem can further improve the network efficacy.

Following [24], Cordonnier et al. pioneer a prototype design called Fully- Attentional Network in their original paper) [28], including a fully vanilla Transformer and a quadratic positional encoding. The authors also theoretically prove that a convolutional layer can be approximated by a single MHSA layer with relative positional encoding and sufficient heads. With the ablations on CIFAR- 10 [166], they further verify that such a prototype design does learn to attend a grid- like pattern around each query pixel, as their theoretical conclusion.

Different from [28] that only focuses on lite scale model, the Vision Transformer (ViT) [29] further explores the effectiveness of the vanilla Transformer with large- scale pre- trained learning, and such a pioneer work impacts the community significantly. Because the vanilla Transformers only accept the sequential inputs, the input image in ViT is firstly split into a series of non- overlapped patches and they are then projected into patch embeddings. Then a 1D learnable positional encoding is added on the patch embeddings to retain the spatial information, and the joint embeddings are then fed into the encoder, as shown in Fig. 6. Similar to BERT [5], a learned [class] token is attached with the patch embeddings to aggregate the global representation and it serves as the final output for classification. Moreover, a 2D interpolation complements the pre- trained positional encoding to maintain the consistent order of the patches when the feeding images are in arbitrary resolution. By pre- training with a large- scale private dataset (JFT- 300M [167]),ViT has achieved similar or even superior results on multiple image recognition benchmarks (ImageNet [168] and CIFAR- 100 [166]) as compared with the most prevailing CNNs methods. However, its generalization capability tends to be eroded with limited training data.

### 3.2 Transformer-enhanced CNNs

As described in Section II, the Transformer has two keys: MHSA and FFN. There exists an approximation between the convolutional layer and the MHSA [28], and Dong et al. suggest that the Transformer can further mitigate the strong bias of MHSA with the help of skip connections and FFN [169]. Recently, some methods attempt to integrate the Transformer into CNNs to enhance representation learning. VTs [52] decouples semantic concepts for the input image into different channels and relates them densely through the encoder block, namely VT- block. Such VT- block substitutes the last convolution stage to enhance the CNN model's ability on semantic modelling. Unlike previous approaches that directly replace convolution with attention structure, Vaswani et al. propose a conceptual redefinition that the successive bottleneck blocks with MHSA can be formulated as the Bottleneck Transformer (BoTNet) [53] blocks. The relative position encoding [170] is adopted to further mimic the original Transformer. Based on ResNet [11], BoTNet outperforms the most CNN models with similar parameter settings on the ImageNet benchmark and further demonstrates the efficacy of hybrid models.

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-11/e729979e-6782-440e-aa44-610b9edd5863/bdedbb1140e0abf71518f9efc3583724ded8352c950a39769f142c0387c61cc2.jpg)  
Fig. 6. Illustration of ViT. The flatten image patches with an additional class token are fed into the vanilla Transformer encoder after positional encoding. Only the class token can be predicted for classification. (from [29].)

### 3.3 CNN-enhanced Transformer

Inductive bias is defined as a set of assumptions on data distribution and solution space, whose manifestations within convolution are the locality and the translation invariance [171]. As the covariance within local neighborhoods is large and tends to be gradually stationary across an image, CNNs can process an image effectively with the help of the biases. Nevertheless, strong biases also limit the upper bound of CNNs when sufficient data are available. Recent efforts attempt to leverage an appropriate CNN bias to enhance Transformer.

Touvron et al. propose a Data- efficient image Transformer (DeiT) [41] to moderate the ViT's dependence on large datasets. In addition to the existing strategies for data augmentation and regularization, a teacher- student distillation strategy is applied for auxiliary representation learning. The student model is the ViT, where a distilled token is attached to the patch embeddings and it is supervised by the pseudo labels from the teacher model. Extensive experiments have demonstrated that CNN is a better teacher model than the Transformer. Surprisingly, the distilled student Transformer even outperforms its teacher CNN model. These observations are explained in [172]: the teacher CNN transfers its inductive bias in a soft way to the student Transformer through knowledge distillation. Based on ViT's architecture, DeiT- B attains the top- 1 accuracy of  $85.2\%$  without external data. ConViT [54] appends a parallel convolution branch with vanilla Transformer to impose inductive biases softly. The main idea of the convolution branch is a learnable embedding that is first initialized to approximate the locality as similar as the convolution and then explicitly gives each attention head freedom to escape the locality by adjusting a learned gating parameter. CeiT [55] and LocalViT [56] extract the locality by directly adding a depth- wise convolution in FFN.

As point- wise convolution is equal to position- wise FFN, they extend FFN to an inverted residual block [173] to build a depth- wise convolutional framework. Based on the assumption of positional encoding [58] and the observation in [174], ResT [58] and CPVT [57] try to adapt the inherent positional information of the convolution to the arbitrary size of inputs instead of interpolating the positional encoding. Including CvT [37], these methods replace the linear patch projection and positional encoding with the convolution stacks. Both methods benefit from such convolutional position embedding, especially for small model.

Besides the "internal" fusion, many approaches focus on an "apparent" combination according to different visual Transformer's structures. For standard columnar structure, Xiao et al. substitute the original patchify stem (single non- overlapped large kernel) with several stacked stride-  $23\times 3$  kernels [59]. Such a Convolutional Stem significantly improves ViT by 1-  $2\%$  on accuracy for ImageNet- 1k and facilitates its stability and generalization for the downstream tasks. For hierarchical structures, Dai et al. [40] investigate an optimal combination of hybrid models to benefit the performance trade- off. By comparing a series of hybrid models, they propose a Convolution and Attention Network (CoAtNet) to leverage the strength of both CNNs and Transformer. The authors observe that depthwise convolution can be naturally integrated into the attention block, and stacking convolution vertically in the shallow layer is more effective than the original hierarchical methods. It has achieved the SoTA performance across multiple datasets.

### 3.4 Local Attention Enhanced Transformer

The coarse patchify process in ViT [29] neglects the local image information. In addition to adding CNNs, various local attention mechanisms are proposed to dynamically attend the neighbour elements and augment the local extraction ability.

One of the representative methods is the Shifted windows (Swin) Transformer [36]. Similar to TSM [175] (Fig. 7(a)), Swin utilizes a shifted window along the spatial dimension to model the global and boundary features. In detail, two successive window- wise attention layers can facilitate the cross- window interactions (Fig. 7(b)- (c)), similar to the receptive field expansion in CNNs. Such operation also reduces the computational complexity from  $O(2n^{2}C)$  to  $O(4M^{2}nC)$  in one attention layer, where  $n$  and  $M$  denote the patch length and the window size, respectively. Swin Transformer achieves  $84.2\%$  accuracy on ImageNet and the latest SoTA on multiple dense prediction benchmarks (see Sec. IV- B).

Inspired by [176], Han et al. leverage a Transformer- iNTransformer (TNT) [60] model to aggregate both patch- and pixel- level representations. Each layer of TNT consists of two successive blocks, an inner block models the pixel- wise interaction within each patch, and an outer block extracts the global information. Twins [61] employs a spatially separable self- attention mechanism, similar to depth- wise convolution [173] or window- wise TNT [60], to model the local- global representation. Another separate form is ViL [62], which replaces the single class token with a series of local embeddings (termed as global memory). These local embeddings only perform an inner attention and an interaction with their corresponding 2D spatial neighbors. VOLO [45] proposes an outlook attention, which is similar to a patch- wise dynamic convolution, to focus on the finer- level features, including three operations: unfold, linear- wights attention, and refold. Based on [44], it achieves SoTA results on ImageNet without external data.

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-11/e729979e-6782-440e-aa44-610b9edd5863/75edce0ad418b88bd38eefb5a0e01d9bbc74f8cf5c480f4978a444b5821a1358.jpg)  
Fig. 7. An overview of Swin Transformer and TSM. (a) TSM with bidirection and uni-direction operation. (b) The shifted window method. (c) Two successive Transformer blocks of Swin Transformer. The regular and shifted window correspond to W-MSA and SW-MSA, respectively. (from [36], [175]).

### 3.5 Hierarchical Transformer

Due to the columnar structure of ViT [29] with a fixed convolution across the entire Transformer layers, it loses the fine- grained features and suffers from heavy computational costs. Followed by the hierarchical models, Tokens- to- Token ViT (T2T- ViT) [64] first introduces a paradigm of hierarchical Transformer and employs an overlapping unfold operation for down- sampling. However, such operation brings heavy memory and computation costs. Therefore, Pyramid Vision Transformer (PVT) [42] leverages a non- overlapping patch partition to reduce feature size. Furthermore a spatial- reduction attention (SRA) layer is applied in PVT to further reduce the computational cost by learning low- resolution key- value pairs. Empirically, PVT adapts the Transformer to the dense prediction tasks on many benchmarks which demand large inputs and fine- grained features with computational efficiency. Moreover, both PiT [65] and CvT [37] utilize pooling and convolution to perform token downsampling, respectively. In detail, CvT [37] improves the SRA of PVT [42] by replacing the linear layer with a convolutional projection. Based on the convolutional bias, CvT [37] can adapt to arbitrary size inputs without positional encodings.

### 3.6 Deep Transformer

Empirically, increasing model's depth always strengthens its learning capacity [11]. Recent works apply a deep structure to Transformer and massive experiments are conducted to investigate its scalability by analyzing cross- patch [68] and cross- layer [38], [67] similarities, and the contribution of residual blocks [43]. In the deep Transformer, the features from the deeper layers tend to be less representative (attention collapse [67]), and the patches are mapped into the indistinguishable latent representations (patch over- smoothing [68]). To address such limitations mentioned above, current methods present the corresponding solutions from two aspects.

From the aspect of model's structure, Touvron et al. present efficient Class- attention in image Transformers (CaiT [43]), including two stages: 1) Multiple self- attention stages without class token. In each layer, a learned diagonal matrix initialized by small values is exploited to update the channel weights dynamically, thereby offering a certain degree of freedom for channel adjustment. 2) Last few class- attention stages with frozen patch embeddings. A later class token is inserted to model global representations, similar to DETR [30] with an encoder- decoder structure. This explicit separation is based on the assumption that the class token is invalid for the gradient of patch embeddings in the forward pass. With distillation training strategy [41], CaiT achieves a new SoTA on imagenet- 1k (86.5% top- 1 accuracy) without external data. Although deep Transformer suffers from attention collapse and oversmoothing problems, it still largely preserves the diversity of the attention map between different heads. Based on this observation, Zhou et al. propose Deep Vision Transformer (DeepViT) [67] that aggregates different head attention maps and re- generates a new one by using a linear layer to increase cross- layer feature diversity. Furthermore, Refiner [38] applies a linear layer to expand the dimension of the attention maps (indirectly increasing the head number) for diversity promotion. Then, a Distributed Local Attention (DLA) is employed to achieve better modeling of both the local features and the global ones, which is implemented by a head- wise convolution effecting on the attention map.

From the aspect of training strategy, Gong et al. present three Patch Diversity losses for deep Transformer that can significantly encourage patches' diversity and offset oversmoothing problem [68]. Similar to [177], a patch- wise cosine loss minimizes pairwise cosine similarity among patches. A patch- wise contrastive loss regularizes the deeper patches by their corresponding one in the early layer. Inspired by Cutmix [178], a patch- wise mixing loss mixes two different images and forces each patch to only attend to the patches from the same image and ignore unrelated ones. Compared with LV- ViT [44], their similar loss function is based on a distinctive motivation. The former focuses on the patch diversity, while the latter focuses on data augmentation about token labeling.

### 3.7 Transformers with Self-Supervised Learning

Following the grateful success of self- supervised in the NLP field [5], recent works also attempt to design various self- supervised learning schemes for the visual Transformers in both generative and discriminative ways.

For the generative models, Chen et al. propose an image Generative Pre- training Transformer (iGPT) [69] for self- supervised visual learning. Different from the patch embedding of ViT [29], iGPT directly resizes and flattens the image to a lower resolution sequences. The resized sequences are then input into a GPT- 2 [4] for auto- regressive pixel prediction. iGPT demonstrates the effectiveness of the Transformer in the visual tasks without any help from image- specific knowledge, but its considerable computation cost is hard to be accepted (roughly 2500 V100- days for pre- training). Instead of generating such raw pixels directly, Bao et al. propose a BERT- style [5] visual Transformer (BEiT) [71] by reconstructing the masked image in the latent space. Similar to the dictionary in BERT [5], dVAE [149] vectorizes the image into discrete visual tokens. The resulting visual token serves as pseudo label to pre- train ViT for masked patch construction.

For the discriminative models, Chen et al. [73] go back to basics and investigate the effects of several fundamental components for stabilized self- supervised ViT training. They observe that the unstable training process mildly affects the eventual performance, and extend MoCo series to MoCo v3, containing a series of training strategies such as freezing projection layer. Following DeiT [41], Caron et al. further extend the teacher- student recipe to self- supervised learning and propose DINO (2021) [74]. The core concepts of DINO can be summarized into three points. A momentum encoder inherited SwAV [179], serves as a teacher model that outputs the centered pseudo labels over a batch. An online encoder without the prediction head serves as a student model to fit the teacher's output. A standard cross- entropy loss connects self- training with knowledge distillation. More interestingly, self- supervised ViT can learn flourishing features for segmentation, which are normally unattainable by the supervised models.

### 3.8 Discussion

1) Algorithm Evaluation and Comparative Analysis: In our taxonomy, all the existing supervised models are grouped into six categories. Tab. I summarizes the performances of these existing visual Transformers on ImageNet-1k benchmarks. To evaluate them objectively and intuitively, we use the following three figures to illustrate their performances on ImageNet-1k under different configurations. Fig. 8(a) summarizes the accuracy of each model under  $224^{2}$  inputs size. Fig. 8(b) takes the FLOPs as the horizontal axis, which focuses on their performances under higher-resolution. Fig. 8(c) focuses on the pre-trained models with external datasets. From these comparison results, we briefly summarize several performance improvements on efficiency and scalability as follows.

- Compared with the most structure-improved methods, the basic training strategies like DeiT [41] and LV-ViT [44], are more universal for various models, tasks, and inputs.- The locality is indispensable for the Transformer, which is reflected by the dominant of VOLO [45] and Swin [36] on classification and dense prediction tasks, respectively.- The convolutional patchify stem (ViT $_c$  [59]) and early convolutional stage (CoAtNet [40]) can significantly boost the accuracy of the Transformers, especially for large models. We speculate the reason is because these designs introduce a more stringent high-level features than the non-overlapping patch projection in ViT [29].- The deep Transformer, such as Refined-ViT [38] and CaiT [43], has great potential. As the model size grows quadratically with the channel dimension, the trade-off in deep Transformer is considered for further investigation.- CeiT [55] and CvT [37] show significant advantages in training a small or medium model (0–40M), which suggests that such kinds of hybrid attention blocks for lightweight models are worth further exploring.

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-11/e729979e-6782-440e-aa44-610b9edd5863/5d46d3fce2bc5dc45897c54cdde7c6b7866cc8a69a0045a788d06f37c954b6d1.jpg)  
Fig. 8. Comparisons of recent visual Transformers on ImageNet-1k benchmark, including ViT [29], DeiT [41], BoTNet [53], VTs [52], ConViT [54], CeiT [55], LocalViT [56], TNT [60], Swin [36], PiT [65], T2T-ViT [64], PVT [42], CVT [37], DeepViT [67], CaiT [43], Cross ViT [180] (best viewed in color). (a) The bubble plot of the mentioned models with  $224^{2}$  resolution input, the size of cycle denotes GFLOPs. (b) Comparison of visual Transformers with high-resolution inputs, the square indicates  $448^{2}$  input resolution. (c) The accuracy plot of some pre-trained models on ImageNet-21k.

TABLEI TOP-1 ACCURACY COMPARISON OF VISUAL TRANSFORMERS ON IMAGENET-1K, 1K ONLY" DENOTES TRAINING ON IMAGENET-1K ONLY; 21K PRE. DENOTES PRE-TRAINING ON MINENET-21K AND FINE-TUNING ON IMAGENET-1K; DISTILL. DENOTES APPLYING DISTILLATION TRAINING SCHEME OF DEIT [41]; THE COLOR OFLEGENDT CORRESPONDING TO EACH MODEL ALSO DENOTES SAME MODEL IN FIG.8.  

<table><tr><td>Method</td><td>#Params.</td><td>#Params.</td><td>ImageNet-1k</td><td>Top-1 Acc.</td><td>Legend</td><td>Method</td><td>#Params.</td><td>#Params.</td><td>ImageNet-1k</td><td>Top-1 Acc.</td><td>Legend</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ViT-B/16T384</td><td>[29]</td><td>(M)</td><td>(G)</td><td>[79]</td><td>21K/Distill(↑)(↑)</td><td>VOLO-D1</td><td>[45]</td><td>(M)</td><td>(G)</td><td>14.1</td><td>21K/Distill(↑)</td><td></td></tr><tr><td>ViT-L/16T384</td><td>[29]</td><td>304.7</td><td>174.8</td><td>76.5</td><td>85.15↑</td><td>VOLO-D2</td><td>[45]</td><td>59</td><td>14.1</td><td>85.2</td><td>-</td><td></td></tr><tr><td>VT-Rest18</td><td>[52]</td><td>11.7</td><td>1.57</td><td>76.8</td><td>-</td><td>VOLO-D3</td><td>[45]</td><td>86</td><td>20.6</td><td>85.4</td><td>-</td><td></td></tr><tr><td>VT-Rest134</td><td>[52]</td><td>19.2</td><td>3.24</td><td>79.9</td><td>-</td><td>VOLO-D4</td><td>[45]</td><td>193</td><td>43.8</td><td>85.7</td><td>-</td><td></td></tr><tr><td>VT-Rest50</td><td>[52]</td><td>21.4</td><td>3.41</td><td>80.6</td><td>-</td><td>VOLO-D5</td><td>[45]</td><td>296</td><td>69.0</td><td>86.1</td><td>-</td><td></td></tr><tr><td>VT-Rest101</td><td>[52]</td><td>41.5</td><td>7.13</td><td>82.3</td><td>-</td><td>VOLO-D3T448</td><td>[45]</td><td>86</td><td>67.9</td><td>86.3</td><td>-</td><td></td></tr><tr><td>BoTNet-T2</td><td>[53]</td><td>33.5</td><td>7.3</td><td>81.7</td><td>-</td><td>VOLO-D4T448</td><td>[45]</td><td>193</td><td>197</td><td>86.8</td><td>-</td><td></td></tr><tr><td>BoTNet-T4</td><td>[53]</td><td>75.1</td><td>10.9</td><td>82.8</td><td>-</td><td>VOLO-D5T448</td><td>[45]</td><td>216</td><td>304</td><td>87.0</td><td>-</td><td></td></tr><tr><td>BoTNet-T5T256</td><td>[53]</td><td>54.7</td><td>19.3</td><td>83.5</td><td>-</td><td>21T-ViT-14 [64]</td><td>29.5</td><td>5.2</td><td>81.5</td><td>-</td><td>-</td><td></td></tr><tr><td>DeiT-Ti [41]</td><td>5.7</td><td>1.1</td><td>72.2</td><td>74.5Y</td><td>-</td><td>21T-ViT-19 [64]</td><td>39.2</td><td>8.9</td><td>81.9</td><td>-</td><td>-</td><td></td></tr><tr><td>DeiT-S [41]</td><td>22.1</td><td>4.3</td><td>79.8</td><td>81.2Y</td><td>-</td><td>PVT-Ti [42]</td><td>13.2</td><td>1.9</td><td>75.1</td><td>-</td><td>-</td><td></td></tr><tr><td>DeiT-B [41]</td><td>86.6</td><td>16.9</td><td>81.8</td><td>83.4Y</td><td>-</td><td>PVT-S [42]</td><td>24.5</td><td>3.7</td><td>79.8</td><td>-</td><td>-</td><td></td></tr><tr><td>DeiT-BT384</td><td>[41]</td><td>86.8</td><td>49.4</td><td>83.1</td><td>84.5Y</td><td>PVT-L [42]</td><td>61.4</td><td>9.8</td><td>81.7</td><td>-</td><td>-</td><td></td></tr><tr><td>ConViT-Ti [54]</td><td>6</td><td>1</td><td>73.1</td><td>-</td><td>-</td><td>PVTv2-B2 [66]</td><td>25.4</td><td>4.0</td><td>82.6</td><td>-</td><td>-</td><td></td></tr><tr><td>ConViT-S [54]</td><td>27</td><td>5.4</td><td>81.3</td><td>-</td><td>-</td><td>PVTv2-B4 [66]</td><td>62.6</td><td>10.1</td><td>83.0</td><td>-</td><td>-</td><td></td></tr><tr><td>LocalViT-T [56]</td><td>5.9</td><td>1.3</td><td>74.8</td><td>-</td><td>-</td><td>PIT-Ti [65]</td><td>4.9</td><td>0.7</td><td>73.0</td><td>74.6Y</td><td>-</td><td></td></tr><tr><td>LocalViT-S [56]</td><td>22.4</td><td>4.6</td><td>80.8</td><td>-</td><td>-</td><td>PIT-XS [65]</td><td>10.6</td><td>1.4</td><td>78.1</td><td>79.1Y</td><td>-</td><td></td></tr><tr><td>CeiT-T [55]</td><td>6.4</td><td>1.2</td><td>76.4</td><td>-</td><td>-</td><td>PIT-S [65]</td><td>23.5</td><td>2.9</td><td>80.9</td><td>81.9Y</td><td>-</td><td></td></tr><tr><td>CeiT-S [55]</td><td>24.2</td><td>4.5</td><td>82.0</td><td>-</td><td>-</td><td>PIT-Ti [55]</td><td>73.8</td><td>12.5</td><td>82.0</td><td>84.0Y</td><td>-</td><td></td></tr><tr><td>CeiT-T↑384</td><td>[55]</td><td>6.4</td><td>13.6</td><td>78.8</td><td>-</td><td>CVT-Ti [37]</td><td>20</td><td>4.5</td><td>81.6</td><td>-</td><td>-</td><td></td></tr><tr><td>CeiT-S↑384</td><td>[55]</td><td>24.2</td><td>12.9</td><td>83.3</td><td>-</td><td>CVT-21 [37]</td><td>32</td><td>7.1</td><td>82.5</td><td>-</td><td>-</td><td></td></tr><tr><td>ResT-Base [58]</td><td>13.7</td><td>1.9</td><td>79.6</td><td>-</td><td>-</td><td>CVT-21↑384</td><td>[37]</td><td>20</td><td>16.3</td><td>83.3</td><td>83.3Y</td><td>-</td></tr><tr><td>ResT-SBase [58]</td><td>30.3</td><td>4.3</td><td>81.6</td><td>-</td><td>-</td><td>CVT-T1↑384</td><td>[37]</td><td>32</td><td>24.9</td><td>83.0</td><td>84.9Y</td><td>-</td></tr><tr><td>ResT-Large [58]</td><td>51.6</td><td>7.9</td><td>83.6</td><td>-</td><td>-</td><td>CVT-W24↑384</td><td>[37]</td><td>277</td><td>193.2</td><td>-</td><td>87.7Y</td><td>-</td></tr><tr><td>ViTc-1GF [62]</td><td>4.6</td><td>1.1</td><td>75.3</td><td>-</td><td>-</td><td>DeepViT-S [67]</td><td>27</td><td>6.2</td><td>82.3</td><td>-</td><td>-</td><td></td></tr><tr><td>ViTc-4GF [62]</td><td>17.8</td><td>4.0</td><td>81.4</td><td>81.2↑</td><td>-</td><td>DeepViT-L [67]</td><td>35</td><td>5.5</td><td>83.7</td><td>-</td><td>-</td><td></td></tr><tr><td>ViTc-18GF [62]</td><td>81.6</td><td>17.7</td><td>83.0</td><td>84.9↑</td><td>-</td><td>CaiT-XS-24 [43]</td><td>26.6</td><td>12.4</td><td>81.8</td><td>82.0Y</td><td>-</td><td></td></tr><tr><td>ViTc-36GF [62]</td><td>167.8</td><td>35</td><td>84.2</td><td>85.8↑</td><td>-</td><td>CaiT-S-24 [43]</td><td>46.9</td><td>9.4</td><td>82.7</td><td>83.5Y</td><td>-</td><td></td></tr><tr><td>CoAtNet-0 [40]</td><td>25</td><td>4.2</td><td>81.6</td><td>-</td><td>-</td><td>CaiT-S-36 [43]</td><td>68.2</td><td>13.9</td><td>83.3</td><td>84.0Y</td><td>-</td><td></td></tr><tr><td>CoAtNet-1 [40]</td><td>42</td><td>8.4</td><td>83.3</td><td>-</td><td>-</td><td>CaiT-M-24 [43]</td><td>285.9</td><td>36.0</td><td>83.8</td><td>84.7Y</td><td>-</td><td></td></tr><tr><td>CoAtNet-2 [40]</td><td>75</td><td>15.7</td><td>84.1</td><td>87.1↑</td><td>-</td><td>CaiT-M-36 [43]</td><td>170.9</td><td>53.7</td><td>83.3</td><td>85.1Y</td><td>-</td><td></td></tr><tr><td>CoAtNet-3 [40]</td><td>168</td><td>34.7</td><td>84.5</td><td>87.6↑</td><td>-</td><td>DiversePatch-S12 [68]</td><td>22</td><td>-</td><td>81.2</td><td>-</td><td>-</td><td></td></tr><tr><td>CoAtNet-4↑384 [40]</td><td>275</td><td>189.5</td><td>-</td><td>88.4↑</td><td>-</td><td>DiversePatch-S24 [68]</td><td>44</td><td>-</td><td>82.2</td><td>-</td><td>-</td><td></td></tr><tr><td>TNT-S [60]</td><td>23.8</td><td>5.2</td><td>81.3</td><td>-</td><td>-</td><td>DiversePatch-B12 [68]</td><td>172</td><td>-</td><td>82.9</td><td>-</td><td>-</td><td></td></tr><tr><td>TNT-B [60]</td><td>65.6</td><td>14.1</td><td>82.8</td><td>-</td><td>-</td><td>DiversePatch-B24 [68]</td><td>86</td><td>-</td><td>83.3</td><td>-</td><td>-</td><td></td></tr><tr><td>TNT-S↑384 [60]</td><td>23.8</td><td>-</td><td>83.1</td><td>-</td><td>-</td><td>DiversePatch-B12↑384 [68]</td><td>86</td><td>-</td><td>84.2</td><td>-</td><td>-</td><td></td></tr><tr><td>TNT-B↑384 [60]</td><td>65.6</td><td>-</td><td>83.9</td><td>-</td><td>-</td><td>Refined-ViT-S [38]</td><td>25</td><td>7.2</td><td>83.6</td><td>-</td><td>-</td><td></td></tr><tr><td>Swin-T [36]</td><td>29</td><td>4.5</td><td>81.3</td><td>-</td><td>-</td><td>Refined-ViT-L [38]</td><td>51</td><td>13.5</td><td>84.9</td><td>-</td><td>-</td><td></td></tr><tr><td>Swin-S [36]</td><td>50</td><td>8.7</td><td>83.0</td><td>-</td><td>-</td><td>Refined-ViT-M [38]</td><td>85</td><td>19.1</td><td>84.6</td><td>-</td><td>-</td><td></td></tr><tr><td>Swin-B [36]</td><td>88</td><td>15.4</td><td>83.3</td><td>85.2↑</td><td>-</td><td>Refined-ViT-M↑384 [38]</td><td>55</td><td>49.2</td><td>85.6</td><td>-</td><td>-</td><td></td></tr><tr><td>Swin-L↑384 [36]</td><td>197</td><td>104</td><td>-</td><td>86.4↑</td><td>-</td><td>Refined-ViT-L↑384 [38]</td><td>81</td><td>69.1</td><td>85.7</td><td>-</td><td>-</td><td></td></tr><tr><td>LV-ViT-S [44]</td><td>26</td><td>6.6</td><td>83.3</td><td>-</td><td>-</td><td>CrossViT-19 [180]</td><td>2.6</td><td>1.8</td><td>73.9</td><td>-</td><td>-</td><td></td></tr><tr><td>LV-ViT-M [44]</td><td>56</td><td>16.0</td><td>84.0</td><td>-</td><td>-</td><td>CrossViT-15 [180]</td><td>87.4</td><td>5.8</td><td>81.5</td><td>-</td><td>-</td><td></td></tr><tr><td>LV-ViT-L↑288 [44]</td><td>150</td><td>59.0</td><td>85.3</td><td>-</td><td>-</td><td>CrossViT-18 [180]</td><td>43.3</td><td>9.0</td><td>82.2</td><td>-</td><td>-</td><td></td></tr><tr><td>LV-ViT-M↑384 [44]</td><td>56</td><td>42.2</td><td>85.4</td><td>-</td><td>-</td><td>CrossViT-15*↑384 [180]</td><td>28.5</td><td>21.4</td><td>83.5</td><td>-</td><td>-</td><td></td></tr><tr><td>LV-ViT-L↑448 [44]</td><td>150</td><td>457.2</td><td>85.9</td><td>-</td><td>-</td><td>CrossViT-18*↑384 [180]</td><td>44.6</td><td>32.4</td><td>83.9</td><td>-</td><td>-</td><td></td></tr></table>

2) Brief Discussion on Alternatives: During the development of the visual Transformers, the most common question is whether the visual Transformers can replace the traditional convolution completely. By reviewing the history of the performance improvements in the last year, there is no sign of relative inferiority here. The visual Transformers have returned from a pure structure to a hybrid form, and the global information has gradually returned to a mixed stage with the locality bias. Although the visual Transformers can be equivalent to CNN or even has a better modeling capability, such a simple and effective convolution operation is enough to process the locality and the semantic features in the shallow layer. In the future, the spirit of combining both of them shall drive more breakthroughs for image classification.

## 4 TRANSFORMER FOR DETECTION

In this section, we review visual Transformers for object detection, which can be grouped into two folds: Transformer as the neck and Transformer as the backbone. For the neck detectors, we mainly focus on a new representation specified to the Transformer structure, called object query, that a set of learned parameters aggregate instance features from input images. The recent variants try to solve an optimal fusion paradigm in terms of either convergence acceleration or performance improvement. Besides these neck design, a proportion of backbone detectors also take specific strategies into consideration. Finally, we evaluate them and analyze some potential methods for these detectors.

### 4.1 Transformer Neck

We first review DETR [30] and Pix2seq [76], the original Transformer detectors that reformulate two different paradigms for object detection. Subsequently, we mainly focus on the DETR- based variants, improving such Transformer detector in accuracy and convergence from five aspects: sparse attention, spatial prior, structural redesign, assignment optimization, and pre- training model.

1) The Original Detectors: DEtection with TRansformer (DETR) [30] is the first end-to-end Transformer detector that eliminates hand-designed representations [181]-[184] and non-maximum suppression (NMS) post-processing, which redefines the object detection as a set prediction problem. In detail, a small set of learnable positional encodings, called object queries, that are parallelly fed into the Transformer decoder to extract the instance information from the image features. Then, these object queries are independently predicted to be a detection result. Instead of the vanilla  $\mathbf{k}$  -class classification, a special class, no object label  $(\emptyset)$  is added for  $\mathrm{k} + 1$  class classification. During the training process, a bipartite matching strategy is applied between the predicted objects and the ground-truth to identify one-to-one label assignment, hence removing the redundant predictions at the inference time without NMS. In back propagation, a Hungarian loss includes a log-likelihood loss for all classification results and a box loss for all the matched pairs. More details about the Hungarian matching strategy are available in the App. B.

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-11/e729979e-6782-440e-aa44-610b9edd5863/7727793644b44a1b5cd9666953150357917bf14a6d367c42adcb0b40e0787e11.jpg)  
Fig. 9. An overview of DETR. (Modified from [30].)

Overall, DETR provides a new paradigm for end- to- end object detection. The object query gradually learns an instance representation during the interaction with image features. The bipartite matching allows a direct set prediction and easily joints to the one- to- one label assignment, hence eliminating traditional post- processing. DETR achieves competitive performance on the COCO benchmark but suffers from slow convergence as well as poor performance on small objects.

Another pioneered work is Pix2seq [76], treating generic object detection as a language modeling task. Given an image input, a vanilla sequential Transformer is executed to extract features and generate a series of object descriptions (i.e. class labels and bounding boxes) auto- regressively. Such a simplified but more elaborate image caption method is derived under the assumption that if a model learns about both location and label of an object, it can be taught to produce a description with specified sequence [76]. Compared with DETR, Pix2seq attains a better result on small objects. How to combine both kinds of concepts is worthy of further consideration.

2) Transformer with Sparse Attention: In DETR, the dense interaction across both object queries and image features costs unbearable resources and slows down the convergence of DETR. Therefore, the most recent efforts aim to design a data-dependent sparse attention to address these issues.

Following [185], Zhu et al. develop Deformable DETR to ameliorate both training convergence and detection performance significantly via multi- scale deformable attention [77]. Compared with original DETR, the deformable attention module only samples a small set of key (reference) points for full features aggregation. Such sparse attention can be easily stretched to multi- scale feature fusion without the help of FPN [186]. Moreover, an iterative bounding box refinement and a two- stage prediction strategy (Sec. IV- A3) are developed to further enhance the detection accuracy. Empirically, Deformable DETR achieves a higher accuracy (especially for small objects) with  $10\times$  less training epochs and reduces the computing complexity to  $O(2N_qC^2 + \min (HWC^2,N_qKC^2))$  with  $1.6\times$  faster inference speed. Please see the App. C for more details of deformable attention mechanism.

By visualizing the attention map of DETR [30], Zheng et al. observe that the semantically similar and spatially close elements always have a similar attention map in the encoder [78]. Then they present an Adaptive Clustering Transformer (ACT), leveraging a multi- round sensitivity hashing to dynamically cluster the queries into different prototypes. The attention map of the prototype is then broadcast to their corresponding queries. Unlike the redesign of on sparse attention, Wang et al. introduce a Poll and Pool (PnP) sampling model [79] to extract the fine foreground features and condense the contextual back

ground features into a smaller one. Such fine- coarse tokens are then fed into DETR to generate the detection results. Instead of the input sparsification, Sparse DETR [80] applies a hysteretic scoring network (corresponding to the Poll operation in [79]) to update the expected tokens selectively within the transformer encoder, where the top- k selected tokens are supervised by pseudo labels from the binarized decoder cross- attention map with BCE loss.

3) Transformer with Spatial Prior: Unlike anchor or other representations directly generated by content and geometry features [181], [187], object queries implicitly model the spatial information with random initialization, which is weakly related with the bounding box. The mainstream for spatial prior applications are the one-stage detector with empirical spatial information and the two-stage detector with geometric coordinates initialization or Region-of-Interest (RoI) features.

In one- stage methods, Gao et al. suggest Spatially Modulated Cross- Attention (SMCA) [81] to estimate the object queries' spatial prior explicitly. Specifically, a Gaussian- like weight map generated by object queries is multiplied with the corresponding cross- attention map to augment the RoI for convergence acceleration. Furthermore, both intra- scale and multi- scale self- attention layers are utilized in the Transformer encoder for multi- scale feature aggregation, and the scale- selection weights generated from object queries are applied for scale- query adaptation. Meng et al. [82] extract the spatial attention map from the cross- attention formulation and observe that the extreme region of such attention map has larger deviations at the early training. Consequently, they propose Conditional DETR where a new spatial prior mapped from reference points is adopted in the cross- attention layer, thereby attending to extreme regions of each object explicitly. The reference point is predicted by the object query or serves as a learned parameter replacing the object query. Following [82], Anchor DETR [83] suggests to explicitly learn the 2D anchor points  $([cx,cy,w,h])$  and different anchor patterns instead of the high- dimensional spatial embedding. Similar to [181], the pattern embeddings are assigned to the meshed anchor points so that they can detect different scale objects anywhere. DAB- DETR [84] then extends the 2D concept to a 4D anchor box  $([cx,cy,w,h])$  to explicitly provide proposal bounding box information during the cross- attention. With the auxiliary decoder loss of the coordinates offset [77], such a 4D box can be dynamically refined layer- by- layer in the decoder. However, the same reference boxes/points may severely deteriorate queries' saliency and confuses detector due to the indiscriminative spatial prior. By assigning query- specific reference points to object queries, SAP- DETR [85] only predicts the distance from each side of the bounding box to these points. Such a query- specific prior discrepancies queries' saliency and pave the way for fast model convergency.

In two- stage methods, Zhe et al. empower the Top- K region proposals from the outputs of the encoder to initialize the decoder embedding instead of the learned content query [77]. Efficient DETR [86] also adopts a similar initialization operation for dense proposals and refines them in the decoder to get sparse prediction by using a shared detection head with the dense parts. More interestingly, it is observed that small stacking decoder layers bring slight performance improvement, but even more stacks yield even worse results. Dynamic DETR [87] regards the object prediction in a coarse- to- fine process. Different from the previous RoI- based initialization detectors, a cross- attention between the pool region features and the object embeddings is used to perform decoder embedding updates. The RoI features associated with the box embedding are then refined accordingly in the next layer.

TABLE II COMPARISON BETWEEN TRANSFORMER NECKS AND REPRESENTATIVE CNNS WITH RESNET-50 BACKBONE ON COCO 2017 VAL SET.  

<table><tr><td>Method</td><td>Epochs</td><td>FLOPs #Para- (G) (M)</td><td>FPS MS AP/AP50/AP75</td><td>APs/APM/APL</td><td></td><td></td><td></td></tr><tr><td colspan="5">CNN Backbone with Other Representations</td><td></td><td></td><td></td></tr><tr><td>FCOS [88], [187]</td><td>36</td><td>177</td><td>47</td><td>✓</td><td>41.0 /59.84/4.1</td><td>26.2/44.6/52.2</td><td></td></tr><tr><td>Faster R-CNN+ [181]</td><td>37</td><td>180</td><td>42</td><td>✓</td><td>40.2/62.1/43.5</td><td>24.2/43.5/52.0</td><td></td></tr><tr><td>Faster R-CNN+ [181]</td><td>109</td><td>180</td><td>42</td><td>✓</td><td>42.0/61.0/45.8</td><td>26.6/45.5/53.4</td><td></td></tr><tr><td>Mask R-CNN+ [188]</td><td>36</td><td>260</td><td>44</td><td>-</td><td>✓</td><td>41.0/61.7/44.9</td><td>- - - / - -</td></tr><tr><td>CNC Mask R-CNN+ [189]</td><td>36</td><td>739</td><td>82</td><td>18</td><td>✓</td><td>46.3/64.3/50.5</td><td>- - - / - -</td></tr><tr><td colspan="8">Transformer Model as Neck</td></tr><tr><td>DETR-R50 [30]</td><td>500</td><td>86</td><td>41</td><td>28</td><td>✓</td><td>42.0/62.4/44.2</td><td>20.5/45.8/61.1</td></tr><tr><td>DETR-DC5 [30]</td><td>500</td><td>187</td><td>41</td><td>-</td><td>✓</td><td>43.3/63.1/45.9</td><td>22.5/47.3/61.1</td></tr><tr><td>DETR-DC5 [76]</td><td>300</td><td>-</td><td>37</td><td>-</td><td>✓</td><td>43.0/61.0/45.6</td><td>25.1/46.9/59.4</td></tr><tr><td>Pix2seq-DC5 [76]</td><td>300</td><td>-</td><td>38</td><td>-</td><td>✓</td><td>43.2/61.0/46.1</td><td>26.6/47.0/58.6</td></tr><tr><td>Defor-DETR [75]</td><td>50</td><td>78</td><td>34</td><td>23</td><td>✓</td><td>39.7/60.1/42.4</td><td>21.2/45.3/56.0</td></tr><tr><td>Defor-DETR-DC5 [77]</td><td>50</td><td>128</td><td>34</td><td>22</td><td>✓</td><td>41.5/61.8/44.9</td><td>24.1/45.3/56.0</td></tr><tr><td>Defor-DETR-Iter [77]</td><td>50</td><td>173</td><td>40</td><td>19</td><td>✓</td><td>43.6/62.6/47.7</td><td>26.4/47.1/58.0</td></tr><tr><td>Defor-DETR-Two [77]</td><td>50</td><td>173</td><td>40</td><td>19</td><td>✓</td><td>46.2/63.2/50.0</td><td>28.8/49.2/61.0</td></tr><tr><td>ACT-DC5 (L=16) [78]</td><td>MTKD [78]</td><td>156</td><td>14</td><td>✓</td><td>40.6/ - - / -</td><td>18.5/44.3/59.7</td><td>18.5/44.3/59.7</td></tr><tr><td>ACT-DC5 (L=16) [78]</td><td>MTKD [78]</td><td>156</td><td>14</td><td>✓</td><td>41.1/ - - / -</td><td>22.8/44.1/60.4</td><td>22.8/44.1/60.4</td></tr><tr><td>PnP-DETR-33 [79]</td><td>MTKD [78]</td><td>77</td><td>-</td><td>16</td><td>✓</td><td>41.1/61.4/5.7</td><td>20.8/47.6/61.0</td></tr><tr><td>PnP-DETR-33 [79]</td><td>500</td><td>169</td><td>-</td><td>-</td><td>✓</td><td>41.8/62.1/44.4</td><td>21.2/45.3/60.8</td></tr><tr><td>PnP-DETR-0.5 [79]</td><td>500</td><td>79</td><td>-</td><td>-</td><td>✓</td><td>43.1/63.4/45.3</td><td>22.7/46.5/61.1</td></tr><tr><td>PnP-DETR-DC5-0.5 [79]</td><td>500</td><td>136</td><td>-</td><td>-</td><td>✓</td><td>43.3/63.4/45.3</td><td>22.7/46.5/61.1</td></tr><tr><td>Sparse-DETR-0.5 [80]</td><td>50</td><td>105</td><td>41</td><td>25</td><td>✓</td><td>46.3/66.8/90.1</td><td>28.0/49.5/60.0</td></tr><tr><td>Sparse-DETR-0.5 [80]</td><td>50</td><td>136</td><td>41</td><td>25</td><td>✓</td><td>45.3/65.0/45.3</td><td>29.4/48.3/60.8</td></tr><tr><td>SMCA [81]</td><td>50</td><td>86</td><td>40</td><td>22</td><td>✓</td><td>41.0/ - - / -</td><td>21.9/44.3/59.1</td></tr><tr><td>SMCA+ [81]</td><td>108</td><td>86</td><td>40</td><td>22</td><td>✓</td><td>42.7/ - - / -</td><td>22.8/46.1/60.0</td></tr><tr><td>SMCA+ [81]</td><td>50</td><td>132</td><td>40</td><td>10</td><td>✓</td><td>43.7/63.6/47.2</td><td>24.2/47.0/60.4</td></tr><tr><td>SMCA+ [81]</td><td>108</td><td>152</td><td>40</td><td>10</td><td>✓</td><td>43.0/64.5/49.7</td><td>22.9/49.7/61.6</td></tr><tr><td>Cond+ [81]</td><td>108</td><td>90</td><td>44</td><td>17</td><td>✓</td><td>45.6/65.4/45.1</td><td>25.7/46.3/62.5</td></tr><tr><td>Cond+ [81]</td><td>108</td><td>195</td><td>44</td><td>11</td><td>✓</td><td>45.1/65.4/48.5</td><td>25.3/49.0/62.2</td></tr><tr><td>Anchor-DETR [83]</td><td>50</td><td>85</td><td>39</td><td>20</td><td>✓</td><td>42.1/63.1/44.9</td><td>22.3/46.2/60.0</td></tr><tr><td>Anchor-DETR-DC5 [83]</td><td>50</td><td>151</td><td>39</td><td>14</td><td>✓</td><td>42.2/64.1/44.5</td><td>24.7/48.2/60.6</td></tr><tr><td>DAB-DETR [84]</td><td>50</td><td>90</td><td>44</td><td>17</td><td>✓</td><td>44.2/63.1/47.7</td><td>21.5/45.7/60.3</td></tr><tr><td>DAB-DETR-DC5 [84]</td><td>50</td><td>194</td><td>44</td><td>11</td><td>✓</td><td>44.5/65.1/47.7</td><td>25.3/48.2/62.3</td></tr><tr><td>SAP-DETR [85]</td><td>50</td><td>92</td><td>47</td><td>16</td><td>✓</td><td>43.1/63.8/45.4</td><td>22.9/47.1/62.1</td></tr><tr><td>SAP-DETR-DC5 [85]</td><td>50</td><td>197</td><td>47</td><td>19</td><td>✓</td><td>46.0/62.5/48.9</td><td>26.4/50.2/56.6</td></tr><tr><td>Efficient DETR [86]</td><td>36</td><td>159</td><td>32</td><td>-</td><td>✓</td><td>44.2/65.1/48.0</td><td>28.4/47.5/52.6</td></tr><tr><td>Efficient DETR+ [86]</td><td>36</td><td>210</td><td>35</td><td>-</td><td>✓</td><td>45.1/63.1/49.1</td><td>28.3/48.4/59.0</td></tr><tr><td>Dynamic DETR [87]</td><td>50</td><td>-</td><td>58</td><td>-</td><td>✓</td><td>47.2/65.9/51.1</td><td>28.6/49.3/59.1</td></tr><tr><td>TSP-FCOS [88]</td><td>36</td><td>188</td><td>52</td><td>15</td><td>✓</td><td>43.8/63.3/47.0</td><td>26.6/46.9/55.9</td></tr><tr><td>TSP-RCNN [88]</td><td>36</td><td>189</td><td>64</td><td>11</td><td>✓</td><td>43.1/62.3/48.3</td><td>26.6/46.8/55.5</td></tr><tr><td>TSP-RCNN+ [88]</td><td>96</td><td>188</td><td>64</td><td>11</td><td>✓</td><td>43.0/64.5/47.6</td><td>29.7/47.5/58.0</td></tr><tr><td>YOLOS-S(800x) [89]</td><td>50</td><td>194</td><td>31</td><td>16</td><td>✓</td><td>36.1/56.4/37.1</td><td>15.3/38.5/56.1</td></tr><tr><td>YOLOS-S(784x) [89]</td><td>150</td><td>172</td><td>28</td><td>6</td><td>✓</td><td>37.6/57.6/39.2</td><td>15.9/40.2/57.3</td></tr><tr><td>YOLOS-S(800x) [89]</td><td>150</td><td>538</td><td>27</td><td>6</td><td>✓</td><td>37.0/57.2/42.5</td><td>19.5/40.2/52.1</td></tr><tr><td>UP-DETR [90]</td><td>150</td><td>86</td><td>41</td><td>28</td><td>✓</td><td>40.5/62.2/44.6</td><td>19.0/44.4/60.0</td></tr><tr><td>UP-DETR+ [90]</td><td>300</td><td>86</td><td>41</td><td>28</td><td>✓</td><td>42.8/63.0/45.3</td><td>20.8/47.1/61.7</td></tr><tr><td>FP-DETR-Base [91]</td><td>50</td><td>-</td><td>36</td><td>-</td><td>✓</td><td>43.3/63.9/47.7</td><td>27.5/46.1/57.0</td></tr><tr><td>DN-DETR [92]</td><td>50</td><td>94</td><td>44</td><td>17</td><td>✓</td><td>44.1/64.4/46.7</td><td>22.9/48.0/63.4</td></tr><tr><td>DN-DETR-DC5 [92]</td><td>50</td><td>202</td><td>48</td><td>18</td><td>✓</td><td>46.3/66.8/49.7</td><td>27.5/50.0/64.3</td></tr><tr><td>DN-DETR-DC5 [92]</td><td>50</td><td>96</td><td>44</td><td>23</td><td>✓</td><td>46.3/66.4/49.7</td><td>22.7/50.0/64.3</td></tr><tr><td>DIN-4-scale [93]</td><td>36</td><td>279</td><td>47</td><td>53</td><td>✓</td><td>50.5/68.3/55.1</td><td>32.7/53.9/64.9</td></tr><tr><td>DIN-5-scale [93]</td><td>36</td><td>860</td><td>47</td><td>10</td><td>✓</td><td>51.0/69.0/55.6</td><td>34.1/53.6/65.6</td></tr></table>

Both GFLOPs and Paras are measured by Detectron2. FPS is measured on a single A100 GPU.

4) Transformer with Redesigned Structure: Besides the modifications focusing on the cross-attention, some works redesign an encoder-only structure to avoid the problem of the decoder directly. TSP [88] inherits the idea of set prediction [30] and dismisses the decoder and the object query to accelerate convergence. Such encoder-only DETR reuses previous representations [181], [187], and generates a set of fixed-size Features of Interests (Fol) [187] or proposals [181] that are subsequently fed into the Transformer encoder. In addition, a matching distillation is applied to resolve the instability of the bipartite matching, especially in the early training stage. Fang et al. [89] combine the encoder-decoder neck of DETR and the encoder-only backbone of ViT into an encoder-only detector and develop YOLOS, a pure sequence-to-sequence

Transformer to unify the classification and detection tasks. It inherits ViT's structure and replaces the single class token with fixed size learned detection tokens. These object tokens are first pre- trained on the transfer ability for the classification tasks and then fine- tuned on the detection benchmark.

5) Transformer with Bipartite Matched Optimization: In DETR [30], the bipartite matching strategy forces the prediction results to fulfil one-to-one label assignment during the training scheme. Such a training strategy simplifies detection pipeline and directly builds up an end-to-end system without the help of NMS. To deeply understand the efficacy of the end-to-end detector, Sun et al. devote to exploring a theoretical view of one-to-one prediction [190]. Based on multiple ablation and theoretical analyses, they conclude that the classification cost for one-to-one matching strategy serves as the key component for significantly avoiding duplicate predictions. Even so, DETR is suffering from multiple problems caused by bipartite matching. Li et al. [92] exploit a denoising DETR (DN-DETR) to mitigate the instability of bipartite matching. Concretely, a series of objects with slight perturbation is supposed to reconstruct their actual coordinates and classes. The main ingredients of the denoising (or reconstruction) part are an attention mask that prevents information leakage between the matching and noised parts, and a specified label embedding to indicate the perturbation. Recently, Zhang et al. [93] present an improved denoising training model called DINO (2022) by incorporating a contrastive loss for the perturbation groups. Based on DN-DETR [92], DINO attaches a "no object" class for the negative example if the distance is far enough from the perturbation, which avoids redundant prediction due to the confusion of multiple reference points near an object. As a result, DINO attains the current SoTA on the COCO dataset.

6) Transformer Detector with Pre-Training: Inspired by the pre-trained linguistic Transformer [3], [5], Dai et al. devise an Unsupervised Pre-training DETR (UP-DETR) [90] to assist the convergence for supervised training. The objective of pretraining is to localize the random cropped patches from a given image. Specifically, each patch is assigned to a set of queries and predicted independently via the attention mask. An auxiliary reconstruction loss forces the detector to preserve the feature discrimination so as to avoid over-bias towards the localization in pre-training. PP-DETR [91] devotes to narrowing the gap between upstream and downstream tasks. During the pre-training, a fully encoder-only DETR like YOLOS [89] views query positional embeddings as a visual prompt to enhance target area attention and object discrimination. A task adapter implemented by self-attention is used to enhance object interaction during fine-tuning.

### 4.2 Transformer Backbone

We have reviewed numerous Transformer- based backbones for image classification [29], [41] in Sec. III. These backbones can be easily incorporated into various frameworks (e.g.,Mask R- CNN [188], RetinaNet [183],DETR [30], etc.) to perform dense prediction tasks. For example, the hierarchical structure like PVT [42], [66], constructs the visual Transformer as a high- to- low resolution process to learn multi- scale features. The locally enhanced structure constructs the backbone as a local- to- global combination, which can efficiently extract both short- range and long- range visual dependencies and avoid quadratic computational overhead, such as Swin- Transformer [36], ViL [62], and Focal Transformer [63]. App. D includes more detailed comparisons of these models for the dense prediction tasks. In addition to the generic Transformer backbone, the Feature Pyramid Transformer (FPT) [94] combines the characteristics across both the spaces and the scales, by using self- attention, top- down cross- attention, and bottom- up cross- channel attention. Following [191], HRFormer [95] introduces the advantages of multiresolution to the Transformer along with non- overlapping local self- attention. HRViT [96] redesigns a heterogeneous branch and a cross- shaped attention block to further optimize the trade- off between efficiency and accuracy.

### 4.3 Discussion

We summarize five folds of the Transformer neck detectors in Tab. II, and more details of Transformer backbone for dense prediction tasks are referred to in Tab. IV. The majority of Transformer neck promotions concentrate on the following five aspects: 1) The sparse attention model and the scoring network are proposed to address the problem of redundant feature interaction. These methods can significantly alleviate computational costs and accelerate model convergence. 2) The explicit spatial prior, which is decomposed into the selected feature initialization and the positional information extracted by learned parameters, would enable the detector to predict the results precisely. 3) Multi- scale features and layer- by- layer updating are extended in the Transformer decoder for small object refinement. 4) The improved bipartite matching strategy is beneficial to avoid redundant prediction as well as perform end- to- end object detection. 5) The encoder- only structure reduces the overall Transformer stack layers but increases the FLOPs excessively, while the encoder- decoder structure is a good trade- off between FLOPs and Parameters, but the deeper decoder layers may cause the problems of long training time and over- smooth.

Moreover, there are many Transformer backbones for improving classification performance, but few works are developed for the dense prediction tasks. In the future, we anticipate that the Transformer backbone would cooperate with the deep high- resolution network to solve dense prediction tasks.

## 5. TRANSFORMER FOR SEGMENTATION

Patch- Based and Query- Based Transformer are the two major ways for Segmentation. The latter can be further grouped into Object Query and Mask Embedding methods.

### 5.1 Patch-Based Transformer

Because of the receptive field expansion strategy [192], CNNs require multiple decoder stacks to map the high- level features into the original spatial resolution. Instead, patch- based Transformer can easily incorporate with a simple decoder for segmented mask prediction because of its global

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-11/e729979e-6782-440e-aa44-610b9edd5863/a315bf09fa75fad5c3cd0edd09c62629021605feab32de85647c60fd77d88eac.jpg)  
Fig. 10. Query-based frameworks for segmentation tasks. (a) Transfer learning for fine-tuning mask head. (b) Multi-task learning for two independent task. (c) Cascade learning for fine-grained mask generation on the coarse region prediction. (d) The query embeddings are independently supervised by mask embeddings and boxes. (e) The box-free model directly predicts masks without box branch and views segmentation task as a mask prediction problem.

modelling capability and resolution invariance. Zheng et al. extend ViT [29] for semantic segmentation tasks, and present SEgmentation TRANSformer (SETR) [97] by employing three fashions of the decoder to perform per- pixel classification: naive up- sampling, progressive up- sampling, and multi- level feature aggregation (MLA). SETR demonstrates the feasibility of the visual Transformer for the segmentation tasks, but it also brings unacceptably extra GPU costs. TransUNet [98] is the first for medical image segmentation. Formally, it can be viewed as either a variant of SETR with MLA decoder [97], or a hybrid model of U- Net [193] and Transformer. Thanks to the strong global modeling capability of Transformer encoder, Segformer [99] designs a lightweight decoder with only four MLP layers. Segformer shows superior performance as well as stronger robustness than CNNs when tested with multiple corrupted types of images.

### 5.2 Query-Based Transformer

Query embeddings are a set of scratch semantic/instance representations gradually learning from the image inputs. Unlike patch embeddings, queries can more "fairly" integrate the information from features and naturally join with the set prediction loss [30] for post- processing elimination. Existed query- based models can be grouped into two categories. One is driven by both the tasks of detection and segmentation, simultaneously (called object queries). The other is only supervised by the segmentation task (called mask embeddings).

1) Object Queries: There are three training manners for object query based methods (Fig. 10(a)-(c)). With the success of DETR [30] for the object detection tasks, the authors extend it to panoptic segmentation (hence termed Panoptic DETR [30]) by training a mask head based on the pre-trained object queries (Fig. 10(a)). In detail, a cross-attention block between the object queries and the encoded features is applied to generate an attention map for each object. After an up-sampling FPN-style CNN, a spatial argmax operation fuses the resulting binary masks to a non-overlapping prediction. Instead of using a multi-stage serial training process, Cell

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-11/e729979e-6782-440e-aa44-610b9edd5863/66b8efb6f02f64bbb6ed5e849ab823c29bd2c13967c75fb21cbdcc6b3c631a7c.jpg)  
Fig. 11. Illustration of Maskformer. (from [32])

DETR and VisTR develop a parallel model for end- to- end instance segmentation (Fig. 10(b)). Based on DETR [30], Cell- DETR leverages a cross- attention block to extract instance- wise features from the box branch and fuses the previous backbone features to augment the CNN decoder for accurate instance mask segmentation of biological cells. Another extension is VisTR [101] that directly formulates the video instance segmentation (VIS) task as parallel sequence prediction. Apart from the similar structure as Cell- DETR [100], the key of VisTR is a bipartite matching loss at the instance sequence level to maintain the order of outputs, so as to adapt DETR [30] to VIS for direct one- to- one predictions. Unlike prior works that treat detection and mask generation branches separately, QueryInst [102] builds a hybrid cascaded network (Fig. 10(c)), where the previous box outputs together with the shared queries serve as the inputs of the mask head for accurate mask segmentation. Notably, QueryInst leverages the shared queries to keep the instance correspondences across multistage, so that mitigating the problem of inconsistent objects in previous non- query based methods [189], [194]. QueryInst obtains the latest SoTA results on the COCO datasets.

2) Mask Embeddings: The other framework makes efforts to use queries to predict mask directly, and we refer to this learned mask-based query as mask embeddings. Unlike the object queries, mask embeddings are only supervised by the segmentation tasks. As shown in Fig. 10(d), two disjoint sets of queries are employed parallelly for different tasks, and the box learning is viewed as an auxiliary loss for further enhancement. For semantic and box-free instance segmentation, a series of query-based Transformers predict the mask directly without the help of the box branch (Fig. 10(e)).

From the auxiliary training perspective, the core is how to enable 1D sequence outputs to be supervised by 2D mask labels directly. To this end, ISTR [103] empowers a mask precoding method to encode the ground- truth spatial mask into low- dimensional mask embedding for instance segmentation. Similarly, Dong et al. propose a more straightforward pipeline SOLQ [104] and explore three reversible compression encoding methods for mask embeddings. In detail, a set of unified queries is applied to perform multiple representation learning parallelly: classification, box regression, and mask encoding. Based on the original DETR [192], SOLQ adds a mask branch to produce mask embedding loss. Both ISTR and SOLQ obtain comparable results and outperform previous methods even with approximation- based suboptimal embeddings. However, there exists a huge gap between  $AP_{box}$  and  $AP_{seg}$  (Tab. III).

From the box- free perspective, Wang et al. pioneer a new paradigm Max- DeepLab [31] that directly predicts panoptic

masks from the query without the help of the box branch. Specifically, it forces the query to predict the corresponding mask via a PQ- style bipartite matching loss and a dualpath Transformer structure. Given a set of mask embeddings and an image input, Max- DeepLab processes them separately in both Transformer and CNN path, and then generates a binary mask and a class for each query, respectively. MaxDeepLab achieves new SoTA with  $51.3\%$  PQ on COCO testdev set, but leads to heavy computational costs due to its dual- path high- resolution processing. Segmenter [105] views the semantic segmentation task as a sequence- to- sequence problem. In detail, a set of mask embeddings that represent different semantic classes are fed into the Transformer encoder together with image patches, and then a set of labeled masks are predicted for each patch via an argmax operation.

Unlike the conventional semantic segmentation methods that predict mask at the pixel level, Cheng et al. reformulate the semantic segmentation task as a mask prediction problem and enable this output format to the query- based Transformer, which is called Maskformer [32]. Different from MaxDeepLab [31], Maskformer leverages a simple Transformer decoder without redundant connection as well as a sigmoid activation for overlapping binary masks selection. It not only outperforms the current per- pixel classification SoTA on largeclass semantic segmentation datasets, but also generalizes the panoptic segmentation task with a new SoTA result (Tab. III).

### 5.3 Discussion

We summarize the aforementioned Transformers according to three different tasks. Table III(a) focuses on ADE20K (170 classes). It can be shown that when trained on the datasets with large numbers of classes, the segmentation performance of visual Transformers is improved significantly. Table III(b) focuses on COCO test dataset for instance segmentation. Clearly, the visual Transformers with mask embeddings surpass most prevailing models for both segmentation and detection tasks. However, there is a huge performance gap between  $AP^{box}$  and  $AP^{seg}$ . With the cascaded framework, QueryInst [102] attains the SoTA among various Transformer models. It is worthy of further study for combining the visual Transformers with the hybrid task cascade structures. Table III(c) focuses on panoptic segmentation. Max- DeepLab [31] is general to solve both foreground and background in the panoptic segmentation task via a mask prediction format, while Maskformer [32] successfully employs this format for semantic segmentation and unifies both semantic and instance segmentation tasks into a single model. Based on their performances in the panoptic segmentation field, we can conclude that the visual Transformers could unify multiple segmentation tasks into one box- free framework with mask prediction.

## 6 TRANSFORMER FOR 3D VISUAL RECOGNITION

With the rapid development of 3D acquisition technology, stereo/monocular images and LiDAR (Light Detection And Ranging) point clouds become the popular sensory data for 3D recognition. Discriminated from the RGB(D) data, point cloud representation pays more attention to distance, geometry, and shape information. Notably, such a geometric feature is significantly suitable for Transformer on account of its characteristic on sparseness, disorder, and irregularity. Following the success of 2D visual Transformers, substantial approaches are developed for 3D visual analysis. This section exhibits a compact review for 3D visual Transformers following Representation learning, Cognition mapping, and Specific processing.

TABLE III COMPARISON BETWEEN CNN-BASED AND TRANSFORMER-BASED MODEL ON ADE20K AND COCO FOR DIFFERENT SEGMENTATION TASKS.  $^{\ast} + \mathrm{MS}^{\ast}$  DENOTES THE PERFORMANCE TRAINED WITH MULTI-SCALE INPUTS.  

<table><tr><td colspan="8">(a) ADE20K Val. Set for Semantic Segmentation</td></tr><tr><td>Method</td><td>Backbone</td><td>image size</td><td>#Params. (M)</td><td>FLOPs (G)</td><td>FPS</td><td>mIoU</td><td>+MS</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="6">UperNet</td><td>[32]</td><td>R-101 [11]</td><td>512</td><td>87</td><td>238</td><td>23.4</td><td>42.1</td></tr><tr><td>[195]</td><td>R-50 [11]</td><td>512</td><td>66</td><td>257</td><td>20.3</td><td>43.8</td></tr><tr><td>[196]</td><td>Swin-T [36]</td><td>512</td><td>60</td><td>236</td><td>18.5</td><td>44.5</td></tr><tr><td></td><td>Swin-B [36]</td><td>512</td><td>71</td><td>259</td><td>15.2</td><td>47.6</td></tr><tr><td></td><td>Swin-L [36]</td><td>640</td><td>121</td><td>471</td><td>8.7</td><td>50.0</td></tr><tr><td></td><td>MiT-B [36]</td><td>640</td><td>234</td><td>647</td><td>6.2</td><td>52.0</td></tr><tr><td rowspan="3">Segformer [99]</td><td>R-101</td><td>MiT-B4</td><td>512</td><td>67</td><td>79</td><td>59.4</td><td>50.5</td></tr><tr><td>R-101</td><td>MiT-B4</td><td>512</td><td>64</td><td>96</td><td>15.4</td><td>50.3</td></tr><tr><td>R-101</td><td>MiT-B5</td><td>640</td><td>85</td><td>183</td><td>9.8</td><td>51.0</td></tr><tr><td rowspan="3">Segmenter [105]</td><td>ViT-S/B16 [29]</td><td>ViT-S/B16 [29]</td><td>512</td><td>106</td><td>-</td><td>24.8</td><td>48.3</td></tr><tr><td>ViT-L/B16 [29]</td><td>ViT-L/B16 [29]</td><td>640</td><td>304</td><td>-</td><td>34.1</td><td>45.5</td></tr><tr><td>R-50</td><td>R-101</td><td>512</td><td>71</td><td>53</td><td>24.5</td><td>44.5</td></tr><tr><td rowspan="5">MaskFormer [32]</td><td>R-101</td><td>Swin-T [36]</td><td>512</td><td>70</td><td>73</td><td>19.5</td><td>45.5</td></tr><tr><td>R-101</td><td>Swin-S [36]</td><td>512</td><td>73</td><td>75</td><td>22.1</td><td>46.7</td></tr><tr><td>Swin-T [36]</td><td>Swin-B [36]</td><td>512</td><td>72</td><td>79</td><td>19.6</td><td>49.8</td></tr><tr><td>Swin-B [36]</td><td>Swin-L [36]</td><td>640</td><td>102</td><td>195</td><td>12.6</td><td>52.7</td></tr><tr><td></td><td>Swin-L [36]</td><td>640</td><td>222</td><td>375</td><td>7.9</td><td>54.1</td></tr></table>

<table><tr><td colspan="8">(b): COCO Test-Dev for Instance Segmentation</td></tr><tr><td>Method</td><td>Backbone</td><td>Eptoch</td><td>AP box/APseg</td><td>APseg</td><td>APseg</td><td>APseg</td><td>FPS</td></tr><tr><td rowspan="3">Mask R-CNN [188]</td><td>R-50-FPN [11]</td><td>36</td><td>41.3/37.5</td><td>21.1</td><td>39.6</td><td>48.3</td><td>15.3</td></tr><tr><td>R-101-FPN [11]</td><td>36</td><td>43.1/38.8</td><td>21.8</td><td>41.4</td><td>50.5</td><td>11.8</td></tr><tr><td>R-101-FPN [11]</td><td>36</td><td>43.0/37.8</td><td>22.8</td><td>40.9</td><td>51.6</td><td>15.0</td></tr><tr><td>Blend Mask [197]</td><td>R-50-FPN [11]</td><td>36</td><td>44.7/39.6</td><td>18.4</td><td>42.2</td><td>53.4</td><td>11.5</td></tr><tr><td rowspan="2">SOLO v2 [198]</td><td>R-50-FPN [11]</td><td>36</td><td>40.7/38.2</td><td>16.0</td><td>41.2</td><td>55.4</td><td>10.5</td></tr><tr><td>R-101-FPN [11]</td><td>36</td><td>42.6/39.7</td><td>17.3</td><td>42.9</td><td>57.4</td><td>9.0</td></tr><tr><td rowspan="2">ISTR [103]</td><td>R-50-FPN [11]</td><td>36</td><td>46.8/38.9</td><td>27.1</td><td>40.4</td><td>50.6</td><td>13.0</td></tr><tr><td>R-101-FPN [11]</td><td>36</td><td>46.8/39.7</td><td>22.8</td><td>41.5</td><td>52.3</td><td>11.8</td></tr><tr><td rowspan="3">SOLQ [104]</td><td>R-50-FPN [11]</td><td>50</td><td>48.7/39.9</td><td>21.5</td><td>42.5</td><td>53.1</td><td>-</td></tr><tr><td>R-101 [11]</td><td>50</td><td>48.7/40.9</td><td>22.5</td><td>43.8</td><td>54.6</td><td>-</td></tr><tr><td>Swin-L [36]</td><td>50</td><td>55.4/45.9</td><td>27.8</td><td>49.3</td><td>60.5</td><td>-</td></tr><tr><td rowspan="4">QueryInst [102]</td><td>R-50-FPN [11]</td><td>36</td><td>44.8/40.1</td><td>23.3</td><td>42.1</td><td>52.0</td><td>7.0</td></tr><tr><td>R-101-FPN [11]</td><td>36</td><td>45.6/40.6</td><td>23.4</td><td>42.5</td><td>52.8</td><td>10.5</td></tr><tr><td>R-101-FPN [11]</td><td>36</td><td>47.0/41.7</td><td>24.2</td><td>43.9</td><td>53.9</td><td>6.1</td></tr><tr><td>Swin-L [36]</td><td>50</td><td>56.1/49.6</td><td>31.5</td><td>51.8</td><td>63.2</td><td>3.3</td></tr><tr><td colspan="8">(c): COCO Panopticon Minimal for Panopticon Segmentation</td></tr><tr><td>Method</td><td>Backbone</td><td>Eptoch</td><td>#Params. (M)</td><td>FLOPs (G)</td><td>PQ</td><td>PQTh</td><td>PQSt</td></tr><tr><td rowspan="2">DETR [30]</td><td>R-50 [11]</td><td>500+25</td><td>73</td><td>137</td><td>43.4</td><td>48.2</td><td>36.3</td></tr><tr><td>R-101 [11]</td><td>500+25</td><td>72</td><td>137</td><td>43.4</td><td>48.2</td><td>37.0</td></tr><tr><td rowspan="3">MaxDeepLab [31]</td><td>Mix-L</td><td>54</td><td>42</td><td>162</td><td>45.1</td><td>50.5</td><td>37.5</td></tr><tr><td>Mix-S</td><td>54</td><td>41</td><td>1846</td><td>47.0</td><td>52.9</td><td>41.1</td></tr><tr><td>Max-L</td><td>54</td><td>41</td><td>1846</td><td>57.0</td><td>42.2</td><td>51.5</td></tr><tr><td rowspan="5">MaskFormer [199]</td><td>R-50 [11]</td><td rowspan="5">300</td><td>45</td><td>181</td><td>46.5</td><td>51.0</td><td>39.8</td></tr><tr><td>R-101 [11]</td><td>64</td><td>248</td><td>47.6</td><td>52.5</td><td>40.3</td></tr><tr><td>Swin-S [36]</td><td>12</td><td>179</td><td>47.7</td><td>51.7</td><td>41.7</td></tr><tr><td>Swin-B [36]</td><td>102</td><td>411</td><td>49.1</td><td>54.3</td><td>42.6</td></tr><tr><td>Swin-L [36]</td><td>212</td><td>259</td><td>51.7</td><td>56.4</td><td>43.2</td></tr></table>

t denotes the model pre-trained on ImageNet-21k

### 6.1 Representation Learning

Compared with conventional hand- designed networks, visual Transformer is more appropriate for learning semantic representations from point clouds, in which such irregular and permutation invariant nature can be transformed into a series of parallel embeddings with positional information. In view of this, Point Transformer [106] and PCT [107] firstly demonstrate the efficacy of the visual Transformer for 3D representation learning. The former merges a hierarchical Transformer [106] with the down- sampling strategy [200] and extends their previous vector attention block [25] to 3D point clouds. The latter first aggregates neighbour points and then processes such neighbour embeddings on a global off- set Transformer where a knowledge transfer from Graph Convolution Network (GCN) is applied for noise mitigation.

Notably, the positional encoding, a significant operation of the visual Transformer, is diminished in both the approaches because of points' inherent coordinate information. PCT directly processes on the coordinates without positional encodings, while Point Transformer adds a learnable relative positional encoding for further enhancement. Lu et al. leverage a localglobal aggregation module 3DCTN [108] to achieve local enhancement and cost- efficiency. Given the multi- stride downsampling groups, an explicit graph convolution with maxpooling operation are used to aggregate the local information within each group. The resulting group embeddings are concatenated and fed into the improved transformer [106], [107] for global aggregation. Park et al. present Fast Point Transformer [109] to optimize the model efficiency by using voxel- hashing neighbor search, voxel- bridged relative positional encoding, and cosine similarity based local attention.

For dense prediction, Pan et al. propose a customized pointbased backbone Pointformer [110] for attending the local and global interactions separately within each layer. Different from previous local- global forms, a coordinate refinement operation after the local attention is adopted to update the centroid point instead of the surface one. And a local- global cross attention model fuses the high- resolution features, followed by global attention. Fan et al. return to a Single- stride Sparse Transformer (SST) [111] rather than the down- sampling operation to address the problem for small scale detection. Similar to Swin [36], a shifted group in continuous Transformer block is adopted to attend to each group of tokens separately, which further mitigates the computation problem. In voxel- based methods, Voxel Transformer (VoTr) [112] separately operate on the empty and non- empty voxel positions effectively via local attention and dilated attention blocks. VoxSeT [113] further decomposes the self- attention layer into two crossattention layers, and a group of latent codes link them to preserve global features in a hidden space.

Following the mentioned methods in Sec. III- G, a series of self- supervised Transformers are also extended to 3D spaces [114]- [116]. Specifically, Point- BERT [114] and PointMAE [115] directly transfer the previous works [71], [72] to point clouds, while MaskPoint [116] changes the generative training scheme by using a contrastive decoder as similar as DINO (2022) [93] for binary noise/part classification. Based on large experiments, we can conclude that such generative/contrastive self- training methods empower visual Transformers to be valid in either images or points.

### 6.2 Cognition Mapping

Given rich representation features, how to directly map the instance/semantic cognition to the target outputs also arouse considerable interests. Different from 2D images, the objects in 3D scenes are independent and can be intuitively represented by a series of discrete surface points. To bridge the gap, some existed methods transfer domain knowledge into 2D prevailing models. Following [30], 3DETR [117] extends an end- to- end module for 3D object detection via farthest point sampling and Fourier positional embeddings for object queries initialization. Group- Free 3D DETR [118] applies a more specified and stronger structure than [117]. In detail, it directly selects a set of candidate sample points from the extracted point clouds as the object queries and updates them in the decoder layerby- layer iteratively. Moreover, the  $K$  - closed inside points are assigned positive and supervised by a binary objectiveness loss in both sampler and decoder heads. Sheng et al. proposes a typical two- stage method that leverages a Channel- wise Transformer 3D Detector (CT3D) [119] to simultaneously aggregate proposal- aware embedding and channel- wise context information for the point features within each proposal.

For monocular sensors, both MonoDTR [120] and MonoDETR [121] utilize an auxiliary depth supervision to estimate pseudo Depth Positional Encodings (DPEs) during the training process. In MonoDETR [120], DPEs are first attached with the image features for Transformer encoder and then serve as the inputs of the DETR- like [30] decoder to initialize the object queries. In MonoDETR [121], both visual features and DPEs are first extracted by two different encoders parallelly and then interact with object queries via two successive cross- attention layers. Based on foreground depth supervision and narrow categorisation interval, MonoDETR obtains the SoTA result on the KITTI benchmark. DETR3D [122] introduces a multicamera 3D object detection paradigm where both 2D images and 3D positions are associated by the camera transformation matrices and a set of 3D object queries. TransFusion [123] further takes the advantages of both LiDAR points and RGB images by interacting with object queries through two Transformer decoder layers successively. More multi- sensory data fusion are introduced in Sec. VII- A.

### 6.3 Specific Processing

Limited by sensor resolution and view angle, point clouds are afflicted with incompletion, noise, and sparsity problems in real- world scenes. To this end, PoinTr [124] represents the original point cloud as a set of local point proxies and leverages a geometry- aware encoder- decoder Transformer to migrate the centre point proxies towards incomplete points direction. SnowflakeNet [125] formulates the process of completing point clouds as a snowflake- like growth, which progressively generates child points from their parent points implemented by a point- wise splitting deconvolution strategy. A skip- Transformer for adjacent layers further refines the spatialcontext features between parents and children to enhance their connection regions. Choe et al. unify various generation tasks (e.g. denosing, completing and super- resolution) into a Point cloud Reconstruction problem, hence termed PointRecon [126]. Based on voxel hashing, it covers the absolute- scale local geometry and utilizes a PointTransformerlike [106] structure to aggregate each voxel (the query) with its neighbours (the value- key pair) for fine- grained conversion from the discrete voxel to a group of point sets. Moreover, an amplified positional encoding is adapted to the voxel local attention scheme, implemented by using a negative exponential function with L1- loss as weights for vanilla positional encodings. Notably, compared with masked generative self- training, the completion task directly generates a set of complete points without the explicit spatial prior of incomplete points.

## 7 TRANSFORMER FOR MULTI-SENSORY DATA STREAM

In the real world, multiple sensors are always used complementarily rather than a single one. To this end, recent works start to explore different fusing methods to cooperate multi-sensory data stream effectively. Compared with the typical CNNs, Transformer is naturally appropriate for multi-stream data fusion because of its nonspecific embedding and dynamically interactive attention mechanism. This section details these methods according to their data stream sources: Homologous Stream and Heterologous Stream.

### 7.1 Homologous Stream

Homologous stream is a set of multi- sensory data with similar inherent characteristics, such as multi- view, multidimension, and multi- modality visual stream data. They can be categorized into two groups: Interactive Fusion and Transfer Fusion, according to their fusion mechanism.

1) Interactive Fusion: The classical fusion pattern of CNN adopts a channel concatenation operation. However, the same positions from different modalities might be anisotropic, which is unsuitable for the translation-invariant bias of CNN. Instead, the spatial concatenation operation of Transformer enables different modalities to interact beyond the local restriction.

For the local interaction, MVT [127] spatially concatenates the patch embeddings from different views and strengthens their interaction via a modal-agnostic Transformer encoder. Considering the redundant features from different modalities, MVDeTr [128] projects each view of features onto the ground plane and extends the multi- scale deformable attention [77] to a multi- view design. TransFuser [129],COTR [130], and mmFormer [134] deploy a hybrid model. TransFuser models image and LiDAR inputs separately by using two different convolution backbones and links the intermediate feature maps via a Transformer encoder together with a residual connection. COTR shares the CNN backbone for each of view images and inputs the resulted features into a Transformer encoder block with a spatially expanded mesh- grid positional encoding. mmFormer exploits a modality- specific Transformer encoder for each sequence of MRI image and a modality- correlated Transformer encoder for multi- modal modeling.

For the global interaction, Wang et al. [131] leverage a shared backbone to extract the features for different views. Instead of pixel/patch wise concatenation in COTR [130], the extracted view- wise global features are spatially concatenated to perform view fusion within a Transformer. Considering the angular and position discrepancy across different camera views, TransformerFusion [133] first converts each view feature into an embedding vector with the intrinsics and extrinsics of their camera views. These embeddings are then fed into a global Transformer whose attention weights are used for a frame selection so as to compute efficiently. To unify the multisensory data in 3D detection, FUTR3D [132] projects the object queries in DETR- like decoder into a set of 3D reference points. These points together with their related features are subsequently sampled from different modalities and spatially concatenated to update the object queries.

2) Transfer Fusion: Unlike the interactive fusion implemented by the Transformer encoder with self-attention, the other fusing form is more like a transfer learning from the source data to the target one via a cross-attention mechanism. For instance, Tulder et al. [135] insert two cooperative cross-attention Transformers into the intermediate backbone features for bridging the unregistered multi-view medical images. Instead of the pixel-wise attention form, a token-pixel cross-attention is further developed to alleviate arduous computation. Long et al. [136] propose a epipolar spatiotemporal Transformer for multi-view image depth estimation. Given a single video containing a series of static multi-view frames, the neighbour frames are first concatenated and the epipolar is then warped into the centre camera space. The resulted frame volume finally serves as the source data to perform fusion with the centre frame through a cross-attention block. With the spatially-aligned data streams, DRT [137] first explicitly models the relation map between different data streams by using a convolution layer. The resulting maps are subsequently fed into a dual-path cross-attention to build both local and global relationships parallelly, thereby it can collect more regional information for glaucoma diagnosis.

### 7.2 Heterologous Stream

Visual Transformers also perform excellently on heterologous data fusion, especially in visual- linguistic representation learning. Although different tasks may adopt different training schemes, such as supervised/self- supervised learning or compact/large- scale datasets, we here categorize them into two representative groups only according to their cognitive forms: 1) Visual- Linguistic Pre- Training including Vision- Language Pre- training (VLP) and Contrastive Language- Image Pretraining (CLIP), 2) and Visual Grounding such as Phrase Grounding (PG), Referring Expression Comprehension (REC). More comparisons please see Tab. V.

1) Visual-Linguistic Pre-Training: Due to limited annotated data, early VLP methods commonly rely on off-the-shelf object detector [201] and text encoder [5] to extract data-specific features for joint distribution learning. Given an image-text pair, an object detector pre-trained on Visual Genome (VG) [202] first extracts a set of object-centric RoI features from the image. The RoI features serving as visual tokens are then merged with text embeddings for pre-defined tasks pre-training. Basically, these methods are grouped into dual-stream and single-stream fusion.

The dual- stream methods, including ViLBERT [139] and LXMERT [140], apply a vision- language cross- attention layer between two data- specific frameworks for multi- modal transferring fusion. Concretely, ViLBERT [139] is pre- trained through Masked Language Modeling (MLM), Masked Region Classification (MRC), and Image Text Alignment (ITA) on Conceptual Captions (CC) [203] with 3M image- text pairs. LXMERT [140] extends the pre- training datasets to a largescale combination and further indicates that the pre- trained task- specific (BERT [5]) weights initialization is harmful to the pre- training of multi- sensory data fusion.

VideoBERT [138] is the first single- stream VLP method, which clusters latent space features of each video frame as vi

sual tokens and organizes their corresponding text embeddings by using a captioning API. Subsequently, these features are together fed into a cross- modality self- attention layer for joint representation learning. Following [138], VisualBERT [141] extends such a single- stream framework for various imagetext tasks and adds a segment embedding to distinguish between textual and visual tokens. VL- BERT [142] suggests that unmatched image- caption pairs over the ITA pre- training may decrease the accuracy of downstream tasks. And the authors further introduce both text- only corpus and unfrozen detector strategies for pre- training enhancement. Instead, such a "harmful" pre- training strategy is refuted by UNITER [143], and the authors deploy an optimal transport loss to explicitly build Word- Region Alignment (wRA) at the instance level. To the same end, Oscar [144] uses shared linguistic semantic embeddings of a salient object class (called tag) as an anchor point to link both region and its paired words. Zhou et al. propose Unified VLP [145] to handle both generation and understanding tasks via a shared Transformer encoder- decoder with two customized attention masks. Without extra auxiliary training, Unified VLP only adopts MLM during pre- training and attains superior results on Visual Question Answering VQA) [204] and Visual Captioning VC) [205] tasks.

However, these methods rely heavily on the visual extrac- . tor or predefined visual vocabulary, leading to a bottleneck of the VLP expressive upper bound. To address this issue, VinVL [147] develops an improved object detector for VLP pre- training on multiple large- scale dataset combination. Instead of the object- centric Rol features, ViLT [146] initializes the interaction Transformer weights from a pre- trained ViT, and adopts whole word masking and image augmentation strategy for VLP pre- training. UniT [151] follows the architecture of DETR and applies a wide range of task for unified Transformer pre- training via different task- specific output heads simultaneously. SimVLM [152] adopts [40] to obtain image features and designs a Prefix Language Modeling as pretraining objective to generalize zero- shot image captioning.

Besides the conventional pre- training scheme with multitask supervision, another recent line has been developed for contrastive learning. The most representative work is CLIP [148]. Based on the 400M Internet image- text pairs datasets, both image and text encoder are jointly trained by a contrastive loss for ITA. Different from previous methods, CLIP enables the pre- trained model with a linear classifier to zero- shot transfer to the most visual downstream tasks efficiently by embedding the whole semantics of the objective dataset's classes. Based on extensive experiments on over 30 existing CV tasks (e.g., classification and action recognition), CLIP attains superior results to classical supervised methods, demonstrating that such task- agnostic pre- training is also generalized well in the CV field. ALIGN [150] further expands a noisy dataset of over one billion image alt- text pairs rather than the elaborate filtering or post- processing steps in CLIP [148]. Combining masked modeling and contrastive learning pre- training strategy, Data2Vec [153] proposes a selfdistilled network treating the masked features as a type of data augmentation, whose structure is analogous to DINO (2021) [74]. By testing on different sensory benchmarks (voice, image, and language), it achieves competitive or better results compared with the existing self- supervised methods.

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-11/e729979e-6782-440e-aa44-610b9edd5863/8e8f0b87830306c8953d4427b085a2d1253a76ffaa8a027f77945e3e0b796815.jpg)  
Fig.12. The overview of CILP (from [148]).

2) Visual Grounding: Compared with VLP, visual grounding has more concrete target signal supervision whose objective is to locate the target objects according to their corresponding descriptions. In the image space, Modulated DETR (MDETR) [154] extends its previous work [30] to phrase grounding pre-training that locates and assigns the bounding box to each instance phrase in one description. Based on the proposed combined dataset from many existing ones, MDETR is first pre-trained on the 1.3M aligned text-image pairs for PG and then fine-tuned on other downstream tasks. During pre-training, the image-text pair features are separately processed by two specific extractors, and fed into a DETR-like Transformer for salient object localization. Besides the box loss, two auxiliary losses are adopted to enforce network to model an alignment between image feature and their corresponding phrase tokens. With the large-scale image-text pairs pre-training, MDETR can be easily generalized in few-shot learning, even on long-tail data. Different from MDETR [154] adding two auxiliary losses for box-phrase alignments, Referring Transformer [157] directly initializes object queries with phrase-specific embeddings for PG, which explicitly reserves an one-to-one phrase assignment for final bounding box prediction. VGTR [156] reformulates the REC as a task for single salient object localization from the language features. In detail, a text-guided attention mechanism encapsulates both self-attention block and text-image cross-attention one to update the image features simultaneously. The resulted image features, which serve as the key-value pairs, interact with language queries when regressing bounding box coordinates in the decoder. Following ViT [29], TransVG [155] keeps the class token to aggregate the image and language features simultaneously for the mentioned object localization in REC. Pseudo-Q [158] focuses on REC for the unsupervised learning, where a pseudo-query generation module based on a pre-trained detector and a series of attributes&relationship generation algorithm is applied to generate a set of pseudo phrase descriptions, and a query prompt is introduced to match feature proposals and phrase queries for REC adaptation.

In the 3D spaces, LanguageReferr [159] redefines the multistream data reasoning as a language modeling problem, whose core idea is to omit point cloud features and infuse the predicted class embeddings together with a caption into a language model to get a binary prediction for object selection. Following the conventional two- stream methods, TransRe- fo3D [160] further enhances the relationship of the object

features by using a cross- attention between asymmetric object relation maps and linguistic features. Considering the specific view for varied descriptions, Huang et al. present a Multi- View Transformer (MVT 2022) [161] for 3D visual grounding. Given a shared point cloud feature for each object, MVT first appends the converted bounding box coordinates to the shared objects in order to get specific view features. These multiview features are then fed into a stack of the Transformer decoders for text data fusion. Finally, the multi- view features are merged by an order- independent aggregation function and converted to the grounding score. MVT achieves the SoTA performance on Nr3D and Sr3D datasets [206]. In the video space, a specific 3D data (with temporal dimension), Yang et al. propose TubeDETR [162] to address the problem of Spatio- Temporal Video Grounding (STVG). Concretely, a slow- fast encoder sparsely samples the frames and performs cross- modal self- attention between the sampled frames and the text features in the slow branch, and aggregates the updated sample features into the full- frame features from fast branch via a broadcast operation. A learnable query attached with different time encodings, called time- specific queries in the decoder is then predicted as either a time- aligned bounding box or "no object". It attains SoTA results on STVG leaderboards.

## 8 DISCUSSION AND CONCLUSION

This section briefly conducts a summary of the performance improvements provided in Sec. VIII- A, some critical issues discussed in Sec. VIII- B, future research directions suggested in Sec. VIII- C, and final conclusion given in Sec. VIII- D.

### 8.1 Summary of Recent Improvements

We briefly summarize the major performance improvements for three fundamental CV tasks as follows.

(1) For classification, a deep hierarchical Transformer backbone is valid for decreasing the computational complexity [42] and avoiding the feature over-smooth [38], [43], [67], [68] in the deep layer. Meanwhile, the early-stage convolution [40] is enough to capture the low-level features, which can significantly enhance the robustness and reduce the computational complexity in the shallow layer. Moreover, both the convolutional projection [53], [56] and the local attention mechanism [36], [45] can improve the locality of the visual Transformers. The former [57], [58] may also be a new approach to replace the positional encoding.

(2) For detection, the Transformer necks benefit from the encoder-decoder structure with less computation than the encoder-only Transformer detector [89]. Thus, the decoder is necessary but it requires more spatial prior [77], [81]- [84], [86], [87] owing to its slow convergence [88]. Furthermore, sparse attention [77] and scoring network [79], [80] for fore-grounding sampling are conducive to reducing the computational costs and accelerating the convergence of visual Transformers.

(3) For segmentation, the encoder-decoder Transformer models may unify three segmentation sub-tasks into a mask prediction problem via a set of learnable mask embeddings [31], [105], [199]. This box-free approach has achieved the latest SoTA performance on multiple benchmarks [199]. Moreover, the specific hybrid task is cascaded with the model [102] of the box- based visual Transformers, which have demonstrated a higher performance for instance segmentation.

(4) For 3D visual recognition, the local hierarchical Transformer with a scoring network could efficiently extract features from the point clouds. Instead of the elaborate local design, the global modeling capability enables the Transformer to easily aggregate surface points. In addition, the visual Transformers can handle multi-sensory data in 3D visual recognition, such as multi-view and multi-dimension data.

(5) The mainstream approaches of visual-linguistic pretraining has gradually abandoned the pre-trained detector [146] and focused on the alignments [148] or similarities [153] among different data streams in the latent space based on the large-scale noised datasets [150]. Another concern is to adapt the downstream visual tasks to the pre-training scheme to perform zero-short transferring [148].

(6) The recent prevailing architecture for multi-sensory data stream fusion is the single-stream method, which spatially concatenates different data streams and performs interaction simultaneously. Based on the single-stream model, numerous recent works devote to finding a latent space to make different data streams semantically consistent.

### 8.2 Discussion on Visual Transformers

Despite that the visual Transformer models are evolved significantly, the "essential" understanding remains insufficient. Therefore, we will focus on reviewing some key issues for a deep and comprehensive understanding.

1) How Transformers Bridge the Gap Between Language and Vision: Transformers are initially designed for machine translation tasks [1], where each word of a sentence is taken as a basic unit representing the high-level semantics. These words can be embedded into a series of vector representations in the N-dimensional feature space. For visual tasks, each single pixel of an image is unable to carry semantic information, which is not full compliance with the feature embedding as done for the traditional NLP tasks. Therefore, the key for transferring such feature embeddings (i.e., word embedding) to image features and applying Transformer to various vision tasks is to build an image-to-vector transformation and maintain the image's characteristics effectively. For example, ViT [29] transforms an image into patch embeddings with multiple low-level information under strong slackness conditions. And its votarist [40], [59] leverages convolution to extract the low-level features and reduce the redundancy from patches.

2) The Relationship between Transformers, Self-Attention and CNNs: From the perspective of CNNs, its inductive bias is mainly shown as locality, translation invariance, weight sharing, and sparse connection. Such a simple convolutional kernel can perform template matching efficiently in lower-level semantic processing but its upper-bound tend to be lower than Transformers due to the excessive bias.

From the perspective of Transformers, as detailed in Sec. III- B and Sec. III- D, attention layer can theoretically express any convolution when a sufficient number of heads

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-11/e729979e-6782-440e-aa44-610b9edd5863/292c061e518cd9d73c3c3ec3cd042e61a68a82fb9cce0db50700fadebb8c4c8d.jpg)  
Fig. 13. Taxonomy of the learnable embedding.

are adopted [28]. Such fully- attentional operation can combine both local- level and global- level attentions, and generate attention weights dynamically according to the feature relationships. Dong et al. demonstrate that the self- attention layer manifests strong inductive bias towards "token uniformity" when it is trained on deep layers without short connection or FFNs [169]. Yu et al. also argue that such an elaborate attention mechanism can be replaced by a pooling operation readily [207]. Therefore, it is concluded that Transformer must consist of two key components: a global token mixer (e.g., self- attention layer) aggregates the relationship of tokens, and a position- wise FFN extracts the features from the inputs.

By comparison, the visual Transformer has a powerful global modelling capability, making it efficiently attend to high- level semantic features. CNNs can effectively process the low- level features [40], [59], enhance the locality of the visual Transformers [54], [82], and append the positional features via padding operations [57], [58], [174].

3) Double Edges of Visual Transformers: We conclude three double-edged properties of visual Transformers as follows. Global property enables Transformer to acquire capacious receptive fields and interact easily between various high-level semantic features, while it becomes inefficiency and debility during low-level processing because of quadratic computing and noised low-level features. Slack bias offers visual Transformer a higher upper bound than CNNs based on sufficient training data without sophistic assumptions but performs inferiority and slow convergency in small datasets [208]. Low-pass is also a significant property of visual Transformer showing excellent robustness, whereas it is insensitive to low-level features (e.g., complicated textures and edges) compared with CNN. Accordingly, it is concluded that Transformers at a high-level stage play a vital role in various vision tasks.

4) Learnable Embeddings for Different Visual Tasks: Various learnable embeddings are designed to perform different visual tasks, such as class token, object query, and mask embedding. These learnable tokens are mainly adopted into two different Transformer patterns, i.e., encoder-only and encoder-decoder ones, as illustrated in Fig. 13. On the quantity level, the number of learned tokens depends on the target prediction. For example, the visual Transformers [29], [41] in the classification task adopt only one class token, and the DETR's votarist in detection [30], [82] and segmentation [199] tasks employ multiple learned queries. On the position level, encoder-only Transformers capitalize on the initial token(s) [29], [89] and later token(s) [43], [105], while

the learned positional encoding [30], [82], [199] and the learned decoder input embedding [77] are applied to the encoder- decoder structure. Different from the vanilla ViT with initial class token, CaiT [43] observes that the later class token can reduce FLOPs of Transformer and improve model performance slightly. Segmenter [105] also shows such strategy efficiency for the segmentation tasks. From the viewpoint of the encoder- decoder Transformer, the decoder input token is considered as a special case of the encoder- only Transformer with later token. It standardizes visual Transformers in the fields of detection [30] and segmentation [199] by using a small set of object queries (mask embeddings). By combing both later tokens and object queries (mask embeddings), the structure like Deformable DETR [77], which takes object queries and the learnable decoder embeddings (equivalent to the later tokens) as the inputs, may unify the learnable embeddings for different tasks into the encoder- decoder Transformer.

### 8.3 Future Research Directions

Visual Transformers have achieved significant progresses and obtained promising results. However, some key technologies are still insufficient to cope with complicated challenges in the CV fields. Based on the above analysis, we point out some promising research directions for future investigation.

1) Set Prediction: Touvron et al. found that multiple class tokens would converge consistently due to the same gradient from the loss function [41], whereas it does not emerge in dense prediction tasks [30], [199]. We conclude that their marked difference lies in the label assignment and the number of targets. Thus, it is natural to consider a set prediction design for the classification tasks, e.g., multiple class tokens are aligned to mix-patches via set prediction, like the data augmentation training strategy in LV-ViT [44]. Furthermore, the label assignment in the set prediction strategy leads to training instability during the early process, which degrades the accuracy of the final results. Redesigning the label assignments and set prediction losses may be helpful for the detection frameworks.

2) Self-Supervised Learning: Self-supervised pre-training of Transformers has standardized the NLP field and obtained tremendous successes in various applications [2], [5]. Because of the popularity of self-supervision paradigms in the CV field, the convolutional Siamese networks employ contrastive learning to perform self-supervised pre-training, which differs from the masked auto-encoders used in the NLP field. Recently, some studies have tried to design self-supervised visual Transformers to bridge the discrepancy of pre-training methodology between vision and language. Most of them inherit the masked auto-encoders in the NLP field or contrastive learning schemes in the CV field. There is no specific supervised method for the visual Transformers, but it has revolutionized the NLP tasks such as GPT-3. As described in Sec. VIII-B4, the encoder-decoder structure may unify the visual tasks by learning the decoder embedding and the positional encoding jointly. Thus it is worth of further investigating the encoder-decoder Transformers for self-supervised learning.

### 8.4 Conclusion

Since ViT demonstrated its effectiveness for the CV tasks, the visual Transformers have received considerable attentions and undermined the dominant of CNNs in the CV field. In this paper, we have comprehensively reviewed more than one hundred of visual Transformer models which have been successively applied to various vision tasks (i.e., classification, detection, and segmentation) and data streams (e.g., images, point clouds, image-text pairs, and other multiple data streams). For each vision task and data stream, a specific taxonomy is proposed to organize the recently-developed visual Transformers and their performances are further evaluated over various prevailing benchmarks. From our integrative analysis and systematic comparison of all these existing methods, a summary of remarkable performance improvements is provided in this paper, four essential issues for the visual Transformers are also discussed, and several potential research directions are further suggested for future investment. We do expect that this review paper can help readers have better understandings of various visual Transformers before they decide to perform deep explorations.

## REFERENCES

## APPENDIX A OVERVIEW OF DEVELOPMENT TREND ON VISUAL TRANSFORMERS

Transformer backbones sprang up within the last year. When our systematics matches the timeline of these models, we can clearly trace the development tendency of Transformer for image classification (the Fig. 1 in main text). As a type of self- attention mechanism, visual Transformers are mainly redesigned according to either the vanilla structure in NLP (ViT [29] and iGPT [69]) or attention- based model in CV (VTs [52] and BoTNet [53]).

Then, many approaches start to extend the hierarchical or deep structure of CNN to visual Transformer. T2T- ViT [64], PVT [42], CvT [37] and PiT [65] share a motivation that transferring the hierarchical structure into Transformer but they perform downsampling differently. CaiT [43], Diverse Patch [68], DeepViT [67], and Refiner [38] focus on the problem in deep Transformer. Moreover, some approaches move on to the internal components to further enhance the image processing capability in previous Transformers, i.e., positional encoding [57], [209], [210], MHSA [28], and MLP [169].

The next wave of Transformers is locality paradigm. Most of them introduce locality into Transformer via introducing local attention mechanism [36], [45], [60], [61] or convolution [54]- [56]. Nowadays, the most recent supervised Transformers are exploring both the structural combination [40], [59] and scaling laws [39], [211]. In addition to supervised Transformers, self- supervised learning accounts for a substantial part of vision Transformers [69]- [71], [73]- [75]. However, it is unclear what tasks and structures are more beneficial to self- supervised Transformer in CV.

## APPENDIX B MORE DETAIL FORMULA OF DETR

The bipartite matching loss  $\mathcal{L}_{\mathrm{match}}$  is applied between prediction  $\hat{y}_{\sigma (i)}$  and ground- truth objects  $y_{i}$  to identify one- to- one

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-11/e729979e-6782-440e-aa44-610b9edd5863/e97f577e30c9975fdd277614463b5a26edcd82aefbce109f7f7f2ccd4691c7a4.jpg)  
Fig. 14. Illustration of Deformable DETR. A fixed number of key samples in each scale feature interacting with all queries.(from [77].)

label assignment  $\hat{\sigma}$  as

$$
\begin{array}{rl} & {\hat{\sigma} = \underset {\sigma \in \mathfrak{G}_N}{\mathrm{argmin}}\sum_i^N\mathcal{L}_{\mathrm{match}}(y_i,\hat{y}_{\sigma (i)}),}\\ & {\mathcal{L}_{\mathrm{match}}(y_i,\hat{y}_{\sigma (i)}) = -\mathbb{1}_{\{c_i\neq \sigma \}}\hat{p}_{\sigma (i)}(c_i)}\\ & {\qquad +\mathbb{1}_{\{c_i\neq \sigma \}}\mathcal{L}_{\mathrm{box}}(b_i,\hat{b}_{\sigma (i)}).} \end{array} \tag{6}
$$

In back propagation, the Hungarian loss  $\mathcal{L}_{\mathrm{H}}$  includes a negative log- likelihood loss for all label predictions  $(i = 1\dots N)$  and a box loss for all matched pairs  $(c_{i}\neq \emptyset)$  as

$$
\mathcal{L}_{\mathrm{H}}(y_i,\hat{y}_{\sigma (i)}) = \sum_{i = 1}^{N}\left[-\mathrm{log}\hat{p}_{\hat{\sigma} (i)}(c_i) + \mathbb{1}_{\{c_i\neq \emptyset \}}\mathcal{L}_{\mathrm{box}}(b_i,\hat{b}_{\sigma (i)})\right]. \tag{7}
$$

## APPENDIX C

### MORE DETAILED FORMULA OF DEFORMABLE DETR

Given  $L$  feature maps  $X^{l}\in \mathbb{R}^{H_{l}\times W_{l}\times C}$  and a query sequence  $\mathbf{z}\in \mathbb{R}^{N_q\times C}$  , MSDA samples offsets  $\Delta \mathbf{p}\in \mathbb{R}^2$  of each query for  $N_{k}$  sample keys at each layer's head via two linear layers. While it is sampling the features of key points  $V_{i}\in \mathbb{R}^{L\times N_{k}\times C_{v}}$  , a linear projection layer is applied to the query to generate an attention map  $A_{i}\in \mathbb{R}^{N_{q}\times L\times N_{k}}$  for key samples, where  $N_{q}$  and  $C$  are query's length and dimension, respectively (see Fig. 14). The process is formulated as

$$
\begin{array}{r}A_{qik}^{l} = \mathbf{z}_{q}W_{ilk}^{A},V_{ik}^{l} = X^{l}(\phi_{t}(\hat{\mathbf{p}}_{q}) + \Delta \mathbf{p}_{ilqk})W_{i}^{V},\\ \mathrm{MSDA}\mathrm{Attn}(A_{qik}^{l},V_{ik}^{l}) = \sum_{i = 1}^{h}\left(\sum_{l = 1}^{L}\sum_{k = 1}^{N_{k}}A_{qik}^{l}V_{ik}^{l}\right)W_{i}^{O}, \end{array} \tag{8}
$$

where  $m$  denotes the attention head,  $W_{il}^{A}\in \mathbb{R}^{C\times N_{l}},W_{i}^{V}\in \mathbb{R}^{C\times C}$  and  $W_{i}^{O}\in \mathbb{R}^{C_{v}\times C}$  are linear matrices.  $\hat{\mathbf{p}}_q\in [0,1]^2$  is normalized coordinates of each query.

## APPENDIX D MORE COMPARISON OF VISUAL TRANSFORMERS FOR DENSE PREDICTION AND VISUAL-LINGUISTIC PRE-TRAINING

Based on RetinaNet [183] and Mask R- CNN [188], Tab. IV compares several visual Transformer backbones on the COCO datasets for the dense prediction tasks. And Tab. V collects the visual Transformers as mentioned in visual- linguistic pretraining (Sec. VII- B1). Concretely, we summarize the main

TABLE IV DENSE PREDICTION RESULTS OF COCO 2017 VAL. SET BASED ON RETINANET [183] AND MASK R-CNN [188], WHEN TRAINED WITH  $3\times$  SCHEDULE AND MULTI-SCALE INPUTS (MS). THE NUMBERS BEFORE AND AFTER  $\mathbf{+}^{\prime \prime}$  CORRESPOND TO THE PARAMETER OF RETINANET AND MASK R-CNN, RESPECTIVELY. (MOST OF DATA FROM [63].)  

<table><tr><td rowspan="2">Backbone</td><td rowspan="2">#Params (M)</td><td rowspan="2">FLOPs (G)</td><td colspan="5">RetinaNet 3× schedule + MS</td><td colspan="6">Mask R-CNN 3× schedule + MS</td><td></td></tr><tr><td>APbox</td><td>APbox 50</td><td>APbox 75</td><td>APbox 50</td><td>APbox 75</td><td>APbox 50</td><td>APbox 75</td><td>APbox 50</td><td>APbox 75</td><td>APseg</td><td>APseg 50</td><td>APseg 75</td></tr><tr><td>ResNet50 [11]</td><td>38 / 44</td><td>239 / 260</td><td>39.0</td><td>58.4</td><td>41.8</td><td>22.4</td><td>42.8</td><td>51.6</td><td>41.0</td><td>61.7</td><td>44.9</td><td>37.1</td><td>58.4</td><td>40.1</td></tr><tr><td>PVTv1-Small [42]</td><td>34 / 44</td><td>226 / 245</td><td>42.2</td><td>62.7</td><td>45.0</td><td>25.4</td><td>45.2</td><td>57.2</td><td>43.0</td><td>65.3</td><td>46.0</td><td>39.9</td><td>62.5</td><td>42.8</td></tr><tr><td>ViL-Small [62]</td><td>36 / 48</td><td>252 / 174</td><td>42.9</td><td>63.8</td><td>45.6</td><td>27.8</td><td>46.4</td><td>56.3</td><td>43.4</td><td>64.9</td><td>47.0</td><td>39.6</td><td>62.1</td><td>42.4</td></tr><tr><td>Swin-Tiny [36]</td><td>39 / 48</td><td>245 / 264</td><td>45.0</td><td>65.9</td><td>48.4</td><td>29.7</td><td>48.9</td><td>58.1</td><td>46.0</td><td>68.1</td><td>30.3</td><td>41.6</td><td>65.1</td><td>44.9</td></tr><tr><td>PVTv2-B2-Li [66]</td><td>32 / 42</td><td>- / -</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>46.8</td><td>68.7</td><td>31.4</td><td>42.3</td><td>65.7</td><td>45.4</td><td></td></tr><tr><td>Focal-Tiny [63]</td><td>39 / 49</td><td>265 / 291</td><td>45.5</td><td>66.3</td><td>48.8</td><td>31.2</td><td>49.2</td><td>58.7</td><td>47.8</td><td>69.4</td><td>31.9</td><td>42.7</td><td>66.5</td><td>45.9</td></tr><tr><td>PVTv2-B2 [66]</td><td>35 / 45</td><td>- / -</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>47.2</td><td>69.7</td><td>52.6</td><td>43.1</td><td>66.8</td><td>46.7</td><td></td></tr><tr><td>ResNet101 [11]</td><td>37 / 63</td><td>315 / 336</td><td>40.9</td><td>60.1</td><td>44.0</td><td>23.7</td><td>43.0</td><td>55.8</td><td>42.8</td><td>65.2</td><td>47.1</td><td>38.3</td><td>60.1</td><td>41.3</td></tr><tr><td>ResNeXt101-32x4d [212]</td><td>36 / 63</td><td>319 / 340</td><td>41.4</td><td>61.0</td><td>44.3</td><td>23.9</td><td>45.5</td><td>53.7</td><td>44.0</td><td>64.4</td><td>48.0</td><td>39.2</td><td>61.4</td><td>41.9</td></tr><tr><td>PVTv1-Medium [42]</td><td>31 / 64</td><td>238 / 301</td><td>43.2</td><td>63.8</td><td>46.1</td><td>27.3</td><td>46.3</td><td>58.9</td><td>44.2</td><td>66.0</td><td>48.2</td><td>40.5</td><td>63.1</td><td>43.5</td></tr><tr><td>ViL-Medium [62]</td><td>34 / 60</td><td>239 / 262</td><td>43.7</td><td>64.6</td><td>46.4</td><td>27.9</td><td>47.1</td><td>56.9</td><td>44.6</td><td>66.3</td><td>38.5</td><td>40.7</td><td>63.8</td><td>43.7</td></tr><tr><td>Swin-Small [36]</td><td>60 / 69</td><td>335 / 354</td><td>46.4</td><td>67.0</td><td>50.1</td><td>31.0</td><td>50.1</td><td>60.3</td><td>48.5</td><td>70.2</td><td>33.5</td><td>43.3</td><td>67.3</td><td>46.6</td></tr><tr><td>Focal-Small [63]</td><td>62 / 71</td><td>367 / 401</td><td>47.3</td><td>67.8</td><td>51.0</td><td>31.6</td><td>50.9</td><td>61.1</td><td>48.8</td><td>70.5</td><td>33.6</td><td>43.8</td><td>67.7</td><td>47.2</td></tr><tr><td>ResNeXt101-64x4d [212]</td><td>96 / 102</td><td>373 / 493</td><td>41.8</td><td>61.5</td><td>44.4</td><td>25.1</td><td>45.4</td><td>54.6</td><td>44.4</td><td>64.9</td><td>38.8</td><td>39.7</td><td>61.9</td><td>42.6</td></tr><tr><td>PVTv1-Large [42]</td><td>71 / 81</td><td>445 / 364</td><td>43.4</td><td>63.6</td><td>46.1</td><td>26.2</td><td>46.0</td><td>59.5</td><td>44.5</td><td>66.0</td><td>48.3</td><td>40.7</td><td>63.4</td><td>43.7</td></tr><tr><td>ViL-Base [62]</td><td>67 / 76</td><td>443 / 365</td><td>44.7</td><td>65.5</td><td>47.6</td><td>29.9</td><td>48.0</td><td>58.1</td><td>45.7</td><td>67.2</td><td>39.9</td><td>41.3</td><td>64.4</td><td>44.5</td></tr><tr><td>Swin-Base [36]</td><td>95 / 107</td><td>477 / 496</td><td>45.8</td><td>66.4</td><td>49.1</td><td>29.9</td><td>49.4</td><td>60.3</td><td>48.5</td><td>69.8</td><td>33.2</td><td>43.4</td><td>66.8</td><td>46.9</td></tr><tr><td>Focal-Base [63]</td><td>101 / 110</td><td>514 / 533</td><td>46.9</td><td>67.8</td><td>50.3</td><td>31.9</td><td>50.3</td><td>61.5</td><td>49.0</td><td>70.1</td><td>33.6</td><td>43.7</td><td>67.6</td><td>47.0</td></tr></table>

TABLE V DETAIS OF VISUAL-LINGUISTIC PRE-TRAINING METHODS, WHERE  $\bullet$  AND  $\bullet \bullet$  DENOTE SINGLE- AND DUAL-STREAM ARCHITECTURE, RESPECTIVELY, AND THE ZERO-SHOT DENOTES THE METHOD CAN BE ZERO-SHOT TRANSFERED INTO DOWN STREAM TASKS. IN THE PRE-TRAINING TASKS, MRM IS MASKED REGION MODELING, OD IS OBJECT DETECTION, SMLM AND BMLM DENOTE BOTH SEQUENTIALLY AND BIDDEECTIONALLY MASKED LANGUAGE MODELING, AND MVM IS MASKED VISUAL-TOKEN MODELING.  

<table><tr><td rowspan="2">Methods</td><td rowspan="2">Arch.</td><td rowspan="2">Visual Token</td><td colspan="3">Pre-training</td><td rowspan="2">Zero Shot</td><td rowspan="2">Publication</td></tr><tr><td>Main Dataset(s)</td><td>Data Size</td><td>Tasks</td></tr><tr><td>Region-Based Methods</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>VideoBERT [138]</td><td>●</td><td>S3D [213]/w k-means</td><td>YouTube Cooking [138]</td><td>312K</td><td>ITA, MLM, MVM</td><td>✓</td><td>ICCV 2019</td></tr><tr><td>ViLBERT [139]</td><td>●</td><td>RoI [201]</td><td>CC3M [203]</td><td>3.1M</td><td>ITA, MLM, MRC, KL</td><td>-</td><td>NeurIPS 2019</td></tr><tr><td>LXMERT [140]</td><td>●</td><td>RoI [201]</td><td>VQ-QA VQAv2 [214], VG [202], COCO [215], GQA [216]</td><td>9.2M</td><td>ITA, MLM, MRM, MRC</td><td>-</td><td>IJCNLP 2019</td></tr><tr><td>VisualBERT [141]</td><td>●</td><td>Faster RCNN [181]</td><td>COCO [215]</td><td>0.9M</td><td>ITA, MLM</td><td>-</td><td>Arxiv 2019</td></tr><tr><td>VL-BERT [142]</td><td>●</td><td>RoI [201]</td><td>CC3M [203], BooksCorpus &amp;amp;English, Wikipedia</td><td>11M</td><td>MLM, MRC</td><td>-</td><td>ICLR 2020</td></tr><tr><td>UNITER [143]</td><td>●</td><td>RoI [201]</td><td>CC3M [203], SBU [217], COCO [215], VG [202], COCO [215], VQA [216]</td><td>9.5M</td><td>ITA, WRAM, MLM/MRM</td><td>-</td><td>ECCV 2020</td></tr><tr><td>Oscar [144]</td><td>●</td><td>RoI [201] +Tags</td><td>CC3M [203], SBU [217], VG [202], Fliker30K [218]</td><td>11.4M</td><td>ITA, MLM</td><td>-</td><td>ECCV 2020</td></tr><tr><td>Unified-VLP [145]</td><td>●</td><td>RoI [201]</td><td>CC3M [203]</td><td>3.1M</td><td>SMLM, BMLM</td><td>-</td><td>AAAI 2020</td></tr><tr><td>VinVL(Oscar+) [147]</td><td>●</td><td>RoI [201]/w NMS +Tags</td><td>SBU [217], VQQDas [202], COCO [215], CC3M [203], GQA [216], Fliker30K [218], VQA [204], OpenImages [219]</td><td>8.9M</td><td>MLM, ITA</td><td>-</td><td>CVPR 2021</td></tr><tr><td>Feature-Based Methods</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ViLT [146]</td><td>●</td><td>Patches from ViT [29]</td><td rowspan="2">SBU [217], CC3M [203], COCO [215], VG [202], COCO [215], VG [202], VQQAv2 [214], SNLI-VE Four LM Datasets</td><td>10M</td><td>ITM, MLM</td><td>✓</td><td>ICML 2021</td></tr><tr><td>UniT [151]</td><td>●</td><td>DETR-ResNet50 [30]</td><td>-</td><td>OD, 4LM, 2ILM</td><td>✓</td><td>ICCV 2021</td></tr><tr><td>CLIP [148]</td><td>●</td><td>ViT [29]</td><td>Internet Pairs [148]</td><td>400M</td><td>Contrasive</td><td>✓</td><td>ICML 2021</td></tr><tr><td>DALL-E [149]</td><td>●</td><td>dVAE</td><td>Extension [149] from COCO</td><td>250M</td><td>Contrasive</td><td>✓</td><td>ICML 2021</td></tr><tr><td>ALIGN [150]</td><td>●</td><td>EfficientNet [12]</td><td>Noise English al-text data [150]</td><td>1.8B</td><td>Contrasive</td><td>✓</td><td>ICML 2021</td></tr><tr><td>SimVLM [152]</td><td>●</td><td>CoAtNet [40]</td><td>Noise English al-text data [150]</td><td>1.8B</td><td>PLM</td><td>✓</td><td>ICLR 2022</td></tr><tr><td>Data2Vec [153]</td><td>●</td><td>ViT [29]</td><td>ImageNet-1k LS-960 Books Corpus &amp;amp; English Wikipedia data</td><td>1k 960h 1M</td><td>Self-Distillation</td><td>✓</td><td>Arxiv 2022</td></tr></table>

datasets, data size, and objective task for each visual Transformer during the pre- training. The architecture (single- /dual- stream) and the type of visual token inputs are also organized for each approach.

## REFERENCES

[1] A. Vaswani et al. Attention is all you need. In NeurIPS, pp. 5998- 6008, 2017. [2] A. Radford et al. Improving language understanding by generative pre- training. 2018. [3] A. Radford et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [4] T. B. Brown et al. Language models are few- shot learners. In NeurIPS, pp. 1877- 1901, 2020. [5] J. Devlin et al. Bert: Pre- training of deep bidirectional transformers for language understanding. In NAACL, pp. 4171- 4186, 2018. [6] Y. Liu et al. Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692, 2019. [7] Z. Lan et al. Albert: A line bert for self- supervised learning of language representations. In ICLR, 2020. [8] Z. Yang et al. XInet: Generalized autoregressive pretraining for language understanding. In NeurIPS, pp. 5753- 5763, 2019. [9] D. W. Otter et al. A survey of the usages of deep learning for natural language processing. IEEE Trans. Neural Netw. Learn. Syst., 32(2):604- 624, 2020. [10] A. Krizhevsky et al. Imagenet classification with deep convolutional neural networks. In NeurIPS, pp. 1097- 1105, 2012. [11] K. He et al. Deep residual learning for image recognition. In CVPR, pp. 770- 778, 2016. [12] M. Tan and Q. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, pp. 6105- 6114, 2019. [13] A. Galassi et al. Attention in natural language processing. IEEE Trans. Neural Netw. Learn. Syst., 32(10):4291- 4308, 2020. [14] X. Wang et al. Non- local neural networks. In CVPR, pp. 7794- 7803, 2018. [15] Z. Huang et al. Ccnet: Criss- cross attention for semantic segmentation. In ICCV, pp. 603- 612, 2019. [16] Y. Cao et al. Ccnet: Non- local networks meet squeeze- excitation networks and beyond. In ICCV Workshop, pp. 1971- 1980, 2019. [17] J. Hu et al. Squeeze- and- excitation networks. In CVPR, pp. 7132- 7141, 2018. [18] S. Woo et al. Cbam: Convolutional block attention module. In ECCV, pp. 3- 19, 2018. [19] Q. Wang et al. Eca- net: Efficient channel attention for deep convolutional neural networks. In CVPR, pp. 11534- 11542, 2020. [20] N. Parmar et al. Image transformer. In ICML, pp. 4055- 4064, 2018. [21] H. Hu et al. Relation networks for object detection. In CVPR, pp. 3588- 3597, 2018. [22] H. Hu et al. Local relation networks for image recognition. In ICCV, pp. 3464- 3473, 2019. [23] I. Bello et al. Attention augmented convolutional networks. In ICCV, pp. 3286- 3295, 2019. [24] P. Ramachandran et al. Stand- alone self- attention in vision models. In NeurIPS, pp. 68- 80, 2019. [25] H. Zhao et al. Exploring self- attention for image recognition. In CVPR, pp. 10076- 10085, 2020. [26] Z. Zheng et al. Global and local knowledge- aware attention network for action recognition. IEEE Trans. Neural Netw. Learn. Syst., 32(1):334- 347, 2020. [27] A. Vaswani et al. Scaling local self- attention for parameter efficient visual backbones. In CVPR, pp. 12894- 12904, 2021. [28] J.- B. Cordonnier et al. On the relationship between self- attention and convolutional layers. In ICLR, 2020. [29] A. Dosovitskiy et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [30] N. Carion et al. End- to- end object detection with transformers. In ECCV, pp. 213- 229, 2020. [31] H. Wang et al. Max- deeplab: End- to- end panoptic segmentation with mask transformers. In CVPR, pp. 5463- 5474, 2021. [32] B. Cheng et al. Per- pixel classification is not all you need for semantic segmentation. In NeurIPS, pp. 17864- 17875, 2021. [33] X. Chen et al. Transformer tracking. In CVPR, pp. 8126- 8135, 2021. [34] Y. Jiang et al. Transgant. Two pure transformers can make one strong gan, and that can scale up. arXiv:2102.07074, 2021.

[35] H. Chen et al. Pre- trained image processing transformer. In CVPR, pp. 12299- 12310, 2021. [36] Z. Liu et al. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, pp. 10012- 10022, 2021. [37] H. Wu et al. Cvt: Introducing convolutions to vision transformers. In ICCV, pp. 22- 31, 2021. [38] D. Zhou et al. Refiner: Refining self- attention for vision transformers. arXiv:2106.03714, 2021. [39] X. Zhai et al. Scaling vision transformers. arXiv:2106.04560, 2021. [40] Z. Dai et al. Coatnet: Marrying convolution and attention for all data sizes. In NeurIPS, pp. 3965- 3977, 2021. [41] H. Touvron et al. Training data- efficient image transformers & distillation through attention. In ICLR, pp. 10347- 10357, 2021. [42] W. Wang et al. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In ICCV, pp. 568- 578, 2021. [43] H. Touvron et al. Going deeper with image transformers. In ICCV, pp. 32- 42, 2021. [44] Z.- H. Jiang et al. All tokens matter: Token labeling for training better vision transformers. In NeurIPS, pp. 18590- 18602, 2021. [45] L. Yuan et al. Volo: Vision outlooker for visual recognition. arXiv:2106.13112, 2021. [46] Y. Tay et al. Efficient transformers: A survey. ACM Comput. Surv. (CSUR), 2020. [47] S. Khan et al. Transformers in vision: A survey. ACM Comput. Surv. (CSUR), 2021. [48] K. Han et al. A survey on vision transformer. IEEE Trans. Pattern Anal. Mach. Intell., 2022. [49] T. Lin et al. A survey of transformers. AI Open, 2022. [50] I. Sutskever et al. Sequence to sequence learning with neural networks. In NeurIPS, pp. 3104- 3112, 2014. [51] K. Greff et al. LSTM: A search space odyssey. IEEE Trans. Neural Netw. Learn. Syst., 28(10):2222- 2232, 2016. [52] B. Wu et al. Visual transformers: Token- based image representation and processing for computer vision. arXiv:2006.03677, 2020. [53] A. Srinivas et al. Bottleneck transformers for visual recognition. In CVPR, pp. 16519- 16529, 2021. [54] S. d'Ascoli et al. Convit: Improving vision transformers with soft convolutional inductive biases. In ICCR, pp. 2286- 2296, 2021. [55] K. Yuan et al. Incorporating convolution designs into visual transformers. In ICCV, pp. 579- 588, 2021. [56] Y. Li et al. Localvit: Bringing locality to vision transformers. arXiv:2104.05707, 2021. [57] X. Chu et al. Conditional positional encodings for vision transformers. arXiv:2102.10882, 2021. [58] Q. Zhang and Y.- B. Yang. Rest: An efficient transformer for visual recognition. In NeurIPS, pp. 15475- 15485, 2021. [59] T. Xiao et al. Early convolutions help transformers see better. In NeurIPS, pp. 30392- 30400, 2021. [60] K. Han et al. Transformer in transformer. In NeurIPS, pp. 15908- 15919, 2021. [61] X. Chu et al. Twins: Revisiting the design of spatial attention in vision transformers. In NeurIPS, pp. 9355- 9366, 2021. [62] P. Zhang et al. Multi- scale vision longformer: A new vision transformer for high- resolution image encoding. In ICCV, pp. 2998- 3008, 2021. [63] J. Yang et al. Focal self- attention for local global interactions in vision transformers. arXiv:2107.00641, 2021. [64] L. Yuan et al. Tokens- to- token vit: Training vision transformers from scratch on imagenet. In ICCV, pp. 558- 567, 2021. [65] B. Heo et al. Rethinking spatial dimensions of vision transformers. In ICCV, pp. 11936- 11945, 2021. [66] W. Wang et al. Pvtv2: Improved baselines with pyramid vision transformer. arXiv:2106.13797, 2021. [67] D. Zhou et al. Deepvit: Towards deeper vision transformer. arXiv:2103.11886, 2021. [68] C. Gong et al. Vision transformers with patch diversification. arXiv:2104.12753, 2021. [69] M. Chen et al. Generative pretraining from pixels. In ICML, pp. 1691- 1703, 2020. [70] Z. Li et al. Mst: Masked self- supervised transformer for visual representation. In NeurIPS, pp. 13165- 13176, 2021. [71] H. Bao et al. Beit: Bert pre- training of image transformers. In ICLR, 2021. [72] K. He et al. Masked autoencoders are scalable vision learners. In CVPR, pp. 16000- 16009, 2022. [73] X. Chen et al. An empirical study of training self- supervised vision transformers. In ICCV, pp. 9640- 9649, 2021.

[74] M. Caron et al. Emerging properties in self- supervised vision transformers. In ICCV, pp. 9650- 9660, 2021. [75] Z. Xie et al. Self- supervised learning with swin transformers. arXiv:2105.04553, 2021. [76] T. Chen et al. Pix2seq: A language modeling framework for object detection. In ICLR, 2021. [77] X. Zhu et al. Deformable detr: Deformable transformers for end- to- end object detection. In ICLR, 2021. [78] M. Zheng et al. End- to- end object detection with adaptive clustering transformer. arXiv:2011.09315, 2020. [79] T. Wang et al. Pnp- detr: towards efficient visual analysis with transformers. In ICCV, pp. 4661- 4670, 2021. [80] B. Roh et al. Sparse detr: Efficient end- to- end object detection with learnable sparsity. In ICLR, 2021. [81] P. Gao et al. Fast convergence of detr with spatially modulated co- attention. In ICCV, pp. 5621- 5630, 2021. [82] D. Meng et al. Conditional detr for fast training convergence. In ICCV, pp. 3651- 3660, 2021. [83] Y. Wang et al. Anchor detr: Query design for transformer- based detector. In AAAI, pp. 2567- 2575, 2022. [84] S. Liu et al. Dab- detr: Dynamic anchor boxes are better queries for detr. In ICLR, 2021. [85] Y. Liu et al. Sap- detr: Bridging the gap between salient points and queries- based transformer detector for fast model convergency. arXiv:2211.02006, 2022. [86] Z. Yao et al. Efficient detr: Improving end- to- end object detector with dense prior. arXiv:2104.01318, 2021. [87] X. Dai et al. Dynamic detr: End- to- end object detection with dynamic attention. In ICCV, pp. 2988- 2997, 2021. [88] Z. Sun et al. Rethinking transformer- based set prediction for object detection. In ICCV, pp. 3611- 3620, 2021. [89] Y. Fang et al. You only look a one sequence: Rethinking transformer in vision through object detection. In NeurIPS, pp. 26183- 26197, 2021. [90] Z. Dai et al. Up- detr: Unsupervised pre- training for object detection with transformers. In CVPR, pp. 1601- 1610, 2021. [91] W. Wang et al. Fp- detr: Detection transformer advanced by fully pretraining. In ICLR, 2021. [92] F. Li et al. Dn- detr: Accelerate detr training by introducing query denoising. In CVPR, pp. 13619- 13627, 2022. [93] H. Zhang et al. Dino: Detr with improved denoising anchor boxes for end- to- end object detection. arXiv:2203.03605, 2022. [94] D. Zhang et al. Feature pyramid transformer. In ECCV, pp. 323- 339, 2020. [95] Y. Yuan et al. Hrformer: High- resolution vision transformer for dense predict. NeurIPS, pp. 7281- 7293, 2021. [96] J. Gu et al. Hrvit: Multi- scale high- resolution vision transformer. arXiv:2111.01236, 2021. [97] S. Zheng et al. Rethinking semantic segmentation from a sequence- to- sequence perspective with transformers. In CVPR, pp. 6881- 6890, 2021. [98] J. Chen et al. Transunet: Transformers make strong encoders for medical image segmentation. arXiv:2102.04306, 2021. [99] E. Xie et al. Segformer: Simple and efficient design for semantic segmentation with transformers. In NeurIPS, pp. 12077- 12090, 2021. [100] T. Prangemeier et al. Attention- based transformers for instance segmentation of cells in microstructures. In BIBM, pp. 700- 707, 2020. [101] Y. Wang et al. End- to- end video instance segmentation with transformers. In CVPR, pp. 8741- 8750, 2021. [102] Y. Fang et al. Instances as queries. In ICCV, pp. 6910- 6919, 2021. [103] J. Hu et al. Istr: End- to- end instance segmentation with transformers. arXiv:2105.00637, 2021. [104] B. Dong et al. Solq: Segmenting objects by learning queries. In NeurIPS, pp. 21898- 21909, 2021. [105] R. Strudel et al. Segmenter: Transformer for semantic segmentation. In ICCV, pp. 7262- 7272, 2021. [106] H. Zhao et al. Point transformer. In ICCV, pp. 16259- 16268, 2021. [107] M.- H. Guo et al. Pct: Point cloud transformer. Comput. Vis. Med., 7(2):187- 199, 2021. [108] D. Lu et al. 3dctn: 3d convolution- transformer network for point cloud classification. IEEE Trans. Intell. Transp. Syst., 2022. [109] C. Park et al. Fast point transformer. In CVPR, pp. 16949- 16958, 2022. [110] X. Pan et al. 3d object detection with pointformer. In CVPR, pp. 7463- 7472, 2021. [111] L. Fan et al. Embracing single stride 3d object detector with sparse transformer. In CVPR, pp. 8458- 8468, 2022.

[112] J. Mao et al. Voxel transformer for 3d object detection. In ICCV, pp. 3144- 3153, 2021. [113] C. He et al. Voxel set transformer: A set- to- set approach to 3d object detection from point clouds. In CVPR, pp. 8417- 8427, 2022. [114] X. Yu et al. Point- bert: Pre- training 3d point cloud transformers with masked point modeling. In CVPR, pp. 19313- 19322, 2022. [115] Y. Pang et al. Masked autoencoders for point cloud self- supervised learning. arXiv:2203.06604, 2022. [116] H. Liu et al. Masked discrimination for self- supervised learning on point clouds. arXiv:2203.11183, 2022. [117] I. Misra et al. An end- to- end transformer model for 3d object detection. In ICCV, pp. 2906- 2917, 2021. [118] Z. Liu et al. Group- free 3d object detection via transformers. In ICCV, pp. 2949- 2958, 2021. [119] H. Sheng et al. Improving 3d object detection with channel- wise transformer. In ICCV, pp. 2743- 2752, 2021. [120] K.- C. Huang et al. Monodtr: Monocular 3d object detection with deep- aware transformer. In CVPR, pp. 4012- 4021, 2022. [121] R. Zhang et al. Monodtr: Depth- aware transformer for monocular 3d object detection. arXiv:2203.13310, 2022. [122] Y. Wang et al. Det3d: 3d object detection from multi- view images via 3d- to- 2d queries. In CoRL, pp. 180- 191, 2022. [123] X. Bai et al. Transfusion: Robust lidar- camera fusion for 3d object detection with transformers. In CVPR, pp. 1090- 1099, 2022. [124] X. Yu et al. Point: Diverse point cloud completion with geometry- aware transformers. In ICCV, pp. 12478- 12487, 2021. [125] P. Xiang et al. Snowflakenet: Point cloud completion by snowflake point deconvolution with skip- transformer. In ICCV, pp. 5479- 5489, 2021. [126] J. H. Choe et al. Deep point cloud reconstruction. arXiv:2111.11704, 2021. [127] S. Chen et al. Mvt: Multi- view vision transformer for 3d object recognition. arXiv:2110.13083, 2021. [128] Y. Hou and L. Zheng. Multiview detection with shadow transformer (and view- coherent data augmentation). In ACMM, pp. 1673- 1682, 2021. [129] A. Prakash et al. Multi- modal fusion transformer for end- to- end autonomous driving. In CVPR, pp. 7077- 7087, 2021. [130] W. Jiang et al. Cot: Correspondence transformer for matching across images. In ICCV, pp. 6207- 6217, 2021. [131] D. Wang et al. Multi- view 3d reconstruction with transformers. In ICCV, pp. 5722- 5731, 2021. [132] X. Chen et al. Futr3d: A unified sensor fusion framework for 3d detection. arXiv:2203.10642, 2022. [133] A. Bozic et al. Transformerfusion: Monocular rgb scene reconstruction using transformers. In NeurIPS, pp. 1403- 1414, 2021. [134] Y. Zhang et al. mmformer: Multimodal medical transformer for incomplete multimodal learning of brain tumor segmentation. In MICCAI, pp. 107- 117, 2022. [135] G. v. Tulder et al. Multi- view analysis of unregistered medical images using cross- view transformers. In MICCAI, pp. 104- 113, 2021. [136] X. Long et al. Multi- view depth estimation using epipolar spatiotemporal networks. In CVPR, pp. 8258- 8267, 2021. [137] D. Song et al. Deep relation transformer for diagnosing glaucoma with optical coherence tomography and visual field function. IEEE Trans. Med. Imaging, 40(9):2392- 2402, 2021. [138] C. Sun et al. Videobert: A joint model for video and language representation learning. In CVPR, pp. 7464- 7473, 2019. [139] J. Lu et al. Vilbert: Pretraining task- diagnostic visiolinguistic representations for vision- and language tasks. In NeurIPS, pp. 13- 23, 2019. [140] H. Tan and M. Bansal. LXmert: Learning cross- modality encoder representations from transformers. In IMNLP- IJCNLP, pp. 5100- 5111, 2019. [141] L. H. Li et al. Visualbert: A simple and performant baseline for vision and language. arXiv:1908.03557, 2019. [142] W. Su et al. V1- bert: Pre- training of generic visual- linguistic representations. arXiv:1908.08530, 2019. [143] Y.- C. Chen et al. Uniter: Universal image- text representation learning. In ECCV, pp. 104- 120, 2020. [144] X. Li et al. Oscar: Object- semantics aligned pre- training for vision- language tasks. In ECCV, pp. 121- 137, 2020. [145] L. Zhou et al. Unified vision- language pre- training for image captioning and vqa. In AAAI, pp. 13041- 13049, 2020. [146] W. Kim et al. Vilt: Vision- and- language transformer without convolution or region supervision. In ICML, pp. 5583- 5594, 2021. [147] P. Zhang et al. Vinyl: Revisiting visual representations in vision- language models. In CVPR, pp. 5579- 5598, 2021.

[148] A. Radford et al. Learning transferable visual models from natural language supervision. In ICML, pp. 8748- 8763, 2021. [149] A. Ramesh et al. Zero- shot text- to- image generation. In ICML, pp. 8821- 8831, 2021. [150] C. Jia et al. Scaling up visual and vision- language representation learning with noisy text supervision. In ICML, pp. 4904- 4916, 2021. [151] R. Hu and A. Singh. Unit: Multimodal multitask learning with a unified transformer. In ICCV, pp. 1439- 1449, 2021. [152] Z. Wang et al. Simvlm: Simple visual language model pretraining with weak supervision. In ICLR, 2021. [153] A. Baevski et al. Data2vec: A general framework for self- supervised learning in speech, vision and language. arXiv:2202.03555, 2022. [154] A. Kamath et al. Mder- modulated detection for end- to- end multimodal understanding. In ICCV, pp. 1780- 1790, 2021. [155] J. Deng et al. Transvg: End- to- end visual grounding with transformers. In ICCV, pp. 1769- 1779, 2021. [156] Y. Du et al. Visual grounding with transformers. In ICME, pp. 1- 6, 2022. [157] M. Li and L. Sigal. Referring transformer: A one- step approach to multi- task visual grounding. In NeurIPS, pp. 19652- 19664, 2021. [158] H. Jiang et al. Pseudo- q: Generating pseudo language queries for visual grounding. In CVPR, pp. 15513- 15523, 2022. [159] J. Roh et al. Languagerefer: Spatial- language model for 3d visual grounding. In CoRL, pp. 1046- 1056, 2022. [160] D. He et al. Transrefer3d: Entity- and- relation aware transformer for fine- grained 3d visual grounding. In ACMM, pp. 2344- 2352, 2021. [161] S. Huang et al. Multi- view transformer for 3d visual grounding. In CVPR, pp. 15524- 15533, 2022. [162] A. Yang et al. Tubelet: Spatio- temporal video grounding with transformers. In CVPR, pp. 16442- 16453, 2022. [163] J. L. Ba et al. Layer normalization. arXiv:1607.06450, 2016. [164] P. P. Brahma et al. Why deep learning works: A manifold disentanglement perspective. IEEE Trans. Neural Netw. Learn. Syst., 27(10):1997- 2008, 2015. [165] Y. Chen et al.  $a^2$  nets: Double attention networks. In NeurIPS, pp. 352- 361, 2018. [166] A. Krizhevsky et al. Learning multiple layers of features from tiny images. 2009. [167] C. Sun et al. Revisiting unreasonable effectiveness of data in deep learning era. In ICCV, pp. 843- 852, 2017. [168] J. Deng et al. Imagenet: A large- scale hierarchical image database. In CVPR, pp. 248- 255, 2019. [169] Y. Dong et al. Attention is not all you need: pure attention loses rank doubly exponentially with depth. In ICLR, pp. 2793- 2803, 2021. [170] P. Shaw et al. Self- attention with relative position representations. In NAACL, pp. 464- 468, 2018. [171] P. W. Battaglia et al. Relational inductive biases, deep learning, and graph networks. arXiv:1806.01261, 2018. [172] S. Abnar et al. Transferring inductive biases through knowledge distillation. arXiv:2006.00555, 2020. [173] M. Sandler et al. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, pp. 4510- 4520, 2018. [174] A. Islam et al. How much position information do convolutional neural networks encode. In ICLR, 2020. [175] J. Lin et al. Tsm: Temporary- shift module for efficient video understanding. In ICCV, pp. 7083- 7093, 2019. [176] Y. Pang et al. Convolution in convolution for network in network. IEEE Trans. Neural Netw. Learn. Syst., 29(5):1587- 1597, 2017. [177] J. Gao et al. Representation degeneration problem in training natural language generation models. In ICLR, 2019. [178] S. Yun et al. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, pp. 6023- 6032, 2019. [179] M. Caron et al. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, pp. 9912- 9924, 2020. [180] C.- F. Chen et al. Cross- vit: Cross- attention multi- scale vision transformer for image classification. In ICCV, pp. 357- 366, 2021. [181] S. Ren et al. Faster r- conn: Towards real- time object detection with region proposal networks. IEEE Trans. Pattern Anal. Mach. Intell., 39(6):1137- 1149, 2017. [182] J. Redmon et al. You only look once: Unified, real- time object detection. In CVPR, pp. 779- 788, 2016. [183] T.- Y. Lin et al. Focal loss for dense object detection. In ICCV, pp. 2980- 2988, 2017. [184] Z.- Q. Zhao et al. Object detection with deep learning: A review. IEEE Trans. Neural Netw. Learn. Syst., 30(11):3212- 3232, 2019. [185] J. Dai et al. Deformable convolutional networks. In ICCV, pp. 764- 773, 2017.

[186] T.- Y. Lin et al. Feature pyramid networks for object detection. In CVPR, pp. 2117- 2125, 2017. [187] Z. Tian et al. Fcos: Fully convolutional one- stage object detection. In ICCV, pp. 9627- 9636, 2019. [188] K. He et al. Mask r- cnn. In ICCV, pp. 2961- 2969, 2017. [189] Z. Cai and N. Vasconcellos. Cascade r- cnn: Delving into high quality object detection. In CVPR, pp. 6154- 6162, 2018. [190] P. Sun et al. What makes for end- to- end object detection? In ICML, pp. 9934- 9944, 2021. [191] K. Sun et al. Deep high- resolution representation learning for human pose estimation. In CVPR, pp. 5693- 5701, 2019. [192] L.- C. Chen et al. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Trans. Pattern Anal. Mach. Intell., 40(4):834- 848, 2017. [193] O. Ronneberger et al. U- net: Convolutional networks for biomedical image segmentation. In MICCAI, pp. 234- 241, 2015. [194] P. Sun et al. Sparse r- cnn: End- to- end object detection with learnable proposals. In CVPR, pp. 14454- 14463, 2021. [195] T. Xiao et al. Unified perceptual parsing for scene understanding. In ECCV, pp. 418- 434, 2018. [196] M. Chen et al. Searching the search space of vision transformer. In NeurIPS, 2021. [197] H. Chen et al. Blendmask: Top- down meets bottom- up for instance segmentation. In CVPR, pp. 8573- 8581, 2020. [198] X. Wang et al. Solov2: Dynamic and fast instance segmentation. In NeurIPS, pp. 17721- 17732, 2020. [199] Y. Wang et al. Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition. In NeurIPS, pp. 11960- 11973, 2021. [200] C. R. Qi et al. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In NeurIPS, 2017. [201] P. Anderson et al. Bottom- up and top- down attention for image captioning and visual question answering. In CVPR, pp. 6077- 6086, 2018. [202] R. Krishna et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Int. J. Comput. Vis., 123(1):32- 73, 2017. [203] P. Sharma et al. Conceptual captions: A cleaned, hyperymed, image alt- text dataset for automatic image captioning. In ACL, pp. 2556- 2565, 2018. [204] S. Antol et al. Vqa: Visual question answering. In ICCV, pp. 2425- 2433, 2015. [205] O. Vinyals et al. Show and tell: A neural image caption generator. In CVPR, pp. 3156- 3164, 2015. [206] P. Achlioptas et al. Referit3d: Neural listeners for fine- grained 3d object identification in real- world scenes. In ECCV, pp. 422- 440, 2020. [207] W. Yu et al. Metaformer is actually what you need for vision. In CVPR, pp. 10819- 10829, 2022. [208] N. Park and S. Kim. How do vision transformers work? In ICLR, 2021. [209] K. Wu et al. Rethinking and improving relative position encoding for vision transformer. In ICCV, pp. 10053- 10041, 2021. [210] M. A. Islam et al. Position, padding and predictions: A deeper look at position information in cnns. arXiv:2101.12322, 2021. [211] C. Riquelme et al. Scaling vision with sparse mixture of experts. arXiv:2106.05974, 2021. [212] S. Xie et al. Aggregated residual transformations for deep neural networks. In CVPR, pp. 1492- 1500, 2017. [213] S. Xie et al. Rethinking spatiotemporal feature learning: Speed- accuracy trade- offs in video classification. In ECCV, pp. 305- 321, 2018. [214] Y. Goyal et al. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, pp. 6904- 6913, 2017. [215] T.- Y. Lin et al. Microsoft coco: Common objects in context. In ECCV, pp. 740- 755, 2014. [216] D. A. Hudson and C. D. Manning. Gqa: A new dataset for real- world visual reasoning and compositional question answering. In CVPR, pp. 6700- 6709, 2019. [217] V. Ordonez et al. In2text: Describing images using 1 million captioned photographs. In NeurIPS, 2011. [218] P. Young et al. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. ACL, pp. 67- 78, 2014. [219] A. Kuznetsova et al. The open images dataset v4. Int. J. Comput. Vis., 128(7):1956- 1981, 2020.