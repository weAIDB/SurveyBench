# Vision-Language-Action Models: Concepts, Progress, Applications and Challenges

Ranjan Sapkota $^{a}$ , Yang Cao $^{b}$ , Konstantinos I. Roumeliotis $^{c}$ , Manoj Karkee $^{a}$

$^{a}$ Cornell University, Biological & Environmental Engineering, Ithaca, New York, USA   $^{b}$ The Hong Kong University of Science and Technology, Department of Computer Science and Engineering, Hong Kong   $^{c}$ University of the Phuconnese, Department of Informatics and Telecommunications, Greece

## Abstract

Vision- Language- Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision- Language- Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross- modal learning architectures to generalist agents that tightly integrate vision- language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameter- efficient training strategies, and real- time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real- time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state- of- the- art, we propose targeted solutions including agentic AI adaptation, cross- embodiment generalization, and unified neuro- symbolic planning. In our forward- looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general- purpose embodied agents. This work serves as a foundational reference for advancing intelligent, real- world robotics and artificial general intelligence.

Keywords: Vision- Language- Action, VLA, Artificial Intelligence, Robotics, Vision- Language Models, AI Agents, Agentic AI


![](images/3771f5df5810385867b81d3c170baf5731f35995ddf676476c53dd62abc389a6.jpg)  
Figure 1: Evolution from Isolated Modalities to Unified Vision-Language-Action Models. This figure illustrates the transition from separate vision, language, and action systems, each limited to its own domain-to integrated VLA models. VLA models enable robots to jointly perceive, understand language, and act, overcoming the fragmentation of earlier approaches and marking a major step toward adaptive, generalizable, and intelligent embodied agents.

## 1 Introduction

Before Vision- Language- Action (VLA) models were developed, progress in robotics and artificial intelligence happened mostly in separate areas: vision systems that could see and recognize images [44, 69], language systems that could understand and generate text [164, 137], and action systems that could control movement [49]. These systems worked well on their own, but they struggled to work together or handle new and unpredictable situations[46, 21]. As a result, "erstand complex environments or respond flexibly to real world challenges.

As illustrated in Figure 1, traditional computer vision models primarily based on convolutional neural networks (CNNs), were tailored for narrowly specified tasks such as object detection or classification, requiring extensive labeled datasets and cumbersome retraining for even slight shifts in environment or objectives [156, 62]. These vision models could "see" (e.g., identifying apples in an orchard, as shown in Figure 1) but lacked any understanding of language or the ability to convert visual insights into purposeful actions. Language models, particularly large language models (LLMs), revolutionized text- based understanding and generation [23]; however, they remained restricted to processing language without the capability to perceive or reason about the physical world [76] (Ripe apples in orchard" in Figure 1 exemplifies this limitation). Meanwhile, action- based systems in robotics, relying heavily on hand- crafted policies or reinforcement learning [122], enabled specific behaviors like object manipulation but demanded painstaking engineering and failed to generalize beyond narrowly scripted scenarios [119].

Despite progress with VLMs, which achieved impressive multimodal understanding by combining vision and language [149, 25, 148], there remained a conspicuous integration gap: the inability to generate or execute coherent actions based on multimodal input [121, 107]. As further visualized in Figure 1, most AI systems specialized at most in two modalities- visionlanguage, vision- action, or language- action- but struggled to fully integrate all three into a unified, end- to- end framework. Consequently, robots could recognize objects visually (apple"), understand a corresponding textual instruction ("pick the apple"), or perform a predefined motor action (grasping), yet orchestrating these abilities into fluid, adaptable behavior was beyond reach. The result was a fragmented pipeline architecture that could not flexibly adapt to new tasks or environments, leading to brittle generalization and labor- intensive engineering efforts. This highlighted a critical bottleneck in embodied AI: without systems that could jointly perceive, understand, and act, intelligent autonomous behavior remained an elusive goal.

The pressing need to bridge these gaps catalyzed the emergence of VLA models. VLA models, conceptualized around 2021- 2022, and pioneered by efforts such as Google DeepMind's Robotic Transformer 2 (RT- 2) [224], introduced a transformative architecture that unified perception, reasoning, and control within a single framework. As a solution to the limitations outlined in Figure 1, VLAs integrate vision inputs, language comprehension, and motor control capabilities, enabling embodied agents to perceive their surroundings, under stand complex instructions, and execute appropriate actions dynamically. Early VLA approaches achieved this integration by extending vision- language models to include action tokens- numerical or symbolic representations of robot motor commands, thereby allowing the model to learn from paired vision, language, and trajectory data [121]. This methodological innovation dramatically improved robots' ability to generalize to unseen objects, interpret novel language commands, and perform multi- step reasoning in unstructured environments [83].

VLA models represent a transformative step in the pursuit of unified multimodal intelligence, overcoming the long- standing limitations of treating vision, language, and action as separate domains [121]. By leveraging internet- scale datasets that integrate visual, linguistic, and behavioral information, VLAs empower robots to not only recognize and describe their environments but also to reason contextually and execute appropriate actions in complex, dynamic settings [196]. The progression illustrated in Figure 1 from isolated vision, language, and action systems to an integrated VLA paradigm- captures a fundamental shift toward the development of truly adaptive and generalizable embodied agents. Given the profound implications of this innovation, it is crucial to undertake a thorough and systematic review that draws from a comprehensive body of literature and critical analysis. First, such a review is necessary to clarify the foundational concepts and architectural principles that distinguish VLAs from their predecessors. Second, it provides a structured account of the rapid progress and key milestones in the field, enabling researchers and practitioners to appreciate the trajectory of technological advancements. Third, an in- depth review is essential for mapping the diverse range of real- world applications- from household robotics to industrial automation and assistive technologies- where VLAs are already demonstrating transformative potential. Furthermore, by critically examining the current challenges, such as data efficiency, safety, generalization, and ethical considerations, the review identifies barriers that must be addressed for widespread deployment. And Fifth, synthesizing these insights helps to inform the broader AI and robotics communities about emerging research directions and practical considerations, fostering collaboration and innovation.

In this review, we systematically analyze the foundational principles, developmental progress, and technical challenges associated with VLA models. Our objective is to consolidate the current understanding of VLAs while identifying limitations and proposing future directions for their evolution. The review begins with a detailed examination of key conceptual foundations (Figure 2), including what constitutes a VLA model, its historical evolution, multimodal integration mechanisms, and language- based tokenization and encoding strategies. These conceptual components set the stage for understanding how VLAs are structured and function across modalities.

Building upon this, we present a unified view of recent progress and training efficiency strategies (Figure 3). This includes architectural innovations that have enabled more capable and generalizable VLA models, as well as data- efficient learning frameworks, parameter- efficient modeling techniques, and

![](images/324c8ccaf343fe91653927a7a55876915b0e28401a9afff74117b498bbfad6fb.jpg)  
Figure 2: Mindmap for VLA Concepts. This diagram outlines the foundational components of Vision-Language-Action models, including their definitions, historical development, integration of multimodal signals, tokenization techniques, and adaptive execution. It sets the conceptual stage for understanding the structure and purpose of VLAs.

model acceleration strategies designed to reduce computational overhead without compromising performance. These advancements are critical for scaling VLA systems to real- world applications.

Following this, we delve into a comprehensive discussion of the current limitations faced by VLA systems (Figure 4). These include inference bottlenecks, safety concerns, high computational demands, limited generalization, and ethical implications. We not only highlight these pressing challenges but also provide an analytical discussion of potential solutions to address them.

Together, these three figures offer a visual framework that supports the textual analysis of this review. By outlining the conceptual landscape, recent innovations, and open challenges, this work aims to guide future research and encourage the development of more robust, efficient, and ethically grounded VLA systems.

## 2 Concepts of Vision-Language-Action Models

VLA models represent a new class of intelligent systems that jointly process visual inputs, interpret natural language, and generate executable actions in dynamic environments. Technically, VLAs combine vision encoders (e.g., CNNs, ViTs), language models (e.g., LLMs, transformers), and policy modules or planners to achieve task- conditioned control. These models typically employ multimodal fusion techniques—such as cross- attention, concatenated embeddings, or token unification—to align sensory observations with textual instructions.

![](images/9fd8a70b5c0ff4f54a8761cd8df3ad2cd0c229065347e755d6fac01fc8fc914a.jpg)  
Figure 3: Mindmap - VLA Progress and Training Efficiency. This figure combines architectural advancements with training optimization methods such as data-efficient learning, parameter reduction, and model acceleration. It provides a visual summary of the technical progress driving the scalability and real-world deployment of VLAs.

![](images/21fea105c6ad66a6211fcfcc271a21c3aafea151d7152a0f72d321d9b814756b.jpg)  
Figure 4: Mindmap - VLA Challenges. This diagram highlights key barriers to robust VLA deployment, including inference limitations, bias, system complexity, generalization gaps, and ethical concerns. It also motivates the need for innovative solutions and future research directions to overcome these challenges.

Unlike traditional visuomotor pipelines, VLAs support semantic grounding, enabling context- aware reasoning, affordance detection, and temporal planning. A typical VLA model observes the environment through camera or sensor data, interprets goals expressed in language (e.g., "pick up the red apple") (Figure 5), and outputs low- level or high- level action sequences. Recent advancements integrate imitation learning, reinforcement learning, or retrieval- augmented modules to improve sample efficiency and generalization. This review examines how VLA models have evolved from foundational fusion architectures to general purpose agents capable of real- world deployment across robotics, navigation, and human- AI collaboration.

VLA models are multimodal artificial intelligence systems that unify visual perception, language comprehension, and physical action generation into a single framework. These models enable robots or AI agents to interpret sensory inputs (e.g., images, text), understand contextual meaning, and autonomously execute tasks in real- world environments- all through end- to- end learning rather than isolated subsystems. As shown conceptually in Figure 5, VLA models bridge the historical disconnect between visual recognition, language comprehension, and motor execution that limited the capabilities of earlier robotic and AI systems.

### 2.1 Evolution and Timeline

The rapid development of VLA models from 2022- 2025 demonstrates three distinct evolutionary phases:

1. Foundational Integration (2022-2023). Early VLAs established basic visuomotor coordination through multimodal fusion architectures. [157] first combined CLIP embeddings with motion primitives, while [141] demonstrated generalist capabilities across 604 tasks. [18] achieved  $97\%$  success rates in manipulation through scaled imitation learning, and [86] introduced temporal reasoning via transformer-based planners. By 2023, [224] enabled visual chain-of-thought reasoning, and [34] advanced stochastic action prediction through diffusion processes. These foundations addressed low-level control but lacked compositional reasoning [216], prompting innovations in affordance grounding [78].

2. Specialization and Embodied Reasoning (2024). Second-generation VLAs incorporated domain-specific inductive biases. [202] enhanced few-shot adaptation through retrieval-augmented training, while [210] optimized navigation via 3D scene-graph integration. [39] introduced reversible architectures for memory efficiency, and [183] addressed partial observability with physics-informed attention. Simultaneously, [5] improved compositional understanding through object-centric disentanglement, and [220] extended applications to autonomous driving via multi-modal sensor fusion. These advances required new benchmarking methodologies [196].

3. Generalization and Safety-Critical Deployment (2025). Current systems prioritize robustness and human alignment. [205] integrated formal verification for risk-aware

decisions, while [42] demonstrated whole- body control through hierarchical VLAs. [19] optimized compute efficiency for embedded deployment, and [102] combined neural- symbolic reasoning for causal inference. Emerging paradigms like [100]'s affordance chaining and [13]'s sim- to- real transfer learning address cross- embodiment challenges, while [108] bridges VLAs with human- in- the- loop interfaces through natural language grounding.

Figure 6 presents a comprehensive timeline highlighting the evolution of 47 VLA models developed between 2022 and 2025. The earliest VLA systems, including CLIPort [157], Gato [141], RT- 1 [18], and VIMA [86], laid the foundation by combining pretrained vision- language representations with task- conditioned policies for manipulation and control. These were followed by ACT [216], RT- 2 [224], and VoxPoser [78], which integrated visual chain- of- thought reasoning and affordance grounding. Models like Diffusion Policy [34] and Octo [167] introduced stochastic modeling and scalable data pipelines. In 2024, systems such as Deer- VLA [202], ReVLA [39], and Uni- NaVid [210] added domain specialization and memory- efficient designs, while Occlama [183] and ShowUI [108] tackled partial observability and user interaction. The trajectory continued with robotics- focused VLAs like Quar- VLA [43] and RoboMamba [111]. Recent innovations emphasize generalization and deployment: SafeVLA [205], Humanoid- VLA [42], and MoManipVLA [190] incorporate verification, full- body control, and memory systems. Models such as G00t N1 [13] and SpatialVLA [136] further bridge sim- to- real transfer and spatial grounding. This timeline illustrates how VLAs have advanced from modular learning to general- purpose, safe, and embodied intelligence.

### 2.2 Multimodal Integration: From Isolated Pipelines to Unified Agents

A central advancement in the emergence of VLA models lies in their ability to perform multimodal integration, the joint processing of vision, language, and action within a unified architecture. Traditional robotic systems treated perception, natural language understanding, and control as discrete modules, often linked through manually defined interfaces or data transformations [109, 20, 168]. For instance, classic pipeline- based frameworks required a perception model to output symbolic labels, which were then mapped by a planner to specific actions—frequently with domain- specific hand engineering [138, 90]. These approaches lacked adaptability, failed in ambiguous or novel environments, and could not generalize instructions beyond pre- encoded templates.

In contrast, modern VLAs fuse modalities end- to- end using large- scale pretrained encoders and transformer- based architectures [188]. This shift enables the model to interpret visual observations and linguistic instructions within the same computational space, allowing flexible, context- aware reasoning [99]. For example, in the task "Pick up the red ripe apple," (Figure 5) the vision encoder—typically a Vision Transformer (ViT) or ConvNeXt—segments and classifies objects in the scene (e.g.,

![](images/7514e40a1eab760bf2c17ea6e6e3c636467ec39aeb1f5b444f63aa4ebceb6aac.jpg)  
Figr 5:    t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t h

apples, leaves, background), identifying color and ripeness attributes [187]. Meanwhile, the language model, often a variant of T5, GPT, or BERT, encodes the instruction into a high- dimensional embedding. These representations are then fused via cross- attention or joint tokenization schemes, producing a unified latent space that informs the action policy [68].

This multimodal synergy was first effectively demonstrated in CLIPort [157], which used CLIP embeddings for semantic grounding and a convolutional decoder for pixel- level manipulation. CLIPort bypassed the need for explicit language parsing and directly conditioned visuomotor policies on natural language. Similarly, VIMA [86] advanced this approach by employing a transformer encoder to jointly process object- centric visual tokens and instruction tokens, enabling few- shot generalization across spatial reasoning tasks.

Recent developments push this fusion further by incorporating temporal and spatial grounding. VoxPoser [78] employs voxel- level reasoning to resolve ambiguities in 3D object selection, while RT- 2 [224] fuses visual- language tokens into a unified transformer that supports zero- shot generalization to unseen instructions. Another noteworthy contribution is Octo [167], which introduces a memory- augmented transformer that enables long- horizon decision- making across diverse scenes, demonstrating the scalability of joint perception- language- action learning.

Crucially, VLAs offer robust solutions to challenges in real- world grounding. For example, Occlama [183] handles occluded object references through attention- based mechanisms, while ShowUI [108] demonstrates natural language interfaces that allow non- expert users to command agents through voice or typed input. These capabilities are only possible because the integration is not limited to surface- level fusion; rather, it captures semantic, spatial, and temporal alignment across modalities.

### 2.3 Tokenization and Representation: How VLAs Encode the World

A core innovation that sets VLA models apart from conventional vision- language architectures lies in their token- based representation framework, which enables holistic reasoning over perceptual [125, 215], linguistic, and physical action spaces [106]. Inspired by autoregressive generative models like transformers, modern VLAs encode the world using discrete tokens that unify all modalities—vision, language, state, and action into a shared embedding space [110]. This allows the model to not only understand “what needs to be done” (semantic reasoning), but also “how to do it” (control policy execution) in a fully learnable and compositional way [192, 117, 170].

- Prefix Tokens: Encoding Context and Instruction: Prefix tokens serve as the contextual backbone of VLA models [195, 83]. These tokens encode the environmental scene (via images or video) and the accompanying natural language instruction into compact embeddings that prime the model's internal representations [16].

For instance, as depicted in Figure 7 in a task such as “stack the green blocks on the red tray,” the image of a cluttered tabletop is processed through a vision encoder like ViT or ConvNeXt, while the instruction is embedded by a large language model (e.g., T5 or LLaMA). These are then

![](images/34525f7b01486e3559f7a58e8fbb6750c71a7c03115a2e36df6914dc7441ffae.jpg)  
Figure 6: Comprehensive timeline of Vision-Language-Action models (2022-2025), showing evolution from foundation to 45 specialized VLA systems. Organized chronologically with thematic grouping.

transformed into a sequence of prefix tokens that establish the model's initial understanding of the goal and environmental layout. This shared representation enables crossmodal grounding, allowing the system to resolve spatial references (e.g., "on the left," "next to the blue cup") and object semantics ("green blocks") across both modalities.

- State Tokens: Embedding the Robot's Configuration: In addition to perceiving external stimuli, VLAs must be aware of their internal physical state [186, 111]. This is achieved through the use of state tokens, which encode real-time information about the agent's configuration—joint positions, force-torque readings, gripper status, end-effector pose, and even the locations of nearby

objects [97]. These tokens are crucial for ensuring situational awareness and safety, especially during manipulation or locomotion [163, 81].

Figure 8 illustrates how VLA models utilize state tokens to enable dynamic, context- aware decision- making in both manipulation and navigation settings. In Figure 8a, a robot arm is shown partially extended near a fragile object. In such scenarios, state tokens play a critical role by encoding real- time proprioceptive information, such as joint angles, gripper pose, and end- effector proximity. These tokens are continuously fused with visual and language- based prefix tokens, allowing the transformer to reason about physical constraints. The model can thus infer that

![](images/bbc0c20c9fa1d33811e5a26ae3dd29a12de1367cc1ff5bdb5d80dfee5b2763fe.jpg)  
Figure 7: The diagram illustrates the end-to-end tokenization and representation process in VLA models. Visual input (e.g., cluttered tabletop) is encoded by a vision encoder (e.g., ViT), while natural language instructions (e.g., "stack the green blocks") are processed by a language encoder (e.g., T5). The system fuses prefix, state, and action tokens through a transformer and autoregressively predicts motor actions.

![](images/66c8fc3c1cb45d8cffced3ea3383a7144909efd44d8a5c21cfde528ce84ed12f.jpg)  
Figure 8: Illustrating how VLA models utilize prefix, state, and action tokens in real-world scenarios. In robotic manipulation, state tokens detect arm extension near fragile objects, enabling path adjustment. In navigation, they represent LiDAR and odometry data. The apple-picking task shows how prefix tokens guide goal understanding, while action tokens generate motion sequences for targeted grasping and execution.

a collision is imminent and adjust the motor commands accordingly—e.g., rerouting the arm trajectory or modulating force output. In mobile robotic platforms, as depicted in Figure 8b, state tokens encapsulate spatial features such as odometry, LiDAR scans, and inertial sen sor data. These are essential for terrain- aware locomotion and obstacle avoidance. The transformer model integrates this state representation with environmental and instructional context to generate navigation actions that dynamically adapt to changing surroundings. Whether grasping objects in cluttered environments or autonomously navigating uneven terrain, state tokens provide a structured mechanism for situational awareness, enabling the autoregressive decoder to produce precise, context- informed action sequences that reflect both internal robot configuration and external sensory data.

- Action Tokens: Autoregressive Control Generation: The final layer of the VLA token pipeline involves action tokens [93, 94], which are autoregressively generated by the model to represent the next step in motor control [186]. Each token corresponds to a low-level control signal, such as joint angle updates, torque values, wheel velocities, or high-level movement primitives [64]. During inference, the model decodes these tokens one step at a time, conditioned on prefix and state tokens, effectively turning VLA models into language-driven policy generators [54, 161]. This formulation allows seamless integration with real-world actuation systems, supports variable-length action sequences [10, 77], and enables model fine-tuning via reinforcement or imitation learning frameworks [214]. Notably, models like RT-2 [224] and PaLM-E [47] exemplify this design, where perception, instruction, and embodiment are merged into a unified token stream.

For instance, in the apple- picking task as depicted in Figure 9, the model may receive prefix tokens that include the image of the orchard and the text instruction. The state tokens describe the robot's current arm posture and whether the gripper is open or closed. Action tokens are then predicted step by step to guide the robotic arm toward the apple, adjust the gripper orientation, and execute a grasp with appropriate force. The beauty of this approach is that it allows transformers, which are traditionally used for text generation, to now generate sequences of physical actions in a manner similar to generating a sentence—only here, the sentence is the motion.

To operationalize the VLA paradigm in robotics, we present in Figure 9 a structured pipeline that demonstrates how multimodal information—specifically vision, language, and proprioceptive state—is encoded, fused, and converted into executable action sequences. This end- to- end loop allows a robot to interpret complex tasks like "pick the ripe apple near the green leaf" and execute precise, context- sensitive manipulations. The system begins with multimodal input acquisition, where three distinct data streams are collected: visual observations (e.g., RGB- D frames), natural language commands, and real- time robot state information (e.g., joint angles or velocity). These are independently tokenized into discrete embeddings using pretrained modules [41, 212]. As depicted in the diagram, the image is processed through a Vision Transformer (ViT) backbone to generate vision tokens, the instruction is parsed by a

![](images/047dabb43f2c3b937bca51880fb8f5c7992951a66a2fab411c6737eea221776b.jpg)  
Figure 9: Illustrating the process of how VLA Encode the World. VLAs encode the world by converting vision, language, and sensor inputs into tokens, fusing them through cross-attention, predicting action sequences via transformers, and executing tasks with real-time feedback enabling robots to interpret scenes, follow instructions, and adapt actions dynamically.

language model such as BERT or T5 to produce language tokens, and state inputs are transformed via a lightweight MLP encoder into compact state tokens.

These tokens are then fused using a cross- modal attention mechanism, where the model jointly reactions over object semantics, spatial layout, and physical constraints [61]. This fused representation forms the contextual basis for decision- making [74, 116]. In Figure 9, this is denoted as the multimodal fusion step. The fused embedding is passed into an autoregressive decoder—typically a transformer—that generates a series of action tokens. These tokens may correspond to joint displacements, gripper force modulation, or high- level motor primitives (e.g., "move to grasp pose", "rotate wrist"). The action tokens are subsequently translated into control commands and passed to the execution loop, which closes the perception- action cycle by feeding back the robot's updated state, thus informing the next inference step. This closed- loop mechanism enables the model to dynamically adapt to perturbations, object shifts, or occlusions in real time [206, 120, 194].

To offer concrete implementation details, Algorithm 1 formalizes the VLA tokenization process. Given an RGB- D frame  $I$  natural language instruction  $T$  and joint angle vector  $\theta$  the algorithm produces a set of action tokens that can be executed in sequence. The image  $I$  is processed via a ViT to produce  $V$  a set of 400 visual tokens. In parallel, the instruction  $T$  is encoded by a BERT model to yield  $L$  a sequence of 12 semantic language tokens. Simultaneously, robot state  $\theta$  is passed through a multilayer perceptron to generate a 64- dimensional state embedding  $S$  .These tokens are then fused via a cross- attention module to produce a shared 512- dimensional representation

$F$  capturing the semantics, intent, and situational awareness needed for grounded action. Finally, a policy decoder such as FAST [133] maps the fused features into 50 discrete action tokens, which can then be decoded into motor commands  $\tau_{1:N}$

The decoding process is implemented using a transformer- based architecture, as shown in the code snippet titled Action Prediction Code. A Transformer object is initialized with 12 layers, a model dimension of 512, and 8 attention heads. The fused tokens are passed to the decoder, which autoregressively predicts the next most likely action token conditioned on previous tokens and context. The final motor command sequence is obtained by detokenizing the output. This implementation mirrors how text generation works in large language models, but here the "sentence" is a motion trajectory—a novel repurposing of natural language generation techniques for physical action synthesis.

Together, Figure 9, Algorithm 1, and the pseudocode illustrate how VLAs unify perception, instruction, and embodiment within a coherent and interpretable token space. This modularity allows the framework to generalize across tasks and robot morphologies, facilitating rapid deployment in real- world applications like apple picking, household tasks, and mobile navigation. Importantly, the clarity and separability of the tokenization steps make the architecture extensible, enabling further research on token learning, hierarchical planning, or symbolic grounding in VLA systems.

Algorithm 1 VLA Tokenization Pipeline

1: Input: RGB- D frame  $I$  text command  $T$  joint angles  $\theta$  2:  $V\gets \mathrm{ViT}(I)$ $\geq 400$  vision tokens 3:  $L\gets \mathrm{BERT}(T)$ $\geq 12$  language tokens 4:  $S\gets \mathrm{MLP}(\theta)$ $\geq 64$  - dim state encoding 5:  $F\gets \mathrm{CrossAttention}(V,L,S)$ $\geq 512$  - dim fused token 6:  $A\gets \mathrm{FAST}(F)$ $\geq 50$  action tokens 7: Output: Motor commands  $\tau_{1:N}$

Action Prediction Code

Python- like pseudocode

def predict_actions(fused_tokens): transformer  $=$  Transformer num_layers  $= 12$  d_model  $= \Xi 12$  nhead  $= 8$  ) action_tokens  $=$  transformer.decode fused_tokens, memory  $\coloneqq$  fused_tokens ) return detokenize(action_tokens)

### 2.4 Learning Paradigms: Data Sources and Training Strategies

Training VLA models requires a hybrid learning paradigm that integrates both semantic knowledge from the web and task- grounded information from robotics datasets [30]. As shown in

![](images/464dc61ec602c4ba39a20a3a49f80f0b696839e9932dfb3b27090f52bab43042.jpg)  
Figure 10: Learning Paradigms: Data Sources and Training Strategies for VLAs.

prior sections, the multimodal architecture of VLAs must be exposed to diverse forms of data that support language understanding, visual recognition, and motor control. This is typically achieved through two primary data sources.

First, as depicted in figure 10, large- scale internet- derived corpora form the backbone of the models semantic prior. These datasets include image- caption pairs (e.g., COCO, LAION400M), instruction- following datasets (e.g., HowTo100M, WebVid), and visual question- answering corpora (e.g., VQA, GQA). Such datasets enable pretraining of the visual and language encoders, helping the model acquire general representations of objects, actions, and concepts [2]. This phase often uses contrastive or masked modeling objectives, such as CLIP- style contrastive learning or language modeling losses, to align vision and language modalities within a shared embedding space [146, 199]. Importantly, this stage gives VLAs a foundational "understanding of the world" that facilitates compositional generalization, object grounding, and zero- shot transfer [28, 15].

However, semantic understanding alone is insufficient for physical task execution [36, 178, 107]. Thus, the second phase focuses on grounding the model in embodied experience [178]. Robot trajectory datasets—collected either from real- world robots or high- fidelity simulators—are used to teach the model how language and perception translate into action [54]. These include datasets like RoboNet [37], BridgeData [50], and RT- X [175], which provide video- action pairs, joint trajectories, and environment interactions under natural language instructions [123]. Demonstration data may come from kinesthetic teaching, teleoperation, or scripted policies [89, 12]. This phase typically employs supervised learning (e.g., behavior cloning) [55], reinforcement learning (RL), or imitation learning to train the autoregressive policy decoder to predict action tokens based on fused visual- language- state embeddings [65].

Recent works increasingly adopt multistage or multitask training strategies. For example, models are often pretrained on vision- language datasets using masked language modeling, then fine- tuned on robot demonstration data using token- level autoregressive loss [94, 221, 195]. Others use curriculum learning, where simpler tasks (e.g., object pushing) precede more complex ones (e.g., multistep manipulation) [217]. Some approaches further leverage domain adaptation such as in OpenVLA [94] or sim- to- real transfer to bridge the gap between synthetic and real- world distributions [96]. By unifying semantic priors with task execution data, these learning paradigms allow VLA models to generalize across tasks, domains, and embodiments—forming the backbone of scalable, instruction- following agents capable of robust real- world operation.

Through co- fine- tuning, these datasets are brought into alignment [179, 52]. The model learns to map from visual and linguistic inputs to appropriate action sequences [136]. This training paradigm not only helps the model understand object affordances (e.g., apples can be grasped) and action outcomes (e.g., lifting requires force and trajectory), but also promotes generalization to novel scenarios [100]. A model trained on kitchen manipulation tasks may be able to infer how to pick an apple in an outdoor orchard if it has learned general principles of object localization, grasping, and following language directives.

Recent architectures, such as Google DeepMind's RT- 2 (Robotic Transformer 2) [224], have demonstrated this principle in action. RT- 2 treats action generation as a form of text generation, where each action token corresponds to a discrete command in a robot's control space. Because the model is

trained on both web- scale multimodal data and thousands of robot demonstrations, it can flexibly interpret novel instructions and perform zero- shot generalization to new objects and tasks—something that was largely impossible with traditional control systems or even early multimodal models.

### 2.5 Adaptive Control and Real-Time Execution

Another strength of VLAs lies in their ability to perform adaptive control, using real- time feedback from sensors to adjust behavior on the fly [153]. This is particularly important in dynamic, unstructured environments like orchards, homes, or hospitals, where unexpected changes (e.g., wind moving an apple, lighting changes, human presence) can alter the task parameters. During execution, state tokens are updated in real time, reflecting sensor inputs and joint feedback [195]. The model can then revise its planned actions accordingly. For instance, in the apple- picking scenario, if the target apple shifts slightly or another apple enters the field of view, the model dynamically reinterprets the scene and adjusts the grasp trajectory. This capability mimics human- like adaptability and is a core advantage of VLA systems over pipeline- based robotics.

## 3 Progress in Vision-Language-Action Models

The inception of VLA models was catalyzed by the remarkable success of transformer- based LLMs, notably ChatGPT, released in November 2022, which demonstrated unprecedented semantic reasoning capabilities (ChatGPT) [139]. This breakthrough inspired researchers to extend language models to multimodal domains, integrating perception and action for robotics. By 2023, GPT- 4 introduced multimodal capabilities, processing both text and images, which spurred efforts to incorporate physical actions (GPT- 4) [1]. Concurrently, VLMs like CLIP (2022) [157] and Flamingo (2022) [3] had established robust visual- text alignment through contrastive learning, enabling zero- shot object recognition and laying the groundwork for VLA models (CLIP). These models leveraged large- scale web datasets to align images with textual descriptions, a critical precursor to integrating actions.

A pivotal development was the creation of large- scale robotic datasets, such as RT- 1's 130,000 demonstrations, which provided action- grounding data essential for co- training vision, language, and action components [18]. These datasets captured diverse tasks and environments, enabling models to learn generalizable behaviors. Architectural breakthroughs followed with Google's RT- 2 in 2023 [17], a landmark VLA model that unified vision, language, and action tokens, treating robotic control as an autoregressive sequence prediction task (RT- 2 Blog). RT- 2 discretized actions using Discrete Cosine Transform (DCT) compression and Byte- Pair Encoding (BPE), achieving a  $63\%$  improvement in performance on novel objects. Multimodal fusion techniques, such as cross- attention transformers, integrated Vision Transformer (ViT)- processed images (e.g., 400 patch tokens) with language embeddings, enabling robots to execute complex commands like "Pick the red cup left of the bowl." Additionally, UC Berkeley's Octo model (2023) introduced an open- source approach with 93M parameters and diffusion decoders, trained on 800,000 robot demonstrations from the OpenX- Embodiment Dataset, further broadening the research landscape [167].

bowl." Additionally, UC Berkeley's Octo model (2023) introduced an open- source approach with 93M parameters and diffusion decoders, trained on 800,000 robot demonstrations from the OpenX- Embodiment Dataset, further broadening the research landscape [167].

### 3.1 Architectural Innovations in VLA Models

From 2023 to 2024, VLA models underwent significant architectural advancements and refined training methodologies. Dual- system architectures emerged as a key innovation, exemplified by NVIDIA's Groot N1 (2025) [13], which combined System 1 (fast diffusion policies with  $10\mathrm{ms}$  latency for low- level control) and System 2 (LLM- based planners for high- level task decomposition). This separation enabled efficient coordination between strategic planning and real- time execution, enhancing adaptability in dynamic environments. Other models, like Stanford's OpenVLA (2024) [94], introduced a 7B- parameter open- source VLA trained on 970k real- world robot demonstrations, using dual vision encoders (DINOv2 [128] and SigLIP [204]) and a Llama 2 language model [172], outperforming larger models like RT- 2- X (55B) [94]. Training paradigms evolved to leverage co- fine- tuning on web- scale vision- language data (e.g., LAION- 5B) [152] and robotic trajectory data (e.g., RTX) [175], aligning semantic knowledge with physical constraints [152]. Synthetic data generation tools like UniSim addressed data scarcity by creating photorealistic scenarios, such as occluded objects, crucial for robust training (UniSim [200]). Parameter efficiency was enhanced through Low- Rank Adaptation (LoRA) adapters [72], which allowed domain adaptation without full retraining, reducing GPU hours by  $70\%$ . The introduction of diffusion- based policies, as seen in Physical Intelligence's pi 0 model (2024) [14], offered improved action diversity but required significant computational resources. These advancements democratized VLA technology, fostering collaboration and accelerating innovation.

Recent VLA models have converged toward three major architectural paradigms: that balance efficiency, modularity, and robustness: early fusion models, dual- system architectures, and self- correcting frameworks. Each of these innovations addresses specific challenges in grounding, generalization, and action reliability in real- world robotic systems.

1. Early Fusion Models: One class of approaches focuses on fusing vision and language representations at the input stage before passing them to the policy module. Huang et al.'s EF-VLA model [74], presented at ICLR 2025, exemplifies this trend by retaining the representational alignment established by CLIP [157]. EF-VLA accepts image-text pairs, encodes them with CLIP's frozen encoders, and fuses the resulting embeddings early in the transformer backbone—prior to action prediction. This design ensures that the semantic consistency learned during CLIP pretraining is preserved, reducing overfitting and enhancing generalization. Notably, EF-VLA demonstrated a  $20\%$  performance improvement on compositional manipulation tasks and reached  $85\%$  success on previously unseen goal descriptions. By avoiding fine-tuning of the vision-language modules, this approach also preserves computational efficiency and

prevents catastrophic forgetting during domain- specific training.

2. Dual-System Architectures: Inspired by dual-process theories of human cognition, models like NVIDIA's Groot N1 (2025) [13] implement two complementary subsystems: a fast-reactive module (System 1) and a slow-reasoning planner (System 2). System 1 comprises a diffusion-based control policy that operates at  $10~\mathrm{ms}$  latency, ideal for fine-grained, low-level control such as end-effector stabilization or adaptive grasping. In contrast, System 2 uses a LLM for task planning, skill composition, and high-level sequencing. The planner parses long-horizon goals (e.g., "clean the table") into atomic subtasks, while the low-level controller ensures real-time execution. This decomposition enables multi-timescale reasoning and improved safety, especially in environments where rapid reaction and deliberation must co-exist. In benchmark tests on multi-stage household manipulation, Groot N1 outperformed monolithic models by  $17\%$  in success rate and reduced collision failures by 28

3. Self-Correcting Frameworks: A third architectural evolution is the development of self-correcting VLA models, designed to detect and recover from failure conditions without external supervision. SC-VLA (2024) introduces a hybrid execution loop featuring a fast inference path and a slow correction path. In this framework, the default behavior is to predict poses or actions directly from the fused embedding using a lightweight transformer. When failures are detected—e.g., unsuccessful grasps or obstacle collisions—the model invokes a secondary process that performs chain-of-thought reasoning [211, 203]. This path queries an internal LLM (or external expert system) to diagnose failure modes and generate correction strategies [48]. For example, if the robot repeatedly misidentifies an occluded object, the LLM may suggest an active viewpoint change or gripper reorientation. In closed-loop experiments, SC-VLA reduced task failure rates by  $35\%$  and significantly improved recoverability in cluttered and adversarial environments.

VLA models exhibit a rich diversity of architectural designs and functional emphases, which can be systematically organized along the dimensions of end- to- end versus modular pipelines, hierarchical versus flat policy structures, and the balance between low- level control and high- level planning (Table 1). End- to- end VLAs, such as CLiPort [157], RT- 1 [18], and OpenVLA [94], process raw sensory inputs directly into motor commands via a single unified network. By contrast, component- focused models like VLIAtest [182] and Chain- of- Affordance [100] decouple perception, language grounding, and action modules, enabling targeted improvements in individual submodules.

Hierarchical architectures have emerged to tackle complex, long- horizon tasks by separating strategic decision making from reactive control. For instance, CogACT [102] and NaVILA [32] employ a two- tier hierarchy where an LLM- based planner issues subgoals to a low- level controller, thereby combining the strengths of System 2 reasoning and System 1 execution. Similarly, ORION [56] integrates a QT- Former for long- term context aggregation with a generative trajectory planner in a cohesive framework.

Low- level policy emphasis is typified by diffusion- based controllers (e.g. Pi- 0 [14], DexGraspVLA [219]), which excel at producing smooth, diverse motion distributions but often incur higher computational cost. In contrast, high- level planners (e.g. FAST Pi- 0 Fast [133], CoVLA [5]) focus on rapid subgoal generation or coarse trajectory prediction, delegating fine- grained control to specialized modules or traditional motion planners. End- to- end dual- system models like HybridVLA [110] and Helix [166] blur these distinctions by jointly training both components while preserving modular interpretability.

Table 1 further highlights how recent VLAs balance these trade- offs. Models such as OpenDriveVLA [220] and CombatVLA [29] prioritize hierarchical planning in dynamic, safety- critical domains, whereas lightweight, edge- targeted systems like Edge VLA [19] and TinyVLA [186] emphasize real- time low- level policies at the expense of high- level reasoning. This classification framework not only clarifies the design space of VLAs but also guides future development by pinpointing underexplored combinations—such as fully end- to- end, hierarchical models optimized for embedded deployment—that promise to advance both the capabilities and the applicability of VLA systems across robotics, autonomous driving, and beyond.

The classification in Table 1 is significant because it provides a clear framework for comparing diverse VLA architectures, highlighting how design choices—such as end- to- end integration versus hierarchical decomposition—impact task performance, scalability, and adaptability. By categorizing models along dimensions like low- level policy execution and high- level planning, researchers can pinpoint strengths and limitations of existing approaches and identify opportunities for innovation. This taxonomy aids in selecting appropriate architectures for specific applications (e.g., real- time control vs. strategic reasoning) and guides future development toward hybrid systems that balance responsiveness with cognitive planning, ultimately accelerating progress in embodied AI. Additionally, to synthesize recent advancements in VLA models, Table 2 presents a comparative summary of notable systems developed from 2022 through 2025. Building upon architectural innovations such as early fusion, dual- system processing, and self- correcting feedback loops, these models incorporate diverse design philosophies and training strategies. Each entry highlights the model's key components—vision and language encoders, action decoders—and the datasets used to ground their capabilities. Models like CLiPort [157] and RT- 2 [224] laid early foundations by assigning semantic embeddings with action policies, while more recent frameworks like Pi- Zero, CogACT [102], and Groot N1 [13] introduce scalable architectures with diffusion- based or high- frequency controllers. Several models leverage multimodal pretraining with internet- scale vision- language corpora and robot trajectory datasets, enhancing generalization and zero- shot capabilities [223, 219, 218, 198]. This tabulated comparison serves as a reference point for researchers seeking to understand the functional diversity, domain applicability, and emerging trends in VLA design across real and simulated environments.

Table 1: Taxonomy of VLA models showing structured classification based on architectural paradigms and scientific priorities. We differentiate models by their support for end-to-end execution, hierarchical planning-control decomposition, or component-focused modularity, and further by their emphasis on low-level motor policies versus high-level task planners.  

<table><tr><td>Model Name</td><td>Year</td><td>End- to-End</td><td>End-Hire rare hi-cal</td><td>Comp Low-onent Fo-cused</td><td>High-Level Policy</td><td>High-Level Plan-ner</td></tr><tr><td>CLIPort [157]</td><td>2022</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>RT-1 [18]</td><td>2022</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>Gato [141]</td><td>2022</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>VIMA [86]</td><td>2022</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>Diffusion Policy [34]</td><td>2023</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>ACT [216]</td><td>2023</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>VoxPoser [78]</td><td>2023</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>Seer [63]</td><td>2023</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>Octo [167]</td><td>2024</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>OpenVLA [94]</td><td>2024</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>CogACT [102]</td><td>2024</td><td>✘</td><td>✓</td><td>✘</td><td>✓</td><td>✓</td></tr><tr><td>VLATest [182]</td><td>2024</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td></tr><tr><td>NaVILA [32]</td><td>2024</td><td>✘</td><td>✓</td><td>✘</td><td>✓</td><td>✓</td></tr><tr><td>RoboNurse-VLA [103]</td><td>2024</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>Mobility VLA [35]</td><td>2024</td><td>✘</td><td>✓</td><td>✘</td><td>✓</td><td>✓</td></tr><tr><td>RevLA [39]</td><td>2024</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td></tr><tr><td>Uni-NaVid [210]</td><td>2024</td><td>✘</td><td>✓</td><td>✘</td><td>✓</td><td>✓</td></tr><tr><td>RDT-1B [112]</td><td>2024</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>RoboMamba [111]</td><td>2024</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>Chain-of-Affordance [100]</td><td>2024</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td></tr><tr><td>Edge VLA [19]</td><td>2024</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td></tr><tr><td>ShowUI-2B [108]</td><td>2024</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>Pi-0 [14]</td><td>2024</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>FAST (Pi-0)</td><td>2025</td><td>✘</td><td>✘</td><td>✓</td><td>✓</td><td>✘</td></tr><tr><td>Fast) [133]</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>OpenVLA-OFT [93]</td><td>2025</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>CoVLA [5]</td><td>2025</td><td>✘</td><td>✓</td><td>✘</td><td>✓</td><td>✓</td></tr><tr><td>OpenDriveVLA [220]</td><td>2025</td><td>✘</td><td>✓</td><td>✘</td><td>✓</td><td>✓</td></tr><tr><td>ORION [56]</td><td>2025</td><td>✘</td><td>✓</td><td>✘</td><td>✓</td><td>✓</td></tr><tr><td>UAV-VLA [150]</td><td>2025</td><td>✘</td><td>✓</td><td>✘</td><td>✓</td><td>✓</td></tr><tr><td>CombatVLA [29]</td><td>2025</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>HybridVLA [110]</td><td>2025</td><td>✘</td><td>✓</td><td>✘</td><td>✓</td><td>✓</td></tr><tr><td>NORA [79]</td><td>2025</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>SpatialVLA [136]</td><td>2025</td><td>✘</td><td>✘</td><td>✓</td><td>✓</td><td>✘</td></tr><tr><td>MoLe-VLA [213]</td><td>2025</td><td>✘</td><td>✘</td><td>✓</td><td>✓</td><td>✘</td></tr><tr><td>JARVIS-VLA [101]</td><td>2025</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>UP-VLA [209]</td><td>2025</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>Shake-VLA [92]</td><td>2025</td><td>✘</td><td>✘</td><td>✓</td><td>✓</td><td>✘</td></tr><tr><td>DexGraspVLA [219]</td><td>2025</td><td>✘</td><td>✓</td><td>✘</td><td>✓</td><td>✓</td></tr><tr><td>DexVLA [185]</td><td>2025</td><td>✘</td><td>✓</td><td>✘</td><td>✓</td><td>✓</td></tr><tr><td>Humanoid-VLA [42]</td><td>2025</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>ObjectVLA [223]</td><td>2025</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr></table>

Table 2: Summary of VLA models, detailing each model's name, architecture features, training dataset, and highlighting their key strengths or unique capabilities in robotics and AI tasks.  

<table><tr><td>Model (Reference)</td><td>Architecture Components</td><td>Training Dataset</td><td>Key Strength / Uniqueness</td></tr><tr><td>CLIPort [157]</td><td>• Vision Encoder: CLIP-ResNet50 + Transporter-ResNet</td><td>Self-collected [SC]</td><td>Combines semantic CLIP features with spatial Transporter network for precise SE(2) manipulation.</td></tr><tr><td rowspan="5">RT-1 [18]</td><td>• Language Encoder: CLIP-GPT</td><td>Action Decoder: LingUNet</td><td rowspan="3">Pioneering Transformer architecture with discretized actions for multi-task kitchen manipulation.</td></tr><tr><td>• Vision Encoder: Efficient-Net</td><td>RT-1-Kitchen [SC]</td></tr><tr><td>• Language Encoder: Universal Sentence Encoder</td><td>• Vision Encoder: Trans-former</td></tr><tr><td>RT-2 [224]</td><td>VQA + RT-1-Kitchen</td><td>First large VLA co-fine tuned on internet VQA data and robot data for emergent capabilities.</td></tr><tr><td>• Vision Encoder: PaLI-X/PaLME</td><td>Self-collected [SC]</td><td rowspan="3">Generalist agent handling Atari, captioning, and robotics through unified tokenization.</td></tr><tr><td rowspan="2">Gato [141]</td><td>• Vision Encoder: VIT</td><td>Self-collected [SC]</td></tr><tr><td>• Language Encoder: Sen-tencePiece</td><td>Action Decoder: Trans-former</td></tr><tr><td rowspan="4">VLMA [86]</td><td>• Vision Encoder: VIT + Mask RCNN</td><td>VLMA-Data [SC]</td><td>Multi-modal prompt-handling with 6 types of vision-language grounding tasks.</td></tr><tr><td>• Language Encoder: T5</td><td>ALOHA [SC]</td><td>Temporal ensem-bling for smooth bin manual manipulation with 0.1mm precision.</td></tr><tr><td>• Vision Encoder: ResNet-18</td><td rowspan="2">Open X-Embodiment</td><td rowspan="2">First policy trained on 4M+ robot trajectories from 22 robot types.</td></tr><tr><td>• Vision Encoder: CVAE-Transformer</td></tr><tr><td rowspan="3">Octo [167]</td><td>• Vision Encoder: CNN</td><td rowspan="3">Zero-shot</td><td rowspan="6">LLM+VLM composition for constraint-aware motion planning without training.</td></tr><tr><td>• Language Encoder: T5-base</td></tr><tr><td>• Action Decoder: Diffusion Transformer</td></tr><tr><td rowspan="3">VoxPoser [78]</td><td>• Vision Encoder: VILD + MDETEP</td><td rowspan="3">Zero-shot</td></tr><tr><td>• Language Encoder: GPT-4</td></tr><tr><td>• Action Decoder: MPC</td></tr><tr><td rowspan="3">Diffusion Pol-icy [34]</td><td>• Vision Encoder: ResNet-18</td><td rowspan="3">Self-collected [SC]</td><td rowspan="3">Pioneering diffusion-based visuomotor policy handling multimodal action distributions.</td></tr><tr><td>• Language Encoder: —</td></tr><tr><td>• Action Decoder: U-Net/Transformer</td></tr><tr><td colspan="4">Continued on next page</td></tr></table>

<table><tr><td>Model (Reference)</td><td>Architecture Components</td><td>Training Dataset</td><td>Key Strength / Uniqueness</td><td>Model (Reference)</td><td>Architecture Components</td><td>Training Dataset</td><td>Key Strength / Uniqueness</td></tr><tr><td>OpenVLA [94]</td><td>· Vision Encoder: DINOv2 + SigLIP
· Language Encoder: Prismatic-7B
· Action Decoder: Symbol-tuning</td><td>DXE + DROID</td><td>Open-source alternative to RT-2 with efficient LoRA fine-tuning.</td><td>CogACT [102]</td><td>· Vision Encoder: DINOv2 VIT-L/1+, SigLIP VIT-So400M/14
· Language Encoder: Llama-2 (via Prismatic-7B VLM)</td><td>Open X-Embodiment (OXE) subset, real-world Realman &amp;amp; Franka tasks</td><td>Componentized VLA with specialized diffusion action transformer; outperformed OpenVLA by 59.1% in real-world success, excels at adaptation and generalization to new robots and unseen objects.</td></tr><tr><td>(Pi-Zero) [14]</td><td>· Vision Encoder: PaliGemma VLM backbone
· Language Encoder: PaliGemma (multimodal)</td><td>Pi-Cross-Embodiment dataset</td><td>Lightweight, efficient VLA model (3B params) excelling at general robot control and bimanual manipulation, with strong open-world generalization across diverse robots and tasks [1][5][7].</td><td>Chain-of-Affordance (CoA) [100]</td><td>· Action Decoder: Diffusion Transformer (DIT-Base, 300M parameters)
· Vision Encoder: Visual encoder with affordance feature extraction
· Language Encoder: Transformer-based language module for sequential reasoning prompts</td><td>LIBERO benchmark, real and simulated manipulation tasks</td><td>Incorporates reasoning via sequential affordances (ob-ject, grasp, spatial, movement); spatially superior LIBERO performance over OpenVLA, excelling in spatial reasoning and obstacle avoid-ance for precise task completion.</td></tr><tr><td>Pi-0 Fast [133]</td><td>· Vision Encoder: PaliGemma VLM backbone
· Language Encoder: PaliGemma (multimodal)</td><td>Pi-Cross-Embodiment dataset</td><td>Variant of Pi-0 optimized for high-frequency, real-time control using com-pressed action to-kenes; achieves up to 15x faster inference for discrete robot actions and strong generalization.</td><td>Edge VLA (EVLA) [19]</td><td>· Action Decoder: Autoregressive and diffusion policy with affordance-conditioned outputs
· Vision Encoder: SigLIP + DINOv2
· Language Encoder: Qwen2 (0.5B parameters)</td><td rowspan="2">Bridge dataset, OXE, 1.2M text-image pairs</td><td rowspan="2">Lightweight VLA model optimized for edge devices (e.g., Jetson Nano) with 30-50 Hz inference; achieves perfor-mance comparable to OpenVLA while enabling efficient, real-time deploy-ment on low-power devices.</td></tr><tr><td>OpenVLA-OFT [93]</td><td>· Vision Encoder: SigLIP + DINOv2 (multi-view)</td><td>LIBERO benchmark, bimanual LOHA</td><td>Optimized fine-tuning variant of OpenVLA achieving 97.1% success on LIBERO, with 26× parallel inference via parallel decoding and action chunking; excels at high-frequency bimanual control.</td><td>ShowUI-2B [108]</td><td>· Action Decoder: Joint control prediction (non-autoregressive)</td></tr><tr><td>RDT-11 [112]</td><td>· Vision Encoder: Multi-view RGB image encoder
· Language Encoder: Transformer-based language module
· Action Decoder: Diffusion Transformer with unified action space</td><td>M+ multi-robot episodes (46 datasets), fine-tuned 6K+ bimanual LOHA episodes</td><td>1.2B-parameter dif-fusion foundation model for biman-ual manipulation; excels at language-conditioned, dex-terous control and zero-shot generalization, with strong but task-specific performance in multi-object settings.</td><td rowspan="2">Groot N1 [13]</td><td rowspan="2">· Vision Encoder: In-tegrated with VLM for high-level planning and reasoning
· Language Encoder: In-tegrated with VLM for high-level planning and reasoning
· Action Decoder: Diffusion Transformer (DIT) for pre-cise, high-frequency action generation</td><td rowspan="2">Multimodal data: human demonstrations, robot trajectories, synthetic simulation, and internet video</td><td rowspan="2">Lightweight 2B-parameter VLA specialized for digital task automation; excels at GUI/web navigation and screen-shot grounding with efficient token selection and unified vision-language-action reasoning.</td></tr><tr><td>Helix 1</td><td>· Vision Encoder: Open-source VLM (System 2) for multimodal scene and language understanding at 7-9 Hz
· Language Encoder: Integrated with VLM for broad generalization and semantic comprehension</td><td>End-to-end or high-pixel data
· Language Encoder: Inte-grated with VLM for broad generalization and semantic comprehension</td><td>First VLA model for real-time, high-DoF humanoid control; enables zero-shot generalization, fine-grained dexterity, and collaborative multi-robot manipu-lation in open-world tasks.</td></tr><tr><td colspan="8">Continued on next page</td></tr></table>

<table><tr><td>Model (Refer-ence)</td><td>Architecture Components</td><td>Training Dataset</td><td>Key Strength / Uniqueness</td><td>Model (Refer-ence)</td><td>Architecture Components</td><td>Training Dataset</td><td>Key Strength / Uniqueness</td></tr><tr><td>Seer [63]</td><td>·Vision Encoder: Visual backbone optimized for grounding and perception</td><td>LIBERO benchmark</td><td>VLA model focused on visual perception and action prediction; achieves competitive results with strong visual grounding for manipulation, but is outperformed by OpenVLA-OFT [1][6][8][10].</td><td>QUAR-VLA [43]</td><td>·Vision Encoder: CLIP + proprioceptive embedding
·Language Encoder: BERT + custom generating adapter
·Action Decoder: Transformer for full-body command decoding</td><td>QUART dataset (locomo-tion + manipulation)</td><td>Quadruped-specific control with strong sim-to-real transfer and fine-grained instruction alignment</td></tr><tr><td rowspan="2">DiffusionVLA [184]</td><td>Vision Encoder: Transformer-based visual encoder for contextual perception</td><td>LIBERO benchmark, activity factoring, zero-shot in-picking tasks</td><td>Leverages diffusion-based action modeling for precise control; demonstrates robustness and in-terpretability, but is less generalizable than CoA in spatial configurations.</td><td>ChatVLA [221]</td><td>·Vision Encoder: Vision encoder integrated with Phase-A signed transformer
·Language Encoder: Prismatic LAM with Mixture-of-Experts</td><td>Unified chat-action dataset (web, robot)</td><td>Exceats at joint VOA and planning; mitigates forgetting; efficient across manipulation and conversational tasks</td></tr><tr><td>·Language Encoder: Au-toregressive reasoning module with next-token prediction</td><td>·Action Decoder: Diffusion policy head for robust action sequence generation</td><td>Real-world logged robot nav demos</td><td>PointVLA [96]</td><td>·Action Decoder: Unified vision-language-action planner
·Vision Encoder: CLIP + 3D Point Cloud (via skip blocks)</td><td>Few-shot spatial tasks (real + sim)</td><td>Exceats at long-horizon and spatial reasoning tasks; avoids retraining by preserving pretrained 2D knowledge</td></tr><tr><td>NaVILA [32]</td><td>·Vision Encoder: CLIP + CNN</td><td>Real-world logged robot nav demos</td><td>Modular hierarchy enables robust terrain generalization and 88% real-world nav success using natural language</td><td>·VLA-Cache [195]</td><td>·Language Encoder: LLaMA 2
·Action Decoder: Transformer with spatial token fusion
·Vision Encoder: SigLIP with token memory buffer
·Language Encoder: Prismatic-Cache: 7B</td><td>ALOHA + real-world sim fusion</td><td>40-50% faster inference with near-zero loss; dynamically reuses static features for real-time robotics</td></tr><tr><td rowspan="2">RoboNanos-VLA [103]</td><td>Vision Encoder: SAM2 + RGB-D</td><td>Surgeover and voice prompts</td><td>Enthusiast encoder real-time surgical tool handover; strong robustness to tool novelty and dynamic OR scenes</td><td>HybridVLA [110]</td><td>·Action Encoder: Transformer with dynamic token reuse
·Vision Encoder: CLIP + DINOva</td><td>RT-X + synthetic task fusion</td><td>Achieves robust control in complex multi-arm settings via dynamic ensemblable; strong sim2real generalization</td></tr><tr><td>·Language Encoder: LLaMA 2 + voice-to-text encoder</td><td>·Action Decoder: Joint pose regression with gripper classifier</td><td>·MINT dataset: vision-language construction</td><td>·Action Encoder: LLaMA 2</td><td>·Language Encoder: Hybrid diffusion + autoregressive ensemble
·Action Decoder: Multi-stage VIT with STAR router
·Language Encoder: CogKD enhanced transformer</td><td>RLBench + real-world manipulation tasks</td><td>Brain-inspired efficiency with 5.6x speedup; selective layer activation with high task success (+8%)</td></tr><tr><td>Mobility VLA [35]</td><td>·Vision Encoder: Long-context ViT + goal image encoder</td><td>·MINT dataset: vision-language construction</td><td>Robust navigation from multimodal input; generalizes across large unobservational mapping</td><td>·Action Encoder: Multi-stage VIT with STAR router
·Language Encoder: CogKD enhanced transformer</td><td>·Action Decoder: Sparse transformer with dynamic routing
·Action Encoder: VIT for aerial imagery</td><td>Satellite + UAV imagery instructions</td><td>Zero-shot aerial task planning; intuitive imagery generation; scalable to large unmapped environments</td></tr><tr><td rowspan="2">TinyVLA [186]</td><td>·Vision Encoder: FastViT with low-latency encoding</td><td>·Mint ALOHA + SC</td><td rowspan="2">Outperforms Open-VLA in speed and precision; does not require pretraining; inference 5x faster with minimal compute</td><td rowspan="2">·UAV-VLA [150]</td><td>·Action Encoder: GPT for instruction parsing</td><td>Satellite + UAV</td><td rowspan="2">Zero-shot aerial task planning; intuitive imagery generation; scalable to large unmapped environments</td></tr><tr><td>·Language Encoder: Compact</td><td>·Action Decoder: Diffusion policy decoder (50M params)</td><td>·Action Encoder: Transformer-based path planner</td><td>Continuous on next page</td></tr></table>

<table><tr><td>Model (Refer-ence)</td><td>Architecture Components</td><td>Training Dataset</td><td>Key Strength / Uniqueness</td></tr><tr><td>DexGraspVLA [200] Vision Encoder: Object-centric spatial VIT</td><td>·Language Encoder: Transformer with grasp sequence reasoning</td><td>Dexterous grasping benchmark (single real)</td><td>90%+ zero-shot success on diverse objects; excels at lighting, background variation, and unseen conditions</td></tr><tr><td>GraspVLA [38]</td><td>·Action Decoder: Diffusion controller for grasp pose generation</td><td>SynGrasp-B (1B synthetic frames), GRIT Internet grounding (dataset)</td><td>First synthetic-data-pretrained grasping VLA; enables sim-to-real generalization, robust grasp policy via PAG; supports zero-shot and few-shot generalization to long-tail object classes and human-centric preferences</td></tr><tr><td rowspan="2">Interleave-VLA [51]</td><td>·Vision Encoder: In-ternVL2.5 and OWLv2 for open-vocabulary and image-text token integration</td><td>Open In-terleaved K-Embodiment 210k episodes (11 out of 11 world datasets)</td><td rowspan="2">First end-to-end VLA model for inter-leave image-text instructions; improves out-of-domain generation zero-shot and enables zero-shot execution from hand-drawn sketches and novel multimodal prompts</td></tr><tr><td>·Language Encoder: Qwen2.5 for instruction parsing and visual-language verification</td><td>·Action Decoder: Continuous action predictor adapted from OpenVL A and r² with diffusion-policy controller</td></tr></table>

### 3.2 Training and Efficiency Advancements in Vision-Language-Action Models

VLA models have seen rapid progress in training and optimization techniques to reconcile multimodal inputs, reduce compute requirements, and enable real- time control. Key areas of advancement include:

- Data-Efficient Learning.

- Co-fine-tuning on massive vision-language corpora (e.g. LAION-5B) and robotic trajectory collections (e.g. Open X-Embodiment) aligns semantic understanding with motor skills. OpenVLA (7 B params) achieves a  $16.5\%$  higher success rate than a  $55\mathrm{B}$  parameter RT-2 variant, demonstrating that co-fine-tuning yields strong generalization with fewer parameters [152, 175, 94].

- Synthetic Data Generation via UniSim produces photorealistic scenes—including occlusions and dynamic lighting—to augment rare edge-case scenarios, improving model robustness in cluttered environments by over  $20\%$  [200, 167].

- Self-Supervised Pretraining adopts contrastive objectives (à la CLIP) to learn joint visual-text embeddings before action fine-tuning, reducing reliance on task-specific labels. Qwen2-VL leverages

self- supervised alignment to accelerate downstream grasp- and- place convergence by  $12\%$  [137, 76].

- Parameter-Efficient Adaptation. Low-Rank Adaptation (LoRA) inserts lightweight adapter matrices into frozen transformer layers, cutting trainable weights by up to 70  $\%$  while retaining performance [72]. The Pi-0 Fast variant uses merely  $10\mathrm{M}$  adapter parameters atop a static backbone to deliver continuous  $200\mathrm{Hz}$  control with negligible accuracy loss [133].

- Inference Acceleration.

- Compressed Action Tokens (FAST) and Parallel Decoding in dual-system frameworks (e.g. Groot N1) yield  $2.5\times$  faster policy steps, achieving sub-5 ms latencies at a modest cost to trajectory smoothness [13, 161].

- Hardware-Aware Optimizations—including tensor-core quantization and pipelined attention kernels—shrink runtime memory footprints below 8 GB and enable real-time inference on embedded GPUs [93].

Together, these methods have transformed VLAs into practical agents capable of handling language- conditioned, vision- guided tasks in dynamic, real- world settings.

### 3.3 Parameter-Efficient Methods and Acceleration Techniques in VLA Models

Building on advances in data- efficient training, recent work has focused on reducing the parameter footprint and improving inference speed of VLA models—critical for deployment on resource- constrained robotic platforms.

1. Low-Rank Adaptation (LoRA). LoRA injects small trainable rank-decomposition matrices into frozen transformer layers, enabling fine-tuning of billion-parameter VLAs with only a few million additional weights. In OpenVLA, LoRA adapters (20 M parameters) tuned a 7 B-parameter backbone on commodity GPUs in under 24 h, cutting GPU compute by  $70\%$  compared to full back-propagation [72, 94]. Crucially, LoRA-adapted models retain their high-level language grounding and visual reasoning capabilities while adapting to new robotic manipulation tasks (e.g. novel object shapes), making large VLAs accessible to labs without supercomputing resources.

2. Quantization. Reducing weight precision to 8-bit integers (INT8) shrinks model size by half and doubles on-chip throughput. OpenVLA experiments show that INT8 quantization on Jetson Orin maintains  $97\%$  of full-precision task success across pick-and-place benchmarks, with only a  $5\%$  drop in fine-grained dexterity tasks [152, 94]. Complementary methods such as post-training quantization with per-channel calibration further minimize accuracy loss in high-dynamic-range sensor inputs [128]. These optimizations allow continuous control loops at  $30\mathrm{Hz}$  on 50 W edge modules.

3. Model Pruning. Structured pruning removes entire attention heads or feed-forward sublayers identified as redundant. While less explored in VLA than in pure vision or language models, early studies on Diffusion Policy demonstrate that pruning up to  $20\%$  of ConvNet-based vision encoders yields negligible performance degradation in grasp stability [34]. Similar schemes applied to transformer-based VLAs (e.g. RDF-1B) can reduce memory footprint by  $25\%$  with under  $2\%$  drop in task success, paving the way for sub-4 GB deployments [112, 102].

4. Compressed Action Tokenization (FAST)-FAST reformulates continuous action outputs as frequency-domain tokens, compressing long control sequences into concise descriptors. The Pi-0 Fast variant achieved  $15\times$  faster inference with a  $300\mathrm{M}$  -parameter diffusion head by tokenizing  $1000~\mathrm{ms}$  action windows into 16 discrete tokens, enabling  $200\mathrm{Hz}$  policy rates on desktop GPUs [133]. This approach trades minimal trajectory granularity for large speedups, suited for high-frequency control in dynamic tasks like bimanual assembly.

5. Parallel Decoding and Action Chunking. Autoregressive VLAs traditionally decode actions token by token, incurring sequential latency. Parallel decoding architectures (e.g. in Groot N1) decode groups of spatial-temporal tokens concurrently, achieving a  $2.5\times$  reduction in end-to-end latency on 7-DoF arms at  $100\mathrm{Hz}$  with less than 3 mm positional error increase [13, 161]. Action chunking further abstracts multi-step routines into single tokens (e.g. "pick-and-place-cup"), cutting inference steps by up to 40  $\%$  in long-horizon tasks like kitchen workflows [86].

6. Reinforcement Learning-Supervised Hybrid Training. The iRe-VLA framework alternates between reinforcement learning (RL) in simulation and supervised finetuning on human demonstrations to stabilize policy updates. By leveraging Direct Preference Optimization (DPO) to shape reward models and Conservative Q-Learning to avoid extrapolation error, iRe-VLA reduces sample complexity by  $60\%$  versus pure RL, while maintaining the semantic fidelity imparted by language-conditioned priors [123, 65]. This hybrid approach yields robust policies for tasks with sparse feedback, such as dynamic obstacle avoidance.

7. Hardware-Aware Optimizations. Compiler-level graph rewrites and kernel fusion (e.g. via NVIDIA TensorRT-LLM) exploit target hardware features—tensor cores, fused attention, and pipelined memory transfers—to accelerate both transformer inference and diffusion sampling. In OpenVLA-OFT, such optimizations reduced inference latency by  $30\%$  on RTX A2000 GPUs and lowered energy per inference by  $25\%$  compared to standard PyTorch execution [93]. This makes real-time VLAs feasible on mobile robots and drones with strict power budgets.

Discussion. Parameter- efficient adaptation and inference acceleration techniques collectively democratize VLA deployment:

LoRA and quantization empower smaller labs to fine- tune and operate billion- parameter VLAs on consumer- grade hardware, unlocking cutting- edge semantic understanding for robots [72, 94]. Pruning and FAST tokenization compress model and action representations, enabling sub- 4 GB, sub- 5 ms control loops without sacrificing precision in dexterous tasks [112, 133]. Parallel decoding and action chunking overcome sequential bottlenecks of autoregressive policies, supporting  $100 - 200\mathrm{Hz}$  decision rates needed for agile manipulation and legged locomotion [13, 161]. Hybrid RL- SL training stabilizes exploration in complex environments, while hardware- aware compilation ensures real- time performance on edge accelerators [123, 93].

Together, these advances make it practical to embed VLA models across industrial manipulators, assistive drones, and consumer robots, bridging the gap from research prototypes to real- world autonomy.

### 3.4 Applications of Vision-Language-Action Models

VLA models are rapidly emerging as foundational building blocks for embodied intelligence, integrating perception, natural language understanding, and motor control within a unified architecture. By encoding visual and linguistic modalities into shared semantic spaces and generating contextually grounded actions, VLA models enable seamless interaction between agents and their environments [102, 220]. This multimodal capacity has positioned VLAs as transformative agents across a wide spectrum of real- world applications. In humanoid robotics, systems like Helix and RoboNurse- VLA combine vision, language, and dexterous manipulation to assist with domestic tasks and surgical operations, demonstrating real- time reasoning and safety- aware control [103, 186]. In autonomous vehicles, models such as OpenDriveVLA and ORION process dynamic visual streams and natural language instructions to make transparent, adaptive driving decisions in complex urban environments [56, 220]. Industrial deployments leverage VLA architectures for high- precision assembly, inspection, and collaborative manufacturing [102]. In agriculture, VLA- powered robotic systems enable vision- guided fruit harvesting, plant monitoring, and anomaly detection, reducing labor dependency and increasing sustainability. Furthermore, recent advances in interactive augmented reality systems utilize VLA models for real- time, language- conditioned spatial navigation, guiding users in indoor and outdoor settings based on voice or visual cues [150, 59]. Across these domains, VLAs offer a unified framework for robust, adaptable, and semantically aligned task execution, marking a pivotal shift toward embodied generalist agents.

Table 3 in the appendix shows the recent VLA models by summarizing their methodologies, application domains, and key innovations.

The following subsections chronologically explore the application areas in depth as shown in Figure 11.

![](images/e17cde1e63b222f5e86f33abc80fd71f6b40827f93d70b6acb61307c0d113b39.jpg)  
Figure 11: Mind-map of application domains for Vision-Language-Action models.

#### 3.4.1 Humanoid Robotics

Humanoid robots, designed to mimic the form and functionality of the human body, represent one of the most demanding yet impactful domains for the deployment of VLA models. These platforms must seamlessly perceive complex environments, understand spoken or written natural language, and perform intricate physical tasks with human- level dexterity [144, 22]. The core strength of VLA models lies in their ability to unify perception, cognition, and control into a single, end- to- end trainable framework—allowing humanoid robots to interpret visual inputs (e.g., RGB- D imagery of cluttered scenes), comprehend linguistic instructions (e.g., "place the spoon in the drawer"), and generate precise motor trajectories [118, 22].

Recent advances have significantly accelerated the deployment of VLAs in humanoid robotics. For example, Helix, a humanoid robot developed by Figure AI, leverages a fully integrated VLA model to perform full- body manipulation at high frequency, controlling arms, hands, torso, and even fine- grained finger motion in real time. The architecture follows a dual- system design: a multimodal transformer processes inputs such as language commands and vision streams, while a real- time motor policy outputs dense action vectors at  $200\mathrm{Hz}$ . This allows Helix to generalize across previously unseen objects and tasks, adapting fluidly to changing environments without the need for task- specific retraining.

The key advantage of VLAs in humanoid systems is their ability to scale across diverse tasks using shared representations [8]. Unlike traditional robotic systems that rely on task- specific programming or modular pipelines, VLA- powered humanoids operate under a unified token- based framework. Vision inputs are encoded via pretrained vision- language models like DINOv2 or SigLIP, while instructions are processed using large language models such as Llama- 2 or GPT- style encoders. These representations are fused into prefix tokens that capture the full context of the scene and task. Action tokens are then generated autoregressively, similar to language decoding, but represent motor commands for the robot's joints and effectors.

This capability enables humanoid robots to operate effectively in human- centric spaces, such as households, hospitals, and retail environments. In domestic settings, VLA- powered robots can clean surfaces, prepare simple meals, or organize objects simply by interpreting voice commands [118, 222]. In healthcare, systems like RoboNurse- VLA [103] have demonstrated the ability to perform precise instrument handovers to surgeons using real- time voice and visual cues. In retail, humanoid platforms equipped with VLAs can assist with customer queries, restock shelves, and navigate store layouts without explicit pre- programming [8].

What distinguishes modern humanoid VLAs is their ability to run on embedded, low- power hardware, making real- world deployment viable. For instance, systems such as TinyVLA [186] and MoManipVLA [190] demonstrate efficient inference pipelines that run on Jetson- class GPUs, enabling mobile deployment without compromising performance. These models exploit techniques like diffusion- based policies, LoRA- based fine- tuning, and dynamic token caching to minimize compute cost while retaining high precision and generalization.

In logistics and manufacturing, humanoid VLAs are already making a commercial impact. Robots like Figure 01 are deployed in warehouses to perform repetitive, physically intensive tasks—such as picking, sorting, and shelving—alongside human workers. Their ability to handle novel object categories and dynamically changing scenes is powered by continual learning and robust multimodal grounding [195, 102].

As VLA models continue to advance in their capacity for diverse action generation, spatial reasoning, and real- time adaptation, humanoid robots are emerging as highly capable assistants across homes, industrial settings, and public spaces. Their strength lies in their ability to unify perception, language comprehension, and motor control through a shared token- based architecture—enabling seamless, context- aware behavior in unstructured human environments.

For example, as depicted in the figure 12, consider 'Helix', a state- of- the- art humanoid robot equipped with a next- generation VLA model. When instructed verbally, "Please take the water bottle from the fridge," Helix activates its integrated perception system, where a foundation vision- language model (e.g., SigLIP or DINOv2) segments the visual scene to identify the refrigerator, its handle, and the bottle. The language input is processed by a large language model such as LLaMA- 2, which tokenizes the instruction and fuses it with the visual context. This fused representation is passed to a hierarchical controller:

![](images/8744d240c42e48bd8e7e691c7f6f0fb28b3ea86b48aa391a6dc6aff440549da7.jpg)

acceleration and trajectory visualizations interpretable to humans. Its end- to- end framework achieves state- of- the- art performance on planning benchmarks and question- answering tasks related to driving scenarios, demonstrating its robustness in urban navigation and behavioral prediction.

Another seminal model, ORION [56], pushes the boundaries of closed- loop autonomous driving by incorporating a QT- Former to retain long- horizon visual context, a large language model for reasoning over traffic narratives, and a generative trajectory planner. ORION excels at aligning the discrete reasoning space of vision- language models with the continuous control space of AV motion. This unified optimization results in accurate visual question answering (VQA) and trajectory planning, crucial for scenarios involving ambiguous human instructions or occluded obstacles (e.g., "take the exit after the red truck").

For example, as depicted in Figure 13 consider an autonomous delivery vehicle, "AutoNav," operating in a dense urban environment using a next- generation VLA architecture. As AutoNav receives a cloud- based instruction—"Drop off the package near the red awning beside the bakery, then return to base avoiding construction zones"—its onboard VLM (e.g., CLIP or SigLIP) parses the visual stream from multiple cameras, identifying dynamic landmarks such as bakery signs, red awnings, and traffic cones. Simultaneously, the LLM module grounded in LLaMA- 2 decodes the instruction and fuses it with real- time sensory context including LiDAR, GPS, and inertial odometry. A hierarchical control stack processes these multimodal signals via an autoregressive VLA decoder that integrates egocentric views and world- centric maps to plan adaptive paths. As the vehicle approaches the delivery location, unexpected pedestrian activity prompts an agentic submodule to trigger trajectory re- planning using a reinforcement learning- inspired policy refinement routine. At the same time, AutoNav audibly warns pedestrians and recalibrates its speed to maintain safety margins. This interplay of semantic understanding, perceptual grounding, and adaptive control exemplifies the power of VLA- based systems in achieving interpretable, human- aligned behavior in safety- critical scenarios. It also demonstrates how such integration can surpass traditional perception- planning- control pipelines in autonomy, transparency, and decision- making agility.

In aerial robotics, VLAs enhance the capabilities of delivery drones and UAVs. Models such as UAV- VLA [150] combine satellite imagery, natural language mission descriptions, and onboard sensing to execute high- level commands (e.g., "deliver to the rooftop pad with the blue tarp"). These systems use modular VLA architectures, where a vision- language planner parses global context and a flight controller executes precise waypoints, supporting applications in logistics, disaster response, and military reconnaissance.

As autonomous systems increasingly operate in unstructured environments, VLAs provide a scalable, interpretable, and data- efficient alternative to traditional pipelines. By learning from large- scale multimodal datasets and modeling decision- making as token prediction, VLAs align human- level semantics with robotic motion, paving the way for safer, smarter autonomous driving and navigation technologies.

![](images/837b84ce3d9a5fbde5fd545e8936586cff19988fc63b6ab57f979cf974b90f9f.jpg)  
Figure 13: This illustration depicts an autonomous delivery vehicle powered by a VLA system, integrating VLMs for visual grounding, LLMs for instruction parsing, and a VLA decoder for path planning. Agentic AI enables adaptive trajectory refinement in dynamic environments, exemplifying how multimodal integration drives safe, interpretable, and autonomous decision-making in real-world navigation tasks.

#### 3.4.3 Industrial Robotics

Industrial robotics is undergoing a paradigm shift with the integration of VLA models, enabling a new generation of intelligent robots capable of high- level reasoning, flexible task execution, and natural communication with human operators [27, 7]. Traditional industrial robots typically operate in highly structured environments using rigid programming, often requiring extensive reconfiguration and manual intervention when adapting to new assembly lines or product variants [6, 142]. Such systems lack the semantic grounding and adaptability required for modern dynamic manufacturing settings.

VLA models, by contrast, offer a more human- interpretable and generalizable framework. Through the joint embedding of visual inputs (e.g., component layout or conveyor belt state), natural language instructions (e.g., "tighten the screw on the red module"), and robot state, VLAs can infer context and execute appropriate control commands in real- time [105, 58, 121]. Vision transformers (e.g., VIT, DINOv2), large language models (e.g., LLaMA- 2), and autoregressive or diffusion- based action decoders form the backbone of these systems, allowing the robot to parse multimodal instructions and perform actions grounded in its environment.

One of the most significant contributions in this domain is CogACT [102], a componentized VLA framework explicitly designed for industrial robotic manipulation. Unlike early VLAs that relied on frozen language- vision embeddings followed by direct action quantization, CogACT introduces a diffusion- based action transformer that models action sequences more robustly and adaptively. The system uses a visual- language encoder (e.g., Prismatic- 7B) to extract high- level scene and instruction embeddings, which are then passed to a diffusion transformer (DiT- Base) to generate fine- grained

motor actions. This modular separation enables superior generalization to unseen tools, parts, and layouts while preserving interpretability and robustness under real- world constraints.

Furthermore, CogACT demonstrates rapid adaptation across different robot embodiments—such as 6- DoF arms or bimanual systems—through efficient fine- tuning, making it suitable for deployment across heterogeneous factory environments [102]. Empirical evaluations show that CogACT outperforms prior models like OpenVLA by over  $59\%$  in real- world task success rates, especially in complex, high- precision tasks such as multistep assembly, screw fastening, and part sorting.

As manufacturing shifts toward Industry 4.0 paradigms, VLAs promise to reduce programming overhead, support voice- commanded robot programming, and facilitate real- time human- robot collaboration on mixed- initiative tasks. While execution precision, safety guarantees, and latency optimizations remain areas of active research, the use of VLA models in industrial robotics marks a substantial step toward autonomous, intelligent, and adaptable robotic factories.

#### 3.4.4 Healthcare and Medical Robotics

Healthcare and medical robotics represent a high- stakes domain where precision, safety, and adaptability are paramount—qualities that VLA models are increasingly well- suited to provide [103, 151]. Traditional medical robotic systems rely heavily on teleoperation or pre- programmed behaviors [130, 158], limiting their autonomy and responsiveness in dynamic surgical or care environments. In contrast, VLA models offer a flexible framework that integrates real- time visual perception, language comprehension, and fine- grained motor control, enabling medical robots to understand high- level instructions and autonomously perform intricate procedures or assistance tasks [102, 43, 174].

In surgical robotics, VLAs can dramatically enhance capabilities in minimally invasive operations [40, 177]. These systems can fuse laparoscopic video feeds [98], anatomical maps [114, 40], and voice commands into a unified tokenized representation using vision encoders (e.g., VIT, SAM- 2) and language models (e.g., LLaMA, T5) [181]. For instance, as depicted in Figure 14a, in a task like "apply a suture to the left coronary artery," the vision module identifies the anatomical target, while the language module contextualizes the instruction. The action decoder then translates the fused semantic embedding into stepwise motion commands with sub- millimeter precision. This enables the robot to adaptively reposition tools, apply dynamic force feedback, and avoid critical structures, reducing the need for surgeon micromanagement and minimizing risk of human error.

Beyond the operating room, VLA models are powering a new generation of patient- assistive robots in eldercare, rehabilitation, and hospital logistics. These systems can autonomously perceive patient behavior, understand spoken or gestural input, and execute responsive tasks such as retrieving medication, guiding mobility aids, or notifying caregivers during emergencies. For example, as depicted in Figure 14b, a VLA- enabled robot can visually detect a patient attempting to rise from bed, interpret a verbal request such as "bring my walker," and generate a context- appropriate motion plan to assist—without predefined scripts or constant supervision.

![](images/701a31a08493a0ccb1f36fa85add117753325b18ca477ba19b61fa009f079f3c.jpg)  
Figure 14: a) This figure illustrates a VLA surgical system executing the task "apply a suture to the left coronary artery." The vision module identifies anatomical targets, the language model interprets the instruction, and the action decoder generates precise motor commands, enabling adaptive tool control, real-time feedback, and safe autonomous operation; b) A VLA-powered assistive robot perceives patient behavior, processes verbal requests (e.g., "bring my walker"), and autonomously executes context-aware motion plans, enabling real-time assistance in eldercare, rehabilitation, and hospital logistics without relying on predefined scripts or manual oversight.

Recent VLA frameworks such as RoboNurse- VLA [103] highlight the real- world feasibility of this approach. RoboNurse employs SAM- 2 for semantic scene segmentation and LLaMA- 2 for command comprehension, integrated into a real- time voice- to- action pipeline that enables robots to assist with surgical instrument handovers in operating rooms [103]. The system demonstrates robustness to diverse tools, varied lighting conditions, and noisy environments—common challenges in clinical settings.

Additionally, VLA architectures offer advantages in explainability and auditability, both critical in regulated medical domains [173, 113]. Scene grounding and trajectory prediction can be visualized and reviewed post- hoc [208], which could facilitate clinical trust and enabling FDA- style validation pipelines. LoRA- based fine- tuning allows adaptation to specific

hospital environments or procedural workflows with minimal data and compute [9, 176, 114].

Importantly, the multimodal foundation of VLA models enables cross- domain transferability: the same model trained on surgical tool manipulation can be adapted to patient mobility tasks with modest retraining [45]. This modularity significantly reduces development time and cost compared to task- specific automation systems [73]. As medical robotics transitions from teleoperated assistance to semi- autonomous and collaborative systems, VLA models stand at the core of this transformation.

By combining high- level semantic understanding with low- level control, VLAs provide a unified solution for scalable, human- aligned, and adaptive robotic healthcare [193, 221, 209]. As healthcare systems face increasing demand and workforce shortages, VLA- driven robotics will play a crucial role in enhancing medical precision, operational efficiency, and patient- centered care.

#### 3.4.5 Precision and Automated Agriculture

As illustrated in Figure 15, VLA models are emerging as transformative tools in precision and automated agriculture, offering intelligent, adaptive solutions for labor- intensive tasks across diverse farming landscapes [57, 150]. Unlike traditional agricultural automation systems that depend on rigid, sensor- driven pipelines and require manual reprogramming for each task or environmental variation [169, 84], VLAs integrate multimodal perception, natural language understanding, and real- time action generation within a unified framework [131, 66]. This enables autonomous ground robots and drones to interpret complex field scenes, follow spoken or text- based farming instructions, and generate context- aware actions such as selective fruit picking or adaptive irrigation. The ability of VLAs to dynamically adjust to occlusions, terrain irregularities, or varying crop types—combined with training on synthetic, photorealistic datasets—allows them to generalize across geographies and seasons. By leveraging action tokenization [189], transformer- based policy generation [11, 67], and techniques like LoRA fine- tuning [72], these systems are redefining the scalability and intelligence of agricultural robotics for sustainable and precision- driven farming.

In modern orchards and crop fields, VLAs can process visual inputs from RGB- D cameras, multispectral sensors, or drones to monitor plant growth, detect diseases, and identify nutrient deficiencies. Vision transformers (e.g., ConvNeXt, DINOv2) encode spatial and semantic information from visual scenes, while large language models (e.g., T5, LLaMA) parse natural language commands—such as “inspect the east plot for powdery mildew” or “harvest ripe apples near the irrigation trench.” Through token fusion, these modalities are aligned in a shared representation space, allowing robots to execute fine- grained, context- aware actions with precision.

For instance, in fruit- picking tasks, as illustrated in Figure 15, a VLA- equipped ground robot can identify ripe produce using image- based ripeness cues, interpret user- specified criteria such as “pick only Grade A fruits,” and execute motion sequences via action tokens that control its end- effector. This approach ensures minimal crop damage, optimizes pick rates, and allows real- time adaptation to unexpected variables like occlusions or terrain shifts. In irrigation management, drones guided by VLA models can interpret field maps and verbal instructions to selectively water stressed zones, reducing water usage by up to  $30\%$ .

Moreover, VLA models support dynamic reconfiguration and lifelong learning. With access to synthetic training datasets generated from photorealistic simulations of crop environments (e.g., 3D orchard renderings), models can be trained to recognize pests, weeds, and crop maturity stages without extensive manual annotation. Techniques like LoRA adapters and diffusion- based policy tuning further enhance generalization to novel crops, seasons, and geographical regions.

The integration of VLAs into agricultural workflows offers significant benefits: reduced dependence on skilled labor, higher yield through targeted intervention, and enhanced environmental sustainability through optimized input usage. As global food systems grapple with climate variability and resource constraints, VLA- enabled agriculture will play a pivotal role in advancing scalable, intelligent, and sustainable farming practices tailored to real- world complexity.

#### 3.4.6 Interactive AR Navigation with Vision-Language-Action Models

Interactive Augmented Reality (AR) navigation represents a frontier where the VLA models can significantly enhance human- environment interaction by providing intelligent, context- aware guidance in real- time [26, 80, 197]. In this paradigm, VLAs process continuous streams of visual data from AR- enabled devices—such as smart glasses or smartphones—alongside natural language queries to generate dynamic navigational cues overlaid directly onto the user's view of the physical world. Unlike traditional GPS- based systems that rely on rigid maps and limited user input [24, 159], VLA- based AR agents interpret complex visual scenes (e.g., intersections, indoor hallways, signage) and respond to free- form instructions such as “take me to the nearest pharmacy with a wheelchair ramp” or “show the quietest route to the conference room.”

Technically, these models integrate a vision encoder (e.g., ViT, DINOv2) that extracts scene representations from first- person RGB frames, a language encoder (e.g., T5 or LLaMA) that processes user prompts or voice commands, and an action decoder that predicts tokenized navigation cues such as directional overlays, waypoints, or voice instructions. A transformer- based architecture fuses these modalities to reason about both the spatial layout and semantic intent, allowing the AR agent to adaptively highlight paths, landmarks, and hazards directly within the user's field of view [163, 129]. For example, as shown in Figure 16, in a crowded airport, the VLA agent could visually identify escalators, gates, or baggage claims while understanding a query like “how do I reach Gate 22 without stairs?”, adjusting the route in response to real- time occupancy and obstacles.

VLAs also support interaction loops that enable users to refine instructions (e.g., “avoid busy areas” or “take the scenic route”) and receive context- aware feedback, improving accessibility for the visually impaired or cognitively challenged. In

![](images/5157e4913e210ec8897b79178104365119d4958f097aa14c4b457c0f813b62b8.jpg)  
Figure 15: This diagram illustrates the application of VLA models in precision and automated agriculture. A ground robot uses vision encoders to detect ripe fruits and interprets instructions such as "pick only Grade A fruits" through language encoders. Action tokens then guide robotic manipulators for efficient, damage-free picking. Dents leverage VLA models to analyze aerial imagery and verbal commands for targeted irrigation. Synthetic training environments and LoRa-based adaptation enable models to generalize across crop types, environmental conditions, and geographies. This VLA-driven pipeline promotes sustainable agriculture by improving productivity, reducing manual labor, and enhancing decision-making through multimodal perception and control.

logistics and indoor navigation, these systems can be integrated with IoT sensors and digital twins to guide warehouse workers, maintenance teams, or delivery robots through complex environments. Furthermore, personalized navigation can be achieved through continual fine- tuning, where VLA models learn user preferences and local spatial layouts over time.

As AR hardware becomes more affordable and integrated into daily life, VLA- powered navigation systems will enable seamless spatial understanding, multimodal interaction, and autonomous guidance in public, industrial, and assistive contexts—redefining how humans perceive, explore, and interact with physical spaces.

## 4 Challenges and Limitations of Vision-Language-Action Models

VLA models face a spectrum of interrelated challenges that impede their translation from research prototypes to robust, real- world systems. First, achieving real- time, resource- aware inference remains difficult: models like DeeR- VLA leverage dynamic early- exit architectures to cut computation  $5 - 6x$  on manipulation benchmarks while preserving accuracy, yet their gains diminish in complex scenarios [202]. Similarly, UniNaVid compresses egocentric video tokens for  $5\mathrm{Hz}$  navigation but still struggles under highly ambiguous instructions and longer horizons [210]. Coupled with limited object generalization, even advanced hybrid vision- language grounding schemes (e.g., ObjectVLA) generalize to only  $64\%$  of novel objects, un

![](images/8e6541bc09b75e7ff69a696b00817dbfbd502a7fa708bd5887f65a9f25062f29.jpg)  
Figure 16: Showing how VLA models enable interactive AR navigation by fusing real-time visual perception, language understanding, and action planning. In dynamic environments such as airports, VLAs interpret user queries like "avoid stairs to Gate 22," analyze visual scenes (e.g., detecting escalators), and adjust navigational paths accordingly, supporting personalized, accessible, and context-aware mobility guidance.

derscoring persistent gaps in open- world robustness [223].

Second, adapting VLA models with minimal supervision and ensuring stable policy updates under scarce, noisy data is nontrivial. ConRFT combines behavior cloning and Qlearning with human- in- the- loop fine- tuning to rapidly converge to  $96.3\%$  success over eight contact- rich tasks, yet it relies heavily on expert interventions and reward shaping [31]. Hierarchical frameworks such as Hi Robot decouple high- level reasoning from low- level execution to improve instruction fidelity, but coordinating these modules and grounding ambiguous feedback remains challenging [155]. Likewise, TLA's fusion of tactile streams with language commands achieves over  $85\%$  success on unseen peg- in- hole tasks, but dataset breadth and real- time multi- step decoding still limit broader generalization [70].

Furthermore, ensuring safety, generalization, and end- to- end reliability in dynamic environments demands new modeling and evaluation standards. Occupancy- Language- Action models like OccLLaMA unify 3D scene understanding with action planning, yet they must scale to richer scene dynamics and semantic consistency across modalities [183]. RaceVLA pushes high- speed drone navigation via quantized, iterative control loops, but its visual- physical generalization trails larger VLAs and dedicated reasoners [153]. Model- merging strategies in ReVLA recover lost out- of- domain visual robustness—improving OOD grasp success by up to  $77\%$  but introduce extra computation and complexity [39]. Finally, SafeVLA formulates constraints via constrained Markov decision processes to cut unsafe behavior by over  $80\%$ , yet defining comprehensive, non- restrictive safety rules for diverse real- world tasks remains an open problem [205]. Addressing these intersecting limitations is critical for VLA models to achieve reliable, autonomous operation against the full complexity of real- world robotics.

Building upon the critical limitations outlined above, it is imperative to map each challenge to targeted mitigation strategies and forecast their system- level impact. Table 4 distills this mapping into three columns—identifying core limitations, proposing concrete technical remedies drawn from recent advances, and articulating the anticipated benefits for real- world VLA deployment. For instance, tackling real- time inference constraints leverages parallel decoding and quantized transformer pipelines with hardware acceleration (e.g., TensorRT) to sustain control loop rates in drones and manipulators [100, 94, 60, 110]. Addressing multimodal action representation via hybrid diffusion—autoregressive policies enriches a model's capacity to produce varied, context- sensitive motor commands for complex tasks [133, 121]. To guarantee safety in open worlds, dynamic risk assessment modules and adaptive planning layers can be integrated, ensuring robust emergency stop behaviors in unpredictable settings [143, 180, 87]. Similarly, dataset bias and grounding are countered through curated debiased corpora and advanced contrastive fine- tuning, bolstering fairness and semantic fidelity when generalizing to novel objects and scenes [145, 16, 136]. Together, these solution pathways—and others spanning simulation- to- real transfer, tactile integration, and energy- efficient architectures—frame a com prehensive roadmap for transitioning VLA research into reliable, scalable autonomy.

The remainder of this section is organized into five focused subsections, each examining a distinct cluster of VLA challenges identified in the literature. First, we analyze real- time inference constraints and the emerging methods that address them. Next, we delve into multimodal action representation alongside safety assurance in open- world settings. We then discuss dataset bias, grounding strategies, and generalization to unseen tasks, followed by an exploration of system integration complexity and computational demands. Finally, we consider robustness and the ethical implications of deploying VLAs in real- world applications.

### 4.1 Real-Time Inference Constraints

Real- time inference remains a significant limitation in deploying VLA models, particularly in latency- critical applications like robotic manipulation, autonomous driving, and drone control. VLAs typically depend on autoregressive decoding strategies, which sequentially generate action tokens based on previous predictions. While effective for many tasks, this method severely restricts inference speed, typically achieving only  $3 - 5\mathrm{Hz}$ . This rate falls dramatically short of the  $100\mathrm{Hz}$  or greater frequency required by robotic systems for precise and fluid real- time control. For instance, when a robotic arm manipulates delicate objects, frequent positional updates are essential to maintain accuracy and prevent damage. Models such as OpenVLA [94] and Pi- 0 [14] face inherent challenges with this sequential token generation approach, thereby limiting their effectiveness in dynamic environments.

Emerging solutions such as parallel decoding, exemplified by NVIDIA's Groot N1 model [13], aim to accelerate inference by predicting multiple tokens simultaneously. Groot N1 achieves approximately a  $2.52\times$  speedup over traditional decoding methods; however, this parallelism often introduces tradeoffs in trajectory smoothness, resulting in jerky or suboptimal robot movements. Such movements are undesirable in sensitive applications like surgical robotics, where precision and fluidity are paramount. Thus, achieving rapid inference without compromising output quality remains an open challenge.

Additionally, hardware limitations exacerbate real- time inference constraints. For example, processing high- dimensional visual embeddings, typically involving over 400 vision tokens at 512 dimensions each, requires approximately  $1.2\mathrm{GB / s}$  memory bandwidth. This demand significantly exceeds the capacity of current embedded systems or edge- AI hardware such as NVIDIA Jetson platforms, thereby restricting practical deployment. Even with efficient quantization techniques, which reduce the precision of floating- point operations to alleviate memory constraints, models frequently experience accuracy degradation, especially in tasks demanding sub- millimeter precision, such as bimanual robotic manipulation or medical robotics.

### 4.2 Multimodal Action Representation and Safety Assurance

Multimodal Action Representation: One significant limitation of current VLA models is accurately representing mul

timodal actions, particularly in scenarios requiring continuous and nuanced control [51, 38]. Traditional discrete tokenization methods, such as those dividing actions into 256 distinct bins, inherently lack precision, creating substantial errors in fine- grained tasks like delicate robotic grasping or intricate surgical procedures [133]. For instance, during precise robotic manipulation in assembly tasks, discrete representations can result in misaligned or imprecise actions, undermining performance and reliability. On the other hand, continuous multilayer perceptron (MLP) based approaches face the risk of mode collapse [126, 179], where models converge prematurely to single action trajectories, despite multiple viable paths available. This diminishes the flexibility necessary for adaptive decision- making in highly dynamic environments. Emerging diffusion- based policies, exemplified by models like Pi- Zero and RDT- 1B [112], offer richer multimodal action representation capable of capturing diverse action possibilities. However, their substantial computational overhead—approximately three times that of conventional transformer- based decoders—renders them impractical for real- time deployment. Consequently, VLA models currently struggle with complex dynamic tasks, such as robotic navigation in densely crowded spaces or sophisticated bimanual manipulations [59, 191], where multiple strategic actions may be equally valid and contextually dependent.

Safety Assurance in Open Worlds: Another critical challenge facing VLAs is ensuring robust safety in dynamic, unpredictable environments characteristic of real- world scenarios [33, 205]. Many current implementations depend heavily on predefined hardcoded force and torque thresholds, significantly constraining their adaptability in encountering unforeseen or novel conditions, such as unexpected obstacles or sudden environmental changes [121]. Models used for collision prediction typically attain only about  $82\%$  accuracy in cluttered and dynamic spaces, posing serious risks in applications such as warehouse logistics or domestic robotics, where safety margins are minimal [217, 94]. Moreover, the essential safety mechanisms like emergency stops incorporate substantial latency—often between 200 and 500 milliseconds—due to comprehensive safety verifications [132, 94]. This delay, although seemingly minor, can prove hazardous in high- speed operations or critical interventions, such as automated driving or emergency robotic responses.

### 4.3 Dataset Bias, Grounding, and Generalization to Unseen Tasks

A significant obstacle limiting the effectiveness of VLA models is the pervasive presence of dataset bias and grounding deficiencies. Current training datasets, predominantly sourced from web- crawled repositories, frequently exhibit inherent biases [165, 91]. Studies indicate that approximately  $17\%$  of associations within standard datasets are skewed toward stereotypical interpretations, such as disproportionately associating terms like "doctor" with male figures [171, 95]. These biases propagate through training, resulting in VLAs that produce semantically misaligned or contextually inappropriate responses when deployed in diverse environments. For instance, models such as OpenVLA have been documented to overlook ap proximately  $23\%$  of object references in novel settings, significantly impairing their practical utility in real- world applications where accurate interpretation of instructions is critical [94]. This grounding issue also extends to challenges in compositional generalization, where VLAs often falter when encountering rare or unconventional combinations, such as interpreting a phrase like "yellow horse" because of underrepresentation in training corpora. These shortcomings highlight an urgent need for carefully curated, balanced, and comprehensive datasets, coupled with advanced grounding algorithms designed to mitigate biases and enhance semantic alignment across varied contexts.

Complementing the challenges posed by dataset bias is the broader issue of generalization to unseen tasks, a critical barrier for the practical deployment of VLAs. While existing models demonstrate proficiency in familiar environments or tasks similar to their training scenarios, their performance significantly degrades—often by as much as  $40\%$ —when encountering entirely novel tasks or unfamiliar variations. For example, a VLA trained specifically on domestic tasks may struggle or outright fail when introduced into industrial or agricultural settings, largely due to discrepancies in object types, environmental dynamics, and operational constraints. This limitation arises primarily from overfitting to narrowly scoped training distributions and insufficient exposure to diverse task representations. Consequently, current VLAs exhibit limited proficiency in zero- shot or few- shot learning scenarios, impeding their adaptability and scalability.

### 4.4 System Integration Complexity and Computational Demands

Integrating VLA models within dual- system architectures, which combine high- level cognitive planning (System 2) and real- time physical control (System 1), presents significant complexity in robotic applications. A primary challenge arises from temporal mismatches between these two systems. Typically, System 2 leverages large language models (LLMs) such as GPT or Llama- 2 for complex task decomposition and strategic planning. These models, due to their substantial computational requirements, often exhibit processing times of approximately  $800~\mathrm{ms}$  or more per inference cycle. Conversely, System 1 components, tasked with executing rapid, low- level motor actions through control loops, operate on millisecond timescales—often around  $10~\mathrm{ms}$  intervals. This stark discrepancy in operational cadence leads to synchronization difficulties, causing delays and potentially suboptimal execution trajectories. For example, NVIDIA's Groot N1 model demonstrates an effective integration of these two systems but still suffers from occasional jerkiness in motion due to asynchronous interaction, highlighting this intrinsic challenge.

Furthermore, the feature space misalignment between high- dimensional vision encoders, such as Vision Transformers (ViT), and lower- dimensional action decoders exacerbates integration complexity. When attempting to reconcile these disparate embeddings, the coherence between perceptual understanding and actionable commands can deteriorate significantly. OpenVLA [94] and RoboMamba [111], which utilize

transformer- based visual processing and subsequent action decoding, illustrate these integration challenges- resulting in diminished performance when ported from simulation environments to physical hardware deployments. Such discrepancies manifest as high as a  $32\%$  reduction in performance, primarily due to mismatches between simulated dynamics and real- world sensor noise or calibration issues.

Energy and compute demands constitute another significant barrier for VLA deployment, particularly in edge computing contexts typical of autonomous drones, mobile robots, and wearable robotic systems. The substantial parameter counts typical of advanced VLAs- for instance, models possessing upwards of 7 billion parameters- necessitate computational resources often exceeding 28 GB of VRAM in their native form. These requirements vastly outpace the capabilities of most current edge- oriented processors and GPUs, restricting the practical applicability of sophisticated VLAs outside specialized, high- resource environments.

### 4.5 Robustness and Ethical Challenges in VLA Deployment

4.5. Robustness and Ethical Challenges in VLA DeploymentThe practical deployment of VLA models faces substantial challenges regarding robustness to environmental variability and ethical considerations. Environmental robustness refers to the VLA's capacity to maintain stable and accurate performance across dynamically changing conditions. Real- world environments frequently introduce unpredictable variations such as fluctuating lighting, weather conditions, or partial occlusions. For instance, vision modules within VLAs, such as those employed by OpenDriveVLA [220], exhibit accuracy reductions of approximately  $20 - 30\%$  under low- contrast or shadow- heavy scenarios due to inadequate processing capabilities of current visual encoders. Similarly, linguistic comprehension in VLAs like CoVLA [5] is adversely affected in acoustically noisy or ambiguous contexts, where instructions can become difficult to interpret accurately, leading to task execution errors. Additionally, robotic manipulation tasks using VLA- equipped systems such as RoboMamba [111] frequently struggle with cluttered environments, misjudging positions or orientations of partially occluded objects, thereby compromising task success.

## 5 Discussion

As illustrated in Figure 17, VLA models face a multifaceted set of challenges that span algorithmic, computational, and ethical dimensions. First, achieving real- time inference on resource- constrained hardware remains difficult due to the sequential nature of autoregressive decoders and the high dimensionality of multimodal inputs. Second, fusing vision, language, and action into coherent policies introduces safety vulnerabilities when encountering unanticipated environmental changes. Third, dataset bias and grounding errors compromise generalization, often causing models to fail on out- of- distribution tasks. Fourth, integrating diverse components- perception, reasoning, control- yields complex architectures that are hard to optimize and maintain. Fifth, the energy and compute demands of large VLA systems hinder de ployment on embedded or mobile platforms. Finally, robustness to environmental variability and ethical considerations, such as privacy and bias mitigation, add layers of societal and regulatory concern. Collectively, these limitations constrain the practical adoption of VLA models in real- world robotics, autonomous systems, and interactive applications. The potential solutions to these challenges are discussed in the below points.

### 5.1 Potential Solutions

- Real-Time Inference Constraints. Future research must develop VLA architectures that harmonize latency, throughput, and task-specific accuracy. One promising direction is the integration of specialized hardware accelerators—such as FPGA-based vision processors and tensor cores optimized for sparse matrix operations—to execute convolutional and transformer layers at sub-millisecond scales [94, 100]. Model compression techniques like Low-Rank Adaptation (LoRA) [72] and knowledge distillation can shrink parameter counts by up to  $90\%$ , reducing both memory footprint and inference time while retaining over  $95\%$  of original performance on benchmark tasks. Progressive quantization strategies that combine mixed-precision arithmetic (e.g., FP16/INT8) with blockwise calibration can further cut computation by \(2 
- 4 \times\) with minimal accuracy loss [93]. Adaptive inference architectures that dynamically adjust network depth or width based on input complexity—akin to early-exit branches in DeeR-VLA [202]—can reduce average compute by selectively bypassing transformer layers when visual scenes or linguistic commands are simple. Finally, efficient tokenization schemes leveraging subword patch embeddings and dynamic vocabulary allocation can compress visual and linguistic input into compact representations, minimizing token counts without sacrificing semantic richness [133]. Together, these innovations can enable sub-  $50 \mathrm{ms}$  end-to-end inference on commodity edge GPUs, paving the way for latency-sensitive applications in autonomous drone flight, real-time teleoperation, and collaborative manufacturing.

- Multimodal Action Representation and Safety Assurance. Addressing multimodal action representation and robust safety requires end-to-end frameworks that unify perception, reasoning, and control under stringent safety constraints. Hybrid policy architectures combining diffusion-based sampling for low-level motion primitives [34] with autoregressive high-level planners [186] enable compact stochastic representations of diverse action trajectories, improving adaptability in dynamic environments. Safety can be enforced via real-time risk assessment modules that ingest multi-sensor fusion streams—visual, depth, and proprioceptive data—to predict collision probability and joint stress thresholds, triggering emergency stop circuits when predefined safety envelopes are breached [143, 180]. Reinforcement learning algorithms augmented with constrained optimization (e.g., Lagrangian methods in SafeVLA [205]) can learn

![](images/7304a7acf7d060d981491f36b59d235c8533ef82f88afbecdf1746d647598603.jpg)  
Figure 17:  ch  t  t  t t  t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t a t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t

policies that maximize task success while strictly respecting safety constraints. Online model adaptation techniques—such as rule- based RL (GRPO) and Direct Preference Optimization (DPO)—further refine action selection under new environmental conditions, ensuring consistent safety performance across scenarios [87]. Crucially, embedding formal verification layers that symbolically analyze planner outputs before execution can guarantee compliance with safety invariants, even for neural- network- based controllers. Integrating these methodologies will produce VLA systems that not only execute complex, multimodal actions but do so with provable safety in unstructured, real- world settings.

- Dataset Bias, Grounding, and Generalization to Unseen Tasks. Robust generalization demands both broadened data diversity and advanced learning paradigms. Curating large-scale, debiased multimodal datasets—combining web-scale image-text corpora like LAION-5B [152] with robot-centric trajectory archives such as Open X-Embodiment [175]—lays the groundwork for equitable semantic grounding. Hard-negative sampling and contrastive fine-tuning of vision-language backbones (e.g., CLIP variants) can mitigate spurious correlations and enhance semantic fidelity [16, 212]. Meta-learning frameworks enable rapid adaptation to novel tasks by learning shared priors across task families, as demonstrated in vision- language robotic navigation models [136]. Continual learning algorithms—with replay buffers and regularization strategies—preserve old knowledge while integrating new concepts, addressing catastrophic forgetting in VLA models [39]. Transfer learning from 3D perception domains (e.g., point cloud reasoning in 3D- VLA [217]) can imbue models with spatial inductive biases, improving out- of- distribution robustness. Finally, simulation- to- real (sim2real) fine- tuning with domain randomization and real- world calibration—such as dynamic lighting, texture, and physics variations—ensures that policies learned in synthetic environments transfer effectively to physical robots [4, 53]. These combined strategies will empower VLAs to generalize confidently to unseen objects, scenes, and tasks in real- world deployments.

- System Integration Complexity and Computational Demands. To manage the intricate orchestration of multimodal pipelines under tight compute budgets, researchers must embrace model modularization and hardware-software co-design. Low-Rank Adaptation (LoRA) adapters can be injected into pre-trained transformer layers, enabling task-specific fine-tuning without modifying core weights [72]. Knowledge distillation from

large "teacher" VLAs into lightweight "student" networks—using student- teacher mutual information objectives—yields compact models with  $5 - 10\times$  fewer parameters while retaining  $90 - 95\%$  task performance [93]. Mixed- precision quantization augmented by quantization- aware training can compress weights to 4- 8 bits, cutting memory bandwidth and energy consumption by over  $60\%$  [94]. Hardware accelerators tailored for VLA workloads—supporting sparse tensor operations, dynamic token routing, and fused vision- language kernels—can deliver sustained  $100+$  TOPS throughput within a 20- 30 W power envelope, meeting the demands of embedded robotic platforms [133, 186]. Toolchains like TensorRT- LLM [100] and TVM can optimize end- to- end VLA graphs for specific edge devices, fusing layers and precomputing static subgraphs. Emerging architectures such as TinyVLA demonstrate that sub- 1 B parameter VLAs can achieve near- state- of- the- art performance on manipulation benchmarks with real- time inference, charting a path for widespread deployment in resource- constrained settings.

- Robustness and Ethical Challenges in VLA Deployment. Ensuring VLA robustness and ethical integrity requires both technical and governance measures. Domain randomization and synthetic augmentation pipelines—like UniSim's closed-loop sensor simulator—generate photorealistic variations in lighting, occlusion, and sensor noise, enhancing model resilience to environmental shifts [200]. Adaptive recalibration modules, which adjust perception thresholds and control gains based on real-time feedback, further mitigate drift and sensor degradation over prolonged operation. On the ethical front, bias auditing tools must scan training datasets for skewed demographic or semantic distributions, followed by corrective fine-tuning using adversarial debiasing and counterfactual augmentation [145, 212]. Privacy-preserving inferencing—via on-device processing, homomorphic encryption for sensitive data streams, and differential privacy during training—safeguards user data in applications like healthcare and smart homes [124, 140]. Socioeconomic impacts can be managed through transparent impact assessments and stakeholder engagement, ensuring that VLA adoption complements human labor through upskilling programs rather than displacing workers en masse. Finally, establishing regulatory frameworks and industry standards for VLA safety and accountability will underpin responsible innovation, balancing technical capabilities with societal values.

### 5.2 Future Roadmap

The future of VLA models lies at the intersection of increasingly powerful multimodal foundations, agentic reasoning, and embodied continual learning. Over the next decade, we anticipate several converging trends that will propel VLAs from capable but narrow task specialists toward the core of truly generalist robotic intelligence.

1. Multimodal Foundation Models as the "Cortex." Today's VLAs typically couple a vision-language backbone with task-specific policy heads. Tomorrow, we expect a single, massive multimodal foundation model—trained on web-scale image, video, text, and affordance data—to serve as a shared perceptual and conceptual "cortex." This foundation will encode not only static scenes but also dynamics, physics, and common-sense world knowledge, enabling downstream action learners to tap into a unified representational substrate rather than reinventing basic perceptual skills for every robot or domain.

2. Agentic, Self-Supervised Lifelong Learning. Rather than static pretraining, future VLAs will engage in continual, self-supervised interaction with their environments. Agentic frameworks—where the model generates its own exploration objectives, hypothesizes outcomes, and self-corrects via simulated or real rollouts—will drive rapid skill acquisition. By formulating internal sub-goals ("learn to open drawers," "map furniture affordances") and integrating reinforcement-style feedback, a VLA-driven humanoid could autonomously expand its capabilities over years of deployment, much like a human apprentice.

3. Hierarchical, Neuro-Symbolic Planning. To scale from low-level motor primitives to high-level reasoning, VLAs will adopt hierarchical control architectures. A top-level language-grounded planner (perhaps an LLM variant finetuned for affordance reasoning) will decompose complex instructions ("prepare a cup of tea") into sequences of sub-tasks ("fetch kettle," "fill water," "heat water," "steep tea bag"). Mid-level modules will translate these into parameterized motion plans, and low-level diffusion or transformer-based controllers will generate smooth, compliant trajectories in real time. This neuro-symbolic blend ensures both the interpretability of structured plans and the flexibility of learned policies.

4. Real-Time Adaptation via World Models. Robustness in unstructured settings demands that VLAs maintain an internal, predictive world model—an up-to-date simulation of objects, contacts, and agent dynamics. As the robot acts, it will continuously reconcile its predictions with sensor feedback, using model-based corrective actions when discrepancies arise (e.g., slipping grasp). Advances in differentiable physics and video-to-state encoders will make these world models both accurate and efficient enough for on-board, real-time use. Cross-Embodiment and Transfer Learning: The era of training separate VLAs for each robot morphology will give way to embodiment-agnostic policies. By encoding actions in an abstract, kinematic-agnostic space (e.g., "apply grasp force at these affordance points"), future VLAs will transfer skills seamlessly between wheeled platforms, quadrupeds, and humanoids. Combined with meta-learning, a new robot can bootstrap prior skills with only a few minutes of calibration data. Safety, Ethics, and Human-Centered Alignment As VLAs gain autonomy, built-in safety and value alignment become non-negotiable. Future systems will integrate real-time risk estimators—assessing potential harm

to humans or property before executing high- risk maneuvers—and seek natural language consent for ambiguous situations. Regulatory constraints and socially aware policies will be baked into the VLA stack, ensuring that robots defer to human preferences and legal norms.

![](images/a2f9529a684adb4c4df71d08b5e36e8667d10342792cd41d51e03d580834b1b2.jpg)  
Figure 18: This conceptual illustration presents "Eva," a future humanoid assistant powered by Vision-Language Models (VLMs), VLA frameworks, and agentic AI systems. VLMs enable semantic scene understanding and object affordance prediction, while VLAs translate language-grounded instructions into hierarchical motor plans. Agentic AI modules ensure adaptive learning, self-refinement, and interactive decision-making in open-ended environments. Together, these components represent a foundational blueprint for Artificial General Intelligence (AGI) in robotics, where perception, language understanding, planning, and safe autonomous behavior converge in real-world, socially aware tasks.

As illustrated in Figure 18, the future of VLA- based robotics lies in the integration of three foundational components: Vision- Language Models (VLMs), VLA architectures, and agentic AI systems. Consider "Eva," a generalist humanoid assistant operating in a household. At the perception layer, Eva's foundation VLM interprets multimodal inputs by segmenting visual scenes into discrete object- level representations, predicting affordances (e.g., graspable, fragile), and simulating dynamic behaviors through an internal world model. This VLM layer enables high- level visual understanding grounded in language semantics and physical properties. Upon receiving a user command such as "Eva, clean the coffee spill and water the plants," the VLA module activates. This core architecture combines tokenized language inputs and sensory feedback to perform hierarchical task planning. A high- level planner decomposes the instruction into actionable subtasks (e.g., locate cloth, wipe spill, retrieve watering can), which are then converted into motion trajectories via a mid- level policy module. These plans are passed to a low- level diffusion- policy controller, responsible for generating smooth, physics- aware joint movements tailored to the robot's embodiment. Complementing these is Eva's agentic AI module, which supports continual learning and adaptation. When confronted with unexpected challenges—like a sticky stain—Eva invokes an internal self- improvement loop, running real- time simulated variations to refine its wiping strategy without human supervision. Safety and alignment are ensured through human- aware policies: proximity sensors, real- time monitoring, and verbal confirmations before high- risk actions. Overnight, Eva performs autonomous review of performance logs, refining sub- policies via simulated rollouts. Together, this VLM- VLA- agentic triad marks a significant leap toward embodied AGI. It enables robots like Eva to perceive, plan, act, adapt, and safely coexist with humans, ultimately transforming how intelligent systems interact with real- world environments in robust, interpretable, and human- aligned ways.

ity sensors, real- time monitoring, and verbal confirmations before high- risk actions. Overnight, Eva performs autonomous review of performance logs, refining sub- policies via simulated rollouts. Together, this VLM- VLA- agentic triad marks a significant leap toward embodied AGI. It enables robots like Eva to perceive, plan, act, adapt, and safely coexist with humans, ultimately transforming how intelligent systems interact with real- world environments in robust, interpretable, and humanaligned ways.

## 6 Conclusion

In this comprehensive review, we systematically evaluated the recent developments, methodologies, and applications of VLA models published over the last three years. Our analysis began with the foundational concepts of VLAs, defining their role as multimodal systems that unify visual perception, natural language understanding, and action generation in physical or simulated environments. We traced their evolution and time- line, detailing key milestones that marked the transition from isolated perception- action modules to fully unified, instruction- following robotic agents. We highlighted how multimodal integration has matured—from loosely coupled pipelines to transformer- based architectures that enable seamless coordination between modalities.

Next, we examined tokenization and representation techniques, focusing on how VLAs encode visual and linguistic information, including action primitives and spatial semantics. We explored learning paradigms, detailing the datasets and training strategies—from supervised learning and imitation learning to reinforcement learning and multimodal pretraining—that have shaped VLA performance. In our section on adaptive control and real- time execution, we addressed how modern VLAs are optimized for dynamic environments, discussing policies that support latency- sensitive tasks. We then categorized major architectural innovations, surveying over 50 recent VLA models. This included advancements in model design, memory systems, and interaction fidelity. We further studied training and efficiency strategies, including parameter- efficient methods like LoRA, quantization, and model pruning, alongside acceleration techniques such as parallel decoding and hardware- aware inference. Our analysis continued with real- world applications of VLA models, showcasing their deployment across six domains: humanoid robotics, autonomous vehicles, industrial automation, healthcare, agriculture, and augmented reality (AR) navigation. Each application was reviewed with examples of model performance, domain- specific challenges, and generalizability.

In addressing challenges and limitations, we focused on five core areas: real- time inference, multimodal action representation and safety, bias and generalization, system integration and compute constraints, and ethical deployment. We proposed potential solutions drawn from current literature, including model compression, cross- modal grounding, domain adaptation, and agentic learning frameworks. Finally, our discussion and future roadmap articulated how the convergence of VLMs, VLA architectures, and agentic AI systems is steering robotics toward artificial general intelligence (AGI). This review provides a unified understanding of VLA advancements, identifies unresolved challenges, and outlines a structured path forward for developing intelligent, embodied, and human- aligned agents.



## References

[1] AchiamJ.AdlerS.AgarwalS.AhmadL.AkkayaI.AlemanF.L AlmeidaD.AltenschmidtJ.AltmanS.AnadkatS.et al.2023. Gpt- 4 technical report. arXiv preprint arXiv:2303.08774. [2] Agarwal, L.VermaB.2024. From methods to datasets:A survey on image- caption generators. Multimedia Tools and Applications 83, 28077- 28123. [3] AlayracJ.B.DonahueJ.LucP.MiechA.BarrI.HassonY.Lenc K.MenschA.MillicanK.ReynoldsM.et al.2022. Flamingo:a visual language model for few- shot learning. Advances in neural information processing systems 35,23716- 23716. [4] AndersonP. ShrivastavaA. TruongJ.MajumdarA.ParikhD. BarraD.LeeS.2021. Sim- to- real transfer for vision- and- language navigation, in: Conference on Robot Learning, PMLR. pp. 671- 681. [5] AraiH.MiwaK.SasakiK.WatanabeK.YamaguchiY.Aoki, S., Yamamoto, I., 2025. Covla: Comprehensive vision- language- action dataset for autonomous driving, in: 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), IEEE. pp. 1933- 1943. [6] AsifS.BuenoM.FerreiraP.AnandanP.ZhangZ.YaoY.RagunathanG.TinklerL.Sotoodeh- BahrainiM.LohseN.et al. 2025. Rapid and automated configuration of robot manufacturing cells, Robotics and Computer- Integrated Manufacturing 92, 102862. [7] AssresG.BhandariG.ShalaginovA.GronliT.M.GhineaG. 2025. State- of- the- art and challenges of engineering ml- enabled software systems in the deep learning era. ACM Computing Surveys. [8] AsuzuK.SinghH.IdrissiM.2025. Human- robot interaction through joint robot planning with large language models. Intelligent Service Robotics, 1- 17. [9] AyazM.KhanM.SaqibM.KhelifiA.SajjadM.ElsaddikA. 2024. Medvlm: Medical vision- language model for consumer devices. IEEE Consumer Electronics Magazine . [10] BartoccioniF.RamziE.BesnierV.VankataramananS.VuT.H. XuY.ChambonL.GidarisS.OdabasS.HurychD.et al.2025. Vavim and vavam: Autonomous driving through video generative modeling. arXiv preprint arXiv:2502.15672 .

[11] Bathula, N.V., Paleti, I., Pagidi, S., Akkumahanthi, S.S., Guduru, N.T., 2024. Policy learning- based image captioning with vision transformer, in: 2024 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS), IEEE. pp. 1- 6. [12] Belkhale, S., Ding, T., Xiao, T., Sermanet, P., Vuong, Q., Tompson, J., Chebotar, Y., Dwiberdi, D., Sadigh, D., 2024. Rt- h: Action hierarchies using language. arXiv preprint arXiv:2403.01823. [13] Bjorck, J., Castañeda, F., Cherniadev, N., Da, X., Ding, R., Fan, L., Fang, Y., Fox, D., Hu, F., Huang, S., et al., 2025. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734 . [14] Black, K., Brown, N., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., Groom, L., Hauman, K., Ichter, B., et al., 2024. Pi- 0: A vision- language- action flow model for general robot control. arXiv preprint arXiv:2410.24164 . [15] Bolya, D., Huang, P.Y., Sun, P., Cho, J.H., Madotto, A., Wei, C., Ma, T., Zhi, J., Rajasegaran, J., Rasheed, H., et al., 2025. Perception encoder: The best visual embeddings are not at the output of the network. arXiv preprint arXiv:2504.13181. [16] Bordes, F., Pang, R.Y., Ajay, A., Li, A.C., Bardes, A., Petryk, S., Mañas, O., Lin, Z., Mahmoud, A., Jayaraman, B., et al., 2024. An introduction to vision- language modeling. arXiv preprint arXiv:2405.17247. [17] Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., Ding, T., Driess, D., Dubey, A., Finn, C., et al., 2023. Rt- 2: Vision- language- action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818. [18] Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et al., 2022. Rt1: Robotics transformer for real- world control at scale. arXiv preprint arXiv:2212.06817. [19] Budzianowski, P., Maa, W., Freed, M., Mo, J., Xie, A., Tipnis, V., Bolte, B., 2024. Edgevla: Efficient vision- language- action models. environments 20, 3. [20] Cangelosi, A., Metta, G., Sagerer, G., Nolfi, S., Nehaniv, C., Fischer, K., Tani, J., Belpaeme, T., Sandini, G., Nori, F., et al., 2010. Integration of action and language knowledge: A roadmap for developmental robots. IEEE Transactions on Autonomous Mental Development 2, 167- 195. [21] Cao, J., Gan, Z., Cheng, Y., Yu, L., Chen, Y.C., Liu, J., 2020. Behind the scene: Revealing the secrets of pre- trained vision- and- language models, in: Computer Vision- ECCV 2020: 16th European Conference, Glasgow, UK, August 23- 28, 2020, Proceedings, Part VI 16, Springer. pp. 565- 580. [22] Cao, L., 2024. Ai robots and humanoid ai: Review, perspectives and directions. arXiv preprint arXiv:2405.15775 . [23] Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., et al., 2024. A survey on evaluation of large language models. ACM transactions on intelligent systems and technology 15, 1- 45. [24] Chatzopoulos, D., Bermejo, C., Huang, Z., Hui, P., 2017. Mobile augmented reality survey: From where we are to where we go. Ieee Access 5, 6917- 6950. [25] Chen, B., Xu, Z., Kermani, S., Ichter, B., Sadigh, D., Guibas, L., Xia, F., 2024a. Spatialvlm: Following vision- language models with spatial reasoning capabilities, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14455- 14465. [26] Chen, H., Hou, L., Wu, S., Zhang, G., Zou, Y., Moon, S., Bhuiyan, M., 2024b. Augmented reality, deep learning and vision- language query system for construction worker safety. Automation in Construction 157, 105158. [27] Chen, H., Li, S., Fan, J., Duan, A., Yang, C., Navarro- Alarcon, D., Zheng, P., 2025a. Human- in- the- loop robot learning for smart manufacturing: A human- centric perspective. IEEE Transactions on Automation Science and Engineering. [28] Chen, H., Liu, B., Wang, S., Wang, X., Han, W., Zhu, Y., Wang, X., Bi, Y., 2025b. Language modulates vision: Evidence from neural networks and human brain- lesion models. arXiv preprint arXiv:2501.13628 . [29] Chen, P., Bu, P., Wang, Y., Wang, X., Wang, Z., Guo, J., Zhao, Y., Zhu, Q., Song, J., Yang, S., et al., 2025c. Combatvl: An efficient vision- language- action model for combat tasks in 3d action role- playing games. arXiv preprint arXiv:2503.09527 . [30] Chen, X., Xu, W., Kan, S., Zhang, L., Jin, Y., Cen, Y., Li, Y., 2025d.

Vision- semantics- label: A new two- step paradigm for action recognition with large language model. IEEE Transactions on Circuits and Systems for Video Technology. [31] Chen, Y., Tian, S., Liu, S., Zhou, Y., Li, H., Zhao, D., 2025e. Conrft: A reinforced fine- tuning method for vla models via consistency policy. arXiv preprint arXiv:2502.05450. [32] Cheng, A.C., Ji, Y., Yang, Z., Gongye, Z., Zou, X., Kautz, J., Byyk, E., Yin, H., Liu, S., Wang, X., 2024a. Navial: Legged robot vision- language- action model for navigation. arXiv preprint arXiv:2412.04453 [33] Cheng, H., Xiao, E., Yu, C., Yao, Z., Cao, J., Zhang, Q., Wang, J., Sun, M., Xu, K., Gu, J., et al., 2024b. Manipulation facing threats: Evaluating physical vulnerabilities in end- to- end vision language action models. arXiv preprint arXiv:2409.13174 [34] Chi, C., Xu, Z., Feng, S., Cousineau, E., Du, Y., Burchfiel, B., Tedrake, R., Song, S., 2023. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, 02783649241273668. [35] Chiang, H.T.L., Xu, Z., Fu, Z., Jacob, M.G., Zhang, T., Lee, T.W.E., Yu, W., Schenck, C., Rendleman, D., Shah, D., et al., 2024. Mobility vla: Multimodal instruction navigation with long- context vlms and topological graphs. arXiv preprint arXiv:2407.07775 [36] Dang, R., Yuan, Y., Zhang, W., Xin, Y., Zhang, B., Li, L., Wang, L., Zeng, Q., Li, X., Bing, L., 2025. Ecbench: Can multi- modal foundation models understand the egocentric world? a holistic embodied cognition benchmark. arXiv preprint arXiv:2501.05031 [37] Dasari, S., Ebert, F., Tian, S., Nair, S., Bucher, B., Schmeckpeper, K., Singh, S., Levine, S., Finn, C., 2019. Robonet: Large- scale multi- robot learning. arXiv preprint arXiv:1910.112151. [38] Deng, S., Yan, M., Wei, S., Ma, H., Yang, Y., Chen, J., Zhang, Z., Yang, T., Zhang, X., Cui, H., Zhang, Z., Wang, H., 2025. Graspvla: a grasping foundation model pre- trained on billion- scale synthetic action data. URL: https://arxiv.org/abs/2505.03233, arXiv:2505.03233. [39] Dey, S., Zaech, J.N., Nikolov, N., Van Gool, L., Paudel, D.P., 2024. Revla: Reverting visual domain limitation of robotic foundation models. arXiv preprint arXiv:2409.13230 [40] Ding, D., Yao, T., Luo, R., Sun, X., 2025a. Visual question answering in robotic surgery: A comprehensive review. IEEE Access. [41] Ding, J., Zhang, Y., Shang, Y., Zhang, Y., Zong, Z., Feng, J., Yuan, Y., Su, H., Li, N., Sukiennik, N., et al., 2024a. Understanding world or predicting future? a comprehensive survey of world models. arXiv preprint arXiv:2411.14499 [42] Ding, P., Ma, J., Tong, X., Zou, B., Luo, X., Fan, Y., Wang, T., Lu, H., Mo, P., Liu, J., et al., 2025b. Humanoid- vla: Towards universal humanoid control with visual integration. arXiv preprint arXiv:2502.14795 [43] Ding, P., Zhao, H., Zhang, W., Song, W., Zhang, M., Huang, S., Yang, N., Wang, D., 2024b. Quar- vla: Vision- language- action model for quadruped robots, in: European Conference on Computer Vision, Springer. pp. 352- 367. [44] Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T., 2015. Long- term recurrent convolutional networks for visual recognition and description, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2625- 2634. [45] Dong, H., Liu, M., Zhou, K., Chatzi, E., Kannala, J., Stachniss, C., Fink, O., 2025. Advances in multimodal adaptation and generalization: From traditional approaches to foundation models. arXiv preprint arXiv:2501.18592 [46] Doveh, S., Arbelle, A., Harary, S., Schwartz, E., Herzig, R., Giryes, R., Feris, R., Panda, R., Ullman, S., Karlinsky, L., 2023. Teaching structured vision & language concepts to vision & language models, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2657- 2668. [47] Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., et al., 2023. Palm- e: An embodied multimodal language model. Openreview. [48] Duan, J., Pumacay, W., Kumar, N., Wang, Y.R., Tian, S., Yuan, W., Krishna, R., Fox, D., Mandlekar, A., Guo, Y., 2024. Aha: A vision- language- model for detecting and reasoning over failures in robotic manipulation. arXiv preprint arXiv:2410.00371.

[49] Duarte, N.F., Rakovek, M., Tasevski, J., Coco, M.I., Billard, A., Santos- Victor, J., 2018. Action anticipation: Reading the intentions of humans and robots. IEEE Robotics and Automation Letters 3, 4132- 4139. [50] Ebert, F., Yang, Y., Schmeckpeper, K., Bucher, B., Georgakis, G., Danilidis, K., Finn, C., Levine, S., 2021. Bridge data: Boosting generalization of robotic skills with cross- domain datasets. arXiv preprint arXiv:2109.13396. [51] Fan, C., Jia, X., Sun, Y., Wang, Y., Wei, J., Gong, Z., Zhao, X., Tomizuka, M., Yang, X., Yan, J., Ding, M., 2025. Interleave- vla: Enhancing robot manipulation with interleaved image- text instructions. URL: https://arxiv.org/abs/2505.02152, arXiv:2505.02152. [52] Fan, L., Chen, K., Xu, Z., Yuan, M., Huang, P., Huang, W., 2024. Language reasoning in vision- language- action model for robotic grasping, in: 2024 China Automation Congress (CAC), IEEE. pp. 6656- 6661. [53] Fang, Y., Yang, Y., Zhu, X., Zheng, K., Bertasius, G., Szafr, D., Ding, M., 2025. Rebot: Scaling robot learning with real- to- sim- to- real robotic video synthesis. arXiv preprint arXiv:2503.14526. [54] Firoozi, R., Tucker, J., Tian, S., Majumdar, A., Sun, J., Liu, W., Zhu, Y., Song, S., Kapoor, A., Hausman, K., et al., 2023. Foundation models in robotics: Applications, challenges, and the future. The International Journal of Robotics Research, 02783649241281508. [55] Foster, D.J., Block, A., Misra, D., 2024. Is behavior cloning all you need? understanding horizon in imitation learning. arXiv preprint arXiv:2407.15007. [56] Fu, H., Zhang, D., Zhao, Z., Cui, J., Liang, D., Zhang, C., Zhang, D., Xie, H., Wang, B., Bai, X., 2025. Orion: A holistic end- to- end autonomous driving framework by vision- language instructed action generation. arXiv preprint arXiv:2503.19755. [57] Gao, B., Liu, Y., Li, Y., Li, H., Li, M., He, W., 2025a. A vision- language model for predicting potential distribution land of soybean double cropping. Frontiers in Environmental Science 12, 1515752. [58] Gao, J., Belkhale, S., Dasari, S., Balakrishna, A., Shah, D., Sadigh, D., 2025b. A taxonomy for evaluating generalist robot policies. arXiv preprint arXiv:2503.01238. [59] Gbagbe, K.F., Cabre, M.A., Alabbas, A., Alyunes, O., Lykov, A., Tsetserakou, D., 2024. P- via: Vision- language- action model- based system for bimanual robotic dexterous manipulations, in: 2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC), IEEE. pp. 2864- 2869. [60] Geens, R., 2024. Bringing generative ai to edge devices through interoperable compute cores, in: Flanders AI Research Day, Location: Ghent. [61] Ghosh, A., Acharya, A., Saha, S., Jain, V., Chadha, A., 2024. Exploring the frontier of vision- language models: A survey of current methodologies and future directions. arXiv preprint arXiv:2404.07214. [62] Gu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., Liu, T., Wang, X., Wang, G., Cai, J., et al., 2018. Recent advances in convolutional neural networks. Pattern recognition 77, 354- 377. [63] Gu, X., Wen, C., Ye, W., Song, J., Gao, Y., 2023. Seer: Language instructed video prediction with latent diffusion models. arXiv preprint arXiv:2303.14897. [64] Gu, Z., Li, J., Shen, W., Yu, W., Xie, Z., McCrory, S., Cheng, X., Shamsah, A., Griffin, R., Liu, C.K., et al., 2025. Humanoid locomotion and manipulation: Current progress and challenges in control, planning, and learning. arXiv preprint arXiv:2501.02116. [65] Guo, Y., Zhang, J., Chen, X., Ji, X., Wang, Y.J., Hu, Y., Chen, J., 2025. Improving vision- language- action model with online reinforcement learning. arXiv preprint arXiv:2501.16664. [66] Guruprasad, P., Sikla, H., Song, J., Wang, Y., Liang, P.P., 2024. Benchmarking vision, language, & action models on robotic learning tasks. arXiv preprint arXiv:2411.05821. [67] Haldar, S., Peng, Z., Pinto, L., 2024. Baku: An efficient transformer for multi- task policy learning. arXiv preprint arXiv:2406.07539. [68] Han, S., Wang, M., Zhang, J., Li, D., Duan, J., 2024. A review of large language models: Fundamental architectures, key technological evolutions, interdisciplinary technologies integration, optimization and compression techniques, applications, and challenges. Electronics 13, 5040. [69] Hanson, A., Riseman, E., 2014. The visions image- understanding system, in: Advances in Computer Vision. Psychology Press, pp. 1- 114. [70] Hao, P., Zhang, C., Li, D., Cao, X., Hao, X., Cui, S., Wang, S., 2025. Tla: Tactile- language- action model for contact- rich manipulation. arXiv

preprint arXiv:2503.08548. [71] Hong, Y., 2025. Building 3D Foundation Models for the Embodied Minds. Ph.D. thesis, University of California, Los Angeles. [72] Hu, E.J., Shen, Y., Wallis, P., Allen- Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al., 2022. Lora: Low- rank adaptation of large language models. ICLR 1, 3. [73] Hu, Y., Tang, J., Gong, X., Zhou, Z., Zhang, S., Elvitigala, D.S., Mueller, F., Hu, W., Quigley, A.J., 2025. Vision- based multimodal interfaces: A survey and taxonomy for enhanced context- aware system design. arXiv preprint arXiv:2501.13443. [74] Huang, H., Liu, F., Fu, L., Wu, T., Mukadam, M., Malik, J., Goldberg, K., Abbeel, P., 2024. Early fusion helps vision language action models generalize better, in: 1st Workshop on X- Embodiment Robot Learning. [75] Huang, H., Liu, F., Fu, L., Wu, T., Mukadam, M., Malik, J., Goldberg, K., Abbeel, P., 2025a. Otter: A vision- language- action model with text- aware visual feature extraction. arXiv preprint arXiv:2503.03734. [76] Huang, S., Dong, L., Wang, W., Hao, Y., Shghal, S., Ma, S., Lv, T., Cui, L., Mohammed, O.K., Patra, B., et al., 2023a. Language is not all you need: Aligning perception with language models. Advances in Neural Information Processing Systems 36, 72096- 72109. [77] Huang, W., Gu, Q., Ye, N., 2025b. Decision spikeformer: Spike- driven transformer for decision making. arXiv preprint arXiv:2504.03800. [78] Huang, W., Wang, C., Zhang, R., Li, Y., Wu, J., Fei- Fei, L., 2023b. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973. [79] Hung, C.Y., Sun, Q., Hong, P., Zadeh, A., Li, C., Tan, U., Majumder, N., Poria, S., et al., 2025. Nora: A small open- sourced generalist vision language action model for embodied tasks. arXiv preprint arXiv:2504.19854. [80] Ikeda, B., Gramopadhye, M., Nekervis, L., Szafr, D., 2025. Marcer: Multimodal augmented reality for composing and executing robot tasks, in: 2025 20th ACM/IEEE International Conference on Human- Robot Interaction (HRI), IEEE. pp. 529- 539. [81] Imran, A., Gopalakrishnan, K., 2025. Foundation models in robotics, in: AI for Robotics: Toward Embodied and General Intelligence in the Physical World. Springer, pp. 159- 216. [82] Intelligence, P., Black, K., Brown, N., Darpinian, J., Dhabalia, K., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., et al., 2025. pi0.5: a vision- language- action model with open- world generalization. arXiv preprint arXiv:2504.16054. [83] Jeong, H., Lee, H., Kim, C., Shin, S., 2024. A survey of robot intelligence with large language models. Applied Sciences 14, 8868. [84] Jha, K., Doshi, A., Patel, P., Shah, M., 2019. A comprehensive review on automation in agriculture using artificial intelligence. Artificial Intelligence in Agriculture 2, 1- 12. [85] Jiang, J., Xiao, W., Lin, Z., Zhang, H., Ren, T., Gao, Y., Lin, Z., Cai, Z., Yang, L., Liu, Z., 2024. Solami: Social vision- language- action modeling for immersive interaction with 3d autonomous characters. arXiv preprint arXiv:2412.00174. [86] Jiang, Y., Gupta, A., Zhang, Z., Wang, G., Dou, Y., Chen, Y., Fei- Fei, L., Anandkumar, A., Zhu, Y., Fan, L., 2022. Vima: General robot manipulation with multimodal prompts. arXiv preprint arXiv:2210.03094 2, 6. [87] Jiang, Y., Zhang, R., Wong, J., Wang, C., Ze, Y., Yin, H., Gokmen, C., Song, S., Wu, J., Fei- Fei, L., 2025. Behavior robot suite: Streamlining real- world whole- body manipulation for everyday household activities. arXiv preprint arXiv:2503.05652. [88] Jones, J., Moos, O., Sforrazza, C., Stachowicz, K., Abbeel, P., Levine, S., 2025. Beyond sight: Finetuning generalist robot policies with heterogeneous sensors via language grounding. arXiv preprint arXiv:2501.04693 [89] Karamcheti, S., Zhai, A.J., Losey, D.P., Sadigh, D., 2021. Learning visually guided latent actions for assistive teleoperation, in: Learning for dynamics and control, PMLR. pp. 1230- 1241. [90] Katiyar, N., 2023. A Model- Driven Framework for Domain- Specific Adaptation of Time Series Forecasting Pipeline. McGill University (Canada). [91] Kelly, C., Hu, L., Yang, B., Tian, Y., Yang, D., Yang, C., Huang, Z., Li, Z., Hu, J., Zou, Y., 2024. Vision- apt: Vision- language understanding agent using generalized multimodal framework. arXiv preprint arXiv:2403.09027.

[92] Khan, M.H., Asfaw, S., Iarchuk, D., Cabrera, M.A., Moreno, L., Tokmurziyev, I., Tsetsepnokou, D., 2025. Shake- vla: Vision- language- action model- based system for bimanual robotic manipulations and liquid mixing. arXiv preprint arXiv:2501.06919. [93] Kim, M.J., Finn, C., Liang, P., 2025. Fine- tuning vision- language- action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645. [94] Kim, M.J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E., Lam, G., Sanketi, P., et al., 2024. Open- vla: An open- source vision- language- action model. arXiv preprint arXiv:2406.09246. [95] Lee, N., Bang, Y., Lovenia, H., Cahyawijaya, S., Dai, W., Fung, P., 2023. Survey of social bias in vision- language models. arXiv preprint arXiv:2309.14381. [96] Li, C., Wen, J., Peng, Y., Peng, Y., Feng, F., Zhu, Y., 2025a. Pointvla: Injecting the 3d world into vision- language- action models. arXiv preprint arXiv:2503.07511. [97] Li, D., Jin, Y., Sun, Y., Yu, H., Shi, J., Hao, X., Hao, P., Liu, H., Sun, F., Zhang, J., et al., 2024a. What foundation models can bring for robot learning in manipulation: A survey. arXiv preprint arXiv:2404.18201. [98] Li, J., Skinner, G., Yang, G., Quaranto, B.R., Schwaitzberg, S.D., Kim, P.C., Xiong, J., 2024b. Llava- surg: towards multimodal surgical assistant via structured surgical video learning. arXiv preprint arXiv:2408.07981. [99] Li, J., Wei, P., Han, W., Fan, L., 2023. Intentqa: Context- aware video intent reasoning, in: Proceedings of the IEEE/CVF international conference on computer vision, pp. 11963- 11974. [100] Li, J., Zhu, Y., Tang, Z., Wen, J., Zhu, M., Liu, X., Li, C., Cheng, R., Peng, Y., Feng, F., 2024c. Improving vision- language- action models via chain- of- affordance. arXiv preprint arXiv:2412.20451. [101] Li, M., Wang, Z., He, K., Ma, X., Liang, Y., 2025b. Jarvis- vla: Posttraining large- scale vision language models to play visual games with keyboards and mouse. arXiv preprint arXiv:2503.16365. [102] Li, Q., Liang, Y., Wang, Z., Luo, L., Chen, X., Liao, M., Wei, F., Deng, Y., Xu, S., Zhang, Y., et al., 2024d. Cogact: A foundational vision- language- action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650. [103] Li, S., Wang, J., Dai, R., Ma, W., Ng, W.Y., Hu, Y., Li, Z., 2024e. Robonurse- vla: Robotic scrub nurse system based on vision- language- action model. arXiv preprint arXiv:2409.19590. [104] Li, Y., Deng, Y., Zhang, J., Jang, J., Memmel, M., Yu, R., Garrett, C.R., Ramos, F., Fox, D., Li, A., et al., 2025c. Hamster: Hierarchical action models for open- world robot manipulation. arXiv preprint arXiv:2502.05485. [105] Li, Y., Gong, Z., Li, H., Huang, X., Kang, H., Bai, G., Ma, X., 2025d. Robotic visual instruction. arXiv preprint arXiv:2505.00693. [106] Li, Y., Lai, Z., Bao, W., Tan, Z., Dao, A., Sui, K., Shen, J., Liu, D., Liu, H., Kong, Y., 2025e. Visual large language models for generalized and specialized applications. arXiv preprint arXiv:2501.02765. [107] Li, Z., Wu, X., Du, H., Nghiem, H., Shi, G., 2025f. Benchmark evaluations, applications, and challenges of large vision language models: A survey. arXiv preprint arXiv:2501.02189 1. [108] Lin, K.Q., Li, L., Gao, D., Yang, Z., Wu, S., Bai, Z., Lei, W., Wang, L., Shou, M.Z., 2024. Showgirl: One vision- language- action model for gui visual agent. arXiv preprint arXiv:2411.17465. [109] Lin, Y., Zhou, H., Chen, M., Min, H., 2019. Automatic sorting system for industrial robot with 3d visual perception and natural language interaction. Measurement and Control 52, 100- 115. [110] Liu, J., Chen, H., An, P., Liu, Z., Zhang, R., Gu, C., Li, X., Guo, Z., Chen, S., Liu, M., et al., 2025a. Hybridvla: Collaborative diffusion and autoregression in a unified vision- language- action model. arXiv preprint arXiv:2503.10631. [111] Liu, J., Liu, M., Wang, Z., An, P., Li, X., Zhou, K., Yang, S., Zhang, R., Guo, Y., Zhang, B., 2024a. Robomamba: Efficient vision- language- action model for robotic reasoning and manipulation. Advances in Neural Information Processing Systems 37, 40085- 40110. [112] Liu, S., Wu, L., Li, B., Tan, H., Chen, H., Wang, Z., Xu, K., Su, H., Zhu, J., 2024b. Rdt- 1b: a diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864. [113] Liu, Y., Cao, X., Chen, T., Jiang, Y., You, J., Wu, M., Wang, X., Feng, M., Jin, Y., Chen, J., 2025b. From screens to scenes: A survey of em

bodied ai in healthcare. arXiv preprint arXiv:2501.07468. [114] Liu, Y., Cao, X., Chen, T., Jiang, Y., You, J., Wu, M., Wang, X., Feng, M., Jin, Y., Chen, J., 2025c. A survey of embodied ai in healthcare: Techniques, applications, and opportunities. arXiv preprint arXiv:2501.07468. [115] Liu, Z., Liang, H., Huang, X., Xiong, W., Yu, Q., Sun, L., Chen, C., He, C., Cui, B., Zhang, W., 2024c. Systhvlm: High- efficiency and high- quality synthetic data for vision language models. arXiv preprint arXiv:2407.20756. [116] Lu, H., Li, H., Shahani, P.S., Herbers, S., Scheutz, M., 2025. Probing a vision- language- action model for symbolic states and integration into a cognitive architecture. arXiv preprint arXiv:2502.04558. [117] Lu, J., Clark, C., Lee, S., Zhang, Z., Khosha, S., Marten, R., Hoiem, D., Kembhavi, A., 2024. Unified- io 2: Scaling autoregressive multimodal models with vision language audio and action, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 26439- 26455. [118] Lu, Y., Liao, Z., 2023. Towards happy housework: Scenario- based experience design for a household cleaning robotic system. EAI Endorsed Transactions on Scalable Information Systems 10. [119] Luo, J., Xu, C., Wu, J., Levine, S., 2024. Precise and dexterous robotic manipulation via human- in- the- loop reinforcement learning. arXiv preprint arXiv:2410.21845. [120] Lyu, J., Li, Z., Shi, X., Xu, C., Wang, Y., Wang, H., 2025. Dywa: Dynamics- adaptive world action model for generalizable non- prehensile manipulation. arXiv preprint arXiv:2503.16806. [121] Ma, Y., Song, Z., Zhuang, Y., Hao, J., King, I., 2024. A survey on vision- language- action models for embodied ai. arXiv preprint arXiv:2405.14093. [122] Mohammed, M.Q., Chung, K.L., Chyi, C.S., 2020. Review of deep reinforcement learning- based object grasping: Techniques, open challenges, and recommendations. Ieee Access 8, 178450- 178481. [123] Moroncelli, A., Soni, V., Shahid, A.A., Maccarini, M., Forgione, M., Piga, D., Spahiu, B., Roveda, L., 2024. Integrating reinforcement learning with foundation models for autonomous robotics: Methods and perspectives. arXiv preprint arXiv:2416.16411. [124] Mumuni, A., Mumuni, F., 2025. Large language models for artificial general intelligence (agi): A survey of foundational principles and approaches. arXiv preprint arXiv:2501.03151. [125] Ni, F., Hao, J., Wu, S., Kou, L., Yuan, Y., Dong, Z., Liu, J., Li, M., Zhuang, Y., Zheng, Y., 2024. Peria: Perspective, reason, imagine, act via holistic language and vision planning for manipulation. Advances in Neural Information Processing Systems 37, 17541- 17571. [126] Nie, Y., Li, L., Gan, Z., Wang, S., Zhu, C., Zeng, M., Liu, Z., Bansal, M., Wang, L., 2021. Mlp architectures for vision- and- language modeling: An empirical study. arXiv preprint arXiv:2112.04453. [127] Noorani, E., Serlin, Z., Price, B., Velasque, A., 2025. From abstraction to reality: Darpa's vision for robust sim- to- real autonomy. arXiv preprint arXiv:2503.11007. [128] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El- Nouby, A., et al., 2023. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193. [129] Pang, J., Zheng, P., Fan, J., Liu, T., 2025. Towatus cognition- augmented human- centric assembly: A visual computation perspective. Robotics and Computer- Integrated Manufacturing 91, 102852. [130] Pantalone, D., Faini, G.S., Cialdai, F., Sereni, E., Bacci, S., Bani, D., Bernini, M., Pratesi, C., Stefanos, P., Orzlozi, L., et al., 2021. Robot- assisted surgery in space: pros and cons. a review from the surgeon's point of view. npj Microgravity 7, 56. [131] Park, S.M., Kim, Y.G., 2023. Visual language integration: A survey and open challenges. Computer Science Review 48, 100548. [132] Patel, D., Eghbalzadeh, H., Kamra, N., Iuzofolino, M.L., Jain, U., Desai, R., 2023. Pretrained language models as visual planners for human assistance, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 15302- 15314. [133] Pertsch, K., Stachowicz, K., Ichter, B., Dress, D., Nair, S., Vuong, Q., Mees, O., Finn, C., Levine, S., 2025. Fast: Efficient action tokenization for vision- language- action models. arXiv preprint arXiv:2501.09747. [134] Plaat, A., van Duijn, M., van Stein, N., Preuss, M., van der Putten, P., Batenburg, K.J., 2025. Agentic large language models, a survey. arXiv preprint arXiv:2503.23037.

[135] Polubarov, A., Lyubaykin, N., Derevyagin, A., Zisman, I., Tarasov, D., Nikulin, A., Kurenkov, V., 2025. Vintix: Action model via in- context reinforcement learning. arXiv preprint arXiv:2501.19400. [136] Qu, D., Song, H., Chen, Q., Yao, Y., Ye, X., Ding, Y., Wang, Z., Gu, J., Zhao, B., Wang, D., et al., 2025. Spatialvla: Exploring spatial representations for visual- language- action model. arXiv preprint arXiv:2501.15830. [137] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al., 2018. Improving language understanding by generative pre- training. [138] Rawal, P.K., 2025. An Intelligent Versatile Pipeline for 6D Localization of Industrial Components in a Production Environment. Ph.D. thesis. Fraunhofer Verlag. [139] Ray, P.P., 2025. Chagget: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. Internet of Things and Cyber- Physical Systems 3, 121- 154. [140] Raza, S., Qureshi, R., Zahid, A., Fioresi, J., Sadak, F., Saeed, M., Sapkota, R., Jain, A., Zafar, A., Hassan, M.U., et al., 2025. Who is responsible? the data, models, users or regulations? responsible generative ai for a sustainable future. arXiv preprint arXiv:2502.08650. [141] Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S.G., Novikov, A., Barth- Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J.T., et al., 2022. A generalist agent. arXiv preprint arXiv:2205.06175. [142] Rodriguez- Guerra, D., Sorrosal, G., Cabanes, I., Calleja, C., 2021. Human- robot interaction. Review: Challenges and solutions for modern industrial environments. Ieee Access 9, 108557- 108578. [143] Rodriguez- Juan, J., Ortiz- Perez, D., Garcia- Rodriguez, J., Tomas, D., Nalepa, G.J., 2025. Integrating advanced vision- language models for context recognition in risks assessment. Neurocomputing 618, 129131. [144] Roychoudhury, A., Khorshidi, S., Agrawal, S., Bennewitz, M., 2023. Perception for humanoid robots. Current Robotics Reports 4, 127- 140. [145] Sahili, Z.A., Patras, I., Purver, M., 2025. Scaling for fairness? analyzing model size, data composition, and multilinguality in vision- language bias. arXiv preprint arXiv:2501.13223. [146] Sameni, S., Kafle, K., Tan, H., Jenni, S., 2024. Building vision- language models on solid foundations with masked distillation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14216- 14226. [147] Samson, M., Muraccioli, B., Kanehiro, F., 2025. Scalable, training- free visual language robotics: a modular multi- model framework for consumer- grade gpus, in: 2025 IEEE/SICE International Symposium on System Integration (SII), IEEE. pp. 193- 198. [148] Sapkota, R., Karkee, M., 2025. Object detection with multimodal large vision- language models: An in- depth review. Available at SSRN 5233953. [149] Sapkota, R., Roumeliotis, K.I., Cheppally, R.H., Calero, M.F., Karkee, M., 2025. A review of 3d object detection with vision- language models. arXiv preprint arXiv:2504.18738. [150] Sautenkov, O., Yaqoot, Y., Lykov, A., Mustafa, M.A., Tadevosyan, G., Akhmetkazy, A., Carpera, M.A., Martynov, M., Karaf, S., Tsetserukou, D., 2025. Uav- vla: Vision- language- action system for large scale aerial mission generation. arXiv preprint arXiv:2501.05014. [151] Schmidgall, S., Cho, J., Sakkka, C., Hiesinger, W., 2024. Gp- vls: A general- purpose vision language model for surgery. arXiv preprint arXiv:2407.19305. [152] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombers, T., Katta, A., Mullis, C., Wortsman, M., et al., 2022. Lasion- 5b: An open- large- scale dataset for training next generation image- text models. Advances in neural information processing systems 35, 25278- 25294. [153] Serpiva, V., Lykov, A., Myshlyaev, A., Khan, M.H., Abdulkarim, A.A., Sautenkov, O., Tsetserukou, D., 2025. Racevla: Vla- based racing drone navigation with human- like behaviour. arXiv preprint arXiv:2503.02572. [154] Sharshar, A., Khan, L.U., Ullah, W., Guizani, M., 2025. Vision- language models for edge networks: A comprehensive survey. arXiv preprint arXiv:2502.07855. [155] Shi, L.X., Ichter, B., Equi, M., Ke, L., Pertsch, K., Vuong, Q., Tanner, J., Walling, A., Wang, H., Fusai, N., et al., 2025. Hi robot: Open- ended instruction following with hierarchical vision- language- action models. arXiv preprint arXiv:2502.19417.

[156] Shin, H.C., Roth, H.R., Gao, M., Lu, L., Xu, Z., Nogues, I., Yao, J., Mollura, D., Summers, R.M., 2016. Deep convolutional neural networks for computer- aided detection: Cnn architectures, dataset characteristics and transfer learning. IEEE transactions on medical imaging 35, 1285- 1298. [157] Shridhar, M., Manuelli, L., Fox, D., 2022. Clioport: What and where pathways for robotic manipulation, in: Conference on robot learning, PMLR. pp. 894- 906. [158] Si, W., Wang, N., Yang, C., 2021. A review on manipulation skill acquisition through teleoperation- based learning from demonstration. Cognitive Computation and Systems 3, 1- 16. [159] Singh, S., Singh, J., Shah, B., Sehra, S.S., Ali, F., 2022. Augmented reality and gps- based resource efficient navigation system for outdoor environments: Integrating device camera, sensors, and storage. Sustainability 14, 12720. [160] Song, M., Deng, X., Zhou, Z., Wei, J., Guan, W., Nie, L., 2025a. A survey on diffusion policy for robotic manipulation: Taxonomy, analysis, and future directions. Authorea Preprints . [161] Song, W., Chen, J., Ding, P., Zhao, H., Zhao, W., Zhong, Z., Ge, Z., Ma, J., Li, H., 2025b. Accelerating vision- language- action model integrated with action chunking via parallel decoding. arXiv preprint arXiv:2503.02310. [162] Sun, H., Wang, H., Ma, C., Zhang, S., Ye, J., Chen, X., Lan, X., 2025a. Prism: Projection- based reward integration for scene- aware real- to- sim- to- real transfer with few demonstrations. arXiv preprint arXiv:2504.20520. [163] Sun, J., Mao, P., Kong, L., Wang, J., 2025b. A review of embodied grasping. Sensors (Basel, Switzerland) 25, 852. [164] Sutskever, I., Martens, J., Hinton, G.E., 2011. Generating text with recurrent neural networks, in: Proceedings of the 28th international conference on machine learning (ICML- 11), pp. 1011- 1024. [165] Szot, A., Mazoure, B., Agrawal, H., Hjelm, R.D., Kira, Z., Toshev, A., 2024. Grounding multimodal large language models in actions. Advances in Neural Information Processing Systems 37, 20198- 20224. [166] Team, G.R., Abeyruwan, S., Ainslie, J., Alayrac, J.B., Arenas, M.G., Armstrong, T., Bankersma, A., Barden, K., Badaa, M., Blokzijr, M., et al., 2025. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020. [167] Team, O.M., Ghosh, D., Walke, H., Pertsch, K., Black, K., Mees, O., Dasari, S., Hejna, J., Kreiman, T., Xu, C., et al., 2024. Octo: An open- source generalist robot policy. arXiv preprint arXiv:2405.12213. [168] Tellex, S., Gopalan, N., Kress- Gazit, H., Matuszek, C., 2020. Robots that use language. Annual Review of Control, Robotics, and Autonomous Systems 3, 25- 55. [169] Tian, H., Wang, T., Liu, Y., Qiao, X., Li, Y., 2020. Computer vision technology in agricultural automation—a review. Information processing in agriculture 7, 1- 19. [170] Tian, K., Jiang, Y., Yuan, Z., Peng, B., Wang, L., 2024. Visual autoregressive modeling: Scalable image generation via next- scale prediction. Advances in neural information processing systems 37, 84839- 84865. [171] Torres, N., Ulloa, C., Araya, I., Ayala, M., Jara, S., 2024. A comprehensive analysis of gender, racial, and prompt- induced biases in large language models. International Journal of Data Science and Analytics, 1- 38. [172] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, R., Bhosale, S., et al., 2023. Llama 2: Open foundation and fine- tuned chat models. arXiv preprint arXiv:2307.09288. [173] Trivedi, C., Bhattacharya, P., Prasad, V.K., Patel, V., Singh, A., Tanwar, S., Sharma, R., Aluvala, S., Pau, G., Sharma, G., 2024. Explainable ai for industry 5.0: vision, architecture, and potential directions. IEEE Open Journal of Industry Applications. [174] Verbaan, L., 2024. Perception and control with large language models in robotic manipulation. TU Delft Library. [175] Vuong, Q., Levine, S., Walke, H.R., Pentsch, K., Singh, A., Doshi, R., Xu, C., Luo, J., Tan, L., Shah, D., et al., 2023. Open x- embodiment: Robotic learning datasets and rt- x models, in: Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition@ CoRL2023. [176] Wang, G., Bai, L., Nah, W.J., Wang, J., Zhang, Z., Chen, Z., Wu, J., Islam, M., Liu, H., Ren, H., 2024a. Surgical- Ivlm: Learning to adapt large vision- language model for grounded visual question answering in robotic surgery. arXiv preprint arXiv:2405.10948. [177] Wang, H., Xing, Z., Wu, W., Yang, Y., Tang, Q., Zhang, M., Xu, Y., Zhu, L., 2024b. Non- invasive to invasive: Enhancing ffa synthesis from cfp with a benchmark dataset and a novel network, in: Proceedings of the 1st International Workshop on Multimedia Computing for Health and Medicine, pp. 7- 15. [178] Wang, J., Guo, D., Liu, H., 2025a. Where to learn: Embodied perception learning planned by vision- language models. IEEE Transactions on Cognitive and Developmental Systems. [179] Wang, S., 2025. Roboflamingo- plus: Fusion of depth and rgb perception with vision- language models for enhanced robotic manipulation. arXiv preprint arXiv:2503.19510. [180] Wang, T., Han, C., Liang, J.C., Yang, W., Liu, D., Zhang, L.X., Wang, Q., Luo, J., Tang, R., 2024c. Exploring the adversarial vulnerabilities of vision- language- action models in robotics. arXiv preprint arXiv:2411.13587. [181] Wang, Y., Wu, S., Zhang, Y., Yan, S., Liu, Z., Luo, J., Fei, H., 2025b. Multimodal chain- of- thought reasoning: A comprehensive survey. arXiv preprint arXiv:2503.12605. [182] Wang, Z., Zhou, Z., Song, J., Huang, Y., Shu, Z., Ma, L., 2024d. Towards testing and evaluating vision- language- action models for robotic manipulation: An empirical study. arXiv preprint arXiv:2409.12894. [183] Wei, J., Yuan, S., Li, J., Hu, Q., Gan, Z., Ding, W., 2024. Occellama: An occupancy- language- action generative world model for autonomous driving. arXiv preprint arXiv:2409.03272. [184] Wen, J., Zhu, M., Zhu, Y., Tang, Z., Li, J., Zhou, Z., Li, C., Liu, X., Peng, Y., Shen, C., et al., 2024. Diffusion- vla: Scaling robot foundation models via unified diffusion and autoregression. arXiv preprint arXiv:2412.03293. [185] Wen, J., Zhu, Y., Li, J., Yang, Z., Shen, C., Feng, F., 2025a. Dexvla: Vision- language model with plug- in diffusion expert for general robot control. arXiv preprint arXiv:2502.05855. [186] Wen, J., Zhu, Y., Li, J., Zhu, M., Tang, Z., Wu, K., Xu, Z., Liu, N., Cheng, R., Shen, C., et al., 2025b. Tinyvla: Towards fast, data- efficient vision- language- action models for robotic manipulation. IEEE Robotics and Automation Letters. [187] Woo, S., Debnath, S., Hu, R., Chen, X., Liu, Z., Kweon, I.S., Xie, S., 2023. Convnext v2: Co- designing and scaling convnets with masked autoencoders, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16133- 16142. [188] Wu, J., Zhong, M., Xing, S., Lai, Z., Liu, Z., Chen, Z., Wang, W., Zhu, X., Lu, L., Lu, T., et al., 2024a. Visionlm v2: An end- to- end generalist multimodal large language model for hundreds of vision- language tasks. Advances in Neural Information Processing Systems 37, 69925- 69975. [189] Wu, W., Feng, X., Gao, Z., Kan, Y., 2024b. Smart: scalable multi- agent real- time motion generation via next- token prediction. Advances in Neural Information Processing Systems 37, 114048- 114071. [190] Wu, Z., Zhou, Y., Xu, X., Wang, Z., Yan, H., 2025. Momanipvla: Transferring vision- language- action models for general mobile manipulation. arXiv preprint arXiv:2503.13446. [191] Xiang, T.Y., Jin, A.Q., Zhou, X.H., Gui, M.J., Xie, X.L., Liu, S.Q., Wang, S.Y., Duang, S.B., Wang, S.C., Lei, Z., et al., 2025. Vla model- expert collaboration for bi- directional manipulation learning. arXiv preprint arXiv:2503.04163. [192] Xiong, J., Liu, G., Kuang, L., Wu, C., Wu, T., Mu, Y., Yao, Y., Shen, H., Wan, Z., Huang, J., et al., 2024. Autoregressive models in vision: A survey. arXiv preprint arXiv:2411.05902. [193] Xu, D., Chen, Y., Wang, J., Huang, Y., Wang, H., Jin, Z., Wang, H., Yue, W., He, J., Li, H., et al., 2024a. Mlevlm: Improve multi- level progressive capabilities based on multimodal large language model for medical visual question answering, in: Findings of the Association for Computational Linguistics ACL 2024, pp. 4977- 4997. [194] Xu, J., Sun, Q., Han, Q.L., Tang, Y., 2025a. When embodied ai meets industry 5.0: human- centered smart manufacturing. IEEE/CAA Journal of Automatica Sinica 12, 485- 501. [195] Xu, S., Wang, Y., Xia, C., Zhu, D., Huang, T., Xu, C., 2025b. Vla- cache: Towards efficient vision- language- action model via adaptive token caching in robotic manipulation. arXiv preprint arXiv:2502.02175. [196] Xu, Z., Wu, K., Wen, J., Li, J., Liu, N., Che, Z., Tang, J., 2024b. A

survey on robotics with foundation models: toward embodied ai. arXiv preprint arXiv:2402.02385. [197] Xue, H., Ren, J., Chen, W., Zhang, G., Fang, Y., Gu, G., Xu, H., Lu, C., 2025. Reactive diffusion policy: Slow- fast visual- tactile policy learning for contact- rich manipulation. arXiv preprint arXiv:2503.02881. [198] Yang, R., Chen, G., Wen, C., Gao, Y., 2025. Fp3: A 3d foundation policy for robotic manipulation. arXiv preprint arXiv:2503.08950. [199] Yang, Y., Huang, W., Wei, Y., Peng, H., Jiang, X., Jiang, H., Wei, F., Wang, Y., Hu, H., Qiu, L., et al., 2023a. Attentive mask clip, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2771- 2781. [200] Yang, Z., Chen, Y., Wang, J., Manivasagam, S., Ma, W.C., Yang, A.J., Urtasun, R., 2023b. Unisim: A neural closed- loop sensor simulator, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1389- 1399. [201] Ye, S., Jang, J., Jeon, B., Joo, S., Yang, J., Feng, B., Mandlekar, A., Tan, R., Chao, Y.W., Lin, B.Y., et al., 2024. Latent action pretraining from videos. arXiv preprint arXiv:2410.11758. [202] Yue, Y., Wang, Y., Kang, B., Han, Y., Wang, S., Song, S., Feng, J., Huang, G., 2024. Deer- vla: Dynamic inference of multimodal large language models for efficient robot execution. Advances in Neural Information Processing Systems 37, 56619- 56643. [203] Zawalski, M., Chen, W., Pertsch, K., Mees, O., Finn, C., Levine, S., 2024. Robotic control via embodied chain- of- thought reasoning. arXiv preprint arXiv:2407.08693. [204] Zhai, X., Mustafa, B., Kolesnikov, A., Beyar, L., 2023. Sigmoid loss for language image pre- training, in: Proceedings of the IEEE/CVF international conference on computer vision, pp. 11975- 11986. [205] Zhang, B., Zhang, Y., Ji, J., Lei, Y., Dai, J., Chen, Y., Yang, Y., 2025a. Safevla: Towards safety alignment of vision- language- action model via safe reinforcement learning. arXiv preprint arXiv:2503.03480. [206] Zhang, H., Ding, P., Lyu, S., Peng, Y., Wang, D., 2025b. Gevrm: Goal- expressive video generation model for robust visual manipulation. arXiv preprint arXiv:2502.09268. [207] Zhang, H., Yu, H., Zhao, L., Choi, A., Bai, Q., Yang, B., Xu, W., 2025c. Simm: Sim- to- rear- legged instructive manipulation via long- horizon visuomotor learning. arXiv preprint arXiv:2201.09905. [208] Zhang, H., Zantout, N., Kachana, P., Wu, Z., Zhang, J., Wang, W., 2024a. Vla- 3d: A dataset for 3d semantic scene understanding and navigation. arXiv preprint arXiv:2411.03540. [209] Zhang, J., Guo, Y., Hu, Y., Chen, X., Zhu, X., Chen, J., 2025d. Up- vla: A unified understanding and prediction model for embodied agent. arXiv preprint arXiv:2501.18867. [210] Zhang, J., Wang, K., Wang, S., Li, M., Liu, H., Wei, S., Wang, Z., Zhang, Z., Wang, H., 2024b. Uni- navid: A video- based vision- language- action model for unifying embodied navigation tasks. arXiv preprint arXiv:2412.06224. [211] Zhang, K., Yin, Z.H., Ye, W., Gao, Y., 2024c. Learning manipulation skills through robot chain- of- thought with sparse failure guidance. arXiv preprint arXiv:2405.13573. [212] Zhang, K., Yun, P., Cen, J., Cai, J., Zhu, L., Yuan, H., Zhao, C., Feng, T., Wang, M.Y., Chen, Q., et al., 2025e. Generative artificial intelligence in robotic manipulation: A survey. arXiv preprint arXiv:2503.03464. [213] Zhang, R., Dong, M., Zhang, Y., Heng, L., Chi, X., Dai, G., Du, L., Wang, D., Du, Y., Zhang, S., 2025f. Mole- vla: Dynamic layer- skipping vision language action model via mixture- of- layers for efficient robot manipulation. arXiv preprint arXiv:2503.20384. [214] Zhao, H., Song, W., Wang, D., Tong, X., Ding, P., Chang, X., Ge, Z., 2025a. More: Unlocking scalability in reinforcement learning for quadruped vision- language- action models. arXiv preprint arXiv:2503.08007. [215] Zhao, Q., Lu, Y., Kim, M.J., Fu, Z., Zhang, Z., Wu, Y., Li, Z., Ma, Q., Han, S., Finn, C., et al., 2025b. Cot- vla: Visual chain- of- thought reasoning for vision- language- action models. arXiv preprint arXiv:2503.22020. [216] Zhao, T.Z., Kumar, V., Levine, S., Finn, C., 2023. Learning fine- grained bimanual manipulation with low- cost hardware. arXiv preprint arXiv:2304.13705. [217] Zhen, H., Qiu, X., Chen, P., Yang, J., Yan, X., Du, Y., Hong, Y., Gan, C., 2024. 3d- vla: A 3d vision- language- action generative world model. arXiv preprint arXiv:2403.09631.

[218] Zheng, J., Li, J., Liu, D., Zheng, Y., Wang, Z., Ou, Z., Liu, Y., Liu, J., Zhang, Y.Q., Zhan, X., 2025. Universal actions for enhanced embodied foundation models. arXiv preprint arXiv:2501.10105. [219] Zhong, Y., Huang, K., Li, R., Zhang, C., Liang, Y., Yang, Y., Chen, Y., 2025. Dextragrsyla: A vision- language- action framework towards general dexterous grasping. arXiv preprint arXiv:2502.20900. [220] Zhou, X., Han, X., Yang, F., Ma, Y., Knoll, A.C., 2025a. Opendrivelva: Towards end- to- end autonomous driving with large vision language action model. arXiv preprint arXiv:2503.23463. [221] Zhou, Z., Zhu, Y., Zhu, M., Wen, J., Liu, N., Xu, Z., Meng, W., Cheng, R., Peng, Y., Shen, C., et al., 2025b. Chatva: Unified multimodal understanding and robot control with vision- language- action model. arXiv preprint arXiv:2502.14420. [222] Zhu, D.H., Chang, Y.P., 2020. Robot with humanoid hands cooks food better? effect of robotic chef anthropomorphism on food quality prediction. International Journal of Contemporary Hospitality Management 32, 1367- 1383. [223] Zhu, M., Zhu, Y., Li, J., Zhou, Z., Wen, J., Liu, X., Shen, C., Peng, Y., Feng, F., 2025. Objectvta: End- to- end open- world object manipulation without demonstration. arXiv preprint arXiv:2502.19250. [224] Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al., 2023. Rt- 2: Vision- language- action models transfer web knowledge to robotic control, in: Conference on Robot Learning, PMLR. pp. 2165- 2183.

Appendix Table

The following appendix tables present a comprehensive overview of recent developments and challenges in VLA models. Table 3 systematically compares state- of- the- art VLA methodologies, their application domains, and key innovations across robotics, autonomous systems, and embodied AI platforms. This comparative summary highlights core architectural advances, deployment contexts, and technical contributions—providing valuable insight into the evolving landscape of generalist and task- specific VLA models. Meanwhile, Table 4 presents a structured synthesis of the major technical and practical challenges facing VLA model deployment, alongside potential solutions and their expected impact. This includes limitations such as real- time inference constraints, multimodal integration issues, and ethical concerns, with proposed resolutions ranging from architectural innovations to scalable training techniques and cross- modal alignment strategies. Together, these tables serve as a detailed reference for researchers, developers, and practitioners aiming to understand both the current capabilities and outstanding barriers in VLA- based intelligent systems.

Table 3: Comparison of VLA methodologis, application areas, and innovations. This comprehensive table compares cutting-edge VLA models by summarizing their methodologies, application domains, and key innovations.  

<table><tr><td>Reference &amp;amp; Year</td><td>VLA Methodology</td><td>VLA Application Area</td><td>Strength and Key Innovation</td></tr><tr><td>CogACT [102] &amp;amp; 2024</td><td>Componentized VLA with specialized action mod-ule using diffusion transformers</td><td>Industrial robotics, language-guided manipulation</td><td>Robust action modeling, rapid adaptation, strong gener-alization, much higher task success rates</td></tr><tr><td>VLATest [182] &amp;amp; 2024</td><td>Automated framework for large-scale VLA model testing in manipulation</td><td>Robotic manipulation: bench-marking VLA robustness and reliability</td><td>Diverse scene generation, multi-model/task evaluation, reveals robustness gaps, guides VLA improvement</td></tr><tr><td>NaVILA [32] &amp;amp; 2024</td><td>Two-level VLA: high-level vision-language gener-ates mid-level nav commands, RL locomotion exe-cutes</td><td>Legged robot navigation via nat-ural language in cluttered, real-world scenes</td><td>Modular mid/low-level split, strong generalization, 88% real-world success, robust to diverse terrains</td></tr><tr><td>RoboNurse-VLA [103] &amp;amp; 2024</td><td>SAM 2 vision, Llama 2 language, real-time voice-to-action pipeline</td><td>Surgical assistance: precise in-strument grasp and handover in OR</td><td>Accurate, real-time handover, robust to unseen tools, ex-cels at complex, dynamic surgical tasks</td></tr><tr><td>Mobility VLA [35] &amp;amp; 2024</td><td>Hierarchical VLA: long-context VLM for multi-modal goal localization, topological graph for low-level navigation</td><td>Multimodal instruction naviga-tion with demonstration tours (MINT) in real-world environ-ments</td><td>High success on complex language+image tasks, robust to novel queries, leverages demonstration videos, scal-able to large spaces</td></tr><tr><td>CoVLA [5] &amp;amp; 2025</td><td>CLIP for vision, Llama-2 for language, trajectory prediction for action</td><td>Multimodal training, dataset for VLA model training</td><td>Large-scale, richly annotated dataset; enables inter-pretable scene understanding and robust path planning</td></tr><tr><td>OpenDriveVLA [220] &amp;amp; 2025</td><td>Hierarchical alignment of PDD/3D visual tokens and language embeddings; autoregressive agent-env-ego modeling</td><td>End-to-end autonomous driving</td><td>Unified semantic space, dynamic interaction modeling, state-of-the-art planning and QA performance</td></tr><tr><td>ORION [56] &amp;amp; 2025</td><td>QT-Former for history context, LLM for reasoning, generative planner for trajectory prediction</td><td>Holistic E2E autonomous driv-ing</td><td>Align reasoning and action spaces, unified optimization for VQA and planning, superior closed-loop results</td></tr><tr><td>QUART-VLA [43] &amp;amp; 2025</td><td>QUART model fuses vision and language for ac-tion generation</td><td>Quadruped robots: navigation, manipulation, whole-body tasks</td><td>Tight vision-language-action sim-to-real, fine-grained instruction alignment, strong generalization</td></tr><tr><td>TinyVLA [186] &amp;amp; 2025</td><td>Compact VLA with fast multimodal backbone, dif-fusion policy decoder</td><td>Robotic manipulation: fast, data-efficient visuomotor con-trol</td><td>No pre-training needed, faster inference, strong generalization, overplatforms OpenVLA on efficiency and accu-ricity</td></tr><tr><td>UAV-VLA [150] &amp;amp; 2025</td><td>Modular VLA: GPT for goal extraction, VLM for object search, GPT for action generation</td><td>Large-scale UAV mission plan-ning from natural language and satellite imagery</td><td>Efficient flight path/action plan generation, no prior training, intuitive human-UAV interaction, bench-marked performance</td></tr><tr><td>Bi-VLA [59] &amp;amp; 2025</td><td>Multimodal transformer links vision, language, and binamical action modules</td><td>Bimannual dexterous manipula-tion for household tasks</td><td>Accurate dexterous generation, high adaptability, strong vision-language-action integration, robust real-world performance</td></tr><tr><td>ChatVLA [221] &amp;amp; 2025</td><td>Phased Alignment Training, Mixture-of-Experts for vision-language-action integration</td><td>Unified multimodal understand-ing and robot control</td><td>Minimizes forgetting/interference, excels at VQA and manipulation, efficient, outperforms SOTA VLA models</td></tr><tr><td>RoboMamba [111] &amp;amp; 2025</td><td>Mamba-based VLA: vision encoder co-trained with SSM for reasoning and SE(3) action</td><td>Robotic reasoning and manipulation, efficient pose prediction</td><td>Low-complexity inference, minimal fine-tuning, fast and accurate reasoning and manipulation, SOTA efficiency</td></tr><tr><td>OTTER [75] &amp;amp; 2025</td><td>Text-aware visual feature extraction with frozen pre-trained VLMs</td><td>Robotic manipulation: zero-shot generalization to novel tasks</td><td>Preserves semantic alignment, no VLM fine-tuning, task-relevant feature selection, strong zero-shot perfor-mance</td></tr><tr><td>PointVLA [96] &amp;amp; 2025</td><td>Injects 3D point cloud features into frozen pre-trained VLA via modular skip-blocks</td><td>Robotic manipulation: spatial reasoning, few-shot, long-horizon tasks</td><td>No retraining, preserves 2D knowledge, strong 3D spa-tial reasoning, excels at few-shot and dynamic tasks</td></tr><tr><td>VLA-Cache [195] &amp;amp; 2025</td><td>Adaptive token caching with selective reuse of static visual tokens</td><td>Robotic manipulation: efficient, real-time VLA inference</td><td>40-50% faster, minimal accuracy loss, dynamic layer-wise token reuse, practical for real-world robots</td></tr><tr><td>CombatVLA [29] &amp;amp; 2025</td><td>Trains on video-action AoT sequences, integrates truncated AoT for fast inference</td><td>3D ARPGs: real-time combat understanding and tactical ac-tion</td><td>50x faster inference, outperforms all baselines, sur-passed human success rate, strong tactical reasoning</td></tr><tr><td>HybridVLA [110] &amp;amp; 2025</td><td>Unified LLM with collaborative diffusion and au-toregressive action policies</td><td>Robotic manipulation: single-arm, dual-arm, diverse real/sim tasks</td><td>Adaptive action ensemble, robust control, strong gener-alization, outperforms SOTA on complex manipulations</td></tr><tr><td>NORA [79] &amp;amp; 2025</td><td>3B-parameter VLA using Qwen-2.5-VL-3B back-bone and FAST+ tokenizer</td><td>Generalist embodied robotics: efficient real-world and simu-lated task execution</td><td>Low computational overhead, strong visual reasoning, fast action decoding, outperforms larger VLA models</td></tr><tr><td>SpatialVLA [136] &amp;amp; 2025</td><td>Ego3D Position Encoding and Adaptive Action Grids for spatially-aware VLA</td><td>Generalist robot manipulation: cross-robot, multi-task, zero-shot control</td><td>3D spatial integration, adaptive action discretization, strong generalization and transfer, open-sourced code</td></tr><tr><td>MoLe-VLA [213] &amp;amp; 2025</td><td>Mixture-of-Layers with dynamic layer-skipping via STAR router and CogKD</td><td>Efficient robot manipulation: RL.Bench and real-world tasks</td><td>Selective layer activation, 5.6x faster, preserves cogni-tion, 8.8% mean success, brain-inspired efficiency</td></tr><tr><td>JARVIS-VLA [101] &amp;amp; 2025</td><td>Post-trains large VLMs with visual-language guid-ance and action head for keyboard/mouse control</td><td>Visual games (Minecraft): 1k+ tasks, open-world, instruction following</td><td>Self-supervised post-training, 40%+ over baselines, strong world knowledge, state-of-the-art generalization, open-sourced</td></tr><tr><td>UP-VLA [209] &amp;amp; 2025</td><td>Unified VLA with joint multi-modal understanding and future prediction objectives</td><td>Understanding agents: manipulation tasks, precise spatial reasoning</td><td>Captures both high-level semantics and low-level spatial dynamics, 33% better on Calvin ABC-D, excels at real-world tasks needing fine spatial control</td></tr><tr><td>Shake-VLA [92] &amp;amp; 2025</td><td>Modular VLA system with vision, speech-to-text, RAG, anomaly detection, and bimanual arms</td><td>Bimannual robotic cocktail mix-ing: ingredient detection, recipe adaptation, liquid measurement</td><td>100% task success, robust in noisy/cluttered settings, accurate ingredient handling, flexible recipe adaptation, real-world deployment</td></tr><tr><td>MoRE [214] &amp;amp; 2025</td><td>Sparse Mixture-of-Experts (MoE) with LoRA modules, RL-based Q-function training</td><td>Quadruped robots: multi-task navigation, locomotion, and ma-nipulation</td><td>Scalable RL fine-tuning on mixed-quality data, strong multi-task and OOD generalization, outperforms base-lines in sim and real-world</td></tr><tr><td>DexGraspVLA [219] &amp;amp; 2025</td><td>Hierarchical VLA: pre-trained vision-language planner + diffusion-based low-level controller</td><td>General dexterous grasping: robust across diverse objects, lighting, and backgrounds</td><td>Iterative domain-invariant representation, strong zero-shot generalization, 90%+ success on unseen scenarios, consistent performance across variations</td></tr><tr><td>DexVLA [185] &amp;amp; 2025</td><td>Plug-in billion-param diffusion action expert, em-bodiment curriculum learning</td><td>General robot control: single-arm, bimanual, dexterous hand, long-horizon tasks</td><td>Cross-embodiment action modeling, efficient curriculum training, rapid adaptation, SOTA on complex tasks without task-specific tuning</td></tr></table>

Table 4: Challenges, Potential Solutions, and Expected Impact of VLA Models  

<table><tr><td>Challenge / Limita-tion</td><td>Potential Solution</td><td>Expected Impact</td></tr><tr><td>Real-Time Inference Constraints</td><td>Adopt parallel decoding, quantized transformers, and hardware acceleration [100, 94](e.g., Ten-sorRT); minimize autoregressive overhead [60, 110]</td><td>Supports real-time robotic control and broader deployment in time-sensitive domains[194, 150] (e.g., drones, manipulators)</td></tr><tr><td>Multimodal Action Representation</td><td>Hybrid tokenization using diffusion and autoregressive policies [133]; train on diverse demonstrations and multi-modal outputs [121]</td><td>Improves handling of complex, dynamic manipulation tasks with multiple viable solutions [59]</td></tr><tr><td>Safety Assurance in Open Worlds</td><td>Integrate dynamic risk assessment modules [143, 180]; low-latency emergency stop circuits and adaptive planning layers [87]</td><td>Ensures reliability and safety in unpre-dictable environments (homes, factories, healthcare settings)</td></tr><tr><td>Dataset Bias and Grounding</td><td>Curate diverse, debiased datasets [145]; apply improved grounding techniques such as CLIP fine-tuning with hard negatives [212, 16]</td><td>Enhances model fairness, semantic fidelity [85], and generalizability to novel real-world inputs [175, 217, 136]</td></tr><tr><td>Limited 3D Percep-tion and Reasoning</td><td>Integrate 3D sensors (e.g., depth, LiDAR), develop 3D-aware architectures, and leverage point cloud fusion with vision-language inputs</td><td>Enables more accurate spatial reasoning, manipulation, and navigation in complex real-world environments [100]</td></tr><tr><td>Cross-Embodiment Generalization</td><td>Train with diverse robot types and morphologies, use embodiment-agnostic representations, and apply cross-domain adaptation techniques [201]</td><td>Facilitates transfer of policies and knowledge across different robot platforms and configurations [209, 94]</td></tr><tr><td>Annotation Com-plexity and Cost</td><td>Employ weak supervision, active learning, and synthetic data generation to reduce reliance on extensive manual annotation [115]</td><td>Lowers development costs and accelerates scaling to new tasks and domains [180, 215]</td></tr><tr><td>Sim-to-Real Transfer Gap</td><td>Use domain adaptation, sim-to-real fine-tuning, and real-world calibration strategies[162, 104]</td><td>Improves reliability and consistency of VLA models when deployed outside sim-ulation environments [4, 53]</td></tr><tr><td>Integration of Physi-cal Knowledge</td><td>Incorporate physics-based priors, simulation en-vironment, and dynamics modeling into training pipelines [42]</td><td>Enhances the model&#x27;s ability to predict and plan actions that respect real-world physi-cal constraints [82]</td></tr><tr><td>Multi-Modal Integra-tion (e.g., tactile, au-dio)</td><td>Fuse additional sensory modalities (tactile, au-dio) with vision and language [88]; develop multi-modal transformers</td><td>Expands task repertoire and robustness to ambiguous or visually occluded scenarios [61, 106, 71]</td></tr><tr><td>Handling Long-Horizon, Multi-Stage Tasks</td><td>Design hierarchical policies, memory-augmented networks, and trajectory planning modules [106]</td><td>Improves performance on complex, se-quential tasks requiring planning and memory [102, 215, 175, 136]</td></tr><tr><td>System Integration Complexity</td><td>Develop unified Transformer backbones [218]; in-corporate temporal alignment layers and sim-to-real transfer learning strategies [127, 207]</td><td>Enables seamless planning-control coor-dination and robust transfer to physical robots [147, 160]</td></tr><tr><td>Energy and Compute Demands</td><td>Apply model pruning, LoRA adapters, quantization-aware training, and deployment on low-power accelerators</td><td>Facilitates scalable, efficient deployment of VLAs in embedded and mobile plat-forms [195, 213, 96, 190]</td></tr><tr><td>Generalization to Unseen Tasks</td><td>Use compositional generalization, few-shot meta-learning, and task-agnostic pretraining pipelines [135, 113]</td><td>Reduces task-specific overfitting, enabling robust zero- and few-shot adaptation [75, 186, 215]</td></tr><tr><td>Robustness to Envi-ronmental Variability</td><td>Use domain randomization, sensor fusion, and real-time recalibration of perception-action pipelines [121]</td><td>Enhances performance in changing or clut-tered environments with minimal degrada-tion [214, 186]</td></tr><tr><td>Ethical and Societal Implications</td><td>Enforce privacy via on-device processing and anonymization [116, 154, 195, 29]; audit model fairness; build regulatory frameworks for trust</td><td>Promotes equitable and trustworthy VLA adoption across social, medical, and labor domains [124, 140, 166, 134]</td></tr></table>