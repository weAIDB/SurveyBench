# A Survey on Efficient Inference for Large Language Models

Zixuan Zhou*, Xuefei Ning*, Ke Hong*, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xuhong Li, Shengen Yan, Guohao Dai, Xiao- Ping Zhang Fellow, IEEE, Huazhong Yang Fellow, IEEE, Yuhan Dong, Yu Wang Fellow, IEEE

Abstract—Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource- constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic- complexity attention operation, and the auto- regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data- level, model- level, and system- level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub- fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.

## 1 INTRODUCTION

Large Language Models (LLMs) have garnered substantial attention from both academia and industry in recent years. The field of LLMs has experienced notable growth and significant achievements. Numerous open- source LLMs have emerged, including the GPT- series (GPT- 1 [1], GPT- 2 [2], and GPT- 3 [3]), OPT [4], LLaMA- series (LLaMA [5], LLaMA 2 [5], Baichuan 2 [6], Vicuna [7], LongChat [8]), BLOOM [9], FALCON [10], GLM [11], and Mistral [12], which are used for both academic research and commercial purposes. The success of LLMs stems from their robust capability in handling diverse tasks such as neural language understanding (NLU), neural language generation (NLG), reasoning [13], [14], and code generation [15], consequently enabling impactful applications like ChatGPT, Copilot, and Bing. There is a growing belief [16] that the rise and achievements of LLMs signify a significant stride towards Artificial General Intelligence (AGI) for humanity.

![](images/c93a3ead981a37f6eeaba0fca2be5a8a77f2abc3124e987becd31c43cd53c4a1.jpg)  
Fig. 1. The challenges of LLM deployment.

However, the deployment of LLMs is not always going smoothly. As shown in Fig. 1, LLMs typically demand higher computational cost, memory access cost and memory usage in their inference process (we will analyse the root causes in the Sec. 2.3), which deteriorates the efficiency indicators (e.g., latency, throughput, power consumption and storage) in the resource- constrained scenarios. This poses challenges for the application of LLMs in both edge and cloud scenarios. For example, the immense storage requirements render the deployment of a 70- billion- parameter model impractical on personal laptops for tasks such as development assistance. Additionally, the low throughput would result in significant costs if LLMs are used for every search engine request, leading to a considerable reduction in the profits of the search engine.

Fortunately, a substantial array of techniques has been proposed to enable efficient inference for LLMs. To gain a comprehensive understanding of existing studies and inspire further research, this survey employs a hierarchical classification and systematic summarization of the current landscape of efficient LLM inference. Specifically, we categorize relevant studies into three levels: data- level optimization, model- level optimization, and system- level optimization (refer to Sec. 3 for elaboration). Moreover, we conduct experimental analyses on representative methods

within critical sub- fields to consolidate knowledge, offer practical recommendations, and provide guidance for future research endeavors.

TABLE1 Comparison of existing surveys.  

<table><tr><td rowspan="2">Survey</td><td colspan="3">Optimization Levels</td><td rowspan="2">Experimental Analysis</td></tr><tr><td>Data-level</td><td>Model-level</td><td>System-level</td></tr><tr><td>[17], [18], [19]</td><td></td><td></td><td></td><td></td></tr><tr><td>[20]</td><td></td><td>✓</td><td></td><td>✓</td></tr><tr><td>[21]</td><td></td><td>✓</td><td></td><td></td></tr><tr><td>[22]</td><td></td><td>✓</td><td></td><td></td></tr><tr><td>[23], [24]</td><td></td><td>✓</td><td></td><td>✓</td></tr><tr><td>[25]</td><td></td><td>✓</td><td></td><td>✓</td></tr></table>

Currently, several surveys [17], [18], [19], [20], [21], [22], [23] have been conducted in the field of efficient LLMs. These surveys primarily focus on different aspects of LLM efficiency but offer opportunities for further improvement. Zhu et al. [17], Park et al. [18], Wang et al. [19] and Tang et al. [20] concentrate on model compression techniques within model- level optimization. Ding et al. [21] center on efficiency research considering both data and model architecture perspectives. Miao et al. [22] approach efficient LLM inference from a machine learning system (MLSys) research perspective. In contrast, our survey provides a more comprehensive research scope, addressing optimization at three levels: data- level, model- level, and system- level, with the inclusion of recent advancements. While Wan et al. [23] and Xu et al. [24] also deliver comprehensive review of efficient LLM research, our work extends by incorporating comparative experiments and offering practical insights and recommendations based on experimental analyses in several critical sub- fields like model quantization and serving systems. A comparison of these surveys is summarized in Table 1.

The remainder of this survey is organized as follows: Sec. 2 introduces the basic concept and knowledge about LLMs and presents a detailed analysis of the efficiency bottlenecks during the inference process of LLMs. Sec. 3 demonstrates our taxonomy. Sec. 4 to Sec. 6 respectively present and discuss studies on efficiency optimization at three distinct levels. Sec. 7 offers broader discussions for several key application scenarios. Sec. 8 concludes the key contributions provided by this survey.

## 2 PRELIMINARIES

### 2.1 Transformer-Style LLMs

Language modeling, as the fundamental function of language models (LMs), involves modeling the likelihood of the word sequence and predicting the distribution of subsequent words. Over recent years, researchers have discovered that scaling up language models not only enhances their language modeling ability but also engenders emergent capabilities for tackling more intricate tasks beyond conventional NLP tasks [25]. These scaled- up language models are referred to as large language models (LLMs).

The mainstream LLMs are designed based on the Transformer architecture [26]. Specifically, a typical Transformer architecture is composed of several stacked Transformer blocks. Typically, a Transformer block consists of a MultiHead Self- Attention (MHSA) block, a Feed Forward Network (FFN), and a LayerNorm (LN) operation. For each block, it receives the output features of the previous one as the input, and passes the features through each submodule to obtain the output. Specifically, before the first block, a tokenizer is used to convert the original input sentence into a sequence of tokens, and a following embedding layer serves to convert the tokens into the input features. Then, the additional position embeddings are added into the input features to encode the sequential order of each input token.

The core concept of the Transformer architecture is the self- attention mechanism, which is adopted in the MHSA block. Specifically, denoted the input features as  $X = [x_{1},x_{2},\dots,x_{n}],$  the MHSA block applies linear projection to them and obtains a set of queries Q, keys K and values  $\mathrm{v}$  as Eq.1:

$$
Q_{i} = XW^{Q_{i}},K_{i} = XW^{K_{i}},V_{i} = XW^{V_{i}}, \tag{1}
$$

where  $W^{Q_i},W^{K_i}$  and  $W^{V_i}$  are the projection matrices corresponding to the  $i$  - th attention head. Then the selfattention operation is applied to each tuple of  $(Q_{i},K_{i},V_{i})$  and get the feature of the  $i$  - th attention head  $Z_{i}$  as Eq.2:

$$
Z_{i} = \mathrm{Attention}(Q_{i},K_{i},V_{i}) = \mathrm{Softmax}\left(\frac{Q_{i}K_{i}^{T}}{\sqrt{d_{k}}}\right)V_{i}, \tag{2}
$$

where  $d_{k}$  is the dimension of the queries (keys). Note that the self- attention operation contains the matrix multiplication operation, its computation complexity is quadratic in the input length. Finally, the MHSA block concatenates the features of all the attention heads and applies a linear projection to them to form its output  $Z$  as Eq.3:

$$
Z = \mathrm{Concat}(Z_1,Z_2,\dots,Z_h)W^O, \tag{3}
$$

where  $W_{O}$  is the projection matrix. As can be seen, the self- attention mechanism allows the model to identify the importance of different input parts regardless of the distance, and thus can capture the long- range dependencies and complex relationships in the input sentence.

Another important module in the Transformer block is the FFN. Typically, FFN is placed after the MHSA block and consists of two linear transformation layers with a nonlinear activation function. It receives the output features  $X$  from the MHSA block and processes them as Eq 4:

$$
\mathrm{FFN}(X) = W_2\sigma (W_1X), \tag{4}
$$

where  $W_{1}$  and  $W_{2}$  denote the weight matrices of the two linear layers, and  $\sigma (\cdot)$  denotes the activation function.

### 2.2 Inference Process of LLMs

The most popular LLMs, i.e., decoder- only LLMs, often adopt the auto- regressive method to generate the output sentence. Specifically, the auto- regressive method generates the tokens one by one. In each generation step, the LLM takes as input the whole token sequences, including the input tokens and previously generated tokens, and generates the next token. With the increase in sequence length, the time cost of the generation process grows rapidly. To address this challenge, a crucial technique, namely key- value

![](images/834c38c67954d9f0ba97dec4ff65879b563d8398b836a9912c966217ee0b6b2c.jpg)  
Fig. 2. Demonstration of the prefilling stage (a) and decoding stage (b).

(KV) cache, has been introduced to expedite the generation process. The KV cache technique, as its name suggests, involves storing and reusing previous key (K) and value (V) pairs within the Multi- Head Self- Attention (MHSA) block. This technique has been widely adopted in LLM inference engines and systems due to its substantial optimization of generation latency. Based on the above methods and techniques, the inference process of LLMs can be divided into two stages:

- Prefilling Stage: The LLM calculates and stores the KV cache of the initial input tokens, and generates the first output token, as shown in Fig. 2(a). Decoding Stage: The LLM generates the output tokens one by one with the KV cache, and then updates it with the key (K) and value (V) pairs of the newly generated token, as shown in Fig. 2(b).

As shown in Fig. 3, we illustrate some critical efficiency indicators. As for the latency, we denote first token latency as the latency to generate the first output token in the prefilling stage, while we denote per- output token latency as the average latency to generate one output token in the decoding stage. Besides, we use generation latency to denote the latency to generate the whole output token sequences. As for the memory, we use model size to denote the memory to store the model weights, and use KV cache size to denote the memory to store the KV cache. Additionally, peak memory denotes the maximum memory usage during the generation process, which is approximately equal to the memory sum of model weights and KV cache. Apart from the latency and memory, throughput is also a widely- used indicator in the LLM serving system. We use token throughput to denote the number of generated tokens per second, and use request throughput to denote the number of completed requests per second.

### 2.3 Efficiency Analysis

Deploying LLMs on resource- constrained scenarios while preserving their powerful capabilities poses a significant challenge for both practitioners and researchers. For instance, let's consider to deploy a LLaMA- 2- 70B model, which contains 70 billion parameters. Storing its weights in FP16 format necessitates 140 GB of VRAM, requiring at least 6 RTX 3090Ti GPUs (each with 24 GB VRAM) or 2 NVIDIA A100 GPUs (each with 80 GB VRAM) for inference. As for latency, generating one token on 2 NVIDIA A100 GPUs requires approximately 100 milliseconds. Consequently, generating a sequence with hundreds of tokens requires more than 10 seconds. In addition to storage and latency, the efficiency indicators, such as throughput, energy and power consumption, also need to be considered. During the LLM inference process, three important factors would largely affect these indicators, i.e., the computational cost, the memory access cost and the memory usage. Yuan et al. [27] provide a more systematic analysis to demonstrate how these factors affect the inference inefficiency with a roofline model. In the following, we further analyze three root causes of inefficiency in the LLM inference process, focusing on the above three key factors:

![](images/af148b6ef4192ab210db32ba99d07ccbe1bba4c54ba7e9a06a2a477ef5bd0237.jpg)  
Fig. 3. Illustration of the memory variation through time (latency) during one generation process. Note that we ignore the activation size in this figure for a simplification.

- Model Size: Mainstream LLMs typically incorporate billions or even trillions of parameters. For instance, the LLaMA-70B model comprises 70 billion parameters, while the GPT-3 model scales up to 175 billion parameters. This considerable model size contributes significantly to the elevated computational cost, memory access cost, and memory usage during the LLM inference process.

- Attention Operation: As illustrated in Sec. 2.1 and Sec. 2.2, in the prefilling stage, the self-attention operation exhibits quadratic computational complexity in the input length. Consequently, as the input length increases, the computational cost, memory access cost, and memory usage of the attention operation escalate rapidly.

- Decoding Approach: The auto-regressive decoding approach generates the tokens one by one. In each decoding step, all the model weights are loaded from the off-chip HBM to the GPU chip, leading to a large memory access cost. In addition, the size of KV cache increases with the growth in the input length, potentially leading to fragmented memory and irregular memory access patterns.

## 3 TAXONOMY

In the aforementioned discussion, we identify key factors (i.e., computational cost, memory access cost and memory usage) that significantly impact the efficiency during

![](images/5183f5bf51a55d93084f90e5779229c3b7fa6c4a07c08a8c277e4984e5d14def.jpg)  
Fig. 4. Taxonomy of efficient inference methods for Large Language Models.

the LLM inference process, and further analyze three root causes (i.e., model size, attention operation and decoding approach). Many efforts have been made to optimize the inference efficiency from different perspectives. By carefully reviewing and summarizing these studies, we classify them into three levels, i.e., data- level optimization, model- level optimization and system- level optimization (as shown in Fig. 4):

- Data-level Optimization refers to improving the efficiency via optimizing the input prompts (i.e., input compression) or better organizing the output content (i.e., output organization). This line of optimization typically does not change the original model, thus is free of costly model training cost (note that a small amount of training for auxiliary models might be required, but this cost can be ignored compared with the training cost for original LLMs).

- Model-level Optimization refers to designing an efficient model structure (i.e., efficient structure design) or compressing the pre-trained models (i.e., model compression) in the inference process to improve its efficiency. This line of optimization (1) often requires costly pre-training or a smaller amount of fine-tuning cost to retain or recover the model ability, and (2) is typically lossy in the model performance.

- System-level Optimization refers to optimizing the inference engine or the serving system. This line of optimization (1) does not involve the costly model training,

and (2) is typically lossless in model performance<sup>1</sup>. In addition, we provide a brief introduction for hardware accelerator design in Sec. 6.3.

## 4 DATA-LEVEL OPTIMIZATION

In the data level, prior studies can be divided into two categories, i.e., input compression and output organization. Input compression techniques directly shorten the model input to reduce the inference cost. While output organization techniques enable batch (parallel) inference via organizing the structure of output content, which can improve the hardware utilization and reduce the generation latency.

### 4.1 Input Compression

In the practical application of LLMs, prompts are crucial. Numerous studies suggest new ways to design prompts effectively and show in practice that well- designed prompts can unleash the capabilities of LLMs. For instance, InContext Learning (ICL) [45] suggests to include multiple relevant examples within the prompt. This approach encourages LLMs to learn through analogy. Chain- of- Thought (CoT) [14] proposes to incorporate a sequence of intermediate reasoning steps within the in- context examples, which help LLMs to conduct complex reasoning. However, these prompting techniques inevitably lead to longer prompts, which poses a challenge because the computational cost and memory usage increase quadratically during the prefilling stage (as illustrated in Sec. 2.3).

To address this challenge, input prompt compression [33] has been proposed to shorten prompts without significantly impacting the quality of answers from LLMs. Within this field, relevant studies are categorized into four groups, as depicted in Figure 5: prompt pruning, prompt summary, soft prompt- based compression, and retrieval- augmented generation.

#### 4.1.1 Prompt Pruning

The core idea behind the prompt pruning is to remove unimportant tokens, sentences, or documents online from each input prompt based on predefined or learnable importance indicators. DYNACL [38] proposes to dynamically decide the optimal number of in- context examples for a given input based on the computational budget via a well- trained LLM- based meta controller. Selective Context [39] proposes to merge tokens into units, and then applies a unit- level prompt pruning based on the self- information indicator (i.e., negative log likelihood). STDC [40] prunes the prompts based on the parse tree, which iteratively removes phrase nodes that cause the smallest performance drop after pruning it. PCRL [41] introduces a token- level pruning scheme based on reinforcement learning. The main idea behind PCRL is to train a policy LLM by combining faithfulness and compression ratio into the reward function. Faithfulness is measured as the output similarity between the compressed prompt and the original prompt. RECOMP [36] implements a sentence- level pruning strategy to compress prompts for Retrieval- Augmented Language Models (RALMs). The approach involves encoding the input question and documents into latent embeddings using a pre- trained encoder. Then, it decides which documents to remove based on the similarity of their embeddings with the question's embedding. LLMLingua [42] introduces a coarse- to- fine pruning scheme for prompt compression. Initially, it performs a demonstration- level pruning followed by token- level pruning based on perplexity. To enhance performance, LLMLingua proposes a budget controller that dynamically allocates the pruning budget across different parts of prompts. Additionally, it utilizes an iterative token- level compression algorithm to address inaccuracies introduced by conditional independence assumptions. Furthermore, LLMLingua incorporates a distribution alignment strategy to align the output distribution of the target LLM with a smaller LLM used for perplexity calculation. LongLLMLingua [43] builds upon LLMLingua with several enhancements: (1) It utilizes perplexity conditioned on the input question as the indicator for prompt pruning. (2) It allocates varying pruning ratios to different demonstrations and reorders the demonstrations within the final prompt based on their indicator values. (3) It restores the original content based on the response. CoT- Influx [44] introduces a coarse- to- grained pruning method for Chain- of- Thought (CoT) prompts using reinforcement learning. Specifically, it prunes unimportant examples, followed by pruning unimportant tokens within the remaining examples.

#### 4.1.2 Prompt Summary

The core idea of prompt summary is to condense the original prompt into a shorter summary while preserving similar semantic information. These techniques also serve as online compression methods for prompts. In contrast to the aforementioned prompt pruning techniques that preserve the unpruned tokens unchanged, this line of methods converts the entire prompt into its summation. RECOMP [36] introduces an Abstractive Compressor that takes an input question and retrieved documents as input, and produces a concise summary. Specifically, it distills a lightweight compressor from the extreme- scale LLMs to perform the summary. SemanticCompression [37] proposes a semantic compression method. It starts by breaking down the text into sentences. Next, it groups sentences together by topic and then summarizes the sentences within each group.

#### 4.1.3 Soft Prompt-based Compression

The core idea of this kind of compression techniques is to design a soft prompt, significantly shorter than the original prompt, for use as input to LLMs. The soft prompt is defined as a sequence of learnable continuous tokens. Some techniques adopt offline compression for the fixed prefix prompt (e.g., system prompt, task- specific prompt). For example, PromptCompression [33] trains a soft prompt to emulate a predetermined system prompt. The approach involves adding several soft tokens before the input tokens and enabling these soft tokens to be adjusted during backpropagation. Following fine- tuning on the prompt dataset, the sequence of soft tokens serves as the soft prompt. Gisting [34] introduces a method to condense task- specific

![](images/937d1f4b8f2126578fbf13c91ec1d47e86cd121dcf3fc765e8efd75f799e6687.jpg)  
Fig. 5. Taxonomy of the input compression methods for Large Language Models.

prompts into a concise set of gist tokens using prefix- tuning [46]. Given that task- specific prompts differ across tasks, prefix- tuning is applied individually for each task. To enhance efficiency, Gisting further introduces a meta- learning approach that predicts gist tokens for new unseen tasks based on the gist tokens of previous tasks.

Other techniques adopt online compression for every new input prompts. For instance, AutoCompressors [30] train a pre- trained LM to compress the prompts into summary vectors via unsupervised learning. ICAE [35] trains an autoencoder to compress the original context into short memory slots. Specifically, ICAE employs a LoRA- adapted LLM as the encoder, and uses the target LLM as the decoder. A set of memory tokens is added before the input tokens and encoded into memory slots.

#### 4.1.4 Retrieval-Augmented Generation

Retrieval- Augmented Generation (RAG) [29] aims to improve the quality of LLMs' responses by incorporating external knowledge sources. RAG can be also viewed as a technique to improve the inference efficiency when handling a large amount of data. Instead of merging all information into an excessively long prompt, RAG only adds relevant retrieved information to the original prompt, ensuring that the model receives necessary information while reducing prompt length significantly. FLARE [30] uses predictions of upcoming sentences to proactively decide when and what information to retrieve. REPLUG [31] treats the LLM as a black box and augments it with a tuneable retrieval model. It prepends retrieved documents to the input for the frozen black- box LLM, and further utilizes the LLM to supervise the retrieval model. Self- RAG [32] enhances LLM's quality and factuality through retrieval and self- reflection. It introduces reflection tokens to make the LLM controllable during the inference phase.

### 4.2 Output Organization

The traditional generation process of LLMs is entirely sequential, leading to significant time consumption. Output organization techniques aim to (partially) parallelize generation via organizing the structure of output content.

Skeleton- of- Thought (SoT) [47] is pioneering in this direction. The core idea behind SoT is to leverage the emerging ability of LLMs to plan the output content's structure. Specifically, SoT consists of two main phases. In the first phase (i.e., skeleton phase), SoT instructs the LLM to generate a concise skeleton of the answer using a predefined "skeleton prompt." For instance, given a question like "What are the typical types of Chinese dishes?", the output at this stage would be a list of dishes (e.g., noodles, hot pot, rice) without elaborate descriptions. Then, in the second phase (i.e., point- expanding phase), SoT instructs the LLM to expand each point in the skeleton simultaneously using a "point- expanding prompt," and then concatenates these expansions to form the final answer. When applied to open- source models, point- expanding can be performed through batch inference, which optimizes hardware utilization and reduces overall generation latency using the same computational resources. To mitigate the additional computation

![](images/697940400a99faa28d0e5f4fc807e822405b9d5f0a0e908c271f6484fd6b89b3.jpg)  
Fig. 6. Demonstration of the inference process of SoT.

overhead brought by the extra prompt (i.e., skeleton prompt and point- expanding prompt), SoT discusses the possibility of sharing the KV cache of the common prompt prefix across multiple points in the point expansion phase. Additionally, SoT uses a router model to decide whether applying SoT is appropriate for specific questions, aiming to limit its use to suitable cases. As a result, SoT achieves up to a  $2.39\times$  speedup on 12 recently released LLMs, and improves the answer quality for many questions by improving the diversity and relevance of their answer.

SGD [48] further extends the idea of SoT by organizing sub- problem points into a Directed Acyclic Graph (DAG) and answering the logic- independent sub- problems in parallel in one turn. Similar to SoT, SGD also leverages the emerging ability of LLMs to generate the output structure by providing manually- crafted prompts along with several examples. SGD relaxes the strict independence assumption among different points to enhance the quality of answers, especially for math and coding problems. Compared with SoT, SGD prioritizes answer quality over speed. Additionally, SGD introduces an adaptive model selection approach,

assigning an optimal model size to handle each sub- problem based on its estimated complexity, thus further improving efficiency.

APAR [49] adopts a similar idea with SoT, leveraging LLMs to output special control tokens (i.e., [fork]) for automatically and dynamically triggering the parallel decoding. To effectively exploit the inherent parallelizable structure within the output content and accurately generate control tokens, APAR fine- tunes the LLMs on carefully- designed data that formed in specific tree structure. As a result, APAR achieves an average  $1.4\sim 2.0\times$  speed- up on benchmarks and cases a negligible impact on the answer quality. Furthermore, APAR combines their decoding approach with the speculative decoding technique (i.e., Medusa [50]) and serving system (i.e. vLLM [51]) to further improve the inference latency and system throughput, respectively.

SGLang [52] introduces a domain- specific language (DSL) in Python featuring primitives that flexibly facilitate LLM programming. The core idea behind SGLang is to analyze dependencies among various generation calls automatically, and perform batch inference and KV cache sharing based on this analysis. With this language, users can implement various prompting strategies easily and benefit from the automatic efficiency optimization of SGLang (e.g., SoT [47], ToT [53]). Furthermore, SGLang introduces and combines several system- level compilation techniques, such as code movement and prefetching annotations.

### 4.3 Knowledge, Suggestions and Future Direction

The growing demand for LLMs to handle longer inputs and generate longer outputs highlights the importance of the data- level optimization techniques. Within these techniques, input compression methods primarily target enhancing the prefilling stage by diminishing the computational and memory cost resulting from the attention operation. Additionally, for API- based LLMs, these methods can reduce the API cost associated with input tokens. In contrast, output organization methods concentrate on optimizing the decoding stage by alleviating the substantial memory access cost associated with auto- regressive decoding approach.

As LLMs become more and more capable, there is potential to utilize them to compress the input prompts or structure the output content. Recent advancements in output organization methods [47], [48], [49] demonstrate the effectiveness of leveraging LLMs to organize the output content into independent points or a dependency graph, facilitating batch inference for improving generation latency. These methods capitalize on the inherent parallelizable structure within output content, enabling LLMs to perform parallel decoding to enhance hardware utilization and thereby reduce end- to- end generation latency.

Recently, diverse prompting pipelines (e.g., ToT [53], GoT [54]) and agent frameworks [55], [56], [57] are emerging. While these innovations enhance LLMs' capabilities, they also extend the length of inputs, leading to increased computational cost. To address this challenge, adopting input compression techniques to reduce input length shows promise as a solution. Simultaneously, these pipelines and frameworks naturally introduce more parallelism into output structures, offering increased potential for parallel decoding and key- value (KV) cache sharing across different decoding threads. SGLang [52] supports flexible LLM programming and offers opportunities for front- end and backend co- optimization, laying the groundwork for further extensions and improvements in this area. In summary, data- level optimization, including input compression and output organization techniques, would become increasingly necessary to enhance efficiency in the foreseeable future. In addition to optimizing the efficiency of existing frameworks, certain studies focus on designing more efficient agent frameworks directly. For example, FrugalGPT [58] proposes a model cascade comprising LLMs of varying sizes, with the inference process being halted early if the model reaches a sufficient level of certainty regarding the answer. This approach aims to achieve efficiency by leveraging a tiered model architecture and intelligent inference termination based on model confidence estimation. Compared with model- level dynamic inference techniques (Sec. 5.2.5), FrugalGPT performs dynamic inference at the pipeline level.

## 5 MODEL-LEVEL OPTIMIZATION

The model- level optimization for LLM efficient inference mainly concentrates on optimizing the model structure or data representation. Model structure optimization involves directly designing efficient model structure, modifying the original model and adjusting the inference- time architecture. In terms of data representation optimization, the model quantization technique is commonly employed.

In this section, we categorize model- level optimization techniques based on the additional training overhead they require. The first category involves designing more efficient model structures (referred to as efficient structure design). Models developed using this approach typically require training from scratch. The second category focuses on compressing pre- trained models (referred to as model compression). Compressed models in this category generally require only minimal fine- tuning to restore their performance.

### 5.1 Efficient Structure Design

Currently, state- of- the- art LLMs commonly employ the Transformer architecture, as discussed in Section 2.1. However, the key components of Transformer- based LLMs, including the Feed Forward Network (FFN) and attention operation, present efficiency challenges during inference. We identify the causes as follows:

- The FFN contributes a substantial portion of the model parameters in Transformer-based LLMs, resulting in significant memory access cost and memory usage, particularly during the decoding stage. For instance, the FFN module accounts for  $63.01\%$  of the parameters in the LLaMA-7B model and  $71.69\%$  in the LLaMA-70B model.- The attention operation demonstrates quadratic complexity in the input length, leading to substantial computational cost and memory usage, especially when dealing with longer input contexts.

To tackle these efficiency challenges, several studies have concentrated on developing more efficient model structures. We categorize these studies into three groups (as depicted in Fig. 7): efficient FFN design, efficient attention design, and Transformer alternates.

![](images/e7ac172485b5d57507b11c7ce4aa6cb2e861cc06d6d874686df6d78d448a88f7.jpg)  
Fig. 7. Taxonomy of the efficient structure design for Large Language Models.

#### 5.1.1 Efficient FFN Design

In this field, many studies concentrate on integrating the Mixture- of- Experts (MoE) technique [98] into LLMs to enhance their performance while maintaining the computational cost. The core idea of MoE is to dynamically allocate varying computational budgets to different input tokens. In MoE- based Transformers, multiple parallel Feed Forward Networks (FFNs), namely experts, are utilized alongside a trainable routing module. During inference, the model selectively activates specific experts for each token controlled by the routing module.

Some researches concentrate on the construction of FFN expert, which mainly focus on optimizing the process of acquiring expert weights or making these experts more lightweight for efficiency. For instance, MoE- Eification [89] devises a method to transform a non- MoE LLM into the MoE version using its pre- trained weights. This approach eliminates the need for expensive pre- training of the MoE model. To accomplish this, MoE- Eification first divides FFN neurons of the pre- trained LLM into multiple groups. Within each group, the neurons are commonly activated simultaneously by the activation function. Then, it restructures each group of neurons as an expert. Sparse Upcycling [91] introduces a method to initialize the weights of MoE- based LLM directly from a dense model's checkpoint. In this approach, the experts within the MoE- based LLM are exact replicas of the FFN from the dense model. By employing this straightforward initialization, Sparse Upcycling can efficiently train the MoE model to achieve high performance. MPOE [90] proposes to reduce the parameters of MoE- based LLMs through Matrix Product Operators (MPO) decomposition. This method involves decomposing each weight matrix of the FFN into a global shared tensor containing common information and a set of local auxiliary tensors that capture specialized features.

Another line of researches focuses on improving the design of the routing module (or strategy) within MoE models. In previous MoE models, the routing module often causes the load imbalance problem, which denotes that some experts are assigned a large number of tokens while the others handle only a few. This imbalance not only wastes the capacities of the under- utilized experts, which degrades model performance, but also degrades the inference efficiency. Current MoE implementations [88], [99], [100] often use batched matrix multiplication to compute all FFN experts simultaneously. This requires that the input matrices of each expert must have the same shape. However, since the load imbalance problem exists, input token sets for these under- utilized experts are needed to be padded to meet the shape constraint, resulting in a waste of computation. Therefore, the major aim of routing module design is achieving better balance in token assignment for MoE experts. Switch Transformers [88] introduces an additional loss, namely the load balancing loss, into the final loss function to penalize imbalanced assignments by the routing module. This loss is formulated as the scaled dot- product between the token assignment fraction vector and a uniform distribution vector. As a result, the loss is minimized only when the token assignment is balanced across all experts. This approach encourages the routing module to distribute tokens evenly among experts, promoting load balance and ultimately improving model performance and efficiency. BASE [92] learns an embedding for each expert in an end- to- end manner and then assigns experts to tokens based on the similarity of their embeddings. To ensure load balance, BASE formulates a linear assignment problem and utilizes the auction algorithm [101] to solve this problem efficiently. Expert Choice [93] introduces a simple yet effective strategy to ensure perfect load balance within MoE- based models. Unlike previous methods that assign experts to tokens, Expert Choice allows each expert to independently select the top-  $k$  tokens based on their embedding similarities. This approach ensures that each expert handles a fixed number of tokens, even though each token might be assigned to a different number of experts.

In addition to the aforementioned researches focusing

on the model architecture itself, there are also studies that concentrate on improving the training methods for MoE- based models. SE- MoE [94] introduces a new auxiliary loss called the router z- loss, which aims to enhance the stability of model training without compromising performance. SE- MoE identifies that the exponential functions introduced by softmax operations in the routing module can exacerbate roundoff errors, leading to training instability. To address this issue, the router z- loss penalizes large logits that are input into exponential functions, thereby minimizing roundoff errors during training. StableMoE [95] points out the routing fluctuation problem existing in the MoE- based LLMs, which denotes the inconsistency of the expert assignment in the training and inference stage. For the same input token, it is assigned to different experts along with training, but only activates one expert at inference time. To address this issue, StableMoE suggests a more consistent training approach. It first learns a routing strategy and then keeps it fixed during both the model backbone training and the inference stage. SMoE- Dropout [96] designs a novel training method for MoE- based LLMs, which proposes to gradually increase the number of activated experts during the training process. This approach enhances the scalability of MoE- based models for inference and downstream fine- tuning. GLaM [97] pre- trains and releases a series of models with various parameter sizes, demonstrating their comparable performance to dense LLMs on few- shot tasks. The largest model in this family has a parameter size of up to 1.2 trillion. Mixtral 8x7B [12] is a remarkable recently released open- source model. During inference, it utilizes only 13 billion active parameters and achieves superior performance compared to the LLaMA- 2- 70B model across different benchmarks. Mixtral 8x7B consists of 8 Feed- Forward Network (FFN) experts in each layer, with each token assigned to two experts during inference.

#### 5.1.2 Efficient Attention Design

The attention operation is a critical component in the Transformer architecture. However, its quadratic complexity in relation to input length leads to substantial computational cost, memory access cost, and memory usage, especially when dealing with long contexts. To address this issue, researchers are exploring more efficient approaches to approximate the functionality of the original attention operation. These studies can be broadly categorized into two main branches: multi- query attention and low- complexity attention.

Multi- Query Attention. Multi- query attention (MQA) [77] optimizes the attention operation by sharing the key (K) and value (V) cache across different attention heads. This strategy effectively reduces both memory access cost and memory usage during inference, contributing to improved efficiency in Transformer models. As introduced in Sec. 2.2, the Transformer- style LLMs typically adopts multi- head attention (MHA) operation. This operation requires storing and retrieving K and V pairs for each attention head during the decoding stage, leading to substantial increases in memory access cost and memory usage. MQA tackles this challenge by using the same K and V pairs across different heads while maintaining distinct query (Q) values. Through extensive testing, it has been demonstrated that

MQA significantly reduces memory requirements with only a minimal impact on model performance, making it a crucial strategy for enhancing inference efficiency. The concept of MQA is further extended by Grouped- query attention (GQA) [78], which can be seen as a blend of MHA and MQA. Specifically, GQA segments the attention heads into groups, storing a single set of K and V values for each group. This method not only sustains the benefits of MQA in reducing memory overhead but also offers an enhanced balance between inference speed and output quality.

Low- Complexity Attention. Low- complexity attention methods aim to design new mechanisms that reduce the computational complexity of each attention head. To simplify the discussion, we assume that the dimensions of the Q (query), K (key), and V (value) matrices are identical, with  $Q, K, V \in \mathbb{R}^{n \times d}$ . Since the following work does not involve altering the number of attention heads like MQA, our discussions focus on the attention mechanism within each head. As introduced in Section 2.2, the computational complexity of the conventional attention mechanism scales as  $\mathcal{O}(n^2)$ , exhibiting quadratic growth with respect to the input length  $n$ . To address the inefficiency issue, kernel- based attention and low- rank attention methods are proposed to reduce the complexity to  $\mathcal{O}(n)$ .

- Kernel-based Attention. Kernel-based attention designs kernel  $\phi$  to approximate the non-linear softmax operation of  $\operatorname {Softmax}(QK^T)$  with a linear dot product between kernel-transformed feature maps, i.e.,  $\phi (Q)\phi (K)^T$ . It avoids the conventional quadratic computation associated with  $QK^T \in \mathbb{R}^{n \times n}$  by prioritizing the computation of  $\phi (K)^T V \in \mathbb{R}^{n \times d}$ , followed by its multiplication with  $\phi (Q) \in \mathbb{R}^{n \times d}$ . Specifically, the input Q and K matrices are first mapped into kernel space using a kernel function  $\phi$ , while maintaining their original dimensions. Leveraging the associative property of matrix multiplication allows for the multiplication of K and V prior to their interaction with Q. The attention mechanism is reformulated as:

$$
\operatorname {Softmax}(QK^T)V\approx \phi (Q)(\phi (K)^T V), \tag{5}
$$

where  $\phi (Q), \phi (K) \in \mathbb{R}^{n \times d}$ . This strategy effectively reduces the computational complexity to  $\mathcal{O}(nd^2)$ , rendering it linear with respect to the input length. Linear Transformer [84] is the first work to propose the kernel- based attention. It adopts  $\phi (x) = \mathrm{elu}(x) + 1$  as the kernel function, where  $\mathrm{elu}(\cdot)$  denotes the exponential linear unit activation function. Performers [85] and RFA [86] proposes to use random feature projection to better approximate the softmax function. PolySketchFormer [87] employs polynomial functions and sketching techniques to approximate the softmax function.

- Low-Rank Attention. Low-Rank Attention technique employs compression on the token dimensions (i.e.,  $n$ ) of the K and V matrices to a smaller, fixed length (i.e.,  $k$ ) before performing the attention computation. The approach is based on the insight that the  $n \times n$  attention matrix often exhibits a low-rank property, making it feasible to compress it in the token dimension. The main focus of this line of researches is to design effective methods for the compression, where  $X$  can be context matrix or K and V matrices:

$$
X\in \mathbb{R}^{n\times d}\to X^{\prime}\in \mathbb{R}^{k\times d}. \tag{6}
$$

One line of work uses linear projection to compress the token dimension. It is done by multiplying K and V matrices with projection matrices  $P_{k},P_{v}\in \mathbb{R}^{k\times n}$  . In this way, the computational complexity of the attention operation is reduced to  $\mathcal{O}(nkd)$  , which is linear to the input length. Linformer [79] first observes and analyses the low- rank property of the attention map, and proposes the low- rank attention framework. LRT [80] proposes to simultaneously apply low- rank transformation to both attention block and FFN to further improve the computational efficiency. FLuRKA [81] combines the low- rank transformation and kernalization to the attention matrices to further improve the efficiency. Specifically, it first reduces the token dimension of K and V matrices, and then applies kernel function to the Q and low- rank K matrices.

Aside from linear projection, other token- dimension compression methods are also proposed. Luna [82] and Set Transformer [83] leverage additional attention computations alongside smaller queries to effectively compress the K and V matrices. Luna [82] involves an extra query matrix of fixed length  $k$  . The small query performs attention with the original context matrix, termed as pack attention, to compress the context matrix to size  $\mathbb{R}^{k\times d}$  Subsequently, the regular attention, termed unpack attention, applies attention to the original Q matrices and the compressed K and  $\mathrm{v}$  matrices. The extra query matrix can be learnable parameters or acquired from previous layers. Set Transformer [83] designs the similar technique by introducing an inducing points vector with fixed length. Unlike previous works that compress K and V, FunnelTransformer [102] uses pooling operation to gradually compress the sequence length of the Q matrix.

#### 5.1.3 Transformer Alternates

In addition to applying efficient techniques to the attention operation, recent studies have also innovated to design sequence modeling architectures that are efficient yet effective. Table 2 compares the efficiency of some representative non- Transformer models. These architectures exhibit sub- quadratic computational complexity with respect to sequence length during both training and inference, enabling LLMs to significantly increase their context length.

Within this research field, two prominent lines of study have garnered significant attention. One line of studies concentrates on the State Space Model (SSM), which formulates sequence modeling as a recurrence transformation based on the HiPPO theory [64]. Additionally, other studies primarily focus on employing long convolutions or designing attention- like formulations to model sequences.

State Space Model. The State Space Model (SSM) has demonstrated competitive modeling capabilities in certain Natural Language Processing (NLP) [75] and and Computer Vision (CV) [103] tasks. Compared to attention- based Transformers, SSM exhibits linear computational and memory complexity with respect to the input sequence length, which enhances its efficiency in handling long- context sequences. In this survey, SSM refers to a series of model architectures that satisfy the following two properties: (1) They model sequence based on the following formulation proposed by HiPPO [64] and LSSL [65]:

$$
\begin{array}{l}{x_k = \overline{A} x_{k - 1} + \overline{B} u_k,}\\ {y_k = \overline{C} x_k,} \end{array} \tag{7}
$$

where  $\overline{A},\overline{B}$  and  $\overline{C}$  denote the transition matrices,  $\mathcal{X}$  denotes the intermediate state and  $u$  denotes the input sequence. (2) They design the transition matrix  $A$  based on the HiPPO theory [64]. Specifically, HiPPO proposes to compress the input sequence into a sequence of coefficients (namely state) by projecting it onto a set of polynomial bases.

Building upon the aforementioned framework, several studies concentrate on improving the parameterization or initialization of the transition matrix  $A$  .This involves refining how the matrix is formulated or initialized within the SSM to enhance its effectiveness and performance in sequence modeling tasks. LSSL [65] firstly proposes to initialize  $A$  with the optimal transition matrix HiPPO- LegS designed by HiPPO. In addition, LSSL also trains the SSM in a convolution manner by unrolling the Eq. 7. Specifically, through a convolution kernel defined as  $\mathcal{K}_L(A,B,C) =$ $(CA^{i}B)_{i\in [L]} = (CB,CAB,\dots,CA^{L - 1}B),$  the Eq. 7 can be rewritten as  $y = \kappa_{L}(\overline{A},\overline{B},\overline{C})*u$  and also can be computed efficiently via Fast Fourier Transform (FFT). However, computing this convolution kernel is expensive, since it requires multiple times of multiplication by  $A$  To this end, S4 [66], DSS [67] and S4D [68] propose to diagonalize the matrix  $A_{i}$  which can accelerate the computing. This can be seen as a parameterization technique to the transition matrix  $A$  . Previous SSMs processed each input dimension independently, resulting in a large number of trainable parameters. To enhance efficiency, S5 [72] proposes to simultaneously process all input dimensions using a single set of parameters. Building upon this structure, S5 introduces a parameterization and initialization method for  $A$  based on the standard HiPPO matrix. Liquid S4 [71] and Mamba [75] parameterize the transition matrices in a input- dependent manner, which further enhances the modeling capability of SSM. Additionally, both S5 [72] and Mamba [75] adopt a parallel scan technique for efficient model training without the need for convolution operations. This technique offers advantages in implementation and deployment on modern CPU hardware.

Another line of research aim to design better model architecture based on SSMs.GSS [69] and BiGS [74] combines the Gated Attention Unit (GAU) [104] with SSM. Specifically, they replace the attention operation in GAU with SSM operation. BST [73] combines the SSM model with the proposed Block Transformer which introduces a strong local inductive bias. H3 [70] observes that SSM is weak in recalling the earlier tokens and comparing a token across the sequence. To this end, it proposes to add a shift SSM operation before the standard SSM operation, which is used to directly shift the input tokens into the state. MambaFormer [76] combines the standard Transformer and SSM model by substituting the FFN layer in the Transformer with an SSM layer. Jamba [105] introduces another approach to combining the Transformer and SSM models by adding four Transformer layers into an SSM model. DenseMamba [106] explores the issue of hidden state degradation

TABLE 2 Efficiency comparison of some novel non-Transformer models. Note that we denote  $n$  as the input length and  $d$  as the input dimension.  

<table><tr><td rowspan="2">Model</td><td rowspan="2">Training Form</td><td rowspan="2">Training Computational Complexity</td><td rowspan="2">Training Memory Complexity</td><td rowspan="2">Inference Form</td><td colspan="2">Inference Computational Complexity</td></tr><tr><td>Prefilling</td><td>Decoding (per token)</td></tr><tr><td>Transformer [26]</td><td>Transformer-like</td><td>O(n²d)</td><td>O(n²+nd)</td><td>Transformer-like</td><td>O(n²d)</td><td>O(nd)</td></tr><tr><td>S4 [66]</td><td>Convolution</td><td>O(nd²logn)</td><td>O(nd)</td><td>Recurrence</td><td>O(nd²)</td><td>O(d²)</td></tr><tr><td>Mamba [75]</td><td>Recurrence</td><td>O(nd²logn)</td><td>O(nd)</td><td>Recurrence</td><td>O(nd²)</td><td>O(d²)</td></tr><tr><td>Hyena [61]</td><td>Convolution</td><td>O(ndlogn)</td><td>O(nd)</td><td>Convolution</td><td>O(ndlogn)</td><td>O(ndlogn)</td></tr><tr><td>RetNet [63]</td><td>Transformer-like</td><td>O(n²d)</td><td>O(n²+nd)</td><td>Recurrence</td><td>O(nd²)</td><td>O(d²)</td></tr><tr><td>RWKV [62]</td><td>Recurrence</td><td>O(nd²)</td><td>O(nd)</td><td>Recurrence</td><td>O(nd²)</td><td>O(d²)</td></tr></table>

in traditional SSMs and introduces dense connections within the SSM architecture to preserve fine- grained information across deeper layers of the model. BlackMamba [107] and MoE- Mamba [108] propose to enhance SSM models with the Mixture- of- Experts (MoE) technique to optimize the training and inference efficiency while maintaining the model performance.

Other Alternates. In addition to SSMs, several other efficient alternates have also garnered significant attention, including long convolution and attention- like recurrence operation.

Several recent studies have applied long convolution in the context of modeling long sequences [59], [60], [61]. These investigations primarily concentrate on refining the parameterization of the convolution kernel. For instance, Hyena [61] employs an data- dependent parameterization method for long convolutions using a shallow feed- forward neural network (FFN).

Other studies [62], [63] aim to design the operation that has a similar form as the attention operation but can be enrolled to the recurrent manner, enabling both efficient training and efficient inference. For instance, RWKV [62] builds upon AFT [109], which proposes to substitute the attention operation in the Transformer model with the following equation:

$$
Y_{t} = \sigma_{q}(Q_{t})\odot \frac{\sum_{t^{\prime} = 1}^{T}\exp(K_{t^{\prime}} + w_{t,t^{\prime}})\odot V_{t^{\prime}}}{\sum_{t^{\prime} = 1}^{T}\exp(K_{t^{\prime}} + w_{t,t^{\prime}})}, \tag{8}
$$

where  $Q,K,$  and  $V$  are the query, key, and value matrices as in Transformer,  $w\in \mathbb{R}^{T\times T}$  denotes a learnable pairwise position bias and  $\sigma_{q}(v)$  denotes a non- linear function. Specifically, it further reparameterizes the position bias as  $w_{t,t^{\prime}} = - (t - t^{\prime})w,$  and thus can rewrite Eq. 8 in a recursive form. In this way, RWKV can combine the effective paral- . lolizable training feature of Transformer and the efficient inference ability of RNN.

Efficiency Analysis. We analyze and compare the computational and memory complexity of several innovative and representative non- transformer architectures in Table 2. In terms of training time, many studies (e.g., S4, Hyena, RetNet) aim to preserve training parallelism by adopting training forms such as the convolution or attention. Notably, Mamba utilizes parallel scan techniques for processing input sequences, thereby leveraging training parallelism as well.

On the other hand, during inference, most studies opt for recurrent architectures to maintain linear computational complexity in the prefilling stage and to remain context length- agnostic in the decoding stage. Furthermore, in the decoding phase, these novel architectures eliminate the need to cache and load features of previous tokens (similar to the key- value cache in Transformer- based language models), resulting in significant memory access cost savings.

### 5.2 Model Compression

Model compression encompasses a range of techniques designed to enhance the inference efficiency of a pre- trained model by modifying its data representation (e.g., quantization) or altering its architecture (e.g., sparsification, structural optimization, and dynamic inference), as depicted in Fig. 8.

#### 5.2.1 Quantization

Quantization is a widely employed technique that reduces the computational and memory cost of LLMs by converting the models' weights and activations from high bit- width to low bit- width representations. Specifically, many methods involve quantizing FP16 tensors into low- bit integer tensors, which can be represented as follows:

$$
\mathbf{X}_{\mathrm{INT}} = \left[\frac{\mathbf{X}_{\mathrm{FP16}} - Z}{S}\right], \tag{9}
$$

$$
S = \frac{\max (\mathbf{X}_{\mathrm{FP16}}) - \min (\mathbf{X}_{\mathrm{FP16}})}{2^{N - 1} - 1}, \tag{10}
$$

where  $X_{\mathrm{FP16}}$  denotes the 16- bit floating- point (FP16) value,  $X_{\mathrm{INT}}$  denotes the low- precision integer value,  $N$  denotes the number of bits, and  $S$  and  $Z$  denote the scaling factor and zero- point.

In the following, we start with an efficiency analysis to illustrate how quantization techniques reduce the end- toend inference latency of LLMs. Subsequently, we offer a detailed introduction to two distinct quantization workflows: Post- Training Quantization (PTQ) and Quantization- Aware Training (QAT), respectively.

Efficiency Analysis. As discussed in Section 2.2, the inference process of LLMs involves two stages: the prefilling stage and the decoding stage. During the prefilling stage, LLMs typically handle long token sequences, and the primary operation is general matrix multiplication (GEMM). The latency of the prefilling stage is primarily constrained by the computation performed by high- precision CUDA Cores. To address this challenge, existing methods quantize both weights and activations to accelerate computation using low- precision Tensor Cores. As illustrated in Figure 9 (b), activation quantization is performed online before each GEMM operation, allowing computation with low- precision

![](images/27406148c4382ceb95376574f1123e253f5631347bd02fee820392450564b707.jpg)  
Fig. 8. Taxonomy of model compression methods for Large Language Models.

Tensor Cores (e.g., INT8). This quantization approach is referred to as Weight- Activation Quantization.

In contrast, during the decoding stage, LLMs process only one token at each generation step using general matrix- vector multiplication (GEMV) as the core operation. The latency of the decoding stage is mainly influenced by the loading of large weight tensors. To tackle this challenge, existing methods focus on quantizing only the weights to accelerate memory access. This method, known as Weight- only Quantization, involves offline quantization of weights, followed by de- quantization of the low- precision weights into FP16 format for computation, as shown in Figure 9 (a).

Post- Training Quantization. Post- training quantization (PTQ) involves quantizing pre- trained models without the need for retraining, which can be a costly process. While PTQ methods have been well- explored for smaller models, applying existing quantization techniques directly to LLMs presents challenges. This is primarily because the weights and activations of LLMs often exhibit more outliers and have a wider distribution range compared to smaller

TABLE 3 Summary of the rrtive stdies on Post-Training Quantization. Quantized Tesor Type denotes which parts of tensors are quantized. Data Format d h  t  t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t the parameters (e.g., scaling factor, zero-point). Quantized Value Update denotes whether to change the model weight (e.g., compensation, re-parameterization) during the quantization process.  

<table><tr><td rowspan="2">Model</td><td colspan="3">Quantized Tensor Type</td><td rowspan="2">Data 
Format</td><td rowspan="2">Quantization Parameter 
Determination Scheme</td><td rowspan="2">Quantized 
Value Update</td></tr><tr><td>Weight</td><td>Activation</td><td>KV Cache</td></tr><tr><td>GPTQ [192]</td><td>✓</td><td></td><td></td><td>Uniform</td><td>Statistic-based</td><td>✓</td></tr><tr><td>LUT-GEMM [193]</td><td>✓</td><td></td><td></td><td>Non-uniform</td><td>Statistic-based</td><td>✓</td></tr><tr><td>AWQ [194]</td><td>✓</td><td></td><td></td><td>Uniform</td><td>Search-based</td><td>✓</td></tr><tr><td>SqueezeLLM [197]</td><td>✓</td><td></td><td></td><td>Non-uniform</td><td>Statistic-based</td><td></td></tr><tr><td>LLM.int8() [204]</td><td>✓</td><td>✓</td><td></td><td>Uniform</td><td>Statistic-based</td><td></td></tr><tr><td>SmoothQuant [205]</td><td>✓</td><td>✓</td><td></td><td>Uniform</td><td>Statistic-based</td><td>✓</td></tr><tr><td>RPTQ [207]</td><td>✓</td><td>✓</td><td></td><td>Uniform</td><td>Statistic-based</td><td></td></tr><tr><td>OmniQuant [210]</td><td>✓</td><td>✓</td><td></td><td>Uniform</td><td>Search-based</td><td></td></tr><tr><td>FlexGen [203]</td><td>✓</td><td></td><td></td><td>Uniform</td><td>Statistic-based</td><td></td></tr><tr><td>Atom [212]</td><td>✓</td><td>✓</td><td></td><td>Uniform</td><td>Statistic-based</td><td></td></tr><tr><td>KVQuanti [219]</td><td></td><td></td><td></td><td>Non-uniform</td><td>Statistic-based</td><td></td></tr><tr><td>KIVI [220]</td><td></td><td></td><td></td><td>Uniform</td><td>Statistic-based</td><td></td></tr></table>

![](images/1ea65fd7b688e2bd92b4037cfa322669b73ab8a01a24282052b9fa7347c51cbf.jpg)  
Fig. 9. (a) The inference workflow of Weight-only Quantization. (b) The inference workflow of Weight-Activation Quantization.

models, making their quantization more challenging. In summary, the complex nature of LLMs, characterized by their size and complexity, requires specialized approaches to effectively handle the quantization process. The presence of outliers and wider distribution ranges in LLMs necessitates the development of tailored quantization techniques that can account for these unique characteristics without compromising model performance or efficiency.

Numerous studies have concentrated on developing effective quantization algorithms to compress LLMs. We provide a synthesis of representative algorithms categorized across four dimensions in Tab. 3. Regarding the types of quantized tensors, certain studies [192], [193], [194], [197] concentrate on weight- only quantization, whereas many others [204], [205], [207] focus on quantizing both weights and activations. Notably, in LLMs, the KV cache represents a distinctive component that impacts memory and memory access. Consequently, some investigations [203], [212], [219] propose KV cache quantization. Regarding data formats, the majority of algorithms adopt a uniform format for straightforward hardware implementation. Concerning the determination of quantized parameters (e.g., scale, zeropoint), most studies rely on statistics derived from weight or activation values. Nevertheless, some research efforts [194], [210] advocate for searching optimal parameters based on reconstruction loss. Furthermore, certain studies [192], [194], [205] suggest updating unquantized weights (referred to as Quantized Value Update) before or during the quantization process to enhance performance.

In weight- only quantization, GPTQ [192] represents an early advancement in LLM quantization, building upon the traditional algorithm OBQ [221]. OBQ utilizes an optimal quantization order per row of the weight matrix, guided by the reconstruction error relative to the Hessian matrix of unquantized weights. After each quantization step, OBQ iteratively adjusts the unquantized weights to mitigate reconstruction errors. However, the frequent updating of the Hessian matrix during quantization escalates computational complexity. GPTQ streamlines this process by adopting a uniform left- to- right order for quantizing each row, thus circumventing the need for extensive Hessian matrix updates. This strategy substantially reduces computational demands by computing the Hessian matrix solely during the quantization of one row, then leveraging the computing results for subsequent rows, expediting the overall quantization procedure. LUT- GEMM [193] presents a novel dequantization method utilizing a Look- Up Table (LUT), aiming to accelerate the inference process of quantized LLMs by reducing the dequantization overhead. Additionally, it adopts a non- uniform quantization approach known as Binary- Coding Quantization (BCQ), which incorporates learnable quantization intervals. AWQ [194] observes that weight channels vary in importance for performance, particularly emphasizing those aligned with input channels exhibiting outliers in activations. To enhance the preservation of critical weight channels, AWQ utilizes a reparameterization method. This technique selects reparameterization coefficients via grid search to minimize reconstruction errors effectively. OWQ [195] observes the difficulty of quantizing weights associated with activation outliers. To address this challenge, OWQ employs a mixed- precision quantization strategy. This method identifies weak columns in

the weight matrix and allocates higher precision to these specific weights, while quantizing the rest of the weights at a lower precision level. SpQR [196] introduces a methodology where weight outliers are identified and allocated higher precision during quantization, while the rest of the weights are quantized to 3 bits. SqueezeLLM [197] proposes to store the outliers in a full- precision sparse matrix, and apply non- uniform quantization to the remaining weights. The values for non- uniform quantization are determined based on quantization sensitivity, which contributes to improved performance of the quantized model. QuIP [198] introduces LDLQ, an optimal adaptive method for a quadratic proxy objective. The study reveals that ensuring incoherence between weight and Hessian matrices can enhance the effectiveness of LDLQ. QuIP utilizes LDLQ and achieves incoherence by employing random orthogonal matrix multiplication. FineQuant [199] utilizes a heuristic approach to determine the granularity of quantization per column, combining empirical insights gained from experiments to design a quantization scheme. QuantEase [200] builds upon GPTQ. When quantizing each layer, it proposes a method based on Coordinate Descent to compensate for the unquantized weights more precisely. Additionally, QuantEase can leverage quantized weights from GPTQ as an initialization and further refine the compensation process. LLMMQ [201] protects the weight outliers with FP16 format, and stores them in Compressed Sparse Row (CSR) format for efficient computation. Besides, LLM- MQ models the bitwidth assignment to each layer as an integer programming problem, and employs an efficient solver to solve it within a few seconds. Moveover, LLM- MQ designs a efficient CUDA kernel to integrate dequantization operators, thereby reducing memory access cost during computation. Inspired by the equivalent transformations used in the previous PTQ methods, AffineQuant [215] firstly introduces equivalent affine transformations in quantization, which extends the optimization scope and further reduces the quantization errors. Recently, many studies [198], [216], [217], [218] follows the computational invariance idea, by multiplying rotation matrices to the weight matrices and activation matrices. In this way, they can effectively eliminate the outliers in the weights and activations, thus help to quantize the LLMs. These studies use different rotation matrices. For example, QuaRot [217] applies randomize Hadamard transformations to the weights and activations. SpinQuant [218] finds the optimal rotation matrices by training on a small validation dataset.

For weight- activation quantization, ZeroQuant [202] employs finer- grained quantization for weights and activations, leveraging kernel fusion to minimize the memory access cost during quantization and conducting layer- bylayer knowledge distillation to recover the performance. FlexGen [203] quantizes weights and KV cache directly into INT4 to reduce the memory footprint during inference with large batch sizes. LLM.int8() [204] identifies that outliers in activations are concentrated within a small subset of channels. Leveraging this insight, LLM.int8() splits activations and weights into two distinct parts based on the outlier distribution within input channels to minimize quantization errors in activations. Channels containing outlier data in both activations and weights are stored in

FP16 format, while other channels are stored in INT8 format. SmoothQuant [205] employs a reparameterization technique to address the challenges of quantizing activation values. This method introduces a scaling factor that expands the data range of weight channels while shrinking the data range of corresponding activation channels. ZeroQuant [202] introduces a group- wise quantization strategy for weights and a token- wise quantization approach for activations. Building upon this methodology, ZeroQuant- V2 [206] presents the LoRC (Low- Rank Compensation) technique, employing low- rank matrices to mitigate quantization inaccuracies. RPTQ [207] identifies substantial variations in the distribution of different activation channels, which present challenges for quantization. To mitigate this issue, RPTQ reorganizes channels with similar activation distributions into clusters and independently applies quantization within each cluster. OliVe [208] observes that the normal values neighboring to the outliers are less critical. Therefore, it pairs each outlier with a normal value, sacrificing the latter to achieve a broader representation range for outliers. OS+ [169] observes that the distribution of outliers is concentrated and asymmetrical, posing a challenge to LLM quantization. To address this, OS+ introduces a channel- wise shifting and scaling technique aimed at alleviating these challenges. The shifting and scaling parameters are determined through a search process to effectively handle the concentrated and asymmetrical outlier distribution. ZeroQuant- FP [209] investigates the feasibility of quantizing weight and activation values into FP4 and FP8 formats. The study reveals that quantizing activations into floating- point types (FP4 and FP8) produces superior results compared to integer types. Omniquant [210] diverges from prior approaches that rely on empirical design of quantization parameters. Instead, it optimizes the boundaries for weight clipping and the scaling factor for equivalent transformation to minimize quantization errors. QLLM [211] addresses the impact of outliers on quantization by implementing channel reassembly. Additionally, it introduces learnable low- rank parameters to minimize quantization errors in the post- quantized model. Atom [212] employs a strategy involving mixed- precision and dynamic quantization for activations. Notably, it extends this approach to quantize the KV cache into INT4 to enhance throughput performance. LLM- FP4 [187] endeavors to quantize the entire model into FP4 format and introduces a pre- shifted exponent bias technique. This approach combines the scaling factor of activation values with weights to address quantization challenges posed by outliers. BiLLM [213] represents one of the lowest- bit PTQ efforts to date. BiLLM identified the bell- shaped distribution of weights and the exceptionally long- tail distribution of weights' Hessian matrix. Based on this, it proposes to categorize weights into salient and nonsalient values structurally based on the Hessian matrix and binarizes them separately. As a result, BiLLM can extensively quantize LLMs to 1.08 bits without significant degradation in perplexity. KVQuant [219] proposes a non- uniform quantization scheme for KV cache quantization, by deriving the optimal datatypes offline on a calibration set. KIVI [220] proposes a tuning- free 2bit KV cache quantization algorithm, which utilizes per- channel quantization for key cache and per- token quantization for value cache in a

TABLE 4 Comparison of speed-ups in different scenarios (e.g., model size, batch size, input context length, inference framework) with W4A16 quantization based on TensorRT-LLM [222] and LMDeploy [23] framework, respectively. We test the speed-ups of prefilling/decoding/end-to-end latency on a single NVIDIA A100 GPU. OOM denotes "Out Of Memory".  

<table><tr><td rowspan="2"></td><td colspan="6">TensorRT-LLM</td></tr><tr><td>B</td><td>128</td><td>256</td><td>512</td><td>1024</td><td>2048</td></tr><tr><td rowspan="5">LLaMA-2-7B</td><td>1</td><td>1.06/2.40/2.37</td><td>0.90/2.38/2.34</td><td>0.92/2.30/2.28</td><td>0.88/2.19/2.17</td><td>0.91/2.00/1.98</td></tr><tr><td>2</td><td>0.88/2.10/2.05</td><td>0.91/2.07/2.04</td><td>0.89/2.01/1.98</td><td>0.91/1.92/1.89</td><td>0.88/1.78/1.76</td></tr><tr><td>4</td><td>0.92/1.72/1.67</td><td>0.89/1.67/1.64</td><td>0.90/1.61/1.58</td><td>0.87/1.53/1.51</td><td>0.84/1.43/1.40</td></tr><tr><td>8</td><td>0.91/1.43/1.36</td><td>0.88/1.38/1.33</td><td>0.83/1.33/1.28</td><td>0.77/1.25/1.21</td><td>0.78/1.16/1.14</td></tr><tr><td>16</td><td>0.91/1.43/1.36</td><td>0.88/1.38/1.33</td><td>0.83/1.33/1.28</td><td>0.77/1.25/1.21</td><td>0.78/1.16/1.14</td></tr><tr><td></td><td>B</td><td>128</td><td>256</td><td>512</td><td>1024</td><td>2048</td></tr><tr><td rowspan="5">LLaMA-2-13B</td><td>1</td><td>1.24/2.51/2.50</td><td>0.89/2.45/2.47</td><td>0.94/2.34/2.42</td><td>0.90/2.18/2.32</td><td>0.83/1.94/2.16</td></tr><tr><td>2</td><td>0.90/2.51/2.50</td><td>0.95/2.45/2.47</td><td>0.90/2.34/2.42</td><td>0.83/2.18/2.32</td><td>0.80/1.94/2.16</td></tr><tr><td>4</td><td>0.96/1.80/1.76</td><td>0.91/1.78/1.74</td><td>0.83/1.73/1.69</td><td>0.80/1.65/1.62</td><td>0.83/1.54/1.52</td></tr><tr><td>8</td><td>0.91/1.86/1.77</td><td>0.83/1.81/1.73</td><td>0.80/1.73/1.66</td><td>0.82/1.62/1.56</td><td>0.75/1.46/1.41</td></tr><tr><td>16</td><td>0.84/1.84/1.69</td><td>0.81/1.77/1.63</td><td>0.82/1.63/1.53</td><td>0.78/1.46/1.39</td><td>OOM</td></tr><tr><td rowspan="2"></td><td rowspan="2">B</td><td colspan="5">LMDeploy</td></tr><tr><td>128</td><td>256</td><td>512</td><td>1024</td><td>2048</td></tr><tr><td rowspan="5">LLaMA-2-7B</td><td>1</td><td>1.30/2.11/2.09</td><td>0.94/2.07/2.05</td><td>0.90/2.03/2.02</td><td>0.88/1.97/1.96</td><td>0.94/1.92/1.91</td></tr><tr><td>2</td><td>1.03/2.24/2.20</td><td>0.90/2.19/2.15</td><td>0.88/2.11/2.08</td><td>0.93/1.97/1.95</td><td>0.85/1.78/1.76</td></tr><tr><td>4</td><td>0.90/2.18/2.10</td><td>0.87/2.12/2.05</td><td>0.93/2.01/1.96</td><td>0.92/1.86/1.83</td><td>0.92/1.64/1.62</td></tr><tr><td>8</td><td>0.92/1.92/1.77</td><td>0.91/1.82/1.71</td><td>0.92/1.65/1.57</td><td>0.93/1.45/1.41</td><td>0.94/1.28/1.26</td></tr><tr><td>16</td><td>0.92/1.92/1.77</td><td>0.91/1.82/1.71</td><td>0.92/1.65/1.57</td><td>0.93/1.45/1.41</td><td>0.94/1.28/1.26</td></tr><tr><td></td><td>B</td><td>128</td><td>256</td><td>512</td><td>1024</td><td>2048</td></tr><tr><td rowspan="5">LLaMA-2-13B</td><td>1</td><td>1.32/2.34/2.32</td><td>0.94/2.31/2.28</td><td>0.92/2.22/2.20</td><td>0.94/2.15/2.13</td><td>0.94/2.01/1.99</td></tr><tr><td>2</td><td>1.06/2.42/2.36</td><td>0.92/2.37/2.32</td><td>0.94/2.29/2.25</td><td>0.94/2.15/2.12</td><td>0.95/1.95/1.93</td></tr><tr><td>4</td><td>0.93/2.36/2.26</td><td>0.94/2.29/2.21</td><td>0.94/2.18/2.12</td><td>0.95/2.01/1.97</td><td>0.96/1.75/1.75</td></tr><tr><td>8</td><td>0.92/2.24/2.10</td><td>0.93/1.93/2.02</td><td>0.94/1.81/1.89</td><td>0.94/1.65/1.71</td><td>0.95/1.43/1.49</td></tr><tr><td>16</td><td>0.93/2.02/1.85</td><td>0.94/1.90/1.76</td><td>0.94/1.73/1.63</td><td>0.95/1.50/1.45</td><td>OOM</td></tr></table>

group- wise manner. Li et al. [214] conducted a thorough evaluation to assess the impact of quantization on different tensor types (including KV Cache), various tasks, 11 LLM families, and SOTA quantization methods.

Quantization- Aware Training. Quantization- aware training (QAT) incorporates the influence of quantization within the model training procedure. By integrating layers that replicate quantization effects, this approach facilitates weight adaptation to quantization- induced errors, leading to enhanced task performance. Nevertheless, training LLMs typically demands substantial training data and considerable computational resources, posing potential bottlenecks for QAT implementation. Consequently, current research endeavors focus on strategies to reduce the training data requirements or alleviate the computational burden associated with QAT implementation.

To reduce the data requirements, LLM- QAT [187] introduces a data- free method to generate the training data by using the original FP16 LLMs. Specifically, LLM- QAT uses every token in the tokenization vocabulary as a starting token to generate sentences. Based on the generated training data, LLM- QAT applies a distillation- based workflow to train the quantized LLM to match the output distribution of the original FP16 LLM. Norm Tweaking [188] limits the selection of the starting token to only those language categories listed among the top languages with the highest proportion. This strategy can effectively improve the generalization of the quantized model on different tasks.

To reduce the computation cost, many methods apply parameter- efficient tuning (PEFT) strategies to accelerate

To reduce the computation cost, many methods apply parameter- efficient tuning (PEFT) strategies to accelerate QAT. QLoRA [189] quantizes the weights of LLMs into 4- bit and subsequently employs LoRA [224] in BF16 for each 4- bit weight matrix to fine- tune the quantized model. QLoRA allows for the efficient fine- tuning of a 65B parameter LLM on one GPU with only 30GB of memory. QA- LoRA [190] proposes to incorporate group- wise quantization into QLoRA. The authors observe that the number of quantization parameters in QLoRA is significantly smaller than the number of LoRA parameters, leading to an imbalance between quantization and low- rank adaptation. They suggest that group- wise operations can address this issue by increasing the number of parameters dedicated to quantization. In addition, QA- LoRA can merge the LoRA terms into the corresponding quantized weight matrices. LoftQ [191] identifies that initializing LoRA matrices with zeros in QLoRA is inefficient for downstream tasks. As an alternative, LoftQ suggests initializing the LoRA matrices using the Singular Value Decomposition (SVD) of the difference between the original FP16 weights and quantized weights. LoftQ iteratively applies quantization and SVD to achieve a more accurate approximation of the original weights. Norm Tweaking [188] proposes to train the LayerNorm layer after quantization and use knowledge distillation to match the output distribution of the quantized model with that of the FP16 model, achieving effects similar to LLM- QAT while avoiding high training costs.

Comparative Experiments and Analysis. In this section, we conduct experiments to evaluate the speed- ups achieved by employing the weight- only quantization technique in various scenarios. Specifically, we focus on two widely- used

large language models (LLMs),LLaMA- 2- 7B and LLaMA- 2- 13B, and quantize their weights to 4- bit using the AWQ [194] algorithm. Subsequently, we deploy these quantized models on a single NVIDIA A100 GPU using two different inference frameworks: TensorRT- LLM [222] and LMDeploy [223]. We then evaluate the speed- ups achieved by these frameworks across different input sequences characterized by varying batch sizes and context lengths.

large language models (LLMs), LLaMA- 2- 7B and LLaMA- 2- 13B, and quantize their weights to 4- bit using the AWQ [194] algorithm. Subsequently, we deploy these quantized models on a single NVIDIA A100 GPU using two different inference frameworks: TensorRT- LLM [222] and LMDeploy [223]. We then evaluate the speed- ups achieved by these frameworks across different input sequence characters by varying batch sizes and context lengths.We present the speed- ups of prefilling latency, decoding latency, and end- to- end latency, as summarized in Tab. 4. From the results, several key observations can be made: (1) Weight- only quantization can substantially accelerate the decoding stage, leading to improvements in end- to- end latency. This enhancement primarily stems from the capability of loading the quantized model with low- precision weight tensors much more swiftly from the High Bandwidth Memory (HBM), as illustrated in the preceding "Efficient Analysis" part. Consequently, this approach markedly diminishes the memory access overhead. (2) Regarding the prefilling stage, weight- only quantization may actually increase the latency. This is due to the fact that the bottleneck in the prefilling stage is the computational cost rather than the memory access cost. Therefore, quantizing only the weights without the activations has minimal impact on latency. Additionally, as illustrated in Fig. 9, weight- only quantization necessitates the de- quantization of low- precision weights to FP16, leading to additional computational overhead and consequently slowing down the prefilling stage. (3) As the batch size and input length increase, the extent of speed- up achieved by weight- only quantization gradually diminishes. This is primarily because, with larger batch sizes and input lengths, the computational cost constitutes a larger proportion of latency. While weight- only quantization predominantly reduces memory access cost, its impact on latency becomes less significant as the computational demands become more prominent with larger batch sizes and input lengths. (4) Weight- only quantization offers greater benefits for larger models due to the significant memory access overhead associated with larger model sizes. As models grow in complexity and size, the amount of memory required to store and access weights increases proportionally. By quantizing the model weights, weight- only quantization effectively reduces this memory footprint and memory access overhead.

#### 5.2.2 Sparsification

Sparsification is a compression technique that increases the proportion of zero- valued elements in data structures such as model parameters or activations. This method aims to decrease computational complexity and memory usage by efficiently ignoring zero elements during computation. In the context of LLMs, sparsification is commonly applied to weight parameters and attention activations. It leads to the development of weight pruning strategies and sparse attention mechanisms.

Weight Pruning. Weight pruning systematically removes less critical weights and structures from models, aiming to reduce computational and memory cost during both prefilling stages and decoding stages without significantly compromising performance. This sparsification approach is categorized into two main types: unstructured pruning and structured pruning. The categorization is based on the granularity of the pruning process, as illustrated in Figure 10.

![](images/ebf1d12fa3c6ae4884b8599988bb70d80f2a4b981f8cd7f51de1f7033c577523.jpg)  
Fig. 10. Illustration of Unstructured Pruning (left) and Structured Pruning (right).

Unstructured pruning prunes individual weight values with fine granularity. Compared with structured pruning, it typically achieves a greater level of sparsity with minimal impact on model prediction. However, the sparse pattern achieved through unstructured pruning lacks high- level regularity, leading to irregular memory access and computation patterns. This irregularity can significantly hinder the potential for hardware acceleration, as modern computing architectures are optimized for dense, regular data patterns. Consequently, despite achieving higher sparsity levels, the practical benefits of unstructured pruning in terms of hardware efficiency and computational speedup may be limited.

The common focus of this line of work is the pruning criterion, including the weight importance and pruning ratio. Considering the huge parameter size of LLMs, improving the pruning efficiency is also crucial. One pruning criterion is to minimize the reconstruction loss of the model. SparseGPT [165] is a representative approach in this field. It follows the idea of OBS [225], which considers the impact of removing each weight on the network's reconstruction loss. OBS iteratively decides a pruning mask to prune the weights and reconstructs the unpruned weights to compensate for the pruning loss. SparseGPT overcomes the efficiency bottleneck of OBS via the Optimal Partial Updates technique, and designs an adaptive mask selection technique based on the OBS reconstruction error. Prune and Tune [168] improves upon SparseGPT by fine- tuning the LLMs with minimal training steps during pruning. ISC [167] designs a novel pruning criterion by combining the saliency criteria in OBS [225] and OBD [226]. It further assigns non- uniform pruning ratios to each layer based on Hessian information. oBERT [171] and FastPruning [172] utilizes the second- order information of the loss function to decide the pruned weights. BESA [170] learns a differentiable binary mask via gradient descent of the reconstruction loss. The pruning ratio for each layer is sequentially decided by minimizing the reconstruction error. The other popular pruning criterion is magnitude- based. Wanda [166] proposes to use the elementwise product between the weight magnitude and the norm of input activation as the pruning criterion. RIA [173] jointly considers the weights and activations by using the metric of Relative Importance and Activations, which evaluates the importance of each weight element based on all its connected weights. In addition, RIA converts the unstructured sparsity pattern to a structured N:M sparsity pattern, which can enjoy the actual speed- up on NVIDIA GPUs. The recent study, Pruner- Zero [185], proposes to automatically identify

the optimal pruning metric for LLMs, going beyond the hand- designed matrices. As a result, the optimal metric tailored for LLaMA and LLaMA- 2 is  $W\odot W\odot \sigma (G)$ , where  $W$  and  $G$  represent the weights and gradients, and  $\sigma (\cdot)$  scales a tensor to [0,1] using its minimum and maximum value. Additionally, OWL [169] focuses on deciding the pruning ratio of each layer. It assigns the pruning ratios to each layer based on its activation outlier ratios. DSOT [186] proposes a training- free approach to fine- tune the pruned LLMs. It builds upon the "pruning- and- growing" workflow adopted in Dynamic Sparse Training [227], which first prunes the model and then iteratively adjusts the network topology without training or weight update. DSOT further designs the pruning and growing metrics tailored for LLMs.

Structured pruning prunes larger structural units of the model, such as entire channels or layers, operating at a coarser granularity compared to unstructured pruning. These methods directly facilitate inference speed- up on conventional hardware platforms due to their alignment with the dense, regular data patterns these systems are optimized to process. However, the coarse granularity of structured pruning often results in a more pronounced impact on model performance. The pruning criterion of this line of work additionally enforces the structured pruning pattern. LLM- Pruner [174] proposes a task- agnostic structured pruning algorithm. Specifically, it first identifies the couple structures in the LLM, based on the connection dependencies between neurons. Then, it decides which structure groups to remove based on a well- designed group- wise pruning metric. After pruning, it further proposes to recover the model performance by a parameter- efficient training technique, i.e., LoRA [224]. Sheared LLaMA [175] proposes to prune the original LLM to a specific target architecture of existing pre- trained LLMs. In addition, it designs dynamic batch- loading techniques to improve post- training performance. ZipLM [176] iteratively identifies and prunes the structural components with the worst trade- off between loss and runtime. LoRAPrune [177] proposes a structured pruning framework for the pre- trained LLMs with LoRA modules to enable fast inference of LoRA- based models. It designs a LoRA- guided pruning criterion that uses the weights and gradients of LoRA, and an iterative pruning scheme to remove the unimportant weights based on the criterion. LoRAShear [178] also designs a pruning method for LoRA- based LLMs with (1) a graph algorithm to identify the minimal removal structures, (2) a progressive structured pruning algorithm LHSPG, and (3) a dynamic knowledge recovery mechanism to recover the model performance. SliceGPT [179] builds on the idea of computational invariance of RMSNorm operation. It proposes to structurally arrange the sparsity in each weight matrix, and to slice out the entire rows or columns. PLATON [180] proposes to prune the weights by considering both their importance and uncertainty. It uses the exponential moving average (EMA) of the importance scores to estimate the importance, and adopts the upper confidence bound (UCB) for the uncertainty. CoFi [181] and SIMPLE [182] propose to prune the attention head, FFN neurons and hidden dimension via learning the corresponding sparsity masks. After pruning, they further adopt knowledge distillation to fine- tune the pruned models for performance recovery. MoE techniques (Sec. 5.1.1) have attracted much attention in the field of efficient LLMs. Recent studies tend to explore the expert pruning methods for MoE- based LLMs. For example, ExpertSparsity [183] proposes to prune some less important FFN experts in each model layer. Specifically, it utilizes the Frobenius norm of the difference between the original output and the output of the pruned layer to quantify the loss of pruned experts. In contrast, SEER- MoE [184] uses the total number of times that one expert gets activated on a calibration dataset, to quantify this expert's importance.

![](images/9e5ad3d5fcdb752be2ea9f65416b8375649bde9fc4676a4f524a0828b2609b1f.jpg)  
Fig. 11. Examples of different sparse attention masks. (a) Static mask with local, global, and random attention pattern. (b) Static mask with dilated attention pattern of different dilated rate. (c) Dynamic token pruning. (d) Dynamic attention pruning.

Sparse Attention. Sparse attention techniques in MultiHead Self- Attention (MHSA) components of transformer models strategically omit certain attention calculations to enhance computational efficiency of the attention operation mainly in the prefilling stage. These mechanisms diverge into static and dynamic categories based on their reliance on specific input data.

Static sparse attention removes activation values independently of specific inputs [150], [152], [153], [154]. These methods pre- determine the sparse attention mask and enforce it on the attention matrix during inference. Previous studies combine different sparse patterns to preserve the most essential elements within each attention matrix. As shown in Figure 11(a), the most common sparse attention patterns are the local and global attention patterns. The local attention pattern captures the local context of each token with a fixed- size window attention surrounding each token. The global attention pattern captures the correlation of specific tokens to all other tokens by computing and attending to all tokens across the sequence. Note that leveraging global patterns can eliminate the need to store key- value (KV) pairs for unused tokens, thereby reducing memory access cost and memory usage during the decoding stage. Sparse Transformer [150] combines these patterns to capture the

local context with a local pattern, and then aggregates the information with the global pattern for every few words. StreamingLLM [151] applies the local pattern, along with the global pattern only for the first few tokens. It shows that such a global pattern serves as the attention sink to keep the strong attention scores toward initial tokens. It helps the LLMs to generalize to infinite input sequence length. Bigbird [153] also uses the random pattern, where all tokens attend to a set of random tokens. The combination of local, global and random patterns is proven to encapsulate all continuous sequence- to- sequence functions, affirming its Turing completeness. As shown in Figure 11(b), Longformer [152] additionally introduces the dilated sliding window pattern. It is analogous to dilated CNNs and makes the sliding window "dilated" to increase the receptive field. To adapt the model to the sparse setting, Structured Sparse Attention [154] advocates an entropy- aware training method that congregates high- probability attention values into denser regions. Unlike previous studies that manually design sparse patterns, SemSA [155] uses gradient- based profiling to identify important attention patterns and automatically optimizes the attention density distribution to further improve model efficiency.

In contrast, Dynamic sparse attention adaptively eliminates activation values based on varying inputs, employing real- time monitoring of neuronal activation values to bypass computations for neurons with negligible impact, thereby achieving pruning. Most dynamic sparse attention methods employ the dynamic token- pruning methods, as Figure 11(c) shows. Spatten [156], SeqBoat [157] and Adaptively Sparse Attention [158] leverage the inherent redundancy in linguistic constructs to propose dynamic token- level pruning strategies. Spatten [156] assesses the cumulative importance of each word by aggregating the attention matrix columns, subsequently pruning tokens with minimal cumulative significance from the input in subsequent layers. SeqBoat [157] trains a linear State Space Model (SSM) with a sparse sigmoid function to determine which token to prune for each attention head. Both Spatten and SeqBoat prune the uninformative tokens for the whole input. Adaptively Sparse Attention [158] gradually prunes the tokens during the generation process. It drops parts of the context that are no longer required for future generation.

In addition to dynamic token pruning, dynamic attention pruning strategies are also employed [159], [160], [161], [162], [163]. As Figure 11(d) shows, instead of pruning all the attention values of certain tokens, these methods dynamically prune the selective part of the attention based on the input. A prominent approach within this domain is dynamically segmenting input tokens into groups, known as buckets, and strategically omitting the attention calculations for tokens that reside in separate buckets. The challenge and focus of these methods lie in the way to cluster related tokens together, thereby facilitating attention computations solely among them to enhance efficiency. Reformer [159] leverages locality- sensitive hashing to cluster keys and queries that share identical hash codes into the same bucket. Following this, Sparse Flash Attention [160] introduces specialized GPU kernels optimized for this hash- based sparse attention mechanism, further improving computational efficiency. Meanwhile, the Routing Transformer [161] employs a spherical k- means clustering algorithm to aggregate tokens into buckets, optimizing the selection process for attention computations. Sparse Sinkhorn Attention [162] adopts a learned sorting network to align keys with their relevant query buckets, ensuring that attention is computed only between the corresponding query- key pairs. Diverging from the bucket- level operation,  $\mathrm{H}_2\mathrm{O}$  [163] introduces the token- level dynamic attention pruning mechanism. It combines static local attention with dynamic computations between the current query and a set of dynamically identified key tokens, termed heavy- hitters  $(\mathrm{H}_2)$ . These heavy- hitters are dynamically adjusted with an eviction policy aimed at removing the least significant keys at each generation step, effectively managing the size and relevance of the heavy- hitter set.

Moreover, viewing each token as a graph node and attention between tokens as edges offers an extended perspective on static sparse attention [153], [164]. The original, full attention mechanism equates to a complete graph with a uniform shortest path distance of 1. Sparse attention, with its random mask, introduces random edges, effectively reducing the shortest path distance between any two nodes to  $O(\log n)$ , thus maintaining efficient information flow akin to full attention. Diffuser [164] utilizes the perspective of graph theory to expand the receptive field of sparse attention with multi- hop token correlations. It also takes inspiration from the expander graph properties to design better sparse patterns that approximate the information flow of full attention.

Beyond the attention- level and token- level sparsity, the scope of attention pruning extends to various granularities. Spatten [156] also extends pruning beyond token granularity to attention head granularity, eliminating computations for inessential attention heads to further reduce computational and memory demands.

#### 5.2.3 Structure Optimization

The objective of structure optimization is to refine model architecture or structure with the goal of enhancing the balance between model efficiency and performance. Within this field of research, two prominent techniques stand out: Neural Architecture Search (NAS) and Low Rank Factorization (LRF).

Neural Architecture Search. Neural Architecture Search (NAS) [228] aims to automatically search the optimal neural architectures that strike an optimized balance between efficiency and performance. AutoTinyBERT [138] utilizes one- shot Neural Architecture Search (NAS) to discover the hyper- parameters of the Transformer architecture. Notably, it introduces a compelling batch- wise training approach to train a Super Pre- trained Language Model (SuperPLM) and subsequently employs an evolutionary algorithm to identify the optimal sub- models. NAS- BERT [139] trains a large super- net on conventional self- supervised pre- training tasks using several innovative techniques, such as blockwise search, search space pruning, and performance approximation. This approach allows NAS- BERT to be applied efficiently across various downstream tasks without requiring extensive re- training. Structure pruning via NAS [140] treats structural pruning as a multi- objective NAS problem,

and solves it via the one- shot NAS method. LiteTransformerSearch [141] proposes to use a training- free indicator, i.e., the number of parameters, as a proxy indicator to guide the search. This method enables efficient exploration and selection of the optimal architectures without the need for actual training during the search phase. AutoDistil [142] presents a fully task- agnostic few- shot NAS algorithm featuring three primary techniques: search space partitioning, task- agnostic SuperLM training, and task- agnostic search. This approach aims to facilitate efficient architecture discovery across various tasks with minimal task- specific adaptations. Typically, NAS algorithms necessitate evaluating the performance of each sampled architecture, which can incur significant training cost. Consequently, these techniques are challenging to apply to LLMs.Low Rank Factorization. Low Rank Factorization (LRF), or Low Rank Decomposition, aims to approximate a matrix  $A^{m \times n}$  with two low- rank matrices  $B^{m \times r}$  and  $C^{r \times n}$  by:

Low Rank Factorization. Low Rank Factorization (LRF), or Low Rank Decomposition, aims to approximate a matrix  $A^{m\times n}$  with two low- rank matrices  $B^{m\times r}$  and  $C^{r\times n}$  by:

$$
A^{m\times n}\approx B^{m\times r}\times C^{r\times n}, \tag{11}
$$

Low Rank Factorization. Low Rank Factorization (LRF), or Low Rank Decomposition, aims to approximate a matrix  $A^{m \times n}$  with two low- rank matrices  $B^{m \times r}$  and  $C^{r \times n}$  by:\[ A^{m \times n} \approx B^{m \times r} \times C^{r \times n}, \]where  $r$  is much smaller than  $m$  and  $n$ . In this way, LRF can diminish memory usage and enhance computational efficiency. Furthermore, during the decoding stage of LLM inference, memory access cost presents a bottleneck to the decoding speed. Therefore, LRF can reduce the number of parameters that need to be loaded, thereby accelerating the decoding speed. LRD [143] shows the potential of compressing the LLMs without largely degrading the performance via LRF. Specifically, it adopts Singular Value Decomposition (SVD) to factorize the weight matrices, and successfully compresses a LLM with 16B parameters to 12.3B with minimal performance drop. TensorGPT [144] introduces a method to compress the embedding layer using Tensor- Train Decomposition. Each token embedding is treated as a Matrix Product State (MPS) and efficiently computed in a distributed manner. LoSparse [145] combines the benefits of LRF and weight pruning for LLM compression. By leveraging low- rank approximation, LoSparse mitigates the risk of losing too many expressive neurons that typically occurs with direct model pruning. LPLR [146] and ZeroQuant- V2 [147] both propose to compress the weight matrix by simultaneously applying LRF and quantization to it. DSFormer [148] proposes to factorize the weight matrix into the product of a semi- structured sparse matrix and a small dense matrix. ASVD [149] designs an activation- aware SVD method. This approach involves scaling the weight matrix based on activation distribution prior to applying SVD for matrix decomposition. ASVD also involves determining an appropriate truncation rank for each layer through a search process. SVD- LLM [229] analyses the relationship between the singular values of the transformed weight matrices and the compression loss. Then, it designs a truncation- aware data whitening technique to identify the singular value that causes the minimal loss after removing it. Additionally, SVD- LLM develops a layer- wise closed- form update strategy to recover the task performance after the factorization.

#### 5.2.4 Knowledge Distillation

Knowledge Distillation (KD) is a well- established technique for model compression, wherein knowledge from large models (referred to as teacher models) is transferred to smaller models (referred to as student models). In the context of LLMs, KD involves using the original LLMs as teacher models to distill smaller LMs. Numerous studies have focused on effectively transferring various abilities of LLMs to smaller models. In this domain, methods can be categorized into two main types: white- box KD and black- box KD (as illustrated in Fig. 12).

![](images/704a37bea813fdd9bcf8b82e137ed8091ad8ae258fcd7b60dd7645f98f332e49.jpg)  
Fig. 12. Illustration of White-Box KD (left) and Black-Box KD (right).

White- box KD. White- box KD refers to distillation methods that leverage access to the structure and parameters of the teacher models. This approach enables KD to effectively utilize the intermediate features and output logits of the teacher models for enhanced performance of the student models. MiniLLM [131] proposes to adopt the standard white- box KD approach but replace the forward Kullback- Leibler divergence (KLD) with the reverse KLD. GKD [132] introduces the use of on- policy data, which includes output sequences generated by the student model itself, to further distill the student model. This method focuses on aligning the output logits between the teacher and student models using these on- policy data. TED [133] presents a task- aware layer- wise KD method. This approach involves adding filters after each layer in both the teacher and student models, training these task- specific filters, and subsequently freezing the teacher model's filters while training the student filters to align their output features with the corresponding teacher filters. MiniMoE [135] mitigates the capacity gap by utilizing a Mixture- of- Experts (MoE) model as the student model. DynaBERT [136] proposes to progressively decrease the models' width and depth, and uses knowledge distillation to train the smaller models. For newly emerging entities, pre- trained language models (LLMs) may lack up- to- date information. To address this, one solution involves incorporating additional retrieved texts into prompts, albeit at an increased inference cost. Alternatively, KPTD [137] suggests transferring knowledge from entity definitions into LLM parameters via knowledge distillation. This method generates a transfer set based on entity definitions and distills the student model to match output distributions with the teacher model based on these definitions.

Black- box KD. Black- box KD refers to the knowledge distillation methods in which the structure and parameters of teacher models are not available. Typically, black- box KD only uses the final results obtained by the teacher models to distill the student models. In the field of LLMs, blackbox KD mainly guides the student models to learn LLMs' generalization ability and emergent ability, including InContext Learning (ICL) ability [45], Chain- of- Thought (CoT) reasoning ability [14] and Instruction Following (IF) ability [230].

Regarding the ICL ability, Multitask- ICT [118] introduces in- context learning distillation to transfer the multitask few

shot ability of Large Language Models (LLMs), leveraging both in- context learning and language modeling proficiency. MCKD [119] observes that student models distilled from incontext learned teacher models often exhibit superior performance on unseen input prompts. Building on this observation, MCKD devises a multi- stage distillation paradigm where the student model from previous stages is employed to generate distillation data for subsequent stages, enhancing the effectiveness of the distillation method.

To distill the Chain- of- Thought (CoT) reasoning ability, several techniques such as Distilling Step- by- Step [120], SCoTD [121], CoT Prompting [122], MCC- KD [123], and Fine- tune- CoT [124] propose distillation methods that incorporate responses and rationales extracted from LLMs to train student models. Socratic CoT [125] also targets reasoning ability transfer to smaller models. Specifically, it fine- tunes a pair of student models, namely a Question Generation (QG) model and a Question Answering (QA) model. The QG model is trained to generate intermediate questions based on input questions, guiding the QA model in producing the final response. PaD [126] observes that faulty reasoning (i.e., correct final answer but incorrect reasoning steps) can be detrimental to student models. To address this, PaD proposes generating synthetic programs for reasoning problems, which can then be automatically checked by an additional interpreter. This approach helps in removing distillation data with faulty reasoning, enhancing the quality of the training data for student models.

For the IF ability, several methods have been proposed to transfer this capability to smaller models. DISCO [128] introduces a technique where phrasal perturbations are generated using a LLM. These perturbations are then filtered by a task- specific teacher model to distill high- quality counterfactual data. LaMini- LM [129] aims to transfer instruction following ability by designing a diverse instruction set for distilling student models. Lion [130] utilizes the teacher model to identify difficult instructions, and generates new and complex instructions to distill the small model.

#### 5.2.5 Dynamic Inference

Dynamic inference involves the adaptive selection of model sub- structures during the inference process, conditioned on input data. This section focuses on early exiting techniques, which enable a LLM to halt its inference at different model layers depending on specific samples or tokens. Notably, while MoE techniques (discussed in Sec. 5.1.1) also adjust model structure during inference, they typically involve expensive pre- training cost. In contrast, early exiting techniques only require training a small module to determine when to conclude the inference. Some previous surveys [231], [232] have reviewed dynamic inference techniques for traditional language models (e.g., RNN, LSTM). In this paper, we categorize studies on early exiting techniques for LLMs into two main types: sample- level early exiting and token- level early exiting (illustrated in Fig. 13). Sample- level. Sample- level early exiting techniques focus on determining the optimal size and structure of Language Models (LLMs) for individual input samples. A common approach is to augment LLMs with additional modules after each layer, leveraging these modules to decide whether to terminate inference at a specific layer. FastBERT [112], DeeBERT [115], MP [233], and MPEE [113] train these modules directly to make decisions (e.g., outputting 0 to continue or 1 to stop) based on features from the current layer. Global Past- Future Early Exit [114] proposes a method that enriches the input to these modules with linguistic information from both preceding and subsequent layers. Given that future layer features are not directly accessible during inference, a simple feed- forward layer is trained to estimate these future features. PABEE [116] trains the modules as output heads for direct prediction, suggesting inference termination when predictions remain consistent. HASHEE [117] employs a non- parametric decision- making approach based on the hypothesis that similar samples should exit inference at the same layer.

![](images/d950e1071977bc5c2d8a51bff02a6c2d0e4ee77f419ee09c52ea33dfcc3a3a8b.jpg)  
Fig. 13. Illustration of Token-level (up) and Sample-level (down) dynamic inference.

Token- level. In the decoding stage of LLM inference, where tokens are generated sequentially, token- level early exiting techniques aim to optimize the size and structure of LLMs for each output token. CALM [110] introduces early exit classifiers after each Transformer layer, training them to output confidence scores that determine whether to halt inference at a specific layer. Notably, in the self- attention block, computing the current token's feature at each layer relies on all previous tokens' features (i.e., KV cache) in the same layer. To address the issue of missing KV cache due to early exiting of previous tokens, CALM proposes directly copying the feature from the exiting layer to subsequent layers, with experimental results showing only minor performance degradation. SkipDecode [111] addresses limitations of previous early exiting methods that hinder their applicability to hatch inference and KV caching, thereby limiting actual speed- up gains. For batch inference, SkipDecode proposes a unified exit point for all tokens within a batch. Regarding KV caching, SkipDecode ensures a monotonic decrease in exit points to prevent recomputation of KV cache, facilitating efficiency gains during inference.

### 5.3 Knowledge, Suggestions and Future Direction

In the field of efficient structure design, the pursuit of alternative architectures to Transformers is a burgeoning area of research. Examples such as Mamba [75], RWKV [62], and their respective variants [103], [106] have demonstrated competitive performance across various tasks, garnering

increasing attention in recent times. Nevertheless, it remains pertinent to investigate whether these non- Transformer models may exhibit certain shortcomings compared to Transformer models. Concurrently, exploring the integration of non- Transformer architectures with the attention operation [76], [105], [234] represents another promising avenue for future research.

In the realm of model compression, quantization stands out as the predominant method employed in Large Language Model (LLM) deployment, primarily due to two key factors. Firstly, quantization presents a convenient means of compressing LLMs. For instance, employing Post- Training Quantization (PTQ) methods can reduce the parameter count of an LLM with seven billion parameters to a compressed form within a matter of minutes. Secondly, quantization holds the potential to achieve substantial reductions in memory consumption and inference speed, while introducing only minor performance trade- offs. This compromise is generally deemed acceptable for numerous real- world applications. However, it's worth noting that quantization may still compromise certain emergent abilities of LLMs, such as self- calibration or multi- step reasoning. Additionally, in specific scenarios like dealing with long contexts, quantization could lead to significant performance degradation [214]. Consequently, it is required to carefully select appropriate quantization methods to mitigate the risk of such degradation in these specialized cases.

Extensive literature has devoted into studying sparse attention techniques for efficient long- context processing. For example, a recent representative work, StreamingLLM [151], can process 4 million tokens by only restoring several attention sink tokens. Nonetheless, these approaches often sacrifice critical information, resulting in performance degradation. Therefore, the challenge of preserving essential information while efficiently managing long contexts remains an important area for future exploration. As for the weight pruning techniques, LLM- KICK [235] notes that current state- of- the- art (SOTA) methods experience considerable performance degradation even at relatively low sparsity ratios. Consequently, developing effective weight pruning methods to maintain LLM performance remains an emerging and critical research direction.

The optimization of model structures often involves the use of Neural Architecture Search (NAS), which typically demands extensive computational resources, posing a potential barrier to its practical application in compressing LLMs. Therefore, investigating the feasibility of employing automatic structure optimization for LLM compression warrants further exploration. Additionally, the challenge remains for techniques like low- rank factorization (LRF) to achieve an optimal balance between compression ratio and task performance. For instance, ASVD [149] achieves only a modest  $10\%$  to  $20\%$  compression ratio without compromising the reasoning capabilities of LLMs.

In addition to employing individual model compression techniques, several studies explore the combination of different methods to compress LLMs, leveraging their respective advantages for improved efficiency. For instance, MPOE [90] applies weight matrix factorization specifically to the expert Feed- Forward Networks (FFNs) in MoE- based LLMs, with the goal of further reducing memory require ments. LLM- MQ [201] utilizes weight sparsity techniques to protect weight outliers during model quantization, thereby minimizing quantization errors. LPLR [146] focuses on quantizing low- rank factorized weight matrices to further decrease memory footprint and memory access cost during LLM inference. Furthermore, LoSparse [145] combines low- rank factorization with weight pruning, leveraging pruning to enhance the diversity of low- rank approximation while using low- rank factorization to retain important weights and prevent loss of critical information. These approaches highlight the potential of integrating multiple compression techniques to achieve better optimization of LLMs.

## 6 SYSTEM-LEVEL OPTIMIZATION

The system- level optimization for LLM inference primarily involves enhancing the model forward pass. Considering the computational graph of an LLM, there exist multiple operators, with attention and linear operators dominating most of the runtime. As mentioned in Sec. 2.3, system- level optimization primarily considers the distinctive characteristics of the attention operator and the decoding approach within LLM. In particular, to address the specific issues related to the decoding approach of LLMs, the linear operator requires special tiling designs, and speculative decoding methods are proposed to improve the utilization. The substantial memory demand of LLMs leads to the offloading of parameters or KV cache to the CPU. Furthermore, in the context of online serving, requests come from multiple users. Therefore, beyond the optimizations discussed earlier, online serving faces challenges related to memory, batching, and scheduling arising from asynchronous requests.

### 6.1 Inference Engine

The optimizations for inference engines are dedicated to accelerating the model forward process. The main operators and the computational graph in LLM inference are highly optimized. Besides, the speculative decoding technique is proposed to accelerate the inference speed without performance degradation, and the offloading technique is introduced to mitigate the memory pressure.

#### 6.1.1 Graph and Operator Optimization

Runtime Profiling. Using HuggingFace [260] implementation, we profile the inference runtime with different models and context lengths. The profiling results in Fig. 15 demonstrate that attention operators and linear operators collectively dominate runtime, with their combined duration often exceeding  $75\%$  of the inference duration. Consequently, a significant portion of optimization efforts at the operator level is dedicated to enhancing the performance of the two operators. Furthermore, there are multiple operators occupying a small proportion of runtime, which fragments the operator execution timeline and increases the cost of kernel launch on the CPU side. To address this issue, at the computational graph level, current optimized inference engines implement highly fused operators.

Attention Operator Optimization. The standard attention computation (e.g., using Pytorch) involves the multiplication of the Query matrix (Q) with the Key matrix (K),

![](images/a1b7e68fbac6e7f614089ff8daf2e8479b0c60ffee343f266c38804bd5a8e0d0.jpg)  
Fig. 14. Taxonomy of the optimization for LLM inference engine.

![](images/d83735256158d414a4129b28b69a0f577cf37275e0f456ddf79ebdc168512ca3.jpg)  
Fig. 15. Inference runtime breakdown over multiple LLMs.

resulting in quadratic time and space complexity in relation to the input sequence length. As shown in Fig. 15, the time proportion of the attention operator increases as the context length grows. This translates to high demands on memory size and computational capability, especially when dealing with long sequences. To address the computational and memory overhead of standard attention computation on GPUs, customized attention operators are essential. FlashAttention [255], [256] fuses the entire attention operation into a single, memory- efficient operator to alleviate memory access overhead. The input matrices  $(Q, K, V)$  and attention matrix are tiled into multiple blocks, which eliminates the need for complete data loading. Built upon Flash Attention, FlashDecoding [259] aims to maximize computational parallelism for decoding. Due to the application of the decoding approach, the Q matrix degrades into a batch of vectors during decoding, which makes it challenging to fill the computational units if the parallelism is limited to the batch size dimension. FlashDecoding addresses this by introducing parallel computation along the sequence dimension. While this introduces some synchronization overhead to softmax computation, it leads to noticeable improvements in parallelism, particularly for small batch sizes and long sequences. The subsequent work, FlashDecoding++ [253], observes that in previous works [255], [256], [259], the maximum value within the softmax only serves as a scaling factor to prevent data overflow. However, the dynamical maximum value incurs significant synchronization overhead. Moreover, extensive experiments indicate that in typical LLM (e.g., Llama2 [261], ChatGLM [262]), over $99.99\%$  of the softmax inputs fall within a certain range. Thus, FlashDecoding++ proposes to determine the scaling factor based on statistics in advance. This eliminates the synchronization overhead in softmax computation, enabling parallel execution of subsequent operations alongside the softmax computation.

Linear Operator Optimization The linear operator plays a pivotal role in LLM inference, performing in feature projection and Feedforward Neural Networks (FFNs). In traditional neural networks, linear operators can be abstracted into General Matrix- Matrix Multiplication (GEMM) operations. However, in the case of LLM, the application of the decoding approach results in a notably reduced dimension, diverging from the conventional GEMM workload. The low- level implementation of traditional GEMM has been highly optimized, and mainstream LLM frameworks (e.g., DeepSpeed [258], vLLM [51], OpenPPI, [263] and etc.) primarily call the GEMM APIs offered by cuBLAS [264] for linear operators. Without an explicitly tailored implementation for GEMMs with a reduced dimension, the linear operators during decoding suffer inefficiency. A notable trend to address the issue is observed in the latest release of TensorRT- LLM [222]. It introduces a dedicated General Matrix- Vector Multiplication (GEMV) implementation, potentially improving efficiency for the decoding step. Recent research FlashDecoding++ [253] makes a further step, addressing the inefficiency of cuBLAS [264] and CUTLASS [265] libraries when dealing with small batch sizes during the decode step. The authors first introduce the concept of the FlatGEMM operation to represent the

workload of GEMM with a highly reduced dimension (dimension size  $< 8$  in FlashDecoding  $^{+ + }$  ). As FlatGEMM poses new computational characteristics, the tiling strategy for traditional GEMMs necessitates modification to be applied. The authors observe that two challenges exist as the workload varies: low parallelism and memory access bottleneck. To tackle the challenges, FlashDecoding  $^{+ + }$  adopts a finegrained tiling strategy to improve parallelism, and leverages the double buffering technique to hide memory access latency. Furthermore, recognizing that the linear operations in typical LLM (e.g., Llama2 [261], ChatGLM [262]) often have fixed shapes, FlashDecoding  $^{+ + }$  establishes a heuristic selection mechanism. This mechanism dynamically chooses between different linear operators based on the input size. The options include FastGEMV [266], FlatGEMM, and GEMM provided by cuBLAS [264], [265] libraries. This approach ensures the selection of the most efficient operator for the given linear workload, potentially leading to better end- toend performance.

Recently, the application of the MoE FFN to enhance the model capability has become a trend in LLMs [12]. This model structure also puts forward new requirements for operator optimization. As shown in Fig. 15, in the Mixtral model with MoE FFN, the linear operator dominates the runtime due to the non- optimized FFN computation in the HuggingFace implementation. Besides, Mixtral's adoption of the GQA attention structure decreases the attention operator's runtime proportion, which further points out the urgent need to optimize the FFN layer. MegaBlocks [254] is the first to optimize the computation for MoE FFN layers. The work formulates the MoE FFN computation into block- sparse operations and proposes tailored GPU kernels for acceleration. However, MegaBlocks concentrates on the efficient training of the MoE models and hence ignores the characteristics of inference (e.g., the decoding approach). Existing frameworks are working hard to optimize the computations of the MoE FFN inference stage. The official repository of vLLM [51] integrates the fused kernels for MoE FFN in Triton [267], seamlessly removing the index overhead.

Graph- Level Optimization. Kernel fusion stands out as a prevalent graph- level optimization because of its capability to reduce runtime. There are three main advantages of applying kernel fusion: (1) To reduce memory access. The fused kernel inherently removes the memory access of intermediate results, mitigating the memory bottleneck for operators. (2) To mitigate kernel launching overhead. For some lightweight operators (e.g., residual adding), the kernel launching time occupies most of the latency, and kernel fusion reduces individual kernel launchings. (3) To enhance parallelism. For those operators without data dependency, when one- by- one kernel execution fails to fill the hardware capacity, it is beneficial to parallel the kernels via fusion.

The technique of kernel fusion proves effective with LLM inference, with all of the aforementioned benefits. FlashAttention [255] formulates the attention operator into one single kernel, removing the overhead of accessing the attention results. Based on the fact that the attention operator is memory- bounded, the reduction of memory access effectively transfers to runtime speed- up. ByteTransformer [257] and DeepSpeed [258] propose to fuse lightweight operators including residual adding, layernorm, and activation functions, into the former linear operators to reduce the kernel launching overhead. As a result, those lightweight operators disappear in the timeline with nearly no extra latency. Moreover, kernel fusion is also adopted to enhance the utilization of LLM inference. The projections of Query, Key, and Value matrices are originally three individual linear operations, and are fused into one linear operator to deploy on modern GPUs. Currently, the kernel fusion technique has been exploited in LLM inference practice, and highly optimized inference engines employ only a few fused kernels within the runtime. For example, in FlashDecoding  $^{+ + }$  [253] implementation, a transformer block integrates merely seven fused kernels. Leveraging the aforementioned operators and kernel fusion optimization, FlashDecoding  $^{+ + }$  achieves up to  $4.86\times$  speed- up over the HuggingFace implementation.

#### 6.1.2 Speculative Decoding

Speculative decoding [268], [269] is an innovative decoding technique for auto- regressive LLMs designed to enhance decoding efficiency without compromising the fidelity of outputs. The core idea of this approach involves employing a smaller model, termed a draft model, to predict several subsequent tokens efficiently, followed by validation of these predictions using the target LLM in parallel. This methodology aims to enable the LLM to generate multiple tokens within the time frame typically required for a single inference. Fig. 16 demonstrates the comparison of the traditional auto- regressive decoding method and the speculative decoding approach. Formally, speculative decoding approach consists of two steps:

1) Draft Construction: It employs the draft model to generate several subsequent tokens, namely draft tokens, in parallel or in the auto-regressive manner. 
2) Draft Verification: It employs the target model to compute the conditional probabilities of all the draft tokens in a single LLM inference step, subsequently determining the acceptance of each draft token sequentially. The acceptance rate, representing the average number of accepted draft tokens per inference step, serves as a key metric for evaluating the performance of a speculative decoding algorithm.

![](images/c2d138f0fd69b4837af1a722f99fb8aa1f3b10b217bccd59e1dcee94d7df02a9.jpg)  
Fig. 16. Comparison of auto-regressive decoding (a) and speculative decoding (b).

Speculative decoding ensures output equivalence with standard auto- regressive decoding methods. Traditional decoding techniques typically employ two primary sampling

TABLE 5 Comparison of several open-source implementations of speculative decoding. In this table, we also show the additional overhead of constructing draft models. Note that for SSD [237], LADE [246], Medusa [50] and Eagle [247], we report the training cost from their original papers. And for SSD [239] and REST [29], we run the sub-LLM search and data are construction with the code they provide, and report the time cost. Besides, for Medusa, we use Medusa-1 [50] which does not fine-tune the original LLM backbone.  

<table><tr><td>Method</td><td>Draft Model</td><td>Draft Construction</td><td>Draft Verifier</td><td>Additional Overhead (GPU hours)</td><td>Acceptance Rate</td><td>Speed-up</td></tr><tr><td>SpD [236], [237]</td><td>small speculative model</td><td>one draft sequence</td><td>speculative sampling</td><td>275</td><td>1.77~2.02×</td><td>1.05~1.77×</td></tr><tr><td>LADE [246]</td><td>LLM + N grams</td><td>one draft sequence</td><td>greedy sampling</td><td>0</td><td>1.92~2.14×</td><td>1.12~1.30×</td></tr><tr><td>REST [239]</td><td>LLM+N grams</td><td>one draft sequence</td><td>speculative sampling</td><td>4</td><td>1.6~1.74×</td><td>1.01~1.23×</td></tr><tr><td>REST [29]</td><td>substore</td><td>token tree</td><td>speculative sampling</td><td>1.5</td><td>1.18~2.31×</td><td>1.72~2.27×</td></tr><tr><td>Medusa-1 [50]</td><td>four LLM heads</td><td>token tree</td><td>speculative sampling</td><td>~24</td><td>2.52~2.62×</td><td>2.04~2.86×</td></tr><tr><td>Eagle [247]</td><td>one Transformer Layer</td><td>token tree</td><td>speculative sampling</td><td>96~192</td><td>3.47~3.72×</td><td>2.77~3.74×</td></tr></table>

strategies: greedy sampling and nucleus sampling. Greedy sampling involves selecting the token with the highest probability at each decoding step to generate a specific output sequence. The initial attempt at speculative decoding, known as Blockwise Parallel Decoding [270], aims to ensure that the draft tokens precisely match the tokens sampled via greedy sampling, thus preserving output token equivalence. In contrast, nucleus sampling involves sampling tokens from a probability distribution, resulting in diverse token sequences with each run. This diversity makes nucleus sampling popular. To accommodate nucleus sampling within speculative decoding frameworks, speculative sampling techniques [236], [237] have been proposed. Speculative sampling maintains output distribution equivalence, aligning with the probabilistic nature of nucleus sampling to generate varied token sequences. Formally, given a sequence of tokens  $x_{1}, x_{2}, \ldots , x_{n}$  and a sequence of draft tokens  $\hat{x}_{n + 1}, \hat{x}_{n + 2}, \ldots , \hat{x}_{n + k}$ , the speculative sampling strategy accepts the  $i$ - th draft token with the following probabilities:

$$
\min \left(1,\frac{p(\hat{x}_i|x_1,x_2,...,x_{i - 1})}{q(\hat{x}_i|x_1,x_2,...,x_{i - 1})}\right), \tag{12}
$$

where  $p(\cdot |\cdot)$  and  $q(\cdot |\cdot)$  denote the conditional probabilities from the target LLM and the draft model, respectively. If the  $i$ - th draft token is accepted, it sets  $x_{i} \leftarrow \hat{x}_{i}$ . Otherwise, it quits the verification of the following draft tokens, and resamples  $x_{i}$  from the following distribution:

$$
\mathrm{norm}(\max (0,p(\cdot |x_1,x_2,\dots,x_{i - 1}) - q(\cdot |x_1,x_2,\dots,x_{i - 1})))
$$

Building upon speculative sampling, several variants [243], [248] have emerged, aimed at validating multiple draft token sequences. Notably, the token tree verifier [243] has become a widely adopted verification strategy within this context. This approach utilizes a tree- structured representation of draft token sets and employs a tree attention mechanism to efficiently perform the verification process.

In the speculative decoding approach, the acceptance rate of draft tokens is significantly influenced by the degree to which the output distributions of draft models align with those of original LLMs. As a result, considerable research efforts have been directed towards improving the design of draft models. DistillSpec [238] directly distills a smaller draft model from the target LLM. SSD [239] involves automatically identifying a sub- model (a subset of model layers) from the target LLM to serve as the draft model, eliminating the need for separate training of the draft model. OSD [240] dynamically adjusts the output distribution of the draft model to match the user query distribution in online LLM services. It achieves this by monitoring rejected draft tokens from the LLM and using this data to refine the draft model through distillation. PaSS [241] proposes utilizing the target LLM itself as the draft model, incorporating trainable tokens (look- ahead tokens) into the input sequence to enable simultaneous generation of subsequent tokens. REST [242] introduces a retrieval- based speculative decoding approach, employing a non- parametric retrieval data store as the draft model. SpecInfer [243] introduces a collective boost- tuning technique to align the output distribution of a group of draft models with that of the target LLM. Lookahead decoding [246] involves generating n- grams of the target LLM in parallel to aid in generating draft tokens. Medusa [50] fine- tunes several heads of the LLM specifically for generating subsequent draft tokens. Eagle [247] adopts a lightweight transformer layer called an auto- regression head to generate draft tokens in an auto- regressive manner, integrating rich contextual features from the target LLM into the draft model's input. Kangaroo [249] uses a fixed shallow subnetwork as the draft model, and trains a lightweight adapter on the top of the sub- network. In this way, it does not need to train a separate draft model.

Another line of studies focuses on designing more effective draft construction strategies. Conventional approaches often yield single draft token sequences, posing challenges for passing verification. In response, Spectr [248] advocates generating multiple draft token sequences and employs a  $k$ - sequential draft selection technique to concurrently verify  $k$  sequences. This method leverages speculative sampling, ensuring equivalence in output distributions. Similarly, SpecInfer [243] adopts a comparable approach. However, unlike Spectr, SpecInfer merges draft token sequences into a "token tree" and introduces a tree attention mechanism for validation. This strategy is called the "token tree verifier". Due to its efficacy, token tree verifier has been widely embraced in numerous speculative decoding algorithms [50], [242], [244], [247]. In addition to these efforts, Stage Speculative Decoding [244] and Cascade Speculative Drafting (CS Drafting) [245] propose accelerating draft construction by integrating speculative decoding directly into the token generation process.

Comparative Experiments and Analysis. We conduct an experiment to evaluate the speed- up performance of

the speculative decoding methods. Specifically, we thoroughly review the studies of this field, and select six of them that have open- sourced their codes, i.e., Speculative Decoding (SpD) [236], [237], Lookahead Decoding (LADE) [246], REST [242], Self- speculative Decoding (SSD) [239], Medusa [50] and Eagle [247]. As for the evaluation dataset, we use Vicuna- 80 [7] to evaluate the above methods, which contains 80 questions that classified into 10 categories. We report the average results on these 80 questions. As for target LLMs, we adopt five fashion open- source LLMs, i.e., Vicuna- 7B- V1.3 [7], Vicuna- 13B- V1.3 [7], Vicuna- 33B- V1.3 [7], LLaMA- 2- 7B [5] and LLaMA- 2- 13B [5]. We report the range of evaluation metrics across these 5 LLMs. As for draft models, we adopt two well- trained draft models, i.e., LLaMA- 68M and LLaMA- 160M [243] for SpD. For other speculative decoding methods, we follow their proposed draft construction approach and use the checkpoints they provide. As for the evaluation metrics, we adopt acceptance rate, which denotes the ratio of the number of accepted tokens to the number of generation steps, and speed- up, which denotes the ratio of the latency of original auto- regressive decoding to the latency of speculative decoding when fixing the total length of output.

Tab. 5 provides a comparison of various speculative decoding methods, highlighting several key observations: (1) Eagle demonstrates exceptional performance, achieving a notable  $3.47\sim 3.72\times$  end- to- end speed- up across multiple LLMs. To understand its success, a deeper analysis of Eagle reveals two key factors. Firstly, Eagle employs an autoregressive approach for decoding draft tokens, leveraging information from previously generated tokens directly. Secondly, Eagle integrates rich features from previous tokens of both original LLMs and draft models to enhance the accuracy of the next draft token generation. (2) The token tree verifier proves to be an effective technique in enhancing the performance of speculative decoding methods. (3) The end- to- end speed- up achieved by these methods is often lower than the acceptance rate. This difference arises due to the practical consideration that the generation cost associated with draft models cannot be overlooked.

#### 6.1.3 Offloading

Current research investigates the potential of offloading to accommodate the substantial memory demand of LLMs (see Sec. 2.3) in resource- constrained environments. The essence of offloading is to offload part of the storage from the GPU to the CPU when it is free of use. Intuitively, the focus of such kind of research lies in hiding the expensive data movement latency between the GPU and the CPU. FlexGen [203] enables the offloading of weights, activations, and the KV cache, and further formulates a graph traversal problem for offloading to maximize the throughput. The data loading of the next batch and the data storing of the previous batch can be overlapped with the computation of the current batch. Another work llama.cpp [250] also assigns computational tasks to the CPU, mitigating the data transfer overhead at the cost of computing with the low- powered CPU. Powerin- . fer [251] exploits the sparsity in activations using ReLU [271] in LLMs, and divides the activations into subsets of cold and hot neurons representing the frequency of computation. The cold neurons are offloaded to the CPU for both storage and computation in Powerinfer. Leveraging adaptive predictors and sparse operators, Powerinfer significantly improves the computational efficiency with offloading. FastDecode [252] proposes to offload the storage and the computation of the entire attention operator to the CPU. Since the attention operation is computed on the CPU, the data movement of KV cache is reduced to merely some activations. The number of CPUs is selected to match the workload latency on GPUs so that the bubbles in the heterogeneous pipeline are mitigated.

### 6.2 Serving System

The optimizations for serving systemworks are dedicated to improving the efficiency in handling asynchronous requests. The memory management is optimized to hold more requests, and efficient batching and scheduling strategies are integrated to enhance the system throughput. Besides, optimizations specific to distributed systems are proposed to exploit distributed computational resources.

#### 6.2.1 Memory Management

The storage of KV cache dominates the memory usage in LLM serving, especially when the context length is long (see Sec. 2.3). Since the generation length is uncertain, it is challenging to allocate the space for KV cache storage in advance. Earlier implementations [286] usually allocate storage space in advance based on the preset maximum length of each request. However, in instances where request generation is terminated early, this approach incurs significant wastage of storage resources. To address the issue,  $S^3$  [284] proposes to predict an upper bound of the generation length for each request, in order to reduce the waste of the pre- allocated space. However, the static way of KV cache memory allocation still fails when no such large contiguous space exists. To deal with the fragmented storage, vLLM [51] proposes to store the KV cache in a paged manner following the operating system. vLLM first allocates a memory space as large as possible and divides it equally into multiple physical blocks. When a request comes, vLLM dynamically maps the generated KV cache to the pre- allocated physical blocks in a discontinuous fashion. In this way, vLLM significantly reduces storage fragmentation and achieves a higher throughput in LLM serving. On the basis of vLLM, LightLLM [278] uses a more fine- grained KV cache storage to cut down the waste happening with the irregular boundary. Instead of a block, LightLLM treats the KV cache of a token as a unit, so that the generated KV cache always saturates the pre- allocated space.

Current optimized service systems commonly employ this paged approach to manage the KV cache storage, thereby mitigating the waste of redundant KV cache memory. However, the paged storage leads to irregular memory access in the attention operator. For the attention operator using the paged KV cache, this necessitates the consideration of the mapping relationship between the virtual address space of the KV cache and its corresponding physical address space. To enhance the efficiency of the attention operator, the loading pattern of the KV cache must be tailored to facilitate contiguous memory access. For instance, in the case of the PagedAttention by vLLM [51], the storage

![](images/225f2cd5fe8e93bf6ca6986d1adf616da6323a5dc5296cfb8e25c12b91b8bb89.jpg)  
Fig. 17. Taxonomy of the optimization for LLM serving system.

of the head size dimension is structured as a 16- byte contiguous vector for K cache, while FlashInfer [285] orchestrates diverse data layouts for the KV cache, accompanied by an appropriately designed memory access scheme. The optimization of the attention operator in conjunction with paged KV cache storage remains a forefront challenge in the advancement of serving systems.

#### 6.2.2 Continuous Batching

The request lengths in a batch can be different, leading to low utilization when shorter requests are finished and longer requests are still running. Due to the asynchronous nature of requests in serving scenarios, there exists an opportunity that such periods of low utilization could be mitigated. The continuous batching technique is proposed to leverage the opportunity by batching new requests once some old requests are finished. ORCA [277] is the first to utilize the continuous batching technique in LLM serving. The computation of each request encompasses multiple iterations, with each iteration representing either a prefilling step or a decoding step. The author suggests that different requests can be batched at the iteration level. The work implements iteration- level batching in linear operators, concatenating different requests together in the sequence dimension. Hence, the spare storage and computational resources corresponding to the completed requests are promptly released. Following ORCA, vLLM [51] extends the technique to the retention computation, enabling requests with different KV cache lengths to be batched together. Sarathi [282], DeepSpeed- FastGen [279] and Sarathi- Serve [283] further introduce a split- and- fuse method to batch together prefilling requests and decoding requests. Specifically, this method first splits the long prefilling request in the sequence dimension, and then batches it together with multiple short decoding requests. The split- and- fuse method balances the workloads among different iterations, and significantly reduces the tail latency via removing the stalls from new requests. LightLLM [278] also adopts the split- and- fuse method.

The split- and- fuse technology operates on the premise that requests during the prefilling stage can be partitioned into discrete chunks. Chunked- prefill methodology involves segmenting prefilling requests along the sequence dimension, thereby preventing the potential bottlenecks for other requests. This strategy capitalizes on the auto- regressive characteristics inherent in LLMs, where attention computation only relies on prior tokens. Consequently, the mathematical equivalence of chunked- prefill technology is guaranteed, positioning it as a leading approach for reducing request latency in LLM serving.

#### 6.2.3 Scheduling Strategy

In LLM serving, the job length of each request exhibits variability, and hence the order of executing requests significantly impacts the throughput of the serving system. The head- of- line blocking [280] happens when long requests are accorded priority. Specifically, memory usage grows rapidly in response to long requests, impeding subsequent requests when the system exhausts its memory capacity. The pioneering work ORCA [277] and open- source systems, including vLLM [51] and LightLLM [278], employ the simple first- come- first- serve (FCFS) principle to schedule requests. DeepSpeed- FastGen [279] gives priority to the decoding requests to enhance the performance. FastServe [280] proposes a preemptive scheduling strategy to optimize the head- of- line blocking problem, achieving low job completion time (JCT) in LLM serving. FastServe employs a multi- level feedback queue (MLFQ) to prioritize the requests with the shortest remaining time. Since the auto- regressive decoding approach poses unknown request lengths, FastServe predicts the length first and utilizes a skip- join fashion to find the proper priority for each request. Unlike previous work, VTC [281] discusses the fairness in LLM serving. VTC introduces a cost function based on token numbers to measure fairness among clients, and further proposes a fair scheduler to ensure fairness.

#### 6.2.4 Distributed Systems

In order to achieve high throughput, LLM services are commonly deployed on distributed platforms. Recent works have additionally focused on optimizing the performance of such inference services by exploiting distributed characteristics. Notably, observing that the computations of prefilling and decoding have interference with each other, splitwise [272], Tetrilnfer [273] and DistServe [274] demonstrate the efficiency of disaggregating the prefilling and the decoding steps of a request. In this way, the two distinct steps are processed independently based on their characteristics. ExeGPT [287] also adopts such disaggregated architecture, and proposes different strategies with controllable variables to maximize system throughput under

TABLE 6 Comparison of multiple open-source inference engines and serving systems. - denotes no serving support. Note that the scheduling method of TensorRT-LLM is not open-sourced.  

<table><tr><td rowspan="2">Model</td><td colspan="4">Inference Optimization</td><td rowspan="2">Inference (token/s)</td><td colspan="3">Serving Optimization</td><td rowspan="2">Serving (req/s)</td></tr><tr><td>Attention</td><td>Linear</td><td>Graph</td><td>Speculative Decoding</td><td>Memory</td><td>Batching</td><td>Scheduling</td></tr><tr><td>HuggingFace [260]</td><td></td><td></td><td></td><td></td><td>38,963</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>DeepSpeed [258]</td><td>✓</td><td></td><td>✓</td><td></td><td>80,947</td><td>blocked</td><td>split-and-fuse</td><td>decode prioritized</td><td>6.78</td></tr><tr><td>vLLM [51]</td><td>✓</td><td></td><td>✓</td><td></td><td>90,052</td><td>paged</td><td>continuous-batching</td><td>prefill</td><td>7.11</td></tr><tr><td>OpenPPL [263]</td><td>✓</td><td></td><td>✓</td><td></td><td>81,169</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>FlashDecoding++ [253]</td><td>✓</td><td>✓</td><td>✓</td><td></td><td>106,636</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>LightLLM [278]</td><td>✓</td><td>✓</td><td>✓</td><td></td><td>94,599</td><td>token-wise</td><td>split-and-fuse</td><td>prefill</td><td>10.29</td></tr><tr><td>TensorRT-LLM [222]</td><td>✓</td><td>✓</td><td>✓</td><td></td><td>92,512</td><td>paged</td><td>continuous-batching</td><td>-</td><td>5.87</td></tr></table>

certain latency constraints. Llumnix [288] reschedules the requests at runtime for different serving objectives including load balancing, de- fragmentation, and prioritization. SpotServe [275] is designed to provide LLM service on clouds with preemptible GPU instances. SpotServe efficiently handles challenges including dynamic parallel control and instance migration, and also utilizes the auto- regressive nature of LLMs to achieve token- level state recovery. Moreover, Infinite- LLM [276] parallels different parts of the sequence in the attention operator across the data center, to address the challenges when serving extremely long contexts. LoongServe [289] proposes the elastic sequence parallelism to manage the elastic resource demand at the iteration level, reducing the data movement of KV cache via elaborately designed scheduling.

### 6.3 Hardware Accelerator Design

Previous research efforts [290], [291], [292] have focused on optimizing Transformer architectures, particularly enhancing the attention operator, often employing sparse methods to facilitate FPGA deployment. The FACT [293] accelerator achieves superior energy efficiency compared to the NVIDIA V100 GPU through mixed- precision quantization for linear operators and algorithm- hardware co- design, yet these approaches are not tailored for generative LLMs.

Recent work like ALLO [294] highlights FPGA advantages in managing the memory- intensive decoding stage and emphasizes the importance of model compression techniques for LLMs' efficient FPGA deployment. Conversely, DFX [295] focuses on decoding stage optimizations but lacks model compression methods, limiting scalability to larger models and longer inputs (up to 1.5B model and 256 tokens). ALLO builds on these insights, further offering a library of High- level Synthesis (HLS) kernels that are composable and reusable. ALLO's implementation demonstrates superior generation speed- up compared to DFX in the prefilling stage, achieving enhanced energy efficiency and speedup over the NVIDIA A100 GPU during decoding.

FlightLLM [296] also leverages these insights, introducing a configurable sparse digital signal processor (DSP) chain for various sparsity patterns with high computational efficiency. It proposes an always- on- chip decode scheme with mixed- precision support to enhance memory bandwidth utilization. FlightLLM achieves  $6.0\times$  higher energy efficiency and  $1.8\times$  better cost efficiency than the NVIDIA V100s GPU for Llama2- 7B models, with  $1.2\times$  higher throughput than the NVIDIA A100 GPU during decoding.

### 6.4 Comparison of LLM Frameworks

We compare the performance of multiple LLM frameworks in Table 6. The inference throughput is measured with Llama2- 7B (batch size=1, input length=1k, output length=128). The serving performance is the maximum throughput measured on the ShareGPT [297] dataset. Both are derived on a single NVIDIA A100 80GB GPU. Among the mentioned frameworks, DeepSpeed [258], vLLM [51], LightLLM [278] and TensorRT- LLM [222] integrate the serving function to serve asynchronous requests from multiple users. We also list the optimizations for each framework in the table. All the frameworks except HuggingFace implement operator- level or graph- level optimizations to enhance performance, and some of them also support the speculative decoding technique. Note that the speculative decoding technique is off when we measure the inference performance for all frameworks. The results of inference throughput show that FlashDecoding++ and TensorRT- LLM outperform others with optimizations covering predominant operators and the computational graph. From the aspect of serving, all the frameworks use fine- grained and discontiguous storage for KV cache, and apply the continuous batching techniques to improve the system utilization. Unlike vLLM and LightLLM, DeepSpeed prioritizes the decoding requests in scheduling, which means no new request is merged if there are enough existing decoding requests in the batch.

### 6.5 Knowledge, Suggestions and Future Direction

The system- level optimization improves efficiency while bringing no accuracy degradation, hence becoming prevalent in the LLM inference practice. The optimization for inference is also applicable to serving. Recently, the operator optimization has been closely combined with practical serving scenarios, e.g., RadixAttention [52] designed specifically for prefix caching, and tree attention [243] to accelerate speculative decoding verification. The iterating of applications and scenarios will continue to put forward new requirements for operator development.

Given the multifaceted objectives inherent in real- world serving systems, such as JCT, system throughput, and fairness, the design of scheduling strategies becomes correspondingly intricate. Within the domain of LLM serving, where the length of requests is indeterminate, extant literature commonly relies on predictive mechanisms to facilitate

the design of scheduling strategies. However, the efficacy of current predictors [273] falls short of ideal standards, indicating the potential for refinement and optimization in serving scheduling strategy development.

## 7 DISCUSSIONS OF KEY APPLICATION SCENARIOS

Current research endeavors have made significant strides in exploring the boundaries of efficient LLM inference across various optimization levels. However, further studies are warranted to enhance LLM efficiency in practical scenarios. We have provided promising future directions for optimization techniques at the data- level (Sec. 4.3), model- level (Sec. 5.3), and system- level (Sec. 6.5). In this section, we summarize four critical scenarios: agent and multi- model framework, long- context LLMs, edge scenario deployment, and security- efficiency synergy, and provide a broader discussion on them.

Agent and Multi- Model Framework. As discussed in Sec. 4.3, recent advancements in agent and multi- model frameworks [55], [56], [57] have significantly improved agents' capabilities to handle complex tasks and human requests by harnessing the powerful abilities of LLMs. These frameworks, while increasing the computational demands of LLMs, introduce more parallelism into the structure of LLMs' output content, thereby creating opportunities for data- level and system- level optimizations such as output organization techniques [52]. Furthermore, these frameworks naturally introduce a new optimization level, i.e., pipeline- level, which holds potential for efficiency enhancements at this level [58].

In addition, there is a growing research trend [298] focused on extending AI agents into the multimodal domain, which often utilizes Large Multimodal Models (LMMs) as the core of these agent systems. To enhance the efficiency of these emerging LMM- based agents, designing optimization techniques for LMMs is a promising research direction.

Long- Context LLMs. Currently, LLMs face the challenge of handling increasingly longer input contexts. However, the self- attention operation, the fundamental component of Transformer- style LLMs, exhibits quadratic complexity in relation to the context length, imposing constraints on maximum context length during both training and inference phases. Various strategies have been explored to address this limitation, including input compression (Sec. 4.1), sparse attention (Sec. 5.2.2), design of low- complexity structures (Sec. 5.1.3), and optimization of attention operators (Sec. 6.1.1). Notably, non- Transformer architectures (Sec. 5.1.3) with sub- quadratic or linear complexity have recently garnered significant interest from researchers.

Despite their efficiency, the competitiveness of these novel architectures compared to the Transformer architecture across various abilities, such as in- context learning ability and long- range modeling ability, is still under scrutiny [76], [299]. Therefore, exploring the capabilities of these new architectures from multiple angles and addressing their limitations remains a valuable pursuit. Moreover, it is crucial to determine the necessary context lengths for various scenarios and tasks, as well as identify the next generation architecture that will serve as the foundational backbone for LLMs in the future.

Edge Scenario Deployment. While considerable efforts have been directed towards enhancing the efficiency of LLM inference, deploying LLMs onto extremely resource- constrained edge devices like mobile phones presents ongoing challenges. Recently, numerous researchers [300], [301], [302], [303], [304], [305], [306], [307], [308], [309], [310] have shown interest in pre- training smaller language models with 1B to 3B parameters. Models of this scale offer reduced resource costs during inference and hold potential for achieving generalization abilities and competitive performance compared to larger models. However, the methods to develop such efficient and powerful smaller language models remain under- explored.

Several studies have initiated this promising direction. For instance, MiniCPM [309] conducts sandbox experiments to determine optimal pre- training hyper- parameters. PanGu-  $\pi$  - Pro [302] suggests initializing model weights from pre- trained LLMs using metrics and techniques from model pruning. MobileLLM [310] adopts a"deep and thin" architecture for small model design and proposes weight sharing across different layers to increase the number of layers without additional memory costs. Nevertheless, a performance gap still exists between small and large models, necessitating future studies to narrow this gap. In the future, there is a crucial need for research aimed at identifying the model scale limited in the edge scenarios, and exploring the boundaries of various optimization methods on designing smaller models.

Beyond designing smaller models, system- level optimization offers a promising direction in LLM deployment. A notable recent project, MLC- LLM [311], successfully deploys the LLaMA- 7B model on mobile phones. MLC- LLM primarily employs compilation techniques like fusion, memory planning, and loop optimization to enhance latency and reduce memory cost during inference. Additionally, adopting the cloud- edge collaboration techniques, or designing more sophisticated hardware accelerators can also help deploy LLMs onto edge devices.

Security- Efficiency Synergy. In addition to task performance and efficiency, security is also a crucial factor that must be considered in LLM applications [312], [313]. Current research primarily focuses on efficiency optimization without adequately addressing security considerations. Therefore, it is critical to investigate the interplay between efficiency and security and determine whether the current optimization techniques compromise the security of LLMs. If these techniques negatively impact LLMs' security, a promising direction would involve developing new optimization methods or refining the existing ones to achieve a better trade- off between LLMs' efficiency and security.

## 8 CONCLUSION

Efficient LLM inference focuses on reducing the computational, memory access, and memory costs during LLM inference processes, aiming to optimize efficiency metrics such as latency, throughput, storage, power, and energy. This survey offers a comprehensive review of efficient LLM inference research, presenting insights, recommendations,

and future directions for key techniques. Initially, we introduce a hierarchical taxonomy encompassing data- , model- , and system- level optimizations. Subsequently, guided by this taxonomy, we meticulously examine and summarize studies at each level and sub- field. For well- established techniques like model quantization and efficient serving systems, we conduct experiments to evaluate and analyze their performance. Based on these analyses, we offer practical suggestions and identify promising research avenues for practitioners and researchers in the field.


## REFERENCES

[1] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., "Improving language understanding by generative pre- training," 2018. [2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., "Language models are unsupervised multitask learners," OpenAI blog, vol. 1, no. 8, p. 9, 2019. [3] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few- shot learners," Advances in neural information processing systems, vol. 33, pp. 1877- 1901, 2020. [4] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin et al., "Opt: Open pre- trained transformer language models," arXiv preprint arXiv:2205.01068, 2022. [5] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.- A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar et al., "Llama: Open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023. [6] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Bian, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan et al., "Baichuan 2: Open large- scale language models," arXiv preprint arXiv:2309.10305, 2023. [7] W.- L. Chiang, Z. Li, Z. Liu, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez et al., "Vicuna: An open- source chatbot impressing gpt- 4 with 90%* chatgpt quality," See https://vicuna.lmsys.org (accessed 14 April 2023), 2023. [8] D. Li, R. Shao, A. Xie, Y. Sheng, L. Zheng, J. Gonzalez, I. Stoica, X. Ma, and H. Zhang, "How long can context length of open- source llms truly promise?" in NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [9] B. Workshop, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagne, A. S. Luccioni, F. Yvon et al., "Bloom: A 176b- parameter open- access multilingual language model," arXiv preprint arXiv:2211.05100, 2022. [10] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, E. Goffinet, D. Hesslow, J. Launay, Q. Malaric et al., "The falcon series of open language models," arXiv preprint arXiv:2311.16867, 2023. [11] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang, "Glm: General language model pretraining with autoregressive blank infilling," arXiv preprint arXiv:2103.10360, 2021. [12] A. Q. Jiang, A. Sabatyrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand et al., "Mixtral of experts," arXiv preprint arXiv:2401.04088, 2024.

[13] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, S. Zhong, B. Yin, and X. Hu, "Harnessing the power of llms in practice: A survey on chatgpt and beyond," ACM Transactions on Knowledge Discovery from Data, 2023. [14] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., "Chain- of- thought prompting elicits reasoning in large language models," Advances in Neural Information Processing Systems, vol. 35, pp. 24824- 24837, 2022. [15] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., "Evaluating large language models trained on code," arXiv preprint arXiv:2107.03374, 2021. [16] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg et al., "Sparks of artificial general intelligence: Early experiments with gpt- 4," arXiv preprint arXiv:2303.12712, 2023. [17] X. Zhu, J. Li, Y. Liu, C. Ma, and W. Wang, "A survey on model compression for large language models," arXiv preprint arXiv:2308.07633, 2023. [18] S. Park, J. Choi, S. Lee, and U. Kang, "A comprehensive survey of compression algorithms for language models," arXiv preprint arXiv:2401.15347, 2024. [19] W. Wang, W. Chen, Y. Luo, Y. Long, Z. Lin, L. Zhang, B. Lin, D. Cai, and X. He, "Model compression and efficient inference for large language models: A survey," arXiv preprint arXiv:2402.09748, 2024. [20] Y. Tang, Y. Wang, J. Guo, Z. Tu, K. Han, H. Hu, and D. Tao, "A survey on transformer compression," arXiv preprint arXiv:2402.05964, 2024. [21] T. Ding, T. Chen, H. Zhu, J. Jiang, Y. Zhong, J. Zhou, G. Wang, Z. Zhu, I. Zharkov, and L. Liang, "The efficiency spectrum of large language models: An algorithmic survey," arXiv preprint arXiv:2312.00678, 2023. [22] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, H. Jin, T. Chen, and Z. Jia, "Towards efficient generative large language model serving: A survey from algorithms to systems," arXiv preprint arXiv:2312.15234, 2023. [23] Z. Wan, X. Wang, C. Liu, S. Alam, Y. Zheng, Z. Qu, S. Yan, Y. Zhu, Q. Zhang, M. Chowdhury et al., "Efficient large language models: A survey," arXiv preprint arXiv:2312.03863, vol. 1, 2023. [24] M. Xu, W. Yin, D. Cai, R. Yi, D. Xu, Q. Wang, B. Wu, Y. Zhao, C. Yang, S. Wang et al., "A survey of resource- efficient llm and multimodal foundation models," arXiv preprint arXiv:2401.08092, 2024. [25] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., "A survey of large language models," arXiv preprint arXiv:2303.18223, 2023. [26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017. [27] Z. Yuan, Y. Shang, Y. Zhou, Z. Dong, C. Xue, B. Wu, Z. Li, Q. Gu, Y. J. Lee, Y. Yan et al., "Llm inference unveiled: Survey and roofline model insights," arXiv preprint arXiv:2402.16363, 2024. [28] A. Golden, S. Hsia, F. Sun, B. Acun, B. Hosmer, Y. Lee, Z. DeVito, J. Johnson, G.- Y. Wei, D. Brooks et al., "Is flash attention stable?" arXiv preprint arXiv:2405.02803, 2024. [29] P. Lewis, E. Perez, A. Pik-tus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W.- t. Yih, T. Rocktäschel et al., "Retrieval- augmented generation for knowledge- intensive nlp tasks," Advances in Neural Information Processing Systems, vol. 33, pp. 9459- 9474, 2020. [30] A. Chevalier, A. Wettig, A. Ajith, and D. Chen, "Adapting language models to compress contexts," arXiv preprint arXiv:2305.14788, 2023. [31] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, and W. tau Yih, "Replug: Retrieval- augmented black- box language models," 2023. [32] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, "Self- rag: Learning to retrieve, generate, and critique through self- reflection," 2023. [33] D. Wingate, M. Shoeybi, and T. Sorensen, "Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models," arXiv preprint arXiv:2210.03162, 2022. [34] J. Mu, X. L. Li, and N. Goodman, "Learning to compress prompts with gist tokens," arXiv preprint arXiv:2304.08467, 2023.

[35] T. Ge, J. Hu, X. Wang, S.- Q. Chen, and F. Wei, "In- context autoencoder for context compression in a large language model," arXiv preprint arXiv:2307.06945, 2023. [36] F. Xu, W. Shi, and L. Choi, "Recomp: Improving retrieval- augmented lms with compression and selective augmentation," arXiv preprint arXiv:2310.04408, 2023. [37] W. Fei, X. Niu, P. Zhou, L. Hou, B. Bai, L. Deng, and W. Han, "Extending context window of large language models via semantic compression," arXiv preprint arXiv:2312.09571, 2023. [38] W. Zhou, Y. E. Jiang, R. Cotterell, and M. Sachan, "Efficient prompting via dynamic in- context learning," arXiv preprint arXiv:2305.11170, 2023. [39] Y. Li, B. Dong, F. Guerin, and C. Lin, "Compressing context to enhance inference efficiency of large language models," in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, pp. 6342- 6353. [40] F. Yin, J. Vig, P. Laban, S. Joty, C. Xiong, and C.- S. J. Wu, "Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning," arXiv preprint arXiv:2306.01150, 2023. [41] H. Jung and K.- J. Kim, "Discrete prompt compression with reinforcement learning," arXiv preprint arXiv:2308.08758, 2023. [42] H. Jiang, Q. Wu, C.- Y. Lin, Y. Yang, and L. Qiu, "Limlingua: Compressing prompts for accelerated inference of large language models," in The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [43] H. Jiang, Q. Wu, X. Luo, D. Li, C.- Y. Lin, Y. Yang, and L. Qiu, "Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression," arXiv preprint arXiv:2310.06839, 2023. [44] X. Huang, L. L. Zhang, K.- T. Cheng, and M. Yang, "Boosting llm reasoning: Push the limits of few- shot learning with reinforced in- context pruning," arXiv preprint arXiv:2312.08901, 2023. [45] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui, "A survey for in- context learning," arXiv preprint arXiv:2301.00234, 2022. [46] X. L. Li and P. Liang, "Prefix- tuning: Optimizing continuous prompts for generation," in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021, pp. 4582- 4597. [47] X. Ning, Z. Lin, Z. Zhou, H. Yang, and Y. Wang, "Skeleton- of- thought: Large language models can do parallel decoding," arXiv preprint arXiv:2307.15337, 2023. [48] S. Jin, Y. Wu, H. Zheng, Q. Zhang, M. Lentz, Z. M. Mao, A. Prakash, F. Qian, and D. Zhuo, "Adaptive skeleton graph decoding," arXiv preprint arXiv:2402.12280, 2024. [49] M. Liu, A. Zeng, B. Wang, P. Zhang, J. Tang, and Y. Dong, "Apar: Llms can do auto- parallel auto- regressive decoding," arXiv preprint arXiv:2401.06761, 2024. [50] T. Cai, Y. Li, Z. Geng, H. Peng, J. D. Lee, D. Chen, and T. Dao, "Medusa: Simple llm inference acceleration framework with multiple decoding heads," 2024. [51] W. Kwon, Z. Li, S. Zhuong, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, "Efficient memory management for large language model serving with pagedattention," in Proceedings of the 29th Symposium on Operating Systems Principles, 2023, pp. 611- 626. [52] L. Zheng, L. Yin, Z. Xie, J. Huang, C. Sun, C. H. Yu, S. Cao, C. Kozyrakis, I. Stoica, J. E. Gonzalez et al., "Efficiently programming large language models using sglang," arXiv preprint arXiv:2312.07104, 2023. [53] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan, "Tree of thoughts: Deliberate problem solving with large language models," Advances in Neural Information Processing Systems, vol. 36, 2024. [54] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski, L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk et al., "Graph of thoughts: Solving elaborate problems with large language models," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 17682- 17690. [55] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou et al., "The rise and potential of large language model based agents: A survey," arXiv preprint arXiv:2309.07864, 2023.

[56] Q. Sun, Z. Yin, X. Li, Z. Wu, X. Qiu, and L. Kong, "Corex: Pushing the boundaries of complex reasoning through multimodel collaboration," arXiv preprint arXiv:2310.00280, 2023. [57] T. Guo, X. Chen, Y. Wang, R. Chang, S. Pei, N. V. Chawla, O. Wiest, and X. Zhang, "Large language model based multi- agents: A survey of progress and challenges," arXiv preprint arXiv:2402.01680, 2024. [58] L. Chen, M. Zaharia, and J. Zou, "Frugalgpt: How to use large language models while reducing cost and improving performance," arXiv preprint arXiv:2305.05176, 2023. [59] Y. Li, T. Cai, Y. Zhang, D. Chen, and D. Dey, "What makes convolutional models great on long sequence modeling?" arXiv preprint arXiv:2210.09298, 2022. [60] D. W. Romero, A. Kuzina, E. J. Bekkers, J. M. Tomczak, and M. Hoogendoorn, "Ckconv: Continuous kernel convolution for sequential data," arXiv preprint arXiv:2102.02611, 2021. [61] M. Poli, S. Massulon, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. Re, "Hiena hierarchy: Towards larger convolutional language models," in International Conference on Machine Learning, PMLR, 2023, pp. 28043- 28078. [62] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung, M. Grella, K. K. GV et al., "Rwkv: Reinventing rnns for the transformer era," arXiv preprint arXiv:2305.13048, 2023. [63] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei, "Retentive network: A successor to transformer for large language models," arXiv preprint arXiv:2307.08621, 2023. [64] A. Gu, T. Dao, S. Ermon, A. Rudra, and C. Re, "Hippo: Recurrent memory with optimal polynomial projections," Advances in neural information processing systems, vol. 33, pp. 1474- 1487, 2020. [65] A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. Re, "Combining recurrent, convolutional, and continuous- time models with linear state space layers," Advances in neural information processing systems, vol. 34, pp. 572- 585, 2021. [66] A. Gu, K. Goel, and C. Re, "Efficiently modeling long sequences with structured state spaces," arXiv preprint arXiv:2111.00396, 2021. [67] A. Gupta, A. Gu, and J. Berant, "Diagonal state spaces are as effective as structured state spaces," Advances in Neural Information Processing Systems, vol. 35, pp. 22982- 22994, 2022. [68] A. Gu, K. Goel, A. Gupta, and C. Re, "On the parameterization and initialization of diagonal state space models," Advances in Neural Information Processing Systems, vol. 35, pp. 35971- 35983, 2022. [69] H. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur, "Long range language modeling via gated state spaces," in International Conference on Learning Representations, 2023. [70] D. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re, "Hungry hungry hippos: Towards language modeling with state space models," arXiv preprint arXiv:2212.14052, 2022. [71] R. Hasani, M. Lechner, T.- H. Wang, M. Chahine, A. Amini, and D. Rus, "Liquid structural state- space models," arXiv preprint arXiv:2209.12951, 2022. [72] J. T. Smith, A. Warrington, and S. W. Linderman, "Simplified field state space layers for sequence modeling," arXiv preprint arXiv:2208.04933, 2022. [73] J. Pilault, M. Fathi, O. Firat, C. Pal, P.- L. Bacon, and R. Goroshin, "Block- state transformers," Advances in Neural Information Processing Systems, vol. 36, 2024. [74] J. Wang, J. N. Yan, A. Gu, and A. M. Rush, "Pretreatment without attention," arXiv preprint arXiv:2212.10544, 2022. [75] A. Gu and T. Dao, "Mamba: Linear- time sequence modeling with selective state spaces," arXiv preprint arXiv:2312.00752, 2023. [76] J. Park, J. Park, Z. Xiong, N. Lee, J. Cho, S. Oymak, K. Lee, and D. Papailiopoulos, "Can mamba learn how to learn? a comparative study on in- context learning tasks," arXiv preprint arXiv:2402.04248, 2024. [77] N. Shazeer, "Fast transformer decoding: One write- head is all you need," arXiv preprint arXiv:1911.02150, 2019. [78] J. Ainslie, J. Lee- Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebron, and S. Sanghai, "Gqa: Training generalized multi- query transformer models from multi- head checkpoints," arXiv preprint arXiv:2305.13245, 2023. [79] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, "Lin- former: Self- attention with linear complexity," arXiv preprint arXiv:2006.04768, 2020.

[80] G. I. Winata, S. Cahyawijaya, Z. Lin, Z. Liu, and P. Fung, "Lightweight and efficient end- to- end speech recognition using low- rank transformer," in ICASSP 2020- 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6144- 6148. [81] A. Gupta, Y. Yuan, Y. Zhou, and C. Mendis, "Flurka: Fast fused low- rank & kernel attention," arXiv preprint arXiv:2306.15799, 2023. [82] X. Ma, X. Kong, S. Wang, C. Zhou, J. May, H. Ma, and L. Zettlemoyer, "Luna: Linear unified nested attention," Advances in Neural Information Processing Systems, vol. 34, pp. 2441- 2453, 2021. [83] J. Lee, Y. Lee, J. Kim, A. Kosiorek, S. Choi, and Y. W. Teh, "Set transformer: A framework for attention- based permutation- invariant neural networks," in International conference on machine learning. PMLR, 2019, pp. 3744- 3753. [84] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, "Transformers are Hurs: Fuse autoregressive transformers with linear attention," in International conference on machine learning. PMLR, 2020, pp. 5156- 5165. [85] K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser et al., "Rethinking attention with performers," in International Conference on Learning Representations, 2020. [86] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. Smith, and L. Kong, "Random feature attention," in International Conference on Learning Representations, 2022. [87] P. Kacham, V. Mirrokni, and P. Zhong, "Polysketchformer: Fast transformers via sketches for polynomial kernels," arXiv preprint arXiv:2310.01655, 2023. [88] W. Fedus, B. Zoph, and N. Shazeer, "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity," The Journal of Machine Learning Research, vol. 23, no. 1, pp. 5232- 5270, 2022. [89] Z. Zhang, Y. Lin, Z. Liu, H. Li, M. Sun, and J. Zhou, "Moefication: Transformer feed- forward layers are mixtures of experts," in Findings of the Association for Computational Linguistics: ACL 2022, 2022, pp. 877- 890. [90] Z.- F. Gao, P. Liu, W. X. Zhao, Z.- Y. Lu, and J.- R. Wen, "Parameter- efficient mixture- of- experts architecture for pre- trained language models," in Proceedings of the 29th International Conference on Computational Linguistics, 2022, pp. 3263- 3273. [91] A. Komatsuzaki, J. Puigcerver, J. Lee- Thorp, C. R. Ruiz, B. Mustafa, J. Ainslie, Y. Tay, M. Dehghani, and N. Houlsby, "Sparse upcycling: Training mixture- of- experts from dense checkpoints," arXiv preprint arXiv:2212.05055, 2022. [92] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer, "Base layers: Simplifying training of large, sparse models," in International Conference on Machine Learning. PMLR, 2021, pp. 6265- 6274. [93] Y. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Zhao, A. M. Dai, Q. V. Le, J. Laudon et al., "Mixture- of- experts with expert choice routing," Advances in Neural Information Processing Systems, vol. 35, pp. 7103- 7114, 2022. [94] B. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer, and W. Fedus, "St- moe: Designing stable and transferable sparse expert models," arXiv preprint arXiv:2202.08906, 2022. [95] D. Dai, L. Dong, S. Ma, B. Zheng, Z. Sui, B. Chang, and F. Wei, "Stablemoe: Stable routing strategy for mixture of experts," in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022, pp. 7085- 7095. [96] T. Chen, Z. Zhang, A. K. JAISWAL, S. Liu, and Z. Wang, "Sparse moe as the new dropout: Scaling dense and self- slammable transformers," in The Eleventh International Conference on Learning Representations, 2022. [97] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat et al., "Glam: Efficient scaling of language models with mixture- of- experts," in International Conference on Machine Learning. PMLR, 2022, pp. 5547- 5569. [98] N. Shazeer, A. Mirhosenei, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, "Outrageously large neural networks: The sparsely- gated mixture- of- experts layer," in International Conference on Learning Representations, 2016. [99] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, "Gshard: Scaling giant models with conditional computation and automatic sharding," arXiv preprint arXiv:2006.16168, 2020.

[100] C. Hwang, W. Cui, Y. Xiong, Z. Yang, Z. Liu, H. Hu, Z. Wang, R. Salas, J. Jose, P. Ram et al., "Tuter: Adaptive mixture- of- experts at scale," Proceedings of Machine Learning and Systems, vol. 5, 2023. [101] D. P. Bertsekas, "Auction algorithms for network flow problems: A tutorial introduction," Computational optimization and applications, vol. 1, pp. 7- 66, 1992. [102] Z. Dai, G. Lai, Y. Yang, and Q. Le, "Funnel- transformer: Filtering out sequential redundancy for efficient language processing," Advances in neural information processing systems, vol. 33, pp. 4271- 4282, 2020. [103] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang, "Vision mamba: Efficient visual representation learning with bidirectional state space model," arXiv preprint arXiv:2401.09417, 2024. [104] W. Hua, Z. Dai, H. Liu, and Q. Le, "Transformer quality in linear time," in International Conference on Machine Learning. PMLR, 2022, pp. 9999- 9117. [105] AI21, "Jamba: AI21's groundbreaking ssm- transformer model," March 2024. [Online]. Available: https://www.ai21. com/blog/announcing- jamba[106] W. He, K. Han, Y. Tang, C. Wang, Y. Yang, T. Guo, and Y. Wang, "Densemamba: State space models with dense hidden connection for efficient large language models," arXiv preprint arXiv:2403.00818, 2024. [107] Q. Anthony, Y. Tokpanov, P. Glorioso, and B. Millidge, "Black- mamba: Mixture of experts for state- space models," arXiv preprint arXiv:2402.01771, 2024. [108] M. Pióro, K. Ciebiera, K. Król, J. Ludziejewski, and S. Jaszczur, "Moe- mamba: Efficient selective state space models with mixture of experts," arXiv preprint arXiv:2401.04081, 2024. [109] S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, and J. Susskind, "An attention- free transformer," arXiv preprint arXiv:2105.14103, 2021. [110] T. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V. Tran, Y. Tay, and D. Metzler, "Confident adaptive language modeling," Advances in Neural Information Processing Systems, vol. 35, pp. 17456- 17472, 2022. [111] I. Del Corro, A. Del Giorno, S. Agarwal, B. Yu, A. Awadallah, and S. Mukherjee, "Skipdecode: Autoregressive skip decoding with batching and caching for efficient film inference," arXiv preprint arXiv:2307.02628, 2023. [112] W. Liu, P. Zhou, Z. Wang, Z. Zhao, H. Deng, and Q. Ju, "Fastbert: a self- distilling bert with adaptive inference time," in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 6035- 6044. [113] J. Kong, J. Wang, L.- C. Yu, and X. Zhang, "Accelerating inference for pretrained language models by unified multi- perspective early exiting," in Proceedings of the 29th International Conference on Computational Linguistics, 2022, pp. 4677- 4686. [114] K. Liao, Y. Zhang, X. Ren, Q. Su, X. Sun, and B. He, "A global past- future early exit method for accelerating inference of pretrained language models," in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021, pp. 2013- 2023. [115] J. Xin, R. Tang, J. Lee, Y. Yu, and J. Lin, "Deebert: Dynamic early exiting for accelerating bert inference," in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 2246- 2251. [116] W. Zhou, C. Xu, T. Ge, J. McAuley, K. Xu, and F. Wei, "Bert loses patience: Fast and robust inference with early exit," Advances in Neural Information Processing Systems, vol. 33, pp. 18330- 18341, 2020. [117] T. Sun, X. Liu, W. Zhu, Z. Geng, L. Wu, Y. He, Y. Ni, G. Xie, X.- J. Huang, and X. Qiu, "A simple hash- based early exiting approach for language understanding and generation," in Findings of the Association for Computational Linguistics: ACL 2022, 2022, pp. 2409- 2421. [118] Y. Huang, Y. Chen, Z. Yu, and K. McKeown, "In- context learning distillation: Transferring few- shot learning ability of pre- trained language models," arXiv preprint arXiv:2212.10670, 2022. [119] J. Zhao, W. Zhao, A. Drozdov, B. Rozonoyer, M. A. Sultan, J.- Y. Lee, M. Iyyer, and A. McCallum, "Multistage collaborative knowledge distillation from large language models," arXiv preprint arXiv:2311.08640, 2023. [120] C.- Y. Hsieh, C.- L. Li, C.- K. Yeh, H. Nakhost, Y. Fujii, A. Ratner, R. Krishna, C.- Y. Lee, and T. Pfister, "Distilling step- by- step!"

outperforming larger language models with less training data and smaller model sizes," arXiv preprint arXiv:2305.02301, 2023. [121] L. H. Li, J. Hessel, Y. Yu, X. Ren, K.- W. Chang, and Y. Choi, "Symbolic chain- of- thought distillation: Small models can also think" step- by- step," arXiv preprint arXiv:2306.14050, 2023. [122] L. C. Magister, J. Mallinson, J. Adamek, E. Malmi, and A. Severyn, "Teaching small language models to reason," arXiv preprint arXiv:2212.08410, 2022. [123] H. Chen, S. Wu, X. Quan, R. Wang, M. Yan, and J. Zhang, "Mcckd: Multi- cot consistent knowledge distillation," arXiv preprint arXiv:2310.14747, 2023. [124] N. Ho, L. Schmid, and S. Y. Yun, "Large language models are reasoning teachers," arXiv preprint arXiv:2212.10071, 2022. [125] K. Shridhar, A. Stolfo, and M. Sachan, "Distilling reasoning capabilities into smaller language models," in Findings of the Association for Computational Linguistics: ACL 2023, 2023, pp. 7059- 7073. [126] X. Zhu, B. Qi, K. Zhang, X. Long, and B. Zhou, "Pad: Program- aided distillation specializes large models in reasoning," arXiv preprint arXiv:2305.1388, 2023. [127] P. Wang, Z. Wang, Z. Li, Y. Gao, B. Yin, and X. Ren, "Scott: Self- consistent chain- of- thought distillation," arXiv preprint arXiv:2305.01879, 2023. [128] Z. Chen, Q. Gao, A. Bosselut, A. Sabharwal, and K. Richardson, "Disco: distilling counterfactuals with large language models," in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 5514- 5528. [129] M. Wu, A. Waheed, C. Zhang, M. Abdul- Mageed, and A. F. Aji, "Lamini- lm: A diverse herd of distilled models from large- scale instructions," arXiv preprint arXiv:2304.14402, 2023. [130] Y. Jiang, C. Chan, M. Chen, and W. Wang, "Lion: Adversarial distillation of proprietary large language models," in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, pp. 3134- 3154. [131] Y. Gu, L. Dong, F. Wei, and M. Huang, "Knowledge distillation of large language models," arXiv preprint arXiv:2306.08543, 2023. [132] R. Agarwal, N. Vieillard, P. Stanczyk, S. Ramos, M. Geist, and O. Bachem, "Gkdr: Generalized knowledge distillation for autoregressive sequence models," arXiv preprint arXiv:2306.13649, 2023. [133] C. Liang, S. Zuo, Q. Zhang, P. He, W. Chen, and T. Zhao, "Less is more: Task- aware layer- wise distillation for language model compression," in International Conference on Machine Learning, PMLR, 2023, pp. 20- 852- 20867. [134] I. Timiryasov and J.- L. Tastet, "Baby llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty," arXiv preprint arXiv:2308.02019, 2023. [135] C. Zhang, Y. Yang, J. Liu, X. Wang, Y. Xian, B. Wang, and D. Song, "Lifting the curse of capacity gap in distilling language models," arXiv preprint arXiv:2305.12129, 2023. [136] L. Hou, Z. Huang, L. Shang, X. Jiang, X. Chen, and Q. Liu, "Dynamabert: Dynamic bert with adaptive width and depth," Advances in Neural Information Processing Systems, vol. 33, pp. 9782- 9793, 2020. [137] S. Padmanabhan, Y. Chou, M. J. Zhang, G. Durrett, and E. Choi, "Propagating knowledge updates to lms through distillation," arXiv preprint arXiv:2306.09306, 2023. [138] Y. Yin, C. Chen, L. Shang, X. Jiang, X. Chen, and Q. Liu, "Autotinybert: Automatic hyper- parameter optimization for efficient pre- trained language models," arXiv preprint arXiv:2107.13686, 2021. [139] J. Xu, X. Tan, R. Luo, K. Song, J. Li, T. Qin, and T.- Y. Liu, "Nasbert: task- agnostic and adaptive- size bert compression with neural architecture search," in Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, 2021, pp. 1933- 1943. [140] A. Klein, J. Golebiowski, X. Ma, V. Perrone, and C. Archambeau, "Structural pruning of large language models via neural architecture search," 2023. [141] M. Javaheripi, G. de Rosa, S. Mukherjee, S. Shah, T. Religa, C. C. Teodoro Mendes, S. Bubeck, F. Koushanfar, and D. Dey, "Litetransformersearch: Training- free neural architecture search for efficient language models," Advances in Neural Information Processing Systems, vol. 35, pp. 24254- 24267, 2022. [142] D. D. Xu, S. Mukherjee, X. Liu, D. Dey, W. Wang, X. Zhang, A. Awadallah, and J. Gao, "Few- shot task- agnostic neural archi

tecture search for distilling large language models," Advances in Neural Information Processing Systems, vol. 35, pp. 28 644- 28 656, 2022. [143] A. Kaushal, T. Vaidhya, and I. Rish, "Lord: Low rank decomposition of monolingual code llms for one- shot compression," arXiv preprint arXiv:2309.14021, 2023. [144] M. Xu, Y. L. Xu, and D. P. Mandic, "Tensorgpt: Efficient compression of the embedding layer in llms based on the tensor- train decomposition," arXiv preprint arXiv:2307.00526, 2023. [145] Y. Li, Y. Yu, Q. Zhang, C. Liang, P. He, W. Chen, and T. Zhao, "Losparse: Structured compression of large language models based on low- rank and sparse approximation," arXiv preprint arXiv:2306.11222, 2023. [146] R. Saha, V. Srivastava, and M. Pilanci, "Matrix compression via randomized low rank and low precision factorization," arXiv preprint arXiv:2310.11028, 2023. [147] Z. Yao, X. Wu, C. Li, S. Yount, and Y. Ye, "Zeroquant- v2: Exploring post- training quantization in llms from comprehensive study to low rank compensation," arXiv preprint arXiv:2303.08302, 2023. [148] R. Chand, Y. Prabhu, and P. Kumar, "Dsformer: Effective compression of text- transformers by dense- sparse weight factorization," arXiv preprint arXiv:2312.13211, 2023. [149] Z. Yuan, Y. Shang, Y. Song, Q. Wu, Y. Yan, and G. Sun, "Asvd: Activation- aware singular value decomposition for compressing large language models," arXiv preprint arXiv:2312.05821, 2023. [150] R. Child, S. Gray, A. Radford, and I. Sutskever, "Generating long sequences with sparse transformers," arXiv preprint arXiv:1904.10509, 2019. [151] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, "Efficient streaming language models with attention sinks," arXiv preprint arXiv:2309.17453, 2023. [152] I. Beltagy, M. E. Peters, and A. Cohan, "Longformer: The long- document transformer," arXiv preprint arXiv:2004.05150, 2020. [153] M. Zaheer, G. Guruganesh, K. A. Ditley, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang et al., "Big bird: Transformers for longer sequences," Advances in neural information processing systems, vol. 33, pp. 17 283- 17 297, 2020. [154] S. Dai, H. Genc, R. Venkatesan, and I. Khailany, "Efficient transformer inference with statically structured sparse attention," in 2023 60th ACM/IEEE Design Automation Conference (DAC). IEEE, 2023, pp. 1- 6. [155] Anonymous, "SemSA: Semantic sparse attention is hidden in large language models." 2023. [Online]. Available: https://openreview.net/forum?id=eG9AkhHtYYH. [156] H. Wang, Z. Zhang, and S. Han, "Spatten: Efficient sparse attention architecture with cascade token and head pruning," in 2021 IEEE International Symposium on High- Performance Computer Architecture (HPCA). IEEE, pp. 97- 110. [157] L. Ren, Y. Liu, S. Wang, Y. Xu, C. Zhu, and C. Zhai, "Sparse modular activation for efficient sequence modeling," arXiv preprint arXiv:2306.11197, 2023. [158] S. Anagnostidis, D. Pavllo, L. Biggio, L. Noci, A. Lucchi, and T. Hoffmann, "Dynamic context pruning for efficient and interpretable autoregressive transformers," arXiv preprint arXiv:2305.15805, 2023. [159] N. Kitaev, L. Kaiser, and A. Levskaya, "Reformer: The efficient transformer," arXiv preprint arXiv:2001.04451, 2020. [160] M. Pagliardini, D. Paliotta, M. Jaggi, and F. Fleuret, "Faster causal attention over large sequences through sparse flash attention," arXiv preprint arXiv:2306.01160, 2023. [161] A. Roy, M. Saffar, A. Vaswani, and D. Grangier, "Efficient content- based sparse attention with routing transformers," Transactions of the Association for Computational Linguistics, vol. 9, pp. 53- 68, 2021. [162] Y. Tay, D. Bahri, L. Yang, D. Metzler, and D.- C. Juan, "Sparse sinkhorn attention," in International Conference on Machine Learning. PMLR, 2020, pp. 9438- 9447. [163] Z. Zhang, Y. Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y. Tian, C. Ré, C. Barrett et al., "H2o: Heavy- hitter oracle for efficient generative inference of large language models," Advances in Neural Information Processing Systems, vol. 36, 2024. [164] A. Feng, I. Li, Y. Jiang, and R. Ying, "Diffuser: efficient transformers with multi- hop attention diffusion for long sequences," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 11, 2023, pp. 12 772- 12 780. [165] E. Frantar and D. Alistarh, "Sparsegpt: Massive language models can be accurately pruned in one- shot," 2023.

[166] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter, "A simple and effective pruning approach for large language models," arXiv preprint arXiv:2306.1165, 2023. [167] H. Shao, B. Liu, and Y. Qian, "One- shot sensitivity- aware mixed sparsity pruning for large language models," arXiv preprint arXiv:2310.09499, 2023. [168] A. Syed, P. H. Guo, and V. Sundarapandiyan, "Prune and tune: Improving efficient pruning techniques for massive language models," 2023. [169] X. Wei, Y. Zhang, Y. Li, X. Zhang, R. Gong, J. Guo, and X. Liu, "Outlier suppression+ Accurate quantization of large language models by equivalent and optimal shifting and scaling," arXiv preprint arXiv:2304.09145, 2023. [170] P. Xu, W. Shao, M. Chen, S. Tang, K. Zhang, P. Gao, F. An, Y. Qiao, and P. Luo, "Desa: Pruning large language models with blockwise parameter- efficient sparsity allocation," in The Twelfth International Conference on Learning Representations, 2023. [171] E. Kurtic, D. Campos, T. Nguyen, E. Frantar, M. Kurtz, B. Fineran, M. Goin, and D. Alistarh, "The optimal bert surgeon: Scalable and accurate second- order pruning for large language models," in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022, pp. 4163- 4181. [172] W. Kwon, S. Kim, M. W. Mahoney, J. Hassoun, K. Keutzer, and A. Gholami, "A fast post- training pruning framework for transformers," Advances in Neural Information Processing Systems, vol. 35, pp. 24101- 24116, 2022. [173] Y. Zhang, H. Bai, H. Lin, J. Zhao, L. Hou, and C. V. Cannistraci, "An efficient plug- and- play post- training pruning strategy in large language models," 2023. [174] X. Ma, G. Fang, and X. Wang, "Llm- pruner: On the structural pruning of large language models," Advances in neural information processing systems, vol. 36, 2024. [175] M. Xia, T. Gao, Z. Zeng, and D. Chen, "Sheared llama: Accelerating language model pre- training via structured pruning," arXiv preprint arXiv:2310.06694, 2023. [176] E. Kurtic, E. Frantar, and D. Alistarh, "Ziplm: Inference- aware structured pruning of language models," Advances in Neural Information Processing Systems, vol. 36, 2024. [177] M. Zhang, H. Chen, C. Shen, Z. Yang, L. Ou, X. Yu, and B. Zhuang, "Loraprune: Pruning meets low- rank parameter- efficient fine- tuning," 2023. [178] T. Chen, T. Ding, B. Yadav, I. Zharkov, and L. Liang, "Lorashear: Efficient large language model structured pruning and knowledge recovery," arXiv preprint arXiv:2310.18356, 2023. [179] S. Ashkboos, M. L. Croci, M. G. d. Nascimento, T. Hoefler, and J. Hensman, "Slicegpt: Compress large language models by deleting rows and columns," arXiv preprint arXiv:2401.15024, 2024. [180] Q. Zhang, S. Zuo, C. Liang, A. Bukharin, P. He, W. Chen, and T. Zhao, "Platon: Pruning large transformer models with upper confidence bound of weight importance," in International Conference on Machine Learning. PMLR, 2022, pp. 26 809- 26 823. [181] M. Xia, Z. Zhong, and D. Chen, "Structured pruning learns compact and accurate models," in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022, pp. 1513- 1528. [182] C. Tao, L. Hou, H. Bai, J. Wei, X. Jiang, Q. Liu, P. Luo, and N. Wong, "Structured pruning for efficient generative pre- trained language models," in Linings of the Association for Computational Linguistics: ACL 2023, 2023, pp. 10 880- 10 895. [183] X. Lu, Q. Liu, Y. Xu, A. Zhou, S. Huang, B. Zhang, J. Yan, and H. Li, "Not all experts are equal: Efficient expert pruning and skipping for mixture- of- experts large language models," arXiv preprint arXiv:2402.14800, 2024. [184] A. Muzio, A. Sun, and C. He, "Seer- moe: Sparse expert efficiency through regularization for mixture- of- experts," arXiv preprint arXiv:2404.05089, 2024. [185] P. Dong, L. Li, Z. Tang, X. Liu, X. Pan, Q. Wang, and X. Chu, "Pruner- zero: Evolving symbolic pruning metric from scratch for large language models," in International Conference on Machine Learning (ICML), 2024. [186] Y. Zhang, L. Zhao, M. Lin, S. Yunyun, Y. Yao, X. Han, J. Tanner, S. Liu, and R. Ji, "Dynamic sparse no training: Training- free fine- tuning for sparse llms," in International Conference on Learning Representations (ICLR), 2024.

[187] S.- y. Liu, Z. Liu, X. Huang, P. Dong, and K.- T. Cheng, "Llm- fp4: 4- bit floating- point quantized transformers," in The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [188] L. Li, Q. Li, B. Zhang, and X. Chu, "Norm tweaking: High- performance low- bit quantization of large language models," arXiv preprint arXiv:2309.02784, 2023. [189] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, "Qlora: Efficient finetuning of quantized llms," Advances in Neural Information Processing Systems, vol. 36, 2024. [190] Y. Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen, X. Zhang, and Q. Tian, "Qa- lora: Quantization- aware low- rank adaptation of large language models," arXiv preprint arXiv:2309.14717, 2023. [191] Y. Li, Y. Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and T. Zhao, "Loftq: Lora- fine- tuning- aware quantization for large language models," arXiv preprint arXiv:2310.08659, 2023. [192] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, "Gptq: Accurate post- training quantization for generative pre- trained transformers," arXiv preprint arXiv:2210.17323, 2022. [193] G. Park, M. Kim, S. Lee, J. Kim, B. Kwon, S. J. Kwon, B. Kim, Y. Lee, D. Lee et al., "Lut- gemm: Quantized matrix multiplication based on luts for efficient inference in large- scale generative language models," in The Twelfth International Conference on Learning Representations, 2023. [194] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, "Awq: Activation- aware weight quantization for llm compression and acceleration," arXiv preprint arXiv:2306.00978, 2023. [195] C. Lee, J. Jin, T. Kim, H. Kim, and E. Park, "Owq: Lessons learned from activation outliers for weight quantization in large language models," arXiv preprint arXiv:2306.02272, 2023. [196] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar, S. Ashkboos, A. Borzunov, T. Hoefler, and D. Alistarh, "Spqr: A sparse- quantized representation for near- lossless llm weight compression," arXiv preprint arXiv:2306.03078, 2023. [197] S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W. Mahoney, and K. Keutzer, "Squeeze- lmm: Dense- and- sparse quantization," arXiv preprint arXiv:2306.07629, 2023. [198] J. Chee, Y. Cai, V. Kuleshov, and C. De Sa, "Quip: 2- bit quantization of large language models with guarantees," in Thirty- seventh Conference on Neural Information Processing Systems, 2023. [199] Y. J. Kim, R. Henry, R. Fahim, and H. H. Awadalla, "Finequant: Unlocking efficiency with fine- grained weight- only quantization for llms," arXiv preprint arXiv:2308.09723, 2023. [200] K. Behdin, A. Acharya, A. Gupta, S. Keerthi, and R. Mazumder, "Quantease: Optimization- based quantization for language models—an efficient and intuitive algorithm," arXiv preprint arXiv:2309.01885, 2023. [201] S. Li, X. Ning, K. Hong, T. Liu, L. Wang, X. Li, K. Zhong, G. Dai, H. Yang, and Y. Wang, "Llm- mq: Mixed- precision quantization for efficient llm deployment," 2023. [202] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He, "Zeroquant: Efficient and affordable post- training quantization for large- scale transformers," in Advances in Neural Information Processing Systems, 2022. [203] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Roabinin, B. Chen, P. Liang, C. Re, I. Stoica, and C. Zhang, "Flexgen: High- throughput generative inference of large language models with a single gpu," 2023. [204] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, "Llm. int8 (): 8- bit matrix multiplication for transformers at scale," arXiv preprint arXiv:2208.07339, 2022. [205] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han, "Smoothquant: Accurate and efficient post- training quantization for large language models," in International Conference on Machine Learning. PMLR, 2023, pp. 38 087- 38 099. [206] Z. Yao, X. Wu, C. Li, S. Youn, and Y. He, "Zeroquant- v2: Exploring post- training quantization in llms from comprehensive study to low rank compensation," arXiv preprint arXiv:2303.08302, 2023. [207] Z. Yuan, L. Niu, J. Liu, W. Liu, X. Wang, Y. Shang, G. Sun, Q. Wu, J. Wu, and B. Wu, "Rptq: Reorder- based post- training quantization for large language models," arXiv preprint arXiv:2304.01089, 2023. [208] C. Guo, J. Tang, W. Hu, J. Leng, C. Zhang, F. Yang, Y. Liu, M. Guo, and Y. Zhu, "Olive: Accelerating large language models via hardware- friendly outlier- victim pair quantization," in Proceedings of the 50th Annual International Symposium on Computer Architecture, 2023, pp. 1- 15.

[209] X. Wu, Z. Yao, and Y. He, "Zeroquant- fp: A leap forward in llms post- training w4a8 quantization using floating- point formats," arXiv preprint arXiv:2307.09782, 2023. [210] W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Zhang, P. Gao, Y. Qiao, and P. Luo, "Omnikuant: Omnidirectionally calibrated quantization for large language models," in The Twelfth International Conference on Learning Representations, 2023. [211] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, "Qllm: Accurate and efficient low- bitwidth quantization for large language models," in The Twelfth International Conference on Learning Representations, 2023. [212] Y. Zhao, C.- Y. Lin, K. Zhu, Z. Ye, L. Chen, S. Zheng, L. Ceze, A. Krishnamurthy, T. Chen, and B. Kasikci, "Atom: Low- bit quantization for efficient and accurate llm serving," arXiv preprint arXiv:2310.19102, 2023. [213] W. Huang, Y. Liu, H. Qin, Y. Li, S. Zhang, X. Liu, M. Magno, and Y. Qi, "Billm: Pushing the limit of post- training quantization for llms," 2024. [214] S. Li, X. Ning, L. Wang, T. Liu, X. Shi, S. Yan, G. Dai, H. Yang, and Y. Wang, "Evaluating quantized large language models," arXiv preprint arXiv:2402.18188, 2024. [215] Y. Ma, H. Li, X. Zheng, F. Ling, X. Xiao, R. Wang, S. Wen, F. Chao, and R. Ji, "Affinequant: Affine transformation quantization for large language models," in International Conference on Learning Representations (ICLR), 2024. [216] A. Tseng, J. Chee, Q. Sun, V. Kuleshov, and C. De Sa, "Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks," arXiv preprint arXiv:2402.04396, 2024. [217] S. Ashkboos, A. Mohtarshami, M. L. Croci, B. Li, M. Jaggi, D. Al- istarh, T. Hoefler, and J. Hensman, "Quarot: Outlier- free 4- bit inference in rotated llms," arXiv preprint arXiv:2404.00456, 2024. [218] Z. Liu, C. Zhao, J. Fedorov, B. Soran, D. Choudhary, R. Krishnamorthi, V. Chandra, Y. Tian, and T. Blankevoort, "Spinquant- llm quantization with learned rotations," arXiv preprint arXiv:2405.16406, 2024. [219] C. Hooper, S. Kim, H. Mohammadzadeh, M. W. Mahoney, Y. S. Shao, K. Keutzer, and A. Gholami, "Kvquant: Towards 10 million context length llm inference with kv cache quantization," arXiv preprint arXiv:2401.10009, 2024. [220] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V. Braverman, B. Chen, and X. Hu, "Kivi: A tuning- free asymmetric 2bit quantization for kv cache," arXiv preprint arXiv:2402.02750, 2024. [221] E. Frantar and D. Alistarh, "Optimal brain compression: A framework for accurate post- training quantization and pruning," in Advances in Neural Information Processing Systems, 2022. [222] N. Vaidya, F. Oh, and N. Comly, "Optimizing inference on large language models with nvidia tensorrt- llm, now publicly available," [Online], 2023, https://github.com/NVIDIA/ TensorRT- LLM. [223] InternLM, "Lmdeploy," 2024. [Online]. Available: https:// github.com/InternLM/Imdeploy [224] E. J. Hu, Y. Shen, P. Wallis, Z. Allen- Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, "Lora: Low- rank adaptation of large language models," arXiv preprint arXiv:2106.09685, 2021. [225] B. Hassibi, D. G. Stork, and G. J. Wolff, "Optimal brain surgeon and general network pruning," in IEEE international conference on neural networks. IEEE, 1993, pp. 293- 299. [226] Y. LeCun, J. Denker, and S. Solla, "Optimal brain damage," Advances in neural information processing systems, vol. 2, 1989. [227] D. C. Mocanu, E. Mocanu, P. Stone, P. H. Nguyen, M. Gibescu, and A. Liotta, "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science," Nature communications, vol. 9, no. 1, p. 2383, 2018. [228] B. Zoph and Q. Le, "Neural architecture search with reinforcement learning," in International Conference on Learning Representations, 2016. [229] X. Wang, Y. Zheng, Z. Wan, and M. Zhang, "Svd- llm: Truncation- aware singular value decomposition for large language model compression," arXiv preprint arXiv:2403.07378, 2024. [230] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., "Training language models to follow instructions with human feedback, 2022," URL https://arxiv.org/abs/2203.02155, vol. 13, 2022. [231] Y. Han, G. Huang, S. Song, L. Yang, H. Wang, and Y. Wang, "Dynamic neural networks: A survey," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 11, pp. 7436- 7456, 2021.

[232] C. Xu and J. McAuley, "A survey on dynamic neural networks for natural language processing," in Findings of the Association for Computational Linguistics: EACL 2023, pp. 2370- 2381. [233] X. He, I. Keivanloo, Y. Xu, X. He, B. Zeng, S. Rajagopalan, and T. Chilimbi, "Magic pyramid: Accelerating inference with early exiting and token pruning," Image, 2023. [234] TogetherAI, "Paving the way to efficient architectures: Stripedhyena- 7b, open source models offering a glimpse into a world beyond transformers," December 2023. [Online]. Available: https://www.togetherai/blog/stripedhyena- 7b[235] A. Jaiswal, Z. Gan, X. Du, B. Zhang, Z. Wang, and Y. Yang, "Compressing llms: The truth is rarely pure and never simple," arXiv preprint arXiv:2310.01382, 2023. [236] Y. Leviathan, M. Kalman, and Y. Matias, "Fast inference from transformers via speculative decoding," in International Conference on Machine Learning, PMLR, 2023, pp. 19274- 19286. [237] C. Chen, S. Borgeaud, G. Irving, J. B. Lesplau, L. Sire, and J. Jumper, "Accelerating large language model decoding with speculative sampling," arXiv preprint arXiv:2302.01318, 2023. [238] Y. Zhou, K. Lyu, A. S. Rawat, A. K. Menon, A. Rostamizadeh, S. Kumar, J.- F. Kagy, and R. Agarwal, "Distillspec: Improving speculative decoding via knowledge distillation," arXiv preprint arXiv:2310.08461, 2023. [239] J. Zhang, J. Wang, H. Li, L. Shou, K. Chen, G. Chen, and S. Mehrotra, "Draft & verify: Lossless large language model acceleration via self- speculative decoding," arXiv preprint arXiv:2309.08168, 2023. [240] X. Liu, L. Hu, P. Bailis, I. Stoica, Z. Deng, A. Cheung, and H. Zhang, "Online speculative decoding," arXiv preprint arXiv:2310.07177, 2023. [241] G. Monea, A. Joulin, and E. Grave, "Pass: Parallel speculative sampling," arXiv preprint arXiv:2311.13581, 2023. [242] Z. He, Z. Zhong, T. Cai, J. D. Lee, and D. He, "Rest: Retrieval- based speculative decoding," arXiv preprint arXiv:2311.08252, 2023. [243] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, R. Y. Y. Wong, Z. Chen, D. Arfeen, R. Abhiyankar, and Z. Jia, "Specinfer: Accelerating generative llm serving with speculative inference and token tree verification," arXiv preprint arXiv:2305.09781, 2023. [244] B. Spector and C. Re, "Accelerating llm inference with staged speculative decoding," arXiv preprint arXiv:2308.04623, 2023. [245] Z. Chen, X. Yang, J. Lin, C. Sun, J. Huang, and K. C.- C. Chang, "Cascade speculative drafting for even faster llm inference," arXiv preprint arXiv:2312.11462, 2023. [246] Y. Fu, P. Bailis, I. Stoica, and H. Zhang, "Breaking the sequential dependency of llm inference using lookahead decoding," November 2023. [Online]. Available: https://lmsys.org/blog/2023- 11- 21- lookahead- decoding/[247] Y. Li, C. Zhang, and H. Zhang, "Eagle: Lossless acceleration of llm decoding by feature extrapolation," December 2023. [Online]. Available: https://sites.google.com/view/eagle- llm[248] Z. Sun, A. T. Suresh, J. H. Ro, A. Beirami, H. Jain, and F. Yu, "Spectr: Fast speculative decoding via optimal transport," arXiv preprint arXiv:2310.15141, 2023. [249] F. Liu, Y. Tang, Z. Liu, Y. Ni, K. Han, and Y. Wang, "Kangaroo: Lossless self- speculative decoding via double early exiting," arXiv preprint arXiv:2404.18911, 2024. [250] ggerganov, "Inference of meta's llama model (and others) in pure c/c++," 2024. [Online]. Available: https://github.com/ggerganov/hama.cpp[251] Y. Song, Z. Mi, H. Xie, and H. Chen, "Powerinfer: Fast large language model serving with a consumer- grade gpu," arXiv preprint arXiv:2312.12456, 2023. [252] J. He and J. Zhai, "Fastdecode: High- throughput gpu- efficient llm serving using heterogeneous pipelines," arXiv preprint arXiv:2403.11421, 2024. [253] K. Hong, G. Dai, J. Xu, Q. Mao, X. Li, J. Liu, K. Chen, Y. Dong, and Y. Wang, "Flashdecoding++: Faster large language model inference on gpus," 2024. [254] T. Gale, D. Narayanan, C. Young, and M. Zaharia, "Megablocks: Efficient sparse training with mixture- of- experts," in Proceedings of Machine Learning and Systems (MLSys), 2023. [255] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Re, "Flashattention: Fast and memory- efficient exact attention with io- awareness," Advances in Neural Information Processing Systems, vol. 35, pp. 16344- 16359, 2022.

[256] T. Dao, "Flashattention- 2: Faster attention with better parallelism and work partitioning," arXiv preprint arXiv:2307.08691, 2023. [257] Y. Zhai, C. Jiang, L. Wang, X. Jia, S. Zhang, Z. Chen, X. Liu, and Y. Zhu, "Bytetransformer: A high- performance transformer boosted for variable- length inputs," in 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS). IEEE, 2023, pp. 344- 355. [258] R. Y. Aminabadi, S. Rajbhandari, A. A. Awan, C. Li, D. Li, E. Zheng, O. Ruwase, S. Smith, M. Zhang, J. Rasley et al., "Deepspeed- inference: enabling efficient inference of transformer models at unprecedented scale," in SC22: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 2022, pp. 1- 15. [259] T. Dao, D. Haziza, F. Massa, and G. Sizov, "Flash- decoding for long- context inference," [Online], 2023, https://crfm.stanford.edu/2023/10/12/flashdecoding.html. [260] Haggie- Grace, "Transfmitters: State- of- the- art machine learning for pytorch, tensorflow, and jax." [Online], 2024, https://github.com/huggingftrace/transformers. [261] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.- A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar et al., "Llama: Open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023. [262] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang, "Glm: General language model pretraining with autoregressive blank infilling," in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022, pp. 320- 335. [263] Sensetime, "Openppl: A high- performance deep learning inference platform," [Online], 2023, https://openppl.ai/home. [264] NVIDIA, "cublas: Basic linear algebra on nvidia gpus," [Online], 2017, https://developer.nvidia.com/cublas. [265] "Cutlass: Cuda templates for linear algebra subroutines," [Online], 2017, https://github.com/NVIDIA/cutlass. [266] S. Wang, "Fastgemv: High- speed gemv kernels," [Online], 2023, https://github.com/wangsigping97/FastGEMV. [267] P. Tillet, H. T. Kung, and D. Cox, "Triton: an intermediate language and compiler for tiled neural network computations," in Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, 2019, pp. 10- 19. [268] C. Zhang et al., "Beyond the speculative game: A survey of speculative execution in large language models," arXiv preprint arXiv:2404.14897, 2024. [269] H. Xia et al., "Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding," arXiv preprint arXiv:2401.07851, 2024. [270] M. Stern, N. Shazeer, and J. Uszkoreit, "Blockwise parallel decoding for deep autoregressive models," Advances in Neural Information Processing Systems, vol. 31, 2018. [271] V. Nair and G. E. Hinton, "Rectified linear units improve restricted boltzmann machines," in Proceedings of the 27th international conference on machine learning (ICML- 10), 2010, pp. 807- 814. [272] P. Patel, E. Choukse, C. Zhang, Inigo Goiri, A. Shah, S. Maleki, and R. Bianchini, "Spitwise: Efficient generative llm inference using phase splitting," arXiv preprint arXiv:2311.18677, 2023. [273] C. Hu, H. Huang, L. Xu, X. Chen, J. Xu, S. Chen, H. Feng, C. Wang, S. Wang, Y. Bao, N. Sun, and Y. Shan, "Inference without interference: Disaggregate llm inference for mixed downstream workloads," arXiv preprint arXiv:2401.11181, 2024. [274] Y. Zhong, S. Liu, J. Chen, J. Hu, Y. Zhu, X. Liu, X. Jin, and H. Zhang, "Distserve: Disaggregating prefill and decoding for goodput- optimized large language model serving," arXiv preprint arXiv:2401.09670, 2024. [275] X. Miao, C. Shi, J. Duan, X. Xi, D. Lin, B. Cui, and Z. Jia, "Spotserve: Serving generative large language models on preemptible instances," arXiv preprint arXiv:2311.15566, 2023. [276] B. Lin, T. Peng, C. Zhang, M. Sun, L. Li, H. Zhao, W. Xiao, Q. Xu, X. Qiu, S. Li, Z. Li, Y. Li, and W. Lin, "Infinite- llm: Efficient llm service for long context with distattention and distributed kvcache," arXiv preprint arXiv:2401.02669, 2024. [277] G.- I. Yu, J. S. Jeong, G.- W. Kim, S. Kim, and B.- G. Chun, "Orca: A distributed serving system for transformer- based generative models," in Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation, 2022, pp. 521- 538. [278] ModelTC, "Lightllm," February 2024. [Online]. Available: https://github.com/ModelTC/lightllm/

[279] C. Holmes, M. Tanaka, M. Wyatt, A. A. Awan, J. Rasley, S. Rajbhandari, R. Y. Aminabadi, H. Qin, A. Bakhtiari, L. Kurilenko, and Y. He, "Deepspeed- fastgen: High- throughput text generation for llms via mii and deeppseed- inference," arXiv preprint arXiv:2401.08671, 2024. [280] B. Wu, Y. Zhong, Z. Zhang, G. Huang, X. Liu, and X. Jin, "Fast distributed inference serving for large language models," arXiv preprint arXiv:2305.05920, 2023. [281] Y. Sheng, S. Cao, D. Li, B. Zhu, Z. Li, and D. Zhuo, "Fairness in serving large language models," arXiv preprint arXiv:2401.00588, 2024. [282] A. Agrawal, A. Panwar, J. Mohan, N. Kwatra, B. Gulavani, and R. Ramjee, "Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills," arXiv preprint arXiv:2308.16369, 2023. [283] A. Agrawal, N. Kedia, A. Panwar, J. Mohan, N. Kwatra, B. S. Gulavani, A. Tumanov, and R. Ramjee, "Taming throughput latency tradeoff in llm inference with sarathi- serve," arXiv preprint arXiv:2403.02310, 2024. [284] Y. Jin, C.- F. Wu, D. Brooks, and G.- Y. Wei, "S3: Increasing gpu utilization during generative inference for higher throughput," arXiv preprint arXiv:2306.06000, 2023. [285] Z. Ye, "flashinfer," March 2024. [Online]. Available: https://github.com/flashinfer- ai/flashinfer [286] NVIDIA, "Fastertransformer: About transformer related optimization, including bert, gpt," [Online], 2017, https://github.com/NVIDIA/FasterTransformer. [287] H. Oh, K. Kim, J. Kim, S. Kim, J. Lee, D.- s. Chang, and J. Seo, "Exegpt: Constraint- aware resource scheduling for llm inference," in Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, 2024, pp. 369- 384. [288] B. Sun, Z. Huang, H. Zhao, W. Xiao, X. Zhang, Y. Li, and W. Lin, "Lumnix: Dynamic scheduling for large language model serving," arXiv preprint arXiv:2406.03243, 2024. [289] B. Wu, S. Liu, Y. Zhong, P. Sun, X. Liu, and X. Jin, "Loongserve: Efficiently serving long- context large language models with elastic sequence parallelism," arXiv preprint arXiv:2404.09526, 2024. [290] B. Li, S. Pandey, H. Yang, Y. Lv, P. Li, J. Chen, M. Xie, E. Wan, H. Liu, and C. Ding, "Ftrans: Energy- efficient acceleration of transformers using fpga," arXiv preprint arXiv:2007.08563, 2020. [291] T. J. Ham, Y. Lee, S. H. Seo, S. Kim, H. Choi, S. J. Jun, and J. W. Lee, "Elsa: Hardware- software co- design for efficient, lightweight self- attention mechanism in neural networks," in ACM/IEEE 48th Annual International Symposium on Computer Architecture, 2021, pp. 692- 705. [292] H. Fan, T. Chau, S. I. Venieris, R. Lee, A. Kouris, W. Luk, N. D. Lane, and M. S. Abdelfattah, "Adaptable butterfly accelerator for attention- based nns via hardware and algorithm co- design," in IEEE/ACM International Symposium on Microarchitecture, 2022, pp. 599- 615. [293] Y. Qin, Y. Wang, D. Deng, Z. Zhao, X. Yang, L. Liu, S. Wei, Y. Hu, and S. Yin, "Fact: Ffn- attention co- optimized transformer architecture with eager correlation prediction," in Proceedings of the 50th Annual International Symposium on Computer Architecture, 2023, pp. 1- 14. [294] H. Chen, J. Zhang, Y. Du, S. Xiang, Z. Yue, N. Zhang, Y. Cai, and Z. Zhang, "Understanding the potential of fpga- based spatial acceleration for large language model inference," arXiv preprint arXiv:2312.15159, 2023. [295] S. Hong, S. Moon, J. Kim, S. Lee, M. Kim, D. Lee, and J.- Y. Kim, "Dfx: A low- latency multi- fpga appliance for accelerating transformer- based text generation," in IEEE Hot Chips 34 Symposium, 2022. [296] S. Zeng, J. Liu, G. Dai, X. Yang, T. Fu, H. Wang, W. Ma, H. Sun, S. Li, Z. Huang et al., "Flightllm: Efficient large language model inference with a complete mapping flow on fpga," arXiv preprint arXiv:2401.03868, 2024. [297] S. teams, "Sharegpt," 2023. [Online]. Available: https://sharegpt.com/ [298] J. Xie, Z. Chen, R. Zhang, X. Wan, and G. Li, "Large multimodal agents: A survey," arXiv preprint arXiv:2402.15116, 2024. [299] I. Lee, N. Jiang, and T. Berg- Kirkpatrick, "Exploring the relationship between model architecture and in- context learning ability," arXiv preprint arXiv:2310.08049, 2023. [300] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth,

E. Raff et al., "Pythia: A suite for analyzing large language models across training and scaling," in International Conference on Machine Learning. PMLR, 2023, pp. 2397-2430. [301] 
J. Bai, 
S. Bai, 
Y. Chu, 
Z. Cui, 
K. Dang, 
X. Deng, 
Y. Fan, 
W. Ge, 
Y. Han, 
F. Huang et al., "Qwen technical report," arXiv preprint arXiv:2309.16609, 2023. [302] 
Y. Tang, 
F. Liu, 
Y. Ni, 
Y. Tian, 
Z. Bai, 
Y.-Q. Hu, 
S. Liu, 
S. Jui, 
K. Han, and 
Y. Wang, "Rethinking optimization and architecture for tiny language models," arXiv preprint arXiv:2402.02791, 2024. [303] 
Y. Li, 
S. Bubeck, 
R. Eldan, 
A. Del Giorno, 
S. Gunasekar, and 
Y. 
T. Lee, "Textbooks are all you need ii: phi-1.5 technical report," arXiv preprint arXiv:2309.05163, 2023. [304] 
S. Gunasekar, 
Y. Zhang, 
J. Aneja, 
C. 
C. 
T. Mendes, 
A. Del Giorno, 
S. Gopi, 
M. Javaheripi, 
P. Kauffmann, 
G. de Rosa, 
O. Saarikivi et al., "Textbooks are all you need," arXiv preprint arXiv:2306.11644, 2023. [305] 
P. Zhang, 
G. Zeng, 
T. Wang, and 
W. Lu, "Tinyllama: An open-source small language model," arXiv preprint arXiv:2401.02385, 2024. [306] 
C. Zhang, 
D. Song, 
Z. Ye, and 
Y. Gao, "Towards the law of capacity gap in distilling language models," arXiv preprint arXiv:2311.07052, 2023. [307] 
X. Geng and 
H. Liu, "Openllama: An open reproduction of llama," May 2023. [Online]. Available: https://github.com/openllm-research/open_llama [308] 
M. Bellagente, 
J. Tow, 
D. Mahan, 
D. Phung, 
M. Zhuravinskyi, 
R. Adithyan, 
J. Baicciannu, 
B. Brooks, 
N. Cooper, 
A. Datta et al., "Stable lm 2 1.6 b technical report," arXiv preprint arXiv:2402.17834, 2024. [309] "Minicpm: Unveiling the potential of end-side large language models," 2024. [310] 
Z. Liu, 
C. Zhao, 
F. Iandola, 
C. Lai, 
Y. Tian, 
I. Fedorov, 
Y. Xiong, 
E. Chang, 
Y. Shi, 
R. Krishnamoorthi et al., "Mobilellm: Optimizing sub-billion parameter language models for on-device use cases," arXiv preprint arXiv:2402.14905, 2024. [311] 
M. team, "MLC-LLM," 2023. [Online]. Available: https://github.com/mlc-ai/mlc-llm [312] 
Y. Yao, 
J. Duan, 
K. Xu, 
Y. Cai, 
Z. Sun, and 
Y. Zhang, "A survey on large language model (llm) security and privacy: The good, the bad, and the ugly," High-Confidence Computing, p. 100211, 2024. [313] 
Y. Li, 
H. Wen, 
W. Wang, 
X. Li, 
Y. Yuan, 
G. Liu, 
J. Liu, 
W. Xu, 
X. Wang, 
Y. Sun et al., "Personal llm agents: Insights and survey about the capability, efficiency and security," arXiv preprint arXiv:2401.05459, 2024.