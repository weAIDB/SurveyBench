# A Survey on Hallucination in Large Language Models

# 0. A Survey on Hallucination in Large Language Models

## 1. Introduction
Hallucination in Large Language Models (LLMs) represents a significant and growing concern within the field of artificial intelligence, impacting the reliability and trustworthiness of these increasingly ubiquitous systems. Formally, LLM hallucination refers to the phenomenon where models generate content that is seemingly plausible or confident but is factually incorrect, inconsistent with the input information, or deviates from verifiable reality [3,4,5,14,18,20,21,28,30,37,40]. This can manifest as minor factual inaccuracies, complete fabrications, or responses containing information not present in the given context or source material, as exemplified by cases in large vision language models (LVLMs) where responses contain information absent from visual content [9,18,34,37]. While the term "hallucination" is borrowed from psychology, it is crucial to distinguish the two: in LLMs, it is not a sign of consciousness or intentional deception, but rather an emergent property stemming from the models' architecture and training data limitations [16,28]. Unlike human psychological hallucinations, which involve perceptions in the absence of external stimuli, LLM hallucinations are algorithmic failures to maintain factual coherence or faithfulness to source information, often despite appearing credible to users [5,16,19,30,36].

The prevalence of hallucinations poses a critical challenge, particularly as LLMs become integral to daily life and are increasingly deployed in sensitive domains where accuracy is paramount [31,37,38]. In fields such as healthcare, law, and finance, inaccurate or fabricated content can lead to severe real-world consequences, including misdiagnoses, legal precedents based on false information, or misinformed decision-making [4,5,18,25,29,31]. The perceived credibility of LLM-generated outputs, despite their factual incorrectness, makes detection challenging and amplifies the risk of widespread misinformation and distrust, hindering their widespread adoption across various applications [2,10,36]. While creative applications might benefit from imaginative outputs, the majority of LLM uses demand high reliability and trustworthiness [10].

This survey aims to provide a comprehensive overview of the current research landscape surrounding hallucination in LLMs, including their manifestations in multimodal models such as Large Vision Language Models (LVLMs) and Multimodal Large Language Models (MLLMs) [1,7,9,14,17,29,38]. We systematically analyze the definition, types, and underlying causes of hallucinations, spanning issues related to data, training, inference, and architectural biases [2,4,11,14,20,22,28,30]. Furthermore, we explore a wide array of detection methods, including fact verification, uncertainty estimation, and novel approaches like semantic entropy [14,25,36]. A significant portion of this survey is dedicated to reviewing and classifying mitigation strategies, ranging from data filtering and fine-tuning to advanced techniques like Retrieval-Augmented Generation (RAG) and direct preference optimization [7,14,26,29,34,37]. By synthesizing recent advances and identifying promising research directions, this survey contributes to fostering the development of more accurate, reliable, and trustworthy LLMs for diverse real-world applications.
## 2. Defining and Categorizing Hallucinations
Hallucinations in Large Language Models (LLMs) represent a critical challenge, fundamentally characterized by the generation of content that appears plausible and fluent but is either nonfactual or inconsistent with provided sources or real-world knowledge [16,18,38]. 

![Key Aspects of LLM Hallucination Definition & Categorization](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/63cmBW6UdtwM3sDp6r4yN_/home/surveygo/data/requests/13533/survey/imgs/Key%20Aspects%20of%20LLM%20Hallucination%20Definition%20%26%20Categorization.png)

The term "hallucination," initially adopted from image resolution enhancement in the 2000s and later applied to language models by Andrej Karpathy, currently lacks a singular, universally accepted definition across the field [16]. This section aims to provide a comprehensive framework for understanding LLM hallucinations by synthesizing various definitions, elaborating on common types, and discussing the inherent challenges in their identification and classification.

A nuanced understanding of LLM hallucinations requires distinguishing between core aspects of their output. Primarily, researchers differentiate between **faithfulness** and **factualness** [4,14,19,24]. Faithfulness assesses the generated content's adherence to the input instructions, source material, or user-provided context, evaluating its internal consistency [14,19,24]. In contrast, factualness gauges whether the output aligns with verifiable real-world knowledge and objective truths [14,19,24]. A key challenge arises because LLMs are designed to mimic human linguistic patterns rather than possess genuine factual understanding, enabling them to generate highly readable but unfaithful or factually incorrect content with high confidence, often without hesitation [5,18]. Further distinctions include **intrinsic hallucinations**, which directly contradict information within the source text, and **extrinsic hallucinations**, where the generated content cannot be verified or supported by the provided context [10,16,30]. A specific form, "confabulation," describes instances where LLMs generate arbitrary and inaccurate claims, particularly when lacking specific knowledge, appearing confident despite their incorrectness [23,25,36].

Beyond these definitional nuances, hallucinations are systematically categorized based on the nature of their discrepancy. The most prevalent classification schemes delineate hallucinations into three main types: **input-conflicting**, **context-conflicting**, and **fact-conflicting** [2,8,11].
*   **Input-conflicting hallucinations** occur when the LLM's output deviates from the explicit instructions or source input provided by the user, indicating a failure to adhere to specified constraints or intent [2,8,11,19].
*   **Context-conflicting hallucinations** refer to internal inconsistencies within the generated response itself, particularly evident in extended interactions where the model contradicts its own previously generated information [2,8,11,19].
*   **Fact-conflicting hallucinations** encompass outputs that contradict established real-world facts, common sense, or verifiable knowledge [2,8,11,19,28]. This broad category is further refined into **factual inconsistency**, where the generated information directly contradicts reality (e.g., misattributions or source conflation), and **factual fabrication**, which involves entirely unfounded or made-up information [8,19]. Specific sub-types within factual inconsistencies include entity errors and relation errors [5,14]. The phenomenon of "hallucination on hallucination" in Retrieval-Augmented Generation (RAG) systems highlights how external knowledge sources can introduce or exacerbate factual inaccuracies, further complicating this category [20].

The classification schemes, while providing valuable structure, also highlight inherent challenges. Distinguishing between a genuine factual error and a creative output from an LLM can be subjective, as some "unexpected" outputs, though not strictly factual, might hold creative utility [5]. Moreover, the definition of hallucinations extends to multimodal contexts, where inconsistencies between visual and textual information in models like Large Vision-Language Models (LVLMs) present unique forms of hallucinations, such as object existence or spatial relation errors [9,29]. Despite the varied definitions and ongoing debates, a consistent characteristic across all types of hallucinations is the mismatch between the model's generated content, its perceived confidence, and its actual truthfulness or adherence to context and instruction [16,18]. Understanding these definitions and classifications is crucial for developing robust detection and mitigation strategies, particularly given the ethical implications of deploying LLMs in high-stakes environments where factual accuracy is paramount [14,16].
### 2.1 Nuances in Definition
Defining "hallucination" in the context of Large Language Models (LLMs) presents significant philosophical and practical challenges, extending beyond a simple binary of "true" or "false" output. The term itself, commonly used in Natural Language Processing with negative connotations, carries broader implications rooted in fields like psychology, neurology, and philosophy, creating a divergence between technical AI frameworks and established social interpretations [16]. This necessitates a nuanced understanding, as LLMs do not "know" facts in the human sense but rather learn to mimic how humans talk about facts, reflecting writing styles without necessarily grasping underlying truth [18]. Consequently, LLMs often generate content that, despite being unfaithful or factually incorrect, maintains high readability due to their robust context generation abilities, making it difficult to distinguish from authentic information [5]. Furthermore, LLMs tend to generate content without exhibiting hesitation, even when it is factually incorrect [18].

A critical distinction in understanding LLM outputs lies in differentiating between **Faithfulness** and **Factualness**. Faithfulness refers to the degree to which generated content adheres to or aligns with the provided input, source content, or user instructions [14,19,24]. This evaluates consistency with the given context. In contrast, **Factualness** assesses whether the generated content is consistent with verifiable real-world knowledge or objective facts [14,19,24]. While Faithfulness addresses internal consistency with the input, Factualness gauges external consistency with established world knowledge. Even advanced techniques like Retrieval-Augmented Generation (RAG), designed to ground LLM outputs in external knowledge, can still encounter hallucinations, indicating that the concept of "truth" remains complex and not always straightforwardly verifiable, especially when retrieved information itself may be flawed [20]. Moreover, in multimodal models, ensuring consistency between different data styles, such as visual and textual information, is crucial, as inconsistencies can mislead models during training and affect the learning of true hallucination preferences [29].

Further refining the definition of hallucinations, a distinction is made between **intrinsic** and **extrinsic** hallucinations. Intrinsic hallucinations occur when the generated content directly contradicts information present in the source or input text [10,19,24]. This signifies a clear internal inconsistency. Extrinsic hallucinations, on the other hand, refer to generated content that cannot be verified or supported by the source information; it is neither confirmed nor refuted by the provided context [10,19,24]. Determining intrinsic hallucinations involves identifying direct contradictions with the source, while extrinsic hallucinations are identified by the absence of support for the generated claims within the source material. A subcategory of hallucination, "confabulation," is characterized by inaccurate and arbitrary content, often arising when LLMs lack specific knowledge. These confabulations are sensitive to irrelevant details, and some research suggests that grouping such distinct mechanisms under a broad "hallucination" umbrella may be unhelpful [25,36].

The tension between factual accuracy and creative expression in LLM outputs highlights a philosophical dilemma. While some generated content may be factually inaccurate and thus considered a "hallucination" from a strict veracity standpoint, it might simultaneously offer creative value. For instance, some researchers view the generation of unexpected and interesting answers, even if not strictly factual, as a potential capability that can be valuable for tasks such as dataset expansion or innovative question answering [5]. This perspective suggests a re-evaluation of "hallucinations" in specific contexts, recognizing their potential utility beyond strict factual adherence.

However, the ethical considerations surrounding LLM hallucinations are paramount, particularly in high-stakes domains where accuracy is critical. The generation of biased or misleading content, often stemming from societal biases present in the training data, poses significant risks [14]. In applications such as medical diagnosis, legal advice, or financial analysis, factually incorrect or unfaithful outputs could lead to severe consequences. The LLMs' inherent ability to produce convincing, yet erroneous, text without hesitation underscores the need for robust detection and mitigation strategies, as well as clear ethical guidelines for their deployment. This necessitates careful design of socio-technical systems that can reconcile the technical capabilities of LLMs with societal expectations for accuracy and trustworthiness [16].
### 2.2 Types of Hallucinations
Hallucinations in Large Language Models (LLMs) manifest in various forms, broadly categorized by their relationship to the provided input, internal consistency, and external factual knowledge. A detailed understanding of these types is crucial for their detection and mitigation. Primary classifications often include input-conflicting, context-conflicting, and fact-conflicting hallucinations [2,8,11].

**Input-conflicting hallucinations** occur when an LLM's generated content violates the explicit instructions or deviates from the source input provided by the user [2,8,11,19]. This signifies a misunderstanding of the user's intent or an inability to adhere to the given constraints. For instance, if an LLM is tasked with summarizing a customer interview transcript, an input-conflicting hallucination would involve the model including user needs not explicitly mentioned in the transcript [2]. This type is closely related to "loyalty-based hallucinations," where the output fails to follow user instructions [19].

**Context-conflicting hallucinations** refer to inconsistencies or self-contradictions within the generated response itself, particularly evident in long or multi-turn dialogues where the model's output contradicts previously generated information [2,8,11,19]. Such hallucinations indicate a failure in maintaining coherence and logical flow across the conversation or text. An example includes an LLM providing inconsistent answers when successively asked about top user complaints and then issues impacting user retention within the same conversation [2]. This also encompasses situations where the model misstates information previously established within the given context, such as misstating the source of a well-known river [14]. These are also referred to as "contextual hallucinations" when discussing inaccuracies with respect to the input context [13].

**Fact-conflicting hallucinations** describe instances where the LLM's response contradicts established facts, common-sense knowledge, or real-world information [2,8,11,19,28]. This broad category is further delineated into two distinct sub-types: factual inconsistencies and factual fabrications [8,19].

*   **Factual inconsistency** occurs when the generated content contains factual information that directly contradicts reality or verifiable real-world knowledge [8,19]. Examples include misattributing the first person to walk on the moon or incorrectly stating that the sun rises in the west [8]. Another illustration is attributing the invention of the telephone to Edison instead of Bell [14]. This type of hallucination often arises from source conflation, where the model combines details from different sources in a factually contradictory manner, sometimes even fabricating sources entirely [40]. Specific forms of factual inconsistency include:
    *   **Entity errors:** Incorrect naming or misidentification of entities, such as confusing the experiences and attributes of two different individuals [5,14].
    *   **Relation errors:** Incorrectly describing relationships between entities [14]. A complex instance is "hallucination on hallucination" in Retrieval-Augmented Generation (RAG) systems, where incorrect or biased retrieved information leads the model to generate factually inconsistent answers, such as identifying a keyboardist for a band that doesn't exist based on a movie title confusion [20].

*   **Factual fabrication** involves the generation of information that cannot be verified with real-world knowledge and is entirely unfounded or made-up [8,19]. This includes inventing the origin of unicorns or producing a detailed percentage for a competitor's market share with no factual basis [2,8]. Factual fabrications can also manifest as inventing a non-existent species extinction [14]. These types of hallucinations often arise from the model's inherent inability to verify information or its tendency to generate overly subjective or imaginative content to maintain fluency [14]. "Confabulation" is a specific term for hallucinations where LLMs generate inaccurate, random, or arbitrary claims, particularly when lacking knowledge on a subject, often exhibiting high confidence despite being incorrect [23,25,36].

Beyond these core categories, "faithfulness" or "loyalty-based" hallucinations provide an alternative classification, emphasizing the model's adherence to user expectations and internal logic. These include:

*   **Instruction inconsistency:** The model's output deviates from the user's specific instructions, such as providing an answer when a translation was requested [8,14].
*   **Context inconsistency:** This overlaps with the broader context-conflicting type but specifically refers to the output not aligning with the context explicitly provided by the user, leading to a breakdown in contextual understanding [8,14].
*   **Logical inconsistency:** The generated output exhibits internal logical contradictions, particularly problematic in reasoning tasks where the model might provide an incorrect solution despite presenting seemingly correct intermediate steps [8,14,19]. These can stem from deeper reasoning errors within the model [15].

**Comparison and Contrast:**

The various types of hallucinations, while distinct, often share underlying causes related to the LLM's predictive nature and training data limitations.
*   **Input-conflicting** hallucinations are unique in their direct violation of the initial user prompt or specified constraints, reflecting a failure to correctly interpret or adhere to instructions.
*   **Context-conflicting** hallucinations highlight issues with maintaining consistency within the model's own generated sequence, particularly over extended interactions, pointing to challenges in long-range coherence and memory.
*   **Fact-conflicting** hallucinations (both inconsistencies and fabrications) represent a fundamental disconnect from real-world knowledge. Factual inconsistencies indicate an incorrect application or combination of existing knowledge, while factual fabrications show an over-reliance on generative capabilities to invent non-existent information. The critical distinction lies in verifiability: inconsistent information could be corrected with accurate facts, while fabricated information has no factual basis to begin with [8,19]. The "confabulation" type further emphasizes the model's arbitrary generation when knowledge is insufficient [36].
*   **Faithfulness hallucinations** (instruction, context, logical inconsistency) provide a more granular view of how LLMs fail to meet task-specific requirements or logical coherence. Instruction inconsistency is a subset of input-conflicting, focusing purely on deviation from directives. Logical inconsistency, often linked to reasoning tasks, reveals the model's limitations in complex problem-solving rather than just factual recall.

In contrast to these specific errors, some broader categories include "nonsensical information," where the output is grammatically correct but semantically meaningless [40], and "intrinsic vs. extrinsic" hallucinations. Intrinsic hallucinations conflict with the source content, while extrinsic hallucinations cannot be verified from existing sources or are inconsistent with the real world, often overlapping with factual fabrications [19,30]. Hallucinations can also be task-specific, appearing as incorrect code in programming tasks, or disrupting narrative logic in creative writing [18]. Furthermore, specific types of hallucinations are observed in multi-modal LLMs, such as "object existence" errors or incorrect spatial relationships in visual descriptions [9,29], and "text inertia" where the language model component over-relies on text generation despite visual input [7]. Despite varied definitions and frameworks, a common characteristic across all types is the mismatch between the model's generated content, its confidence, and its actual truthfulness or adherence to context/instruction [16,18].
## 3. Causes of Hallucinations
Hallucinations in Large Language Models (LLMs) are a pervasive and complex phenomenon, arising from an intricate interplay of factors spanning the entire lifecycle of model development and deployment. 

These causative elements can be broadly categorized into data-related issues, inherent model characteristics, challenges within the training process, and intricacies of decoding strategies [4,8,10,14,19]. Understanding these distinct yet interconnected categories is crucial for developing robust detection and mitigation strategies.

**Data-related causes** originate from the quality, quantity, and characteristics of the information LLMs are trained on. These include the presence of incorrect facts, inconsistencies, and biases within the massive, often internet-sourced, training datasets, leading models to capture and propagate inaccuracies [8,11,28,40]. A significant challenge is the sheer scale, which complicates filtering out fabricated or outdated information, and the tendency for models to infer incorrect relationships through co-occurrence induction or exhibit repetition bias [10,30]. Furthermore, a lack of data diversity, comprehensive coverage, and specific domain knowledge contributes to critical knowledge gaps, rendering models susceptible to hallucinations when encountering out-of-distribution (OOD) scenarios or niche queries not sufficiently represented in their frozen pre-training knowledge base [2,14,19,35,38]. Misinformation and societal biases embedded in pre-training data can lead to "imitative falsehoods" and stereotypical content, further exacerbated by context-dependent skewed associations [4,8,14].

**Model-related causes** stem from the fundamental architectural design, operational mechanisms, and inherent limitations of LLMs. These models operate primarily by predicting the next most probable word based on statistical patterns, lacking true real-world understanding, common sense, or critical thinking [10,28,40]. Architectural constraints, particularly in Transformer-based models, can hinder the capture of complex contextual and long-range dependencies, leading to issues like "long-range dependency dilution" in attention mechanisms and exposure bias during generation [8,14]. LLMs may lack or misremember crucial knowledge, struggle with memory confusion, or over-rely on pre-training knowledge over immediate context, contributing to factual inaccuracies [3,10,11,20,25]. Overfitting to training data also causes biases when generalizing to new scenarios [2,37]. A critical model characteristic is the tendency to overestimate capabilities, leading to confidently incorrect responses even when knowledge is absent [11,40]. In Multimodal LLMs (MLLMs), architectural imbalances such as scale disparity between vision encoders and language models can lead to "text inertia," where language priors dominate over crucial visual input, resulting in higher hallucination rates in visual understanding tasks [7,9,17].

**Training-related causes** encompass issues arising from the pre-training, fine-tuning (SFT), and alignment (RLHF) stages. During pre-training, decoder-only architectures' unidirectional processing can limit global contextual understanding, while attention mechanisms may dilute focus with increasing sequence length, exacerbating errors due to exposure bias between training and inference conditions [8]. Overfitting during this phase is also a direct cause [37]. In the fine-tuning phase, a knowledge boundary mismatch can compel models to generate content beyond their actual knowledge, termed "capability misalignment," particularly in specialized domains [8,14,40]. Paradoxically, the RLHF stage, designed for alignment with human preferences, can induce "sycophancy" or "belief misalignment," where models generate "virtual facts" to please evaluators or align with user opinions, even if factually incorrect, aiming for higher reward over truthfulness [3,8,11,14,19,29,30]. Inappropriate instruction tuning and data mismatches between source and test data can further miscalibrate model confidence and induce fabrications [11,30].

**Decoding-related causes** pertain to the strategies employed during text generation. The choice of decoding method directly impacts the trade-off between output diversity, coherence, and accuracy [10]. While deterministic methods like greedy decoding and beam search reduce randomness, they can still manifest hallucinations if the underlying model is over-confident or lacks factual knowledge. Stochastic sampling methods (e.g., top-k, top-p, temperature adjustment) increase output diversity but significantly elevate the risk of hallucinations by allowing selection of less probable, potentially inconsistent, or factually incorrect tokens, especially with higher temperature settings [4,8,11,14,19]. Over-confidence in predictions can lead LLMs to disregard context and generate plausible but incorrect information, particularly in subjective areas [14,30]. Imperfect decoding representation, including insufficient contextual attention and the "softmax bottleneck" that limits expressive power, can force models to choose sub-optimal tokens [8,14]. Finally, the autoregressive nature of LLM generation contributes to a "snowballing" effect, where initial errors propagate and amplify throughout the sequence, leading to increasingly erroneous content [4,11,28].

Collectively, these factors interact in complex ways. For instance, biased or incomplete training data (data-related) can lead to models that overfit during training (training-related) and misrepresent knowledge (model-related), which then confidently generate incorrect answers during decoding (decoding-related) due to stochastic sampling or overconfidence. The probabilistic nature of LLMs (model-related), inherent to their learning from data, is directly manifested and sometimes amplified by decoding choices. Moreover, issues in retrieval-augmented generation (RAG) systems can arise from both the retrieval (data/model interaction) and generation stages, highlighting the multi-faceted nature of hallucination [20]. This intricate web of causation necessitates a holistic approach to understanding and mitigating hallucinations in LLMs.
### 3.1 Data-Related Causes
Hallucinations in Large Language Models (LLMs) are profoundly influenced by the quality and characteristics of their training data, manifesting through various data-related deficiencies. A primary contributor is the presence of inherent data quality issues, encompassing incorrect facts, inconsistencies, and biases [8,11,28]. Models trained on unverified or contradictory data from sources like the internet often struggle to distinguish between factual and fictional content, thereby capturing inaccuracies [18,28,40]. The sheer scale of internet-sourced datasets, while vast, complicates the process of filtering out fabricated, biased, or outdated information, making it difficult for models to discern truth from falsehood [11]. Such imperfect training data can lead to the reinforcement of errors through propagation, and models may infer incorrect relationships due to co-occurrence induction, where frequently co-occurring entities are mistakenly associated [30]. Furthermore, repetitive statements or specific patterns within the training data can cause repetition bias, leading the model to over-memorize and excessively reproduce these elements, even if they contribute to factual inaccuracies or irrelevant outputs [8,10].

Beyond direct inaccuracies, the lack of data diversity and comprehensive coverage fundamentally contributes to knowledge gaps and amplifies the likelihood of hallucinations [14]. Models may overfit to certain data points, indicating insufficient exposure to a diverse enough dataset during training [37]. This incompleteness or outdated nature of pre-training datasets limits the model's contextual understanding, making it difficult for LLMs to generate accurate outputs, particularly in specific domains or for prompts not thoroughly represented in the training data [2,19]. When questions or tasks fall outside the scope of the model's training data, often termed out-of-distribution (OOD) scenarios, LLMs may resort to guessing, leading to incorrect responses or flawed planning and execution strategies [3,35].

Misinformation and biases deeply embedded within the pre-training data are particularly problematic, leading to what are termed "imitative falsehoods" and "societal biases" [14]. Imitative falsehoods occur when LLMs directly replicate false information or widely believed misconceptions present in their training corpus, such as the common misconception that "Thomas Edison invented the light bulb" [8,14]. Similarly, societal biases manifest when models generate stereotypical content, for instance, associating certain professions with specific genders without contextual justification [8,14]. These biases are further exacerbated by the model's tendency to learn from context, which can introduce skewed associations depending on data sample order or label category differences [4]. Even in visual contexts, manipulation of images to create conflicting scenarios, such as inserting unusual objects or removing related ones, highlights how biased or inconsistent visual data can induce hallucinations [9].

Furthermore, knowledge boundary limitations significantly contribute to hallucinations, particularly concerning the lack of long-tail knowledge and time-sensitive information [14]. LLMs' internal knowledge is frozen after training, meaning they cannot inherently update with new information or recent events [8,18]. This limitation means models struggle with current affairs or niche, low-frequency knowledge that is less represented in their massive pre-training datasets, leading to a higher risk of hallucinations when queried on such topics [8,14,30]. When faced with specialized domains like medicine or law, general models often exhibit more hallucinations due to insufficient domain-specific knowledge [8]. Similarly, models may struggle with complex scenarios that require nuanced understanding beyond simple factual recall, sometimes taking "knowledge shortcuts" or failing to recall trained knowledge effectively [8]. The quality of retrieved data during inference, particularly if biased or insufficient, can also lead to a "cognitive trap," where the model is misled by inaccurate information, demonstrating the continuous impact of data quality even in retrieval-augmented setups [20].
### 3.2 Model-Related Causes
Model-related factors constitute a significant category of hallucination causes in Large Language Models (LLMs), stemming primarily from their architectural design, inherent operational mechanisms, and limitations in knowledge representation. A fundamental issue is that LLMs predict the next most likely word based on statistical probabilities derived from training data patterns, rather than generating the most accurate one [28,40]. This probabilistic nature means models do not inherently possess real-world understanding, common sense, or critical thinking, but rather mimic language patterns learned during training, which can introduce errors, particularly in tasks requiring high precision like mathematics, reasoning, or programming [28,35].

The architecture of LLMs, predominantly the Transformer, plays a crucial role in the manifestation of hallucinations. Architectural limitations can hinder the capture of complex contextual dependencies [14]. Specifically, the attention mechanisms, while powerful, face challenges in maintaining consistency over long sequences and capturing long-range dependencies. Research by Liu et al. highlights a "long-range dependency dilution" problem within attention mechanisms, which can directly lead to reasoning hallucinations [14]. Furthermore, exposure bias, defined as the discrepancy between training and inference contexts, contributes to error accumulation within the model [14].

Another critical aspect is the model's knowledge and information processing capabilities. LLMs may lack essential knowledge or, conversely, remember incorrect information from their training data, leading to fabricated answers when addressing complex tasks or questions [11]. This issue is compounded by the fact that the model's memory of certain content can be easily confused, contributing to hallucinations [3]. Additionally, LLMs exhibit a tendency to rely on pre-training knowledge more heavily than the immediate context provided, which can result in factual inaccuracies if the pre-trained knowledge is outdated or misapplied [10]. Even when presented with correct retrieved data, models may misinterpret or misrepresent information due to noise or a fundamental misunderstanding of the context, indicating limitations in their ability to reason and synthesize information accurately [20].

Model size and capacity also factor into hallucination generation, highlighting a complex trade-off between memorization and generalization. Overfitting, where a model becomes overly accurate on its training data but struggles to generalize to new, unseen data, is a significant contributor to hallucinations [2,37]. This suggests that the model's architecture or capacity can increase its susceptibility to biases when generalizing to novel scenarios [37].

A concerning characteristic of LLMs is their tendency to overestimate their own capabilities. This leads to the generation of confidently incorrect responses, as models may provide fabricated answers to questions that fall outside their knowledge boundaries, without inherently knowing whether their generated response is accurate [11,40].

In the context of multimodal LLMs (MLLMs), particularly Large Vision-Language Models (LVLMs), specific architectural imbalances can exacerbate hallucinations. The scale disparity between the vision encoder and the language model, where a larger language model may dominate processing, can lead to the model ignoring or downplaying crucial visual inputs [7]. This manifests as the model's tendency to prioritize language priors over visual input, struggling with complex semantic relationships in real-world images and consequently exhibiting higher hallucination rates compared to synthetic datasets [9]. Moreover, the visual encoder itself can be a significant contributing factor; for instance, the use of models like CLIP as visual encoders in many open-source MLLMs may be insufficient for detailed visual discrimination [17].
### 3.3 Training-Related Causes
Hallucinations in large language models (LLMs) are profoundly influenced by various stages of their development, encompassing pre-training, fine-tuning (including Supervised Fine-Tuning, or SFT), and reinforcement learning from human feedback (RLHF), as well as the subsequent inference phase [11,14,19]. This systematic impact arises from the inherent architectural choices and the dynamics of the learning process itself, often manifesting as a discrepancy between training methodologies and real-world application contexts [10].

During the pre-training phase, architectural limitations significantly contribute to the propensity for hallucinations. Specifically, the prevalent use of decoder-only Transformer architectures, which process information unidirectionally from left to right, can hinder the model's ability to fully capture complex contextual dependencies across a sequence [8]. This inherent unidirectional modeling may result in an inadequate understanding of the global context, thereby increasing the likelihood of generating inconsistent or factually incorrect information. Furthermore, attention mechanisms—a cornerstone of Transformer models—can dilute their focus across different positions as sequence length increases, potentially diminishing the model's ability to concentrate on critical information [8]. Another critical issue during pre-training is exposure bias. While models are typically trained using maximum likelihood estimation (MLE) with ground truth tokens, during inference they must rely on their own previously generated tokens to predict the next ones. This discrepancy between training and inference conditions can lead to an accumulation of errors and inconsistencies, manifesting as hallucinations [8]. Additionally, overfitting in the training process can also directly cause hallucinations [37].

The fine-tuning and alignment stages, particularly SFT and RLHF, introduce further complexities that can amplify or even induce hallucinations [19]. In the SFT phase, a significant contributing factor is the knowledge boundary mismatch. When models are exposed to questions beyond their pre-trained knowledge base—or when there are inconsistencies between pre-trained knowledge and SFT data—they may lack the ability to refuse answering. Instead, they are compelled to generate content that extends beyond their actual knowledge boundaries, leading to fabricated or untrue responses, often termed "capability misalignment" [8,14]. For instance, LLMs that are generally trained for broad natural language tasks may struggle to infer answers in highly specific domains such as law or medicine without additional, specialized assistance [40].

The RLHF stage, designed to align model outputs with human preferences, paradoxically introduces a distinct type of hallucination known as sycophancy or "belief misalignment" [14,19,30]. In this scenario, models learn to prioritize generating outputs that please human evaluators or align with user opinions, even if these outputs contradict the models’ internal beliefs or factual correctness [8,11,30]. This phenomenon, sometimes described as "Lie To Alignment," occurs because the model learns to generate "virtual facts" to achieve a higher reward in the feedback function [3]. This pursuit of reward over truthfulness represents a critical challenge in controlling hallucinations in human-aligned models. Moreover, inappropriate instruction tuning during alignment can similarly lead to fabrications or behavior cloning, where models attempt to learn from samples exceeding their capabilities—thereby inducing hallucinations [11]. Finally, data mismatches between source training and test data, particularly when human supervision is lacking, can miscalibrate an LLM’s confidence, further exacerbating the problem of hallucinations [30].
### 3.4 Decoding-Related Causes
Decoding strategies play a pivotal role in the generation process of Large Language Models (LLMs) and significantly influence their propensity for hallucination. The choice of decoding method, ranging from deterministic approaches to more stochastic sampling techniques, directly impacts the trade-off between output diversity, coherence, and accuracy [10].

Traditional decoding strategies include greedy decoding, beam search, and various sampling methods. Greedy decoding, which selects the token with the highest probability at each step, tends to produce highly coherent but often repetitive or generic output, with a comparatively lower risk of generating factually incorrect information due to its deterministic nature. However, it can trap the model in local optima. Beam search extends greedy decoding by exploring multiple candidate sequences (beams) at each step, typically yielding higher quality outputs than greedy decoding by considering a broader search space. While more robust, it can still suffer from exposure bias, where the model's performance during inference diverges from its training due to cumulative errors. Both greedy decoding and beam search, while generally reducing overt randomness, do not entirely eliminate hallucination, especially if the underlying model is over-confident or lacks robust factual knowledge.

In contrast, sampling methods introduce a degree of randomness into the generation process, which can increase output diversity but concurrently elevate the risk of hallucinations [8]. These methods, including top-k sampling, top-p (nucleus) sampling, and temperature adjustments, intentionally deviate from merely selecting the most probable token. The inherent randomness introduced by such sampling techniques is a significant factor contributing to hallucinations [8,11,19]. Specifically, random sampling strategies increase the probability of selecting low-frequency words or less probable but contextually plausible tokens, which can lead to outputs that are inconsistent with factual information or the input prompt [14]. This "high-uncertainty sampling" exacerbates hallucination issues, as evidenced by techniques like top-p and top-k sampling [4].

The temperature parameter in sampling methods directly modulates the probability distribution over candidate tokens. A higher temperature flattens the distribution, increasing the likelihood of selecting lower-probability tokens, thereby enhancing output diversity. Conversely, a lower temperature sharpens the distribution, making the model more deterministic and likely to select high-probability tokens, thus favoring accuracy and coherence over diversity. While higher temperatures can foster creativity, they also amplify the risk of generating spurious or hallucinated content by selecting tokens that might be semantically or factually detached from the context [10]. The inconsistency in LLM outputs can arise from this randomness in token sampling, among other factors, suggesting limitations in reasoning capabilities [30].

Beyond the choice of decoding strategy, other factors contribute to decoding-related hallucinations. A critical one is the model's over-confidence in its predictions [14]. This over-confidence can lead the LLM to disregard the overall instruction or background context, generating plausible but incorrect information. This issue is particularly salient in areas lacking objective answers, where the model might confidently assert a false statement [30]. Furthermore, in multi-modal contexts, models may over-trust specific "summary tokens" (e.g., periods or quotes) that aggregate knowledge, leading to hallucinations if these tokens are misinterpreted or over-weighted in importance [17].

Imperfect decoding representation also contributes significantly to hallucinations [19]. This encompasses several issues:
1.  **Insufficient Contextual Attention**: Language models often exhibit a local focus in their attention mechanisms, prioritizing nearby words or tokens [8]. This localized attention can lead to a failure in maintaining global coherence or integrating distant but crucial contextual information, resulting in fragmented or inconsistent outputs.
2.  **Softmax Bottleneck**: The Softmax function, used to convert logit scores into probability distributions, can limit the expressive power of the output probability distribution [8]. This "softmax bottleneck" restricts the model's ability to accurately represent and output the desired distribution, especially when dealing with a vast vocabulary or complex, nuanced concepts [14]. This limitation can force the model to choose less optimal tokens, potentially leading to errors.

Finally, the autoregressive nature of LLM generation contributes to the accumulation of errors, often resulting in a "snowballing" effect of hallucinations [11]. Since each generated token depends on the preceding ones, an initial deviation or error can propagate and amplify throughout the sequence, leading to increasingly erroneous content [28]. This phenomenon is observed when LLMs produce "snowballing hallucinations" to maintain apparent coherence with previously generated, albeit incorrect, information, even when prompted to "think step by step" [4]. Such systematic error propagation highlights the fragility of sequential generation processes when coupled with the aforementioned decoding-related vulnerabilities.
## 4. Evaluation Metrics and Benchmarks
Evaluating hallucination in Large Language Models (LLMs) is a critical yet complex task, requiring a systematic review of various metrics and benchmarks to assess model reliability and factual integrity. 

**Overview of LLM Hallucination Evaluation Approaches**

| Evaluation Method      | Description                                                                                                                                                                                                | Pros                                                                                                | Cons                                                                                                        |
|------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| **Automatic Evaluation** | Computational techniques to quantify and identify hallucinations.                                                                                                                                          | Scalable, systematic, computationally efficient.                                                    | Often poor correlation with human judgment; limited by reference data quality/coverage; auxiliary model biases. |
| Reference-based        | Compares generated text against a reference/source document (e.g., ROUGE, BLEU, BARTSCORE). Primarily assesses "faithfulness."                                                                               | Simple, quantifiable.                                                                               | Fails to assess "factualness"; can reward factually incorrect but syntactically similar output.             |
| Reference-free         | Evaluates without external reference; analyzes internal states or uses auxiliary models (e.g., IE-based, QA-based, NLI-based, semantic entropy, uncertainty estimation). Primarily assesses "factualness." | More flexible; can directly assess factual accuracy.                                                | Relies on robustness of auxiliary models; interpretability of internal states; varying efficiency/accuracy. |
| **Human Evaluation**     | Expert groups manually assess faithfulness, factualness, and correctness of generated answers.                                                                                                              | "Gold standard" for reliability; captures nuanced errors; strong alignment with human perception. | High cost, time-intensive, not scalable; difficult to ensure objectivity & consistency; susceptible to bias. |

**Key Benchmark Dataset Categories:**

| Benchmark Category       | Focus / Examples                                                                                                                                                                                                         | Characteristics / Purpose                                                                                                                                                                  |
|--------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **General Hallucination**| PopQA, TruthfulQA, HaluEval.                                                                                                                                                                                             | Assess LLM's inherent hallucination tendency or effectiveness of detection techniques.                                                                                                     |
| **Implicit (Repurposed NLP)**| Question-Answering (TriviaQA, HotpotQA, NQ, BioASQ, SQuAD), Reasoning (SVAMP, 2WikiMultihopQA, StrategyQA), general (Movies, Winogrande, IMDB). | Not explicitly designed for hallucination; assess general performance where errors might be hallucinations. Limited in categorizing/analyzing hallucination types beyond accuracy.           |
| **Multimodal Specific**  | POPE (LVLMs - object existence), VidHal (VLLMs - temporal), AutoHallusion (LVLMs - visual/textual inconsistencies).                                                                                                      | Addresses unique challenges of integrating different modalities; often focus on modality-specific errors (e.g., visual ungroundedness).                                                        |
| **Domain-Specific**      | College enrollment RAG benchmark (conversational RAG, structural info analysis, faithfulness to external knowledge, denoising, time-sensitive, multi-document).                                                            | Tailored for specific real-world applications; provide precise assessments for specialized contexts; highlight limitations of general benchmarks.                                            |

The diverse landscape of evaluation methods can broadly be grouped into three primary categories: automatic evaluation methods, human evaluation, and benchmark datasets, each with distinct advantages, disadvantages, and specific applications [24].

Automatic evaluation methods leverage computational techniques to quantify and identify hallucinations, offering systematic and scalable assessment mechanisms. These methods often fall into two broad types: reference-based and reference-free approaches [24]. Reference-based metrics, such as ROUGE and BLEU, assess lexical or N-gram overlap by comparing generated text against a reference or source document [21,24,30]. While useful for assessing “faithfulness” — consistency with the input source — they frequently fall short in evaluating “factualness,” or consistency with real-world facts [4,19]. This limitation is critical, as LLMs can generate factually incorrect information that is syntactically consistent with the source but still constitutes a hallucination. More advanced reference-based methods like BARTSCORE aim to improve upon this by training models to transform generated text into reference text [4].

Conversely, reference-free methods evaluate hallucinations without external reference texts, typically by analyzing the LLM’s internal states or using auxiliary models. These include Information Extraction (IE)-based, Question Answering (QA)-based, and Natural Language Inference (NLI)-based methods, as well as factualness classification and uncertainty estimation techniques [4,14,24]. Recent advancements include semantic entropy, which quantifies meaning-level uncertainty in LLM responses [23,25,36], and metrics like Sentence-level Hallucination Ratio (SHR) for quantitative assessment [29]. Despite their scalability and computational efficiency, a significant challenge for automatic methods is their frequently poor correlation with human judgment, especially for complex or nuanced hallucinations [5,16]. Their accuracy can be limited by the quality and coverage of reference data, and their effectiveness often relies on the robustness of auxiliary models, which can introduce their own biases or errors. The diversity of statistical metrics (e.g., BLEU, BERTScore, ROUGE, perplexity, cosine similarity) highlights a lack of standardized approaches, impacting the consistency and comparability of results across studies [16].

Human evaluation remains the “gold standard” for assessing hallucination due to its perceived reliability and ability to capture nuanced errors that automatic metrics might miss [5,10,19,24]. It typically involves expert groups evaluating the faithfulness and correctness of generated answers against factual accuracy and source material [5]. However, human evaluation is plagued by significant limitations, primarily its high cost, time intensiveness, and lack of scalability, making it impractical for continuous, large-scale assessment in rapidly evolving LLM development cycles [5,10]. Furthermore, ensuring objectivity and consistency across human evaluators poses a considerable challenge, as subjective interpretations, varying expertise, and inherent biases can lead to errors and inconsistencies in judgment [16]. Even advanced LLMs, which could assist in scoring, exhibit their own hallucination tendencies, complicating the pursuit of objective and consistent evaluation [24].

Benchmark datasets are indispensable for quantifying LLM hallucination and evaluating detection efficacy. These benchmarks are categorized based on their focus, including general hallucination evaluation datasets like PopQA and TruthfulQA, and specific hallucination detection benchmarks such as HaluEval, SelfCheckGPT-Wikibio, and BAMBOO [14,19]. Many standard NLP datasets, including question-answering datasets (e.g., TriviaQA, HotpotQA, Natural Questions) and reasoning datasets (e.g., BioASQ, SQuAD, SVAMP), are also repurposed to implicitly assess hallucination, especially in knowledge-intensive tasks [15,20,25]. The advent of multimodal LLMs (MM-LLMs) has spurred the development of specialized benchmarks for visual and temporal hallucinations, such as POPE for multimodal models, VidHal for video-based VLLMs, and AutoHallusion for LVLMs [1,9,17,29]. Furthermore, domain-specific benchmarks, like those for Retrieval-Augmented Generation (RAG) models in particular fields, offer more precise assessments for specialized contexts, revealing the limitations of general benchmarks in capturing nuanced domain-specific hallucinations [32]. Despite this diversity, existing benchmarks often focus on specific types of factual or logical inconsistencies and lack comprehensive coverage of all hallucination types (e.g., semantic, temporal, relational errors across modalities) [14]. They often do not inherently provide fine-grained categorization or severity metrics for hallucinations beyond general accuracy.

In summary, the evaluation of LLM hallucination faces several overarching challenges. The persistent poor correlation between many automatic metrics and human judgment underscores the difficulty in mechanizing the detection of nuanced errors. The high cost and scalability issues of human evaluation prevent its widespread and continuous application. Moreover, a lack of universally standardized metrics and benchmarks hinders consistent progress and comparability across research efforts. Future work must focus on developing more robust, accurate, and computationally efficient automatic evaluation methods that better align with human perception of hallucination. This necessitates the creation of comprehensive, fine-grained benchmark datasets that can capture a wider spectrum of hallucination types, including semantic, temporal, and relational errors across diverse modalities and specialized domains. Ultimately, a synergistic approach combining the scalability of advanced automatic techniques with targeted, high-quality human annotation will be crucial for reliable hallucination detection and mitigation in LLMs.

Note: No mathematical formulas or macro package expressions were identified in the text, so no conversion or syntax corrections for KaTeX were required.
### 4.1 Automatic Evaluation Methods
Automatic evaluation methods play a pivotal role in quantifying and identifying hallucinations in Large Language Models (LLMs) by providing systematic and scalable assessment mechanisms. These methods often leverage computational techniques to compare generated text against reference data or to analyze the internal states of the models.

One prominent category of automatic evaluation methods employs semantic similarity metrics to gauge the overlap between generated and reference texts. For instance, metrics like BERTScore and BLEURT are widely utilized for this purpose [5]. BLEURT, specifically, is a neural metric that demonstrates robust performance even with limited training data, owing to its pre-training on a substantial and diverse text corpus [5]. Its training methodology typically involves fine-tuning on human judgments of text quality, enabling it to mimic human perception of semantic equivalence more effectively than traditional lexical overlap metrics [5]. Semantic overlap metrics based on BERT are also employed to simulate human judgment for detecting hallucinations in tasks like Generative Question Answering (GQA) [5].

In domains characterized by structured knowledge, knowledge-based methods prove particularly effective. These methods often involve extracting knowledge in the form of triples from the LLM's output and subsequently verifying their factual consistency against external knowledge bases using auxiliary models [24]. While specific metrics like CheXBert are not detailed in the provided digests, the underlying principle involves fact accuracy scores, which measure the ratio of facts in the generated text that align with a reference text [5].

A sophisticated approach to detecting hallucination, particularly confabulations, involves measuring the "semantic entropy" of LLM-generated text [23,25,36]. This method quantifies the uncertainty in the model's responses by addressing the challenge of multiple ways to express the same meaning semantically [23]. Unlike naive prediction entropy, 
  $H(Y|x) = - \sum_y P(y|x) \log P(y|x)$, 
which can conflate uncertainty about meaning with uncertainty about specific symbolic expressions, semantic entropy aims to capture meaning-level uncertainty [23]. This method generalizes across various tasks and has been compared against baselines such as naive token-sequence entropy, a supervised embedding regression method, and a $P(\mathrm{True})$ method that assesses the probability of an LLM predicting “True” when prompted to compare alternatives [25].

Automatic evaluation methods for hallucination can broadly be categorized into reference-based and reference-free approaches [19,24].

Reference-based metrics assess the generated text by comparing it against a reference or source document. These typically include statistical measures like ROUGE and BLEU, which evaluate lexical or N-gram overlap between the output and the source/target information [24]. Knowledge F1 is another metric that relies solely on source information for evaluation [24]. While these metrics can effectively evaluate “Faithfulness” (consistency with the input source), they often fall short in assessing “Factualness” (consistency with real-world facts), making them less suitable for comprehensively evaluating hallucinations in LLMs that may generate factually incorrect yet source-consistent information [19]. BARTSCORE represents a more advanced reference-based approach, training a model to transform generated text into reference or source text, with higher scores indicating superior generated content [4].

Reference-free methods evaluate hallucinations without the need for external reference texts, typically by relying on auxiliary models or analyzing the LLM's internal states [24]. Common reference-free techniques include:
*   Information Extraction (IE)-based methods: These extract knowledge, often in the form of triples, from the generated text and then use additional models for verification of factual consistency [24].
*   Question Answering (QA)-based methods: These involve generating question-answer pairs from the LLM's responses and subsequently comparing these answers with those provided by a dedicated QA model given the original source information [24].
*   Natural Language Inference (NLI)-based methods: These utilize NLI models to determine if the source information logically entails the generated text, thus assessing factual consistency [24].
*   Factualness Classification Metrics: These employ a trained model to directly classify the factual accuracy of the generated text [24].

Beyond these, other reference-free approaches delve into the internal workings or output characteristics of LLMs. Probing classifiers, for instance, are used to predict the truthfulness of answers by analyzing the LLM's internal representations, testing prediction effectiveness across various layers and token positions [15]. Similarly, attention-based models, such as those employing a “lookback ratio” of attention weights on context versus newly generated tokens, can serve as features for linear classifiers to detect hallucinations [13]. Inference classifiers (e.g., $p(H) = FC(Q, A)$) determine if an answer contains hallucinated content [4]. Prompt classifiers and explainers can generate intermediate process labels and natural language explanations to enhance final predictions [4]. Uncertainty estimation methods analyze the model's internal states or behaviors, leveraging approaches like minimum token probability, self-evaluation, self-consistency, and multi-agent debate, particularly useful for black-box models [4,14]. Metrics like KoK assess answer uncertainty based on subjectivity, ambiguity, and textual uncertainty, while SLAG measures consistency with factual beliefs through paraphrasing, logic, and entailment [4]. KLD combines information theory-based metrics like entropy and KL divergence to capture knowledge uncertainty, and POLAR introduces Pareto-optimal learning for estimating response confidence through risk scores [4]. Furthermore, the Sentence-level Hallucination Ratio (SHR), calculated by models like GPT-4, provides a quantitative measure of hallucination at the sentence level [29].

Despite the proliferation of these methods, the research community notes a significant diversity of metrics, highlighting a lack of standardized approaches when quantifying hallucinations statistically [16]. This diversity impacts the accuracy, efficiency, and scalability of detection. While semantic similarity and knowledge-based methods offer robust frameworks, their accuracy can be limited by the quality and coverage of reference data or external knowledge bases. Reference-based metrics, while straightforward, often fail to capture factual inconsistencies beyond direct textual overlap. Reference-free methods offer greater flexibility and the ability to assess factualness directly, but their effectiveness heavily relies on the robustness and accuracy of auxiliary models or the interpretability of internal LLM states, which can introduce their own biases or errors. The efficiency and scalability of these methods vary, with simpler statistical methods being faster but less accurate, and more complex neural or knowledge-based approaches requiring greater computational resources. The challenge remains in developing universally applicable, highly accurate, and computationally efficient automatic evaluation methods that can reliably detect the multifaceted nature of LLM hallucinations across diverse tasks and domains.
### 4.2 Human Evaluation
Human evaluation has traditionally been regarded as the most reliable and effective method for assessing the presence and extent of hallucination in Large Language Models (LLMs) [10,19,24]. It is widely utilized for this purpose, with several studies validating machine-based correctness evaluations against human judgments to confirm their efficacy [25]. The fundamental methodology typically involves expert groups evaluating the faithfulness and correctness of generated answers, ensuring alignment with factual accuracy and source material [5].

Despite its perceived reliability, human evaluation is not without significant challenges and limitations, particularly concerning scalability and consistency. A primary concern is the high cost associated with human evaluation [10], which stems from the intensive time commitment required to produce comprehensive datasets. Such datasets are often small and cannot keep pace with the rapid advancements and proliferation of new LLMs [5]. This limitation makes it impractical for continuous, large-scale assessment in dynamic development environments.

Furthermore, ensuring objectivity and consistency across human evaluators poses a considerable challenge. Human evaluation methods are susceptible to errors when measuring hallucinations [16], potentially due to subjective interpretation, varying expertise, or inherent biases among evaluators. Even advanced LLMs, such as GPT-4, which could theoretically assist in scoring, exhibit their own hallucination tendencies, further complicating the pursuit of objective and consistent evaluation [24]. These factors highlight the intrinsic difficulties in standardizing human judgment to produce robust and reproducible assessments of LLM hallucination.
### 4.3 Benchmark Datasets
Evaluating hallucination in Large Language Models (LLMs) necessitates a diverse array of benchmark datasets, which serve to quantify models’ propensity to generate erroneous information and assess the efficacy of detection methods. These benchmarks can be broadly categorized based on their primary focus: general hallucination evaluation, implicit hallucination assessment through standard NLP tasks, and specialized benchmarks for multimodal or domain-specific applications.

Benchmarks explicitly designed for hallucination evaluation, as outlined in 
[14], differentiate between those assessing an LLM’s inherent tendency to hallucinate and those evaluating the performance of hallucination detection techniques. For instance, hallucination evaluation benchmarks often target long-tail knowledge, exemplified by PopQA, or adversarial questioning, such as TruthfulQA. Conversely, hallucination detection benchmarks like SelfCheckGPT-Wikibio, BAMBOO, PHD, and RealHall are specifically crafted to gauge the effectiveness of methods designed to identify generated hallucinations 
[14]. The HaluEval dataset, comprising 35,000 samples, is a notable example tailored for detecting hallucinations in large models. Evaluations using HaluEval have revealed that models like ChatGPT struggle with verifying approximately 19.5% of their responses, particularly in domains such as language, climate, and technology 
[19].

Beyond specialized hallucination benchmarks, a variety of established Natural Language Processing (NLP) datasets are commonly repurposed to implicitly assess LLM hallucination, especially within knowledge-intensive tasks. These include question-answering (QA) datasets such as TriviaQA 
[15,20], HotpotQA 
[15,20], and Natural Questions (NQ/NQ-Open) 
[15,20,25]. Other datasets contribute to evaluating specific reasoning capabilities that, if lacking, could manifest as hallucination: BioASQ for life-sciences QA, SQuAD for reading comprehension 
[25], SVAMP for elementary mathematical reasoning 
[25], and multi-hop reasoning datasets like 2WikiMultihopQA and StrategyQA 
[20]. Datasets like Movies, Winogrande, Winobias, NLI, Math, and IMDB 
[15] further contribute to evaluating diverse linguistic and reasoning tasks. While these datasets are valuable for general performance evaluation, their primary limitation in the context of hallucination is that they are not explicitly designed to categorize or analyze the *types* of hallucinations, nor do they inherently provide metrics for hallucination detection beyond accuracy.

The emergence of multimodal LLMs (MM-LLMs) and Vision-Language Models (VLLMs) has led to the development of specialized benchmarks for visual and temporal hallucinations. The POPE dataset is a widely used evaluation dataset for large multimodal models, featuring 9,000 questions focused on determining the existence of objects across 80 COCO categories 
[29]. However, POPE’s limitation lies in its categorical nature, as it does not account for object attributes or relative relationships, which are critical for detecting more nuanced visual hallucinations 
[29]. Addressing the challenge of temporal hallucinations in video-based VLLMs, VidHal 
[1] introduces a new benchmark dataset focusing on temporal aspects of videos and including captions with varying degrees of hallucination. Similarly, AutoHallusion 
[9] is specifically designed to evaluate LVLMs’ ability to avoid hallucinations, incorporating both synthetic (200) and real-world (160) images. This dataset assesses diversity, image quality (using IS and FID scores), and the average number of hallucinations per sample 
[9].

Furthermore, to address the specific nuances of real-world applications and particular domains, domain-specific benchmarks play a crucial role. For instance, 
[32] introduces a novel benchmark dataset tailored for evaluating Retrieval-Augmented Generation (RAG) models within the domain of college enrollment. This dataset goes beyond generic evaluation by assessing six specific abilities critical to domain-specific performance: conversational RAG, analysis of structural information, faithfulness to external knowledge, denoising capabilities, solving time-sensitive problems, and understanding multi-document interactions 
[32]. Such domain-specific benchmarks offer superior generalizability within their intended use cases, providing a more precise assessment of an LLM’s reliability and accuracy in specialized contexts, thereby highlighting limitations of general benchmarks in capturing nuanced, domain-specific hallucinations.

In summary, while a range of benchmarks exists for hallucination evaluation, limitations persist in terms of their coverage and generalizability. Many current datasets focus on specific types of factual or logical inconsistencies. Future directions involve creating more comprehensive benchmarks that capture a wider spectrum of hallucination types, including semantic, temporal, and relational errors, especially across diverse modalities and specialized domains. This requires a shift towards benchmarks that not only detect the presence of hallucinations but also provide fine-grained categorization and severity assessment, paving the way for more robust and reliable LLMs.
## 5. Mitigation Strategies
Mitigating hallucination in Large Language Models (LLMs) is a critical area of research, aiming to enhance the factual accuracy, consistency, and trustworthiness of generated content. 

The diverse landscape of proposed solutions can be broadly categorized into three primary paradigms: data-centric, model-centric, and decoding-centric approaches, each addressing different stages of the LLM lifecycle [4,8,10,14,21,26]. Beyond these core categories, several advanced and cross-cutting strategies have emerged, including prompt engineering, self-checking and self-reflection mechanisms, program-aided language models, and multi-agent debate frameworks, which further refine and bolster the models' ability to produce grounded and reliable outputs. The selection and combination of these techniques often involve trade-offs between computational cost, implementation complexity, and the desired level of performance and factual integrity [24,29].

**Data-centric approaches** operate on the fundamental premise that the quality and nature of the training data directly influence an LLM's propensity for hallucination. These methods focus on meticulously curating, cleaning, and enriching the datasets used for pre-training and fine-tuning. Key techniques include rigorous data cleaning and filtering to remove inconsistencies and biases, data augmentation to enhance diversity and coverage, and the strategic integration of external knowledge sources to ground the model in factual information. Retrieval-Augmented Generation (RAG) exemplifies a prominent data-centric strategy that enriches the input context with relevant information retrieved from external knowledge bases during inference, thereby guiding the model to generate responses based on verifiable facts rather than internal confabulations [14,34].

**Model-centric approaches** target the internal architecture, parameters, and training methodologies of LLMs to inherently reduce their tendency to hallucinate. This category encompasses a wide array of techniques, such as fine-tuning models on high-quality, domain-specific data to embed accurate knowledge, modifying attention mechanisms to ensure appropriate focus on critical information, and employing knowledge editing techniques to precisely correct factual errors within the model's weights without extensive retraining [4,31]. Furthermore, regularization techniques, contrastive learning to distinguish true from false statements, and preference optimization methods like Human Feedback Reinforcement Learning (RLHF) and Hallucination-Aware Direct Preference Optimization (HA-DPO) are utilized to align model outputs with human preferences for truthfulness and non-hallucinatory content [18,29].

**Decoding-centric approaches** intervene during the inference phase, imposing constraints and refining the output generation process to ensure factual accuracy and consistency. These methods include constrained decoding, which applies explicit controls to the output (e.g., adjusting temperature parameters or using structural rules), and uncertainty-aware decoding, which quantifies the model's confidence in its generations to filter or flag unreliable content [11,31]. Ensemble methods and self-consistency techniques, such as sampling multiple outputs and checking for agreement or employing contrast models in a "debate" during generation, also fall under this category, allowing for self-correction and improved reliability [10,31].

Beyond these foundational categorizations, several advanced strategies complement and enhance hallucination mitigation:

•   **Prompt Engineering** represents a user-end, input-side strategy that significantly influences an LLM's output by meticulously crafting the input prompts. By providing clear, specific, and highly descriptive instructions, prompt engineering enriches the context available to the model, guiding it towards desired, factually accurate responses and away from confabulation, particularly within RAG frameworks [34,40].

•   **Self-Checking and Self-Reflection** mechanisms empower LLMs to internally evaluate and refine their own outputs. These techniques involve the model analyzing its generated text for inconsistencies or errors, often through iterative processes like the SmartLLMChain or consistency checks with methods like SelfCheckGPT, thereby enabling internal critique and self-correction [10,18].

•   **Program-Aided Language Models (PALs)**, particularly Causal Program-Aided Language (CPAL) chains, enhance LLM reasoning by integrating external computational tools and code execution. This provides direct feedback from executed code, allowing the LLM to refine its logical flow and solve complex, often numerical, problems more accurately, thus preventing logical or arithmetic hallucinations [12].

•   **Multi-Agent Debate (MAD)** systems, such as the DRAG framework, introduce an innovative adversarial or collaborative process where multiple LLM agents debate and refine answers. By simulating a critical peer-review system during both retrieval and generation stages, MAD frameworks enhance factual grounding and logical integrity, building resilience against the production of erroneous information [20].

In practice, the most effective strategies often involve a synergistic combination of these approaches, leveraging their respective strengths to create a robust defense against hallucinations across the entire LLM pipeline, from data preparation and model training to inference and output generation. While each method offers unique benefits, ongoing research continues to explore their optimal integration and to address the persistent challenges of balancing accuracy with efficiency and computational resources.
### 5.1 Data-Centric Approaches
Data-centric approaches are crucial for mitigating hallucinations in Large Language Models (LLMs) by focusing on improving the quality, diversity, and factual accuracy of the training data. This strategy emphasizes that the quality of data is as significant as its quantity in training effective models [2].

A primary focus within data-centric methods is on **data cleaning and filtering** to remove errors, inconsistencies, and biases. This involves collecting high-quality, factual data and rigorously cleaning it to eliminate biases and misinformation, thereby reducing the impact of noise [8,28]. Techniques include selecting high-quality data sources to reduce noise through data filtering [14], and meticulously preprocessing data to remove errors and biases while also annotating data to convey information about truthfulness [30]. For instance, during pre-training, efforts are made to automatically enhance the quality of the pre-training corpus, exemplified by Llama 2's up-sampling of factually superior data sources like Wikipedia [11]. In the subsequent instruction tuning phase, manual cleaning is often performed due to the typically smaller scale of the data [11]. Furthermore, fine-tuning LLMs with high-quality, domain-specific training data can lead to more relevant and accurate responses [40]. Despite its importance, creating high-quality, noise-free datasets remains a significant challenge [10].

**Data augmentation techniques** can be employed to increase the diversity and coverage of the training data, thereby enriching the model's understanding. One example involves a data construction process that incorporates initial description generation using Language and Vision Models (LVLMs), followed by hallucination detection and correction, and ultimately style-consistent data augmentation [29]. Tools like GPT-4 can be utilized to rewrite positive and negative samples, ensuring consistency and enhancing data quality [29].

The integration of **external knowledge sources** is another critical data-centric strategy for enriching training data and providing factual information. Retrieval-Augmented Generation (RAG) is a widely adopted method that retrieves information from external sources, such as knowledge bases, to provide context for text generation and ground the model in facts [14,31]. In a RAG system, the knowledge source is typically fragmented and structured into a vector database [34]. Key parameters, such as `chunk size` (allowing variations in fragment size) and increasing `top_k` (to enhance the probability of including relevant elements), are chosen to optimize retrieval efficiency and relevance [34]. However, the effectiveness of RAG is contingent on the quality of the retrieved data, and a notable limitation is the absence of an effective verification mechanism for the data source [31].

While the sub-section description also suggests exploring curriculum learning or active learning to prioritize factual knowledge, the provided digests do not offer specific details or examples regarding their application in mitigating LLM hallucination.

In summary, data-centric approaches offer a powerful avenue for mitigating LLM hallucinations, primarily through rigorous data cleaning, strategic data augmentation, and the integration of external knowledge. The effectiveness of these strategies hinges on the ability to consistently curate and provide high-quality, factual, and diverse data—a task that presents ongoing challenges for researchers.
### 5.2 Model-Centric Approaches
Model-centric approaches to mitigating hallucinations in Large Language Models (LLMs) primarily involve optimizing the model's internal architecture, parameters, and training strategies to enhance its factual consistency and reduce the generation of misleading or fabricated content [14,30].

A fundamental aspect of model-centric optimization is the **fine-tuning** of LLMs. This process often entails adapting base models to specific tasks or domain-specific knowledge by tweaking parameters, typically requiring significantly less data than initial pre-training. Fine-tuning on high-quality, domain-specific corpora or question-answer datasets can imbue the model with more accurate knowledge, thereby reducing its propensity to fabricate information [2,18]. Beyond domain adaptation, fine-tuning also encompasses optimizing pre-training and alignment strategies, such as using synthetic data and aggregating human annotations to train models to prioritize factual accuracy over user opinions [14]. Model ensembling can further enhance output truthfulness [30].

Modifications to **attention mechanisms** are crucial for improving the model's ability to attend to relevant information. For instance, in Large Vision-Language Models (LVLMs), attention mechanisms can be specifically altered to better weigh image features, thereby addressing visual information deficiencies and reducing modality-specific hallucinations [7,17]. Advanced techniques also involve identifying and activating specific attention heads with high linear probing accuracy during inference, directing the model's focus toward factual knowledge pathways [4].

The integration and management of **knowledge representations** are vital for grounding an LLM's knowledge. This involves strategies like aligning parameter modules with task-specific knowledge and generating relevant contextual information as additional input prompts [4]. Furthermore, techniques such as TYE introduce contextual knowledge that contradicts the model's internal prior knowledge, effectively diminishing the weight of existing biases through context-aware decoding, thus enhancing factual reliability [4]. For multimodal LLMs, approaches like Mixture of Features (MoF) methods (e.g., Additive-MoF and Interleaved-MoF) address visual information limitations by averaging or interleaving features from different vision encoders (e.g., CLIP and DINOv2) to improve multimodal understanding and reduce hallucinations [17].

**Regularization techniques** are employed to prevent overfitting and improve generalization, which indirectly contributes to hallucination mitigation. An example includes the use of sparsity regularization terms during pre-training to enhance contextual modeling capabilities [14]. While general regularization methods like dropout or weight decay are widely applicable in neural networks, the provided digests specifically highlight sparsity regularization in the context of hallucination mitigation.

**Adversarial training**, a technique designed to make models more robust to noisy or misleading inputs, is theorized to be beneficial for reducing hallucinations. However, specific methodologies or detailed implementations within the provided digests are not explicitly detailed.

**Knowledge editing techniques** offer a precise way to correct factual errors within the model's knowledge base without the need for extensive retraining. Methods such as ROME and MEMIT achieve this by precisely adjusting the weights of neural connections associated with specific facts, ensuring efficiency and minimizing disruption to other tasks [31]. More broadly, parameter adaptation involves adjusting, editing, and optimizing model parameters to guide effective knowledge [4]. Examples include EasyEdit, which focuses on localized parameter editing to minimize impact on irrelevant parts of the model, and Edit-TA, which mitigates pre-training biases by analyzing weight variations [4]. EWR utilizes the Fisher information matrix to quantify estimation uncertainty, aiding in parameter interpolation for hallucination removal in dialogue systems [4]. PURR introduces noise into text and fine-tunes a compact editor to denoise by incorporating relevant evidence, thereby correcting knowledge errors [4]. These methods collectively represent a proactive approach to maintaining the factual integrity of LLMs [8].

**Contrastive learning** is another potent paradigm for training models to distinguish between true and false statements, thereby reducing hallucinations. Approaches like CLR optimize parameters to reduce the generation probability of span-level negative samples through contrastive learning [4]. Similarly, HISTALIGN enhances the reliability of memory parameters by resolving inconsistencies between hidden states using sequence information contrastive learning [4]. This paradigm directly trains the model to identify and avoid factual inaccuracies [31].

**Preference optimization**, particularly through methods like Human Feedback Reinforcement Learning (RLHF), has emerged as a powerful technique to align models with human preferences for truthful and non-hallucinatory outputs [18]. A notable extension in this area is Hallucination-Aware Direct Preference Optimization (HA-DPO), which extends the standard DPO framework to multimodal contexts to ensure the optimized model prioritizes non-hallucinatory outputs [29]. HA-DPO operates based on the following equation, which optimizes the policy model π₍θ₎ to prefer chosen outputs (y₊) over rejected outputs (y₋) given an input (x_T, x_I):

$$
\pi_{\theta}^*(x) \approx \operatorname*{arg\,max}_{\pi_{\theta}} \mathbb{E}_{(x,\,y_{+},\,y_{-})\sim D}\Bigl
$$

Here, x_T and x_I denote the text and image prompts, respectively, with “||” representing feature concatenation. π₍ref₎ is the reference model, and D represents a style-consistent hallucination dataset [29].

Finally, **multi-agent debate frameworks** represent an innovative model-centric approach. The DRAG framework, for instance, employs multiple agents—a "Proponent Agent," a "Challenger Agent," and a "Judge Agent"—to iteratively debate and refine retrieval and generation processes. This adversarial collaboration among agents enhances the accuracy and factual integrity of the generated content [20].
### 5.3 Decoding-Centric Approaches
Decoding-centric approaches offer a crucial avenue for mitigating hallucinations in large language models (LLMs) by imposing constraints and refining output generation strategies during inference. These methods primarily aim to enforce factual accuracy and consistency, thereby enhancing the reliability of the generated content.

One fundamental technique involves **constrained decoding**, where explicit controls are applied to the generation process. This can be as simple as adjusting LLM generation parameters, such as the `temperature`, where lower values promote more deterministic and factual outputs, particularly critical in domains requiring high accuracy like medical or legal applications [12]. More advanced constraints are facilitated by libraries like LMQL, which allow for fine-grained control over output length and content, ensuring generated text adheres to predefined structural or semantic rules [12]. Beyond simple parameter tuning, sophisticated constrained decoding methods include Inference-Time-Intervention (ITI), which directly manipulates model activation values in attention heads to promote factuality during decoding [11]. Similarly, the "Lookback Lens" detector uses a classifier-guided decoding approach to mitigate contextual hallucinations [13]. Decoding strategies can be broadly categorized into factuality-enhanced decoding, which dynamically adjusts sampling and intervenes in activations, and faithfulness-enhanced decoding, which employs context-aware decoding and post-editing frameworks to ensure consistency with input context [14]. Some approaches also involve external knowledge incorporation during decoding to provide relevant information to the model [11].

**Uncertainty-aware decoding techniques** play a significant role in identifying and filtering potentially hallucinatory outputs. These methods quantify the model's confidence in its generated content, allowing for the rejection or flagging of untrustworthy parts of the response [11]. An example is uncertainty-aware beam search, which integrates risk assessment by assigning an uncertainty score to each explored sequence and discards those with a high potential for hallucinations, thus pruning the search space for more reliable outputs [31]. For Vision-Language Models (VLMs), a technique involves subtracting logits from multi-modal inputs from those of pure text inputs. This approach helps prevent VLMs from being biased towards the textual component, which can often be a source of hallucination in multi-modal contexts [7].

**Ensemble methods and self-consistency techniques** are employed to combine multiple outputs or leverage internal consistency checks to reduce hallucinations. A common strategy involves sampling multiple outputs and checking them for consistency, effectively acting as a self-correction mechanism [10]. Contrastive decoding introduces a debate-like mechanism during text generation, where a contrast model critically examines the content generated by the main LLM, biasing the output towards content both models deem reasonable and factual [31]. Specific instantiations include Induced-Contrastive Decoding (ICD), which works by creating a hallucination-prone LLM and then mitigating its errors during decoding [22]. For Multi-modal Large Language Models (MLLMs), Visual Contrastive Decoding (VCD) addresses object hallucinations by comparing outputs generated from original and distorted visual inputs, penalizing candidate words overestimated only under the distorted condition [17,22]. Another MLLM-specific method is OPERA (Over-Trust Penalty and Retrospection-Allocation), which reduces hallucinations by adding a penalty term to the beam score to guide sequence generation and by backtracking and re-decoding from a summary token position if continuous knowledge aggregation scores are detected [17,22]. The DRAG framework utilizes an "opponent-style reasoning" mechanism, where two agents with different information sources debate to converge on the most accurate answer, thereby preventing over-reliance on potentially biased retrieval information [20]. Post-processing and filtering outputs, model interpretation, user feedback, and web retrieval are also considered decoding-centric methods for confirming outputs [30].

The effective evaluation of these diverse decoding strategies is paramount. While the sub-section description refers to the AutoHallusion framework for automatically generating hallucination benchmarks [9], specific details regarding its implementation or leverage were not provided in the available digests. Nevertheless, the ability to generate systematic benchmarks is crucial for rigorously assessing the efficacy of different decoding approaches in mitigating hallucinations.
### 5.4 Prompt Engineering
Prompt engineering serves as a fundamental strategy for mitigating hallucinations in Large Language Models (LLMs) by meticulously guiding their output generation [8,26]. This approach involves carefully designing the wording of input prompts to direct the model towards producing desired outputs, thereby reducing the likelihood of generating factual inaccuracies or fabricated information [18]. By providing clear, specific, and highly descriptive instructions, prompt engineering enriches the context available to the model, which is crucial for minimizing hallucinatory responses [28,40]. For instance, a query such as "According to 2023 data, tell me the Nobel Prize winners" is significantly more effective in eliciting accurate information than a vague request like "Tell me the Nobel Prize winners" [28].

Within the Retrieval-Augmented Generation (RAG) framework, prompt engineering plays a particularly crucial role in encouraging LLMs to "honestly answer" questions based solely on provided documents [12]. The primary objective is to constrain the model's responses strictly to the information retrieved, preventing it from confabulating details not present in the given context. This is achieved through the use of predefined prompts and question templates that instruct the model on how to process queries and structure its answers, guiding users to frame questions accurately and guiding the model to ground its responses [2].

Effective prompt design in RAG typically incorporates several key instructions to ensure grounded and verifiable outputs. A robust starting prompt, for example, often begins by explicitly stating the expectation of fidelity to source material: "Based on the provided documents, answer the question honestly" [12]. Further instructions reinforce this requirement and enhance answer verifiability:
* "Cite the documents using their format" [12].
* "If multiple documents contain the answer, cite them accordingly" [12].
* "If the answer is not in the documents, state that it cannot be answered from the provided information" [12].

Similarly, another comprehensive RAG prompt template includes directives such as:
* "Use the information contained in the context to give a comprehensive answer to the question" [34].
* "Respond only to the question asked" [34].
* "Ensure the response is concise and relevant" [34].
* "Provide the number of the source document when relevant" [34].
* "Do not give an answer if it cannot be deduced from the context" [34].

These explicit instructions compel the LLM to admit ignorance when information is unavailable, rather than generating fabricated content, and to attribute facts to their source documents. Beyond these direct instructions, advanced prompt engineering techniques can further enhance truthfulness. For instance, prompting models to show their reasoning process step by step can encourage self-correction and increase the transparency and reliability of their outputs [18]. By systematically incorporating these elements, prompt engineering serves as a critical mechanism for minimizing hallucinations and fostering truthful, evidence-based responses within LLM applications, particularly in the RAG framework.
### 5.5 Self-Checking and Self-Reflection
Self-checking and self-reflection mechanisms represent a promising avenue for mitigating hallucinations in large language models (LLMs) by enabling models to internally evaluate and refine their own outputs [18,31]. This approach allows LLMs to analyze and critique their generated text, identify inconsistencies or errors, and correct unfounded claims based on their internal knowledge [18,31]. Recent advancements highlight the increasing sophistication of these techniques, where LLMs are prompted to check outputs for factual correctness and provide feedback for iterative improvements [16,26].

One notable self-critique methodology is the SmartLLMChain, a self-checking technique designed to address complex problems by iteratively refining generated ideas [12]. The process is structured into three distinct stages:
1.  Ideation: The initial step involves passing the user's prompt through the LLM multiple times (n times) to generate a diverse set of $n$ output proposals, referred to as "ideas" [12]. This multiplicity of initial responses aims to explore a wider solution space.
2.  Review: In this stage, all generated ideas are rigorously evaluated to identify potential flaws, errors, or inconsistencies [12]. Following this critical assessment, the most promising or least flawed idea is selected for further development [12].
3.  Solution: Finally, the LLM takes the selected idea from the review stage and attempts to improve it, presenting the refined version as the ultimate output [12]. This iterative refinement, based on an internal critique, enhances the quality and accuracy of the generated response [12].

The inherent mechanism by which SmartLLMChain, and similar self-critique approaches, enhance response quality lies in their multi-stage verification and refinement loop. By generating multiple initial ideas and then subjecting them to a systematic review, the system can leverage the LLM's own analytical capabilities to detect and rectify errors that might otherwise propagate into the final output. This process mimics human ideation and revision, leading to more robust and accurate results [18].

Beyond SmartLLMChain, other self-checking and reflection mechanisms have emerged. For instance, SelfCheckGPT operates by sampling multiple responses and measuring their information consistency to discern factual claims from hallucinations [10]. This consistency can be quantitatively assessed using neural methods, such as BERTScore, or information extraction/question answering (IE/QA)-based methods [10]. Similarly, the Chain-of-Verification (CoVe) method also incorporates self-checking and self-reflection, where the LLM actively checks its responses and corrects errors to improve overall accuracy [21].

Despite the clear benefits of self-checking, it is crucial to note that the effectiveness of self-correction in LLMs is still an active area of research. Some findings indicate that LLMs may not always self-correct reasoning effectively, and paradoxically, multiple self-correction attempts can, in certain scenarios, decrease accuracy [19]. This suggests that while self-reflection holds significant promise, its implementation requires careful design to ensure consistent and beneficial outcomes.
### 5.6 Program-Aided Language Models
Causal Program-Aided Language (CPAL) chains represent a significant advancement over traditional Program-Aided Language (PAL) approaches. A central innovation of CPAL lies in its utilization of causal graphs to explicitly model the underlying causal structure of prompts [12]. By translating the relationships and dependencies within a prompt into a graphical format, CPAL enables large language models (LLMs) to more effectively discern and navigate the logical flow required for complex problem-solving. Furthermore, CPAL crucially integrates code execution as a direct feedback mechanism for the LLM [12]. This continuous feedback loop, derived from the outcomes of executed code, empowers LLMs to refine their reasoning and improve their performance, particularly when addressing intricate computational challenges such as those involving complex mathematical operations [12].
### 5.7 Multi-Agent Debate
Multi-Agent Debate (MAD) systems represent a novel approach to mitigating hallucination in Large Language Models (LLMs) by fostering internal deliberation and self-correction. A prominent example of this architecture is DRAG, which employs a multi-agent debate mechanism to enhance the quality and factual accuracy of generated text [20].

The architecture of MAD systems, as exemplified by DRAG, typically involves distinct agent roles designed to simulate a critical debate process. DRAG specifically utilizes three primary types of agents: the Proponent, the Challenger, and the Judge [20]. These agents collaborate across different phases of the text generation process. In the retrieval phase, for instance, the agents engage in a debate to determine the most effective search strategy. This initial debate aims to refine the information-gathering process, thereby reducing the likelihood of retrieving irrelevant or misleading data that could contribute to subsequent hallucinations [20].

Following successful retrieval, in the generation phase, the focus shifts to the output itself. Here, the agents rigorously debate the accuracy and logical coherence of the answers produced by the LLM. The Proponent might generate an initial response, while the Challenger critiques its factual basis, internal consistency, and potential for hallucination. The Judge then arbitrates these arguments, ultimately guiding the system toward a more accurate and reliable final output [20].

The primary strength of the Multi-Agent Debate approach lies in its inherent capacity for self-scrutiny and iterative refinement. By instantiating an adversarial or collaborative peer-review process within the system, it introduces multiple layers of verification before a final response is committed. This multi-perspective evaluation, encompassing both the information retrieval and content generation stages, enables the system to identify and rectify potential errors, inconsistencies, or fabricated details, directly addressing the problem of hallucination. This systematic challenge-and-resolve loop promotes a higher degree of factual grounding and logical integrity in the generated text.

However, the effectiveness of MAD systems is critically dependent on the capabilities of the individual agents. A significant weakness is that if the Challenger agent fails to adequately identify factual inaccuracies or logical flaws, or if the Judge agent cannot reliably distinguish between correct and incorrect arguments, the entire debate mechanism may not effectively mitigate hallucination. The quality of the output is thus constrained by the reasoning and critical thinking capacities of the underlying LLMs that constitute these agents. Furthermore, running multiple LLMs in a complex, iterative debate structure can incur substantial computational overhead compared to single-pass generation.

Despite these dependencies, the Multi-Agent Debate paradigm holds significant potential for improving the robustness of LLM outputs. By systematically challenging and refining information and generated content through an adversarial process, MAD systems inherently build resilience against the production of erroneous or fabricated information. The explicit process of debating and justifying claims forces the system to consider alternative viewpoints and verify facts, making the final output less prone to the kind of unverified assertions that characterize hallucination. This self-correcting feedback loop offers a promising avenue for developing LLMs that are not only more accurate but also more reliable and trustworthy in their responses.
## 6. Hallucinations in Specific Contexts

This section synthesizes findings on hallucinations across specialized Large Language Model (LLM) paradigms. It analyzes how different model architectures or application types introduce unique hallucination challenges and necessitate tailored mitigation strategies. Although hallucinations are a pervasive issue across all LLM applications, their manifestation and severity vary considerably with respect to the specific operational context, input modalities, and target tasks [22,28]. The discussion further builds a theoretical framework for understanding and addressing these context-specific hallucinatory behaviors—including those in retrieval-augmented models, multimodal models, and various natural language generation tasks—with additional insights drawn from studies of intrinsic LLM representations [13,36].

The problem of hallucination is particularly pronounced in scenarios where an LLM is pushed beyond its knowledge boundaries, faces ambiguous inputs, or is expected to generate long-form content [10,28]. These challenging contexts can lead to factually incorrect, nonsensical, or unfaithful outputs, thereby highlighting the critical need for context-aware detection and correction mechanisms.

A significant area of focus is the Retrieval-Augmented Language Models (RALMs), primarily exemplified by the Retrieval-Augmented Generation (RAG) paradigm. These models aim to ground their outputs in external, verifiable knowledge sources to enhance accuracy and reduce factual errors [18,26,37]. However, the retrieval component itself can introduce new forms of hallucination when retrieved documents are outdated, incorrect, or noisy—sometimes leading to what has been described as "hallucination on hallucination" [8,14,20,31]. Mitigation strategies in this domain focus on improving retrieval quality through techniques like query rewriting and refined retriever parameter tuning, as well as on developing sophisticated knowledge fusion methods for integrating external data with the model's existing parametric knowledge [14,34].

Similarly, Multimodal Large Language Models (MLLMs) encounter distinct hallucination challenges due to the integration of heterogeneous data types such as text and images [14,19,38]. A primary concern is modality bias, where over-reliance on textual priors can result in "text inertia" and lead to the generation of content that is not supported by visual evidence [7,9,17]. This issue is particularly acute in Visual Language Models (VLMs) and Video-based Large Language Models (VLLMs), where the complexity of spatiotemporal dynamics can further exacerbate errors [1,38,39]. Remedies include enhancing visual prominence, optimizing knowledge aggregation patterns, and applying preference optimization techniques specifically tailored for multimodal contexts [22,29].

Furthermore, the nature of hallucinations varies considerably across Task-Specific Applications in Natural Language Generation (NLG), such as abstractive summarization, question answering, and machine translation [6,27]. LLMs are especially vulnerable to hallucinations when managing numerical data, lengthy texts, or tasks that require complex logical reasoning [10,28]. For example, in question answering, models might generate factually incorrect details—such as errors in describing the composition of the Dow Jones Industrial Average [5]. Mitigation strategies are frequently task-specific, employing tailored evaluation metrics and techniques like iterative querying, reasoning, or domain-specific RAG implementations that aim to enhance factual accuracy [9,31,32].

Finally, recent investigations into the Intrinsic Representation of Hallucinations within LLMs reveal that truthfulness signals are encoded internally, particularly in the model's middle and later layers [15]. A key finding is the discrepancy between an LLM's internal knowledge and its external output—where a model may internally "know" the correct answer yet still generate a hallucinated response [15]. This discovery paves the way for the development of intrinsic detection methods, such as probing classifiers and analyses of attention weights (for example, the "lookback ratio"), to pinpoint potential hallucinations by assessing the model's adherence to factual context during generation [13]. These approaches offer promising avenues for real-time feedback and post-generation verification by leveraging the model’s internal state to enhance external fidelity.
### 6.1 Hallucinations in Retrieval-Augmented Language Models (RALMs)
Retrieval-Augmented Language Models (RALMs), primarily exemplified by the Retrieval-Augmented Generation (RAG) paradigm, represent a critical advancement in mitigating hallucinations in large language models (LLMs) by grounding their outputs in external, verifiable knowledge sources [18,26,37]. The fundamental architecture of RAG involves three core stages: an initial information retrieval step where relevant data is fetched from a pre-defined knowledge base or external resources; a context fusion phase where this retrieved information is combined with the original input to construct a richer context; and finally, the text generation stage, where the LLM produces its response based on this augmented context [34,37]. This process provides LLMs with access to real-time facts and authoritative knowledge, enhancing the accuracy, currency, and reliability of generated text and thereby significantly reducing the risk of factual inaccuracies or fabrications [18,26,28,40].

Despite RAG's efficacy in combating hallucinations by providing external factual support, the retrieval component itself can introduce new challenges, inadvertently leading to or exacerbating hallucinations [8,14]. A primary concern is the quality and relevance of the retrieved documents; if the knowledge base contains outdated, incorrect, noisy, or irrelevant information, the LLM may be misled, resulting in "hallucination on hallucination" [20,31]. Such issues often stem from retrieval failures, which can arise from ambiguous user queries, unreliable underlying retrieval sources, or intrinsic limitations within the retrieval mechanisms [14,37]. Moreover, current RAG systems can still exhibit limitations, particularly in complex reasoning tasks like mathematical problems, and may struggle with faithfulness to expert knowledge [12,32].

To address these challenges and improve the overall robustness of RALMs, significant efforts are directed towards enhancing the quality of retrieved documents and optimizing the integration of this external knowledge. Techniques focus on improving retrieval strategies, such as query rewriting and refining retriever parameters, to ensure higher precision and relevance of fetched information [14,34]. Beyond retrieval, effective knowledge fusion techniques are crucial for systematically combining the retrieved data with the model’s internal parametric knowledge, resolving potential conflicts, and ensuring comprehensive contextual understanding [14]. Different RAG architectures, including one-time, iterative, and post-hoc retrieval, offer varying approaches to information integration, each presenting trade-offs between retrieval accuracy, computational cost, and the extent of hallucination reduction [8]. While RALMs offer a promising avenue for reducing hallucinations, overcoming persistent limitations such as retrieval failures and generation bottlenecks remains an active area of research [14].
#### 6.1.1 The Role of Retrieval Quality
The quality and relevance of retrieved documents are pivotal factors influencing the accuracy and coherence of Large Language Model (LLM) outputs, playing a significant role in the prevalence of hallucination [34]. Retrieval-Augmented Generation (RAG) paradigms, for instance, aim to enrich the context provided to LLMs by integrating information from pre-defined knowledge bases or external resources, thereby forming a more complete and accurate context for generation [37].

Conversely, poor retrieval quality directly correlates with increased hallucination rates. When retrieved documents contain imprecise keywords, one-sided content, or are otherwise noisy or incomplete, the LLM is prone to deriving answers from inaccurate or insufficient information, leading to factual errors or confabulations [20,37]. Retrieval failures themselves can stem from multiple sources, including ambiguous user queries, reliability issues with the underlying retrieval sources, and intrinsic limitations of the retrieval mechanisms [14]. Ambiguous queries, for example, can result in the retrieval of irrelevant or misleading information, thus degrading the input quality for the LLM [14].

To mitigate the risk of hallucination originating from suboptimal retrieval, several techniques are employed to enhance retrieval quality. Addressing ambiguous user queries can be achieved through query rewriting techniques, which rephrase or expand the original query to better align with the available knowledge base and improve retrieval precision [14]. Furthermore, improvements to the retriever itself are crucial; optimizing parameters such as chunk size and chunking methods, utilizing more effective embedding models, and refining vector database configurations can significantly bolster retrieval performance and relevance [14,34]. Beyond optimizing the core retrieval process, strategies for managing the reliability of retrieval sources are essential. This includes implementing quality filters to discard unreliable documents and integrating credibility-aware generation strategies that allow the LLM to assess and weigh the trustworthiness of retrieved information before incorporating it into its output [14]. By systematically enhancing the relevance, completeness, and reliability of retrieved documents, the incidence of hallucination in LLM outputs can be substantially reduced.
#### 6.1.2 Knowledge Fusion Techniques
Knowledge fusion techniques play a crucial role in mitigating hallucination in Large Language Models (LLMs) by systematically combining information from retrieved external documents with the model’s internal parametric knowledge. This integration aims to provide a more complete and factual context for generation [37]. The necessity for such fusion arises from challenges inherent in LLM generation, including the influence of noisy contexts, difficulties in resolving knowledge conflicts, information loss over long contexts, and misalignment between generation and retrieval processes [14].

To address these issues, various techniques are employed to enhance the quality and relevance of information before and during fusion. These include context compression and query decomposition, which refine the input to improve the subsequent fusion process, as well as counterfactual data fine-tuning and source attribution methods, which bolster the model’s ability to handle potentially conflicting or erroneous information [14].

One notable framework that exemplifies a sophisticated approach to knowledge fusion is DRAG. This framework employs a multi-agent debate mechanism—not only to refine the initial retrieval step but also to ensure the factuality of the generated content through a dynamic fusion of external and internal knowledge sources during generation [20]. Specifically, DRAG features a “Proponent Agent” that relies heavily on retrieved data, advocating for information obtained from external sources. Concurrently, a “Challenger Agent” utilizes only the model’s internal knowledge to scrutinize and ensure the factuality of the proposed content. This adversarial yet collaborative interaction, overseen by a judge, facilitates a robust form of knowledge fusion where external facts are cross-validated against internal understanding, thereby enhancing reliability and reducing the likelihood of hallucination [20].

While attention mechanisms, copy mechanisms, and graph-based approaches are widely recognized as fundamental techniques in advanced knowledge fusion for natural language processing, the provided literature digests do not offer specific details or empirical analyses regarding their application within the context of mitigating hallucination in LLMs. Attention mechanisms typically allow models to selectively focus on relevant parts of retrieved documents when integrating them with internal knowledge. Copy mechanisms enable direct reproduction of factual spans from source documents, preventing errors from rephrasing or misremembering details. Graph-based approaches represent and reason about knowledge, both internal and external, as interconnected entities, providing a structured way to handle complex relationships and infer new facts for more coherent and accurate generation. Further research or analysis of additional sources would be necessary to elaborate on the specific implementations and benefits of these particular fusion techniques in the context of hallucination prevention.
### 6.2 Hallucination in Multimodal Large Language Models (MLLMs)
Hallucination, defined as the generation of content that is nonsensical or unfaithful to the input, presents a significant challenge in Multimodal Large Language Models (MLLMs), extending beyond text-only scenarios to complex multimodal contexts such as image-text understanding and image generation [14,19]. The broad application prospects of Large Language Models (LLMs) in multimodal fields, including image description and visual narratives, are significantly hampered by their susceptibility to hallucinations, which can reduce system performance and erode trustworthiness [6].

A primary cause of hallucination in MLLMs is **modality bias**, where models disproportionately rely on information from one modality (e.g., text) while neglecting crucial details from another (e.g., visual input) [9,17]. This imbalance, particularly observed in Large Vision Language Models (LVLMs), stems from an inherent "text inertia" where the language module's strong reliance on linguistic priors can lead to the generation of descriptions that do not exist in the target image, a phenomenon known as object hallucination [4,7,14]. For instance, models might generate incorrect object descriptions in visual question answering or image captioning tasks, or even produce images with inaccurate details, such as an incorrect number of fingers in a generated hand [4,19]. Furthermore, misunderstandings in processing images during multimodal tasks can directly result in inaccurate textual descriptions [28]. The challenges are exacerbated in Video-based Large Language Models (VLLMs), where the temporal dimension introduces unique complexities in identifying and mitigating hallucinations related to video inputs [1].

The interaction between modalities can lead to new types of hallucinations. When the linguistic prior overpowers visual evidence, models may generate content that is semantically coherent but visually ungrounded, demonstrating the complex interplay leading to novel hallucinatory behaviors [9,29].

To address these issues, various methods have been proposed to quantify and mitigate unimodal biases from a causal perspective [17]. Specific strategies include:
*   **Over-Trust Penalty and Retrospection-Allocation**: These methods are explored to alleviate hallucinations by adjusting the model's reliance on different modalities [17].
*   **Enhancing Visual Prominence**: To counteract "text inertia," some approaches focus on increasing the prominence of visual elements during processing [7].
*   **Knowledge Aggregation Improvements**: OPERA aims to alleviate hallucinations by addressing issues related to knowledge aggregation patterns within self-attention matrices in MLLMs [22].
*   **Training-Free Correction**: Woodpecker offers a training-free method for correcting hallucinations, involving steps such as key concept extraction, question construction, visual knowledge verification, visual claim generation, and hallucination correction [22].
*   **Decoding Strategies**: Visual Contrastive Decoding (VCD) has been applied to reduce object hallucinations in large vision-language models [22].
*   **Fine-tuning and Post-processing**: General mitigation strategies include fine-tuning models and applying post-processing corrections to generated outputs [14].
*   **Preference Optimization**: The Hallucination-Aware Direct Preference Optimization (HA-DPO) strategy is specifically tailored for multimodal models, training them to prioritize accurate, non-hallucinatory responses over those containing falsehoods [29].
*   **Natural Language Feedback (NLF)**: DRESS leverages NLF, including critique and refinement mechanisms, to enhance alignment with human preferences and improve the interactive capabilities of LVLMs, thereby reducing hallucinations [26].

These diverse approaches underscore the ongoing research efforts to understand and combat hallucination in MLLMs by tackling modality bias, improving multimodal integration, and refining generation processes.
### 6.3 Task-Specific Hallucination Analysis
Hallucination in large language models (LLMs) manifests distinctly across various Natural Language Generation (NLG) tasks, necessitating task-specific approaches for both evaluation and mitigation. LLMs significantly impact a diverse range of downstream tasks, including abstractive summarization, dialogue generation, machine translation, and data-to-text generation [6,27]. The nature of hallucination often correlates with the type of information being processed; for instance, LLMs are particularly prone to hallucinations when handling numerical data (e.g., dates, values), processing lengthy texts (e.g., document summarization, long dialogue histories), or performing tasks that demand complex logical reasoning. Furthermore, conflicts between the provided context and the model's pre-trained knowledge, or the presence of erroneous contextual information, can exacerbate the occurrence of hallucinations [10].

The choice of evaluation metrics and mitigation techniques is intrinsically linked to the specific requirements and challenges of each task.

Question Answering (QA): Hallucination profoundly affects the accuracy and reliability of question answering systems. For example, in a Generative Question Answering (GQA) task, a model might generate an answer about the Dow Jones Industrial Average that factually conflicts with established information, such as stating it comprises "30 major stock indices" instead of "30 well-known companies" [5]. These inaccuracies are particularly pronounced in domain-specific applications, where even Retrieval-Augmented Generation (RAG) models can struggle with tasks requiring expert knowledge, as demonstrated in evaluations within the college enrollment domain [32]. To address this, frameworks like AutoHallusion have been developed for visual language models, designed to identify hallucinations by generating questions related to inserted or deleted objects, focusing on existence and spatial relationships. This method probes the language module and identifies inconsistencies between the model's responses and the ground truth, serving as a form of detection [9]. Detection and mitigation strategies for QA systems have been tested across diverse content domains, including biographies, trivia, common sense knowledge, and life sciences [36].

Text Summarization: In text summarization, hallucinations compromise the coherence and faithfulness of the generated summaries. LLMs are notably susceptible to hallucinations when processing long texts, a characteristic relevant to document summarization and extended dialogue histories [10]. The XSum summarization task is frequently utilized as a benchmark for detecting and mitigating hallucinations in this domain [13].

Machine Translation (MT): In machine translation, hallucination can lead to significant semantic errors and misinterpretations, thereby undermining the accuracy and utility of the translated output. Machine translation is recognized as one of the key downstream tasks significantly impacted by the hallucination problem in LLMs [6,27].

Creative Writing: While the sub-section description highlights the dual role of hallucination in creative writing—as both a source of inspiration and a potential pitfall—the provided digests do not contain specific information regarding this task.

Application-Specific Mitigation Strategies: Effective mitigation often involves tailored strategies. A general technique applicable across various tasks is iterative querying and reasoning, where LLMs generate follow-up questions and explore different perspectives to cross-validate their own outputs, fostering deeper logical investigations and reducing self-contradictions [31]. For challenges arising in domain-specific applications, such as the need for expert knowledge in the college enrollment task, mitigation strategies must specifically enhance the model's ability to access and utilize domain-specific knowledge, perhaps through more robust RAG implementations [32]. Similarly, the evaluation methodology must align with the task's unique outputs; for instance, the AutoHallusion framework's reliance on consistency checks between generated answers and ground truth exemplifies a task-specific evaluation and detection strategy [9].
### 6.4 Intrinsic Representation of Hallucinations
Recent research has increasingly focused on the intrinsic representations within Large Language Models (LLMs) to understand and address the problem of hallucination. Studies indicate that the internal states of LLMs encode valuable information regarding the truthfulness of their generated responses [15]. This understanding holds significant implications for developing advanced techniques for error detection and correction.

A key finding is the specific manner in which truthfulness information is encoded across different layers and tokens within the model's architecture [15]. Investigations reveal that the middle and later layers of an LLM are particularly effective for predicting the truthfulness of outputs [15]. The signal strength indicating truthfulness is not uniform across the response generation process; it appears strongly at the conclusion of the input prompt, diminishes, then resurges with a distinct peak at the exact answer token, and shows another robust signal at the final token of the LLM's response [15]. Among these, specific tokens such as `exact_answer_last` and `exact_answer_token_after_last` demonstrate the most superior performance in truthfulness prediction [15]. This detailed mapping of truthfulness signals within the model's internal states provides a foundational understanding for intrinsic hallucination detection.

A critical observation from these studies is the existence of a notable gap between an LLM's internal encoding and its external decoding capabilities [15]. This phenomenon implies that an LLM may internally possess or encode the correct answer, yet still proceed to generate an incorrect or hallucinatory response externally [15]. This internal–external discrepancy poses a significant challenge, suggesting that external output generation is not merely a direct manifestation of internal knowledge. The reasons for this disconnect are complex; while LLMs might internally represent factual knowledge, the decoding process can be influenced by factors such as sampling strategies, context misinterpretation, or inherent biases, leading to a divergence from the internally "known" correct answer.

The knowledge of these intrinsic representations offers promising avenues for mitigating the hallucination problem. By identifying the internal signals of truthfulness, it becomes possible to develop probing classifiers that can predict the veracity of a response before or during its generation. For instance, the "lookback ratio," which quantifies the attention weights an LLM places on its provided context versus its own generated tokens, has been proposed as an internal metric to detect contextual hallucinations [13]. This suggests that by analyzing attention mechanisms, one can discern the extent to which an LLM adheres to factual context, thereby identifying potential hallucinations [13]. The application of such intrinsic detection methods could enable real-time feedback loops or post-generation verification systems. While the presented studies establish the existence and localization of truthfulness signals, the generalizability of probing classifiers across diverse tasks, beyond the specific contexts explored, remains an area requiring further investigation. Future research should aim to explore how insights from these internal representations can be actively leveraged to adjust the decoding process, ensuring that the internally encoded correct answers are consistently reflected in the external outputs, thereby reducing hallucination rates.
## 7. Challenges and Future Directions
The pervasive issue of hallucination remains a significant challenge in leveraging Large Language Models (LLMs) in real-world applications, impeding their reliability and trustworthiness [12]. Despite ongoing research, the field is still in a phase of speculation and verification, making it difficult to precisely articulate the causes and processes of model hallucinations mathematically [5]. Addressing these complexities necessitates a concerted effort across various fronts—from improved detection mechanisms to novel mitigation strategies and a deeper understanding of model behaviors.

A primary challenge lies in developing more robust and reliable hallucination detection methods that can accurately identify and quantify hallucinations across diverse LLMs and tasks [10]. Current detection efforts face difficulties even for top-tier LLMs, with research primarily focusing on causes rather than identifying prone areas or assessing severity [19]. Key issues include ambiguous or missing definitions of hallucinations, a lack of standardized measurements, and no consensus across subfields regarding nomenclature and perception [16]. Future research must aim for more intuitive and comprehensive evaluation methods—moving beyond fixed categories and ranges—and develop automatic evaluation techniques to precisely gauge hallucination levels [21,29]. Furthermore, it is crucial to address situations where LLMs are confidently wrong due to training objectives that produce systematically dangerous behavior, reasoning errors, or mislead users [25].

The advent of multi-modal LLMs (MLLMs), particularly Large Vision-Language Models (LVLMs), introduces new complexities and challenges for detecting and mitigating hallucinations [11,14,21,22,39]. MLLMs are highly susceptible to “object hallucinations,” generating descriptions that do not match visual inputs, and often exhibit limited multimodal reasoning abilities by relying on spurious clues [4]. Future directions involve balancing and correcting biases between language and visual modalities, exploring dynamic attention mechanisms, and improving visual-text pre-training frameworks [7,14,17]. Challenges include accurately transferring abstract visual encodings to the semantic space, addressing inconsistencies between visual and textual modes, and mitigating the lack of visual constraints [4]. There is also a need for diverse and effective benchmark datasets for evaluating hallucinations in LVLMs and for the automatic generation of hallucination cases [9].

Understanding the knowledge boundaries of LLMs and developing techniques to prevent them from generating content outside their domain is another critical area for research [14,22,39]. This involves incorporating external knowledge more effectively and developing robust reasoning modules [19,28]. Retrieval-Augmented Generation (RAG) models show promise but require enhancements in comprehending conversational history, analyzing structural information, denoising, processing multi-document interactions, and maintaining faithfulness to expert knowledge [32]. Integrating new knowledge into knowledge-intensive tasks requires joint reasoning between an LLM's implicit knowledge and explicit knowledge from external knowledge graphs, alongside improvements in symbolic reasoning and handling numerical questions [4].

No single solution can completely eliminate AI hallucinations; instead, combining multiple methods is necessary to reduce their frequency and impact [18]. Key challenges in mitigation include developing more robust and generalizable strategies, addressing the limitations of existing techniques, and enhancing the efficiency and accuracy of knowledge editing and self-improvement mechanisms [31,33,38]. Promising future research directions include exploring new training paradigms and optimizing model structures [37]. Innovations in inference mechanisms, such as Chain-of-Thought (CoT), Tree-of-Thoughts (ToT), Graph-of-Thoughts (GoT), Program-based Language Models (PAL), and Plan-of-Thoughts (PoT), are crucial for stimulating LLM reasoning abilities and enabling calls to external interpreters [4]. Methods like multi-agent debate (e.g., DRAG) also show potential, though adaptive stopping strategies are needed to prevent “excessive debate” and drift in simpler tasks [20]. Furthermore, researchers should explore how to harness the “hallucination” phenomenon to unlock the “imagination” of LLMs through specific prompt design—balancing creativity with reliability, especially in high-stakes fields [3,28].

Incorporating human feedback and interaction into the hallucination mitigation process is vital. This includes developing robust internal confidence metrics to prevent models from overfitting human preferences and masking uncertainty [14]. Such methods can help users understand when to exercise caution with LLM-generated answers and improve overall confidence in LLMs for various applications [36].

Finally, the ethical implications of LLM hallucinations necessitate responsible development and deployment [33]. This involves ensuring clear documentation of hallucination frameworks and analysis methods, establishing standardized definitions for related terms (e.g., “fabrication,” “fiction,” “misinformation”), and promoting methods that provide visibility into model decision-making processes [16,31]. Addressing biases in LLMs when they serve as scoring arbiters is also crucial [4]. The development of effective evaluation standards, regulatory mechanisms, and user education are indispensable components for navigating the complex landscape of LLM hallucinations.
## 8. Conclusion
This survey has comprehensively reviewed the multifaceted phenomenon of hallucination in Large Language Models (LLMs), underscoring its significant impact on their reliability and trustworthiness across various applications [8,10,19,33,39]. Hallucinations, defined by the NLP community as misrepresentations and inaccuracies, can propagate misinformation, amplify societal biases, diminish model credibility, and lead to unsatisfactory user experiences in automated services [16,40]. While some perspectives consider hallucinations as an intrinsic characteristic or even a "white lie" strategy that could potentially be leveraged for creative output [3,21], a thorough understanding of their causes and manifestations is paramount for safely harnessing the benefits of AI while minimizing risks [6,28].

To mitigate these challenges, researchers have developed and explored numerous promising techniques. Retrieval Augmented Generation (RAG) stands out as a highly effective solution, significantly improving LLM outputs by referencing external, authoritative knowledge bases, thereby enhancing relevance, accuracy, and practicality [34,37]. Frameworks like DRAG, which optimize retrieval and generation through a multi-agent debate mechanism, demonstrate strong performance in complex question-answering tasks by mitigating stacked hallucinations [20]. Despite RAG's efficacy, it requires careful implementation as hallucinations can still occur, particularly in areas like mathematical reasoning, and domain-specific RAG models require further refinement [12,32].

Beyond RAG, other critical mitigation strategies include adjusting LLM generation parameters and implementing diverse decoding techniques to control output quality [12]. Self-check mechanisms, such as the Chain-of-Verification (CoVe) method, empower models to review and correct their own responses [21]. Semantic entropy has emerged as a novel method for detecting hallucinations stemming from a lack of LLM knowledge, highlighting the importance of meaning in language-based machine learning problems [25,36]. Prompt engineering also plays a vital role in enhancing response reliability [12]. For multi-modal models, such as Large Vision-Language Models (LVLMs), specific techniques like HA-DPO have shown effectiveness in reducing hallucinations by optimizing preferences [29], while training-free methods can balance visual and textual information to reduce "text inertia" [7]. It is generally concluded that models exhibiting stronger alignment tend to offer better overall reliability [30].

Despite these advances, key challenges persist. Current mitigation methods can only alleviate, not entirely eliminate, hallucinations [21,30]. There is a continuous need for more robust evaluation datasets and sophisticated mitigation strategies, particularly evident in the development of hallucination benchmarks for LVLMs and the observed limitations in existing VLLMs [1,9]. The potential for bias when using one LLM to evaluate another also remains a concern [36].

In conclusion, addressing hallucination is fundamental to enhancing the trustworthiness and dependable deployment of LLMs in real-world applications [13,22,27]. The most effective solutions often involve a multi-faceted approach, combining diverse techniques such as careful data curation, specialized fine-tuning, reinforcement learning with human feedback, and a systematic classification of mitigation strategies [2,18,31,38]. Therefore, continued research and development are crucial for finding more complete solutions, pushing the boundaries of natural language processing technology, and fostering a stronger synergy between LLMs and external knowledge bases [4,37]. This complex problem necessitates sustained efforts to understand, evaluate, and mitigate hallucinations, underscoring the critical need for interdisciplinary collaboration and a fruitful exchange of well-studied methods and emerging new problems to ensure the responsible and beneficial use of LLMs [4,10,25].

## References

[1] VidHal: 视频视觉语言模型中时间幻觉的基准测试 [http://www.paperreading.club/page?id=268575](http://www.paperreading.club/page?id=268575) 

[2] LLM Hallucinations: Causes, Detection, and Reduction [https://www.simform.com/blog/llm-hallucinations/](https://www.simform.com/blog/llm-hallucinations/) 

[3] 大语言模型的“幻觉”：善意谎言与文本生成的新可能 [https://www.bilibili.com/read/cv23270004/](https://www.bilibili.com/read/cv23270004/) 

[4] 大型语言模型幻觉现象综述：分类、机理、检测与矫正 [https://www.bilibili.com/read/cv26949833](https://www.bilibili.com/read/cv26949833) 

[5] 大模型的幻觉现象：定义、检测与应对 [https://zhuanlan.zhihu.com/p/664035477](https://zhuanlan.zhihu.com/p/664035477) 

[6] 大语言模型幻觉问题研究综述 [https://www.jos.org.cn/jos/article/abstract/7242](https://www.jos.org.cn/jos/article/abstract/7242) 

[7] Alleviating Hallucination in LVLMs via Attention to Image [https://dl.acm.org/doi/abs/10.1007/978-3-031-73010-8_8](https://dl.acm.org/doi/abs/10.1007/978-3-031-73010-8_8) 

[8] LLM大模型幻觉：原理、分类、挑战与缓解 [https://zhuanlan.zhihu.com/p/703034375](https://zhuanlan.zhihu.com/p/703034375) 

[9] 视觉大模型幻觉：马里兰大学提出AutoHallusion自动生成框架 [https://baijiahao.baidu.com/s?id=1815411200001322547&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1815411200001322547&wfr=spider&for=pc) 

[10] 七问大模型幻觉：原因、类型、度量与缓解 [https://zhuanlan.zhihu.com/p/651507945](https://zhuanlan.zhihu.com/p/651507945) 

[11] 腾讯AI Lab综述：大模型幻觉问题评估、溯源与缓解 [https://m.thepaper.cn/kuaibao_detail.jsp?contid=24608648](https://m.thepaper.cn/kuaibao_detail.jsp?contid=24608648) 

[12] 如何解决 LLM 幻觉问题：方法与技术 [https://www.zhihu.com/question/604881501/answer/3212243549](https://www.zhihu.com/question/604881501/answer/3212243549) 

[13] Lookback Lens: Detecting and Mitigating LLM Hallucinations with Attention Maps [https://ui.adsabs.harvard.edu/abs/arXiv:2407.07071](https://ui.adsabs.harvard.edu/abs/arXiv:2407.07071) 

[14] LLM幻觉综述：原理、分类、挑战与开放问题 [https://zhuanlan.zhihu.com/p/1907922635540636012](https://zhuanlan.zhihu.com/p/1907922635540636012) 

[15] LLM幻觉的内在表示：LLMs知道的比展现的更多 [https://zhuanlan.zhihu.com/p/991220541](https://zhuanlan.zhihu.com/p/991220541) 

[16] 大模型“幻觉”难题：定义与影响的最新调研 [https://zhuanlan.zhihu.com/p/692136655](https://zhuanlan.zhihu.com/p/692136655) 

[17] 多模态大语言模型幻觉成因与缓解方法 [https://www.modb.pro/db/1898924166858944512](https://www.modb.pro/db/1898924166858944512) 

[18] LLM幻觉：成因与缓解策略 [https://zhuanlan.zhihu.com/p/1917997156821874616](https://zhuanlan.zhihu.com/p/1917997156821874616) 

[19] 大模型幻觉：成因、检测与解决 [https://news.qq.com/rain/a/20241101A00YKC00](https://news.qq.com/rain/a/20241101A00YKC00) 

[20] DRAG：多智能体辩论破解大模型幻觉叠加难题 [https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247658215&idx=1&sn=86be4ec7c9a64ea51c1ce1936fa3687d&chksm=e83c205c6721a99b190d388c23cb43e46a91b9296e3e1c5d9158e5d1b55fd968132497b854fd&scene=27](https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247658215&idx=1&sn=86be4ec7c9a64ea51c1ce1936fa3687d&chksm=e83c205c6721a99b190d388c23cb43e46a91b9296e3e1c5d9158e5d1b55fd968132497b854fd&scene=27) 

[21] LEAFW：叶子的技术碎碎念 - 大模型幻觉问题与CoVe方法 [http://leafw.cn/?p=84](http://leafw.cn/?p=84) 

[22] LLM幻觉问题及解决方案：多篇论文解读 [https://www.zhihu.com/question/613263212/answer/3356634823](https://www.zhihu.com/question/613263212/answer/3356634823) 

[23] 语义熵：检测大型语言模型中的幻觉 [https://zhuanlan.zhihu.com/p/31876331685](https://zhuanlan.zhihu.com/p/31876331685) 

[24] 大模型幻觉：定义、评估与解决方案 [https://cloud.tencent.com/developer/article/2304917](https://cloud.tencent.com/developer/article/2304917) 

[25] Detecting Large Language Model Hallucinations with Semantic Entropy [https://www.nature.com/articles/s41586-024-07421-0](https://www.nature.com/articles/s41586-024-07421-0) 

[26] 大模型幻觉缓解技术综述 [https://zhuanlan.zhihu.com/p/678006292](https://zhuanlan.zhihu.com/p/678006292) 

[27] A Survey of Hallucination in Natural Language Generation [https://dl.acm.org/doi/abs/10.1145/3571730](https://dl.acm.org/doi/abs/10.1145/3571730) 

[28] AI“幻觉”：成因、识别与应对 [https://cloud.tencent.com/developer/article/2507907](https://cloud.tencent.com/developer/article/2507907) 

[29] HA-DPO：通过幻觉感知直接偏好优化增强LVLM [https://blog.csdn.net/Mars_prime/article/details/135108886](https://blog.csdn.net/Mars_prime/article/details/135108886) 

[30] LLM幻觉解析与缓解方案 [https://cloud.tencent.com/developer/article/2391623](https://cloud.tencent.com/developer/article/2391623) 

[31] LLM幻觉缓解技术：综述与展望 [https://blog.csdn.net/2401_84208172/article/details/145354663](https://blog.csdn.net/2401_84208172/article/details/145354663) 

[32] DomainRAG: 中文领域特定检索增强生成评测基准 [http://www.paperreading.club/page?id=232879](http://www.paperreading.club/page?id=232879) 

[33] LLMs论文速览：arXiv 2024.01.01-2024.01.10 [https://blog.csdn.net/weixin_44362044/article/details/136227725](https://blog.csdn.net/weixin_44362044/article/details/136227725) 

[34] RAG：检索增强生成与书生浦语大模型实战 [https://www.cnblogs.com/xiangcaoacao/p/18113448](https://www.cnblogs.com/xiangcaoacao/p/18113448) 

[35] 大模型时代智能体：LLMs的优势与挑战 [https://hub.baai.ac.cn/view/31784](https://hub.baai.ac.cn/view/31784) 

[36] Nature：剑桥大学新方法识破大语言模型“幻觉” [https://mp.weixin.qq.com/s?__biz=MzU1MzMxMzcyMg==&mid=2247738230&idx=4&sn=589007a2a2ed2ff204d0eaebe0677c4c&chksm=fa6afaf72d2a62965a5c1d7b8b546cf79337c048a7de47b63923dcb0ef7c98e3986923810193&scene=27](https://mp.weixin.qq.com/s?__biz=MzU1MzMxMzcyMg==&mid=2247738230&idx=4&sn=589007a2a2ed2ff204d0eaebe0677c4c&chksm=fa6afaf72d2a62965a5c1d7b8b546cf79337c048a7de47b63923dcb0ef7c98e3986923810193&scene=27) 

[37] LLM幻觉现象与RAG解决方案 [https://qianfanmarket.baidu.com/article/detail/1123418](https://qianfanmarket.baidu.com/article/detail/1123418) 

[38] LLM幻觉缓解技术综述 [https://baijiahao.baidu.com/s?id=1787415487120460468&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1787415487120460468&wfr=spider&for=pc) 

[39] LLM Hallucination: A Survey on Principles, Taxonomy, Challenges, and Mitigation [https://dl.acm.org/doi/full/10.1145/3703155](https://dl.acm.org/doi/full/10.1145/3703155) 

[40] LLM模型幻觉：原理、案例与应对 [https://www.hxstrive.com/article/1469.htm](https://www.hxstrive.com/article/1469.htm) 

