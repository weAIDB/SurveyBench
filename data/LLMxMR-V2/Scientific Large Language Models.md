# A Survey on Scientific Large Language Models

# 0. A Survey on Scientific Large Language Models

## 1. Introduction
The rapid emergence of Large Language Models (LLMs) marks a profound shift in artificial intelligence, extending their influence far beyond traditional linguistic tasks into specialized scientific domains [1,35]. This evolution signifies a move from highly specialized computational methods to more generalized, LLM-driven approaches, fundamentally transforming scientific research paradigms [19]. Unlike earlier AI systems that addressed specific problems, modern LLMs, such as GPT-4, demonstrate advanced capabilities in natural language understanding, generation, and complex reasoning, grounded in vast datasets and innovative architectural designs [19,20,28]. These capabilities directly address long-standing challenges in scientific productivity, such as the slowing pace of discovery and the limitations of traditional methods for complex scientific tasks [22]. By efficiently processing vast scientific literature, identifying key concepts, and synthesizing knowledge, LLMs are poised to accelerate literature reviews, enhance information integration, and facilitate tasks from molecular property prediction to scientific hypothesis generation [3,22,23]. The overarching goal of Scientific Large Language Models (SciLLMs) is therefore to enhance scientific research, innovation, and policy-making across diverse fields including biomedicine, chemistry, materials science, agriculture, and drug discovery [2,5,7,11,26].

Despite their transformative potential, the integration of LLMs into scientific practice presents significant challenges and necessitates careful consideration [2,7]. Concerns include the inherent limitations of LLMs in generating truly novel or profound scientific hypotheses, their reliance on potentially outdated or incomplete training data, and the pervasive issue of "hallucinations" where models produce plausible but incorrect information [20,23]. Technical hurdles like substantial computational requirements, data deficiency in specialized domains, and multimodal data integration challenges further complicate their deployment [7,20]. Broader implications encompass data privacy, algorithmic bias, ethical considerations of AI-driven decision-making, and a general lack of interpretability, which are critical for maintaining good scientific practices and trust in science [20,30,34]. These issues underscore the imperative for developing robust, responsible frameworks and ethical guidelines for the effective and safe deployment of LLMs in scientific contexts.



![Survey Structure and Key Focus Areas](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/hs3Ciq3OhhLXwDN_fbVgs_Survey%20Structure%20and%20Key%20Focus%20Areas.png)

This survey provides a comprehensive and systematic analysis of SciLLMs, serving as a roadmap for researchers and practitioners in this rapidly evolving field [19,31]. We begin by delineating the **foundational principles** of SciLLMs, covering model architectures, pre-training strategies, adaptation techniques, and robust evaluation methodologies, alongside the unique characteristics of scientific datasets [1,12,17,18,20,28,30,33]. Subsequently, we examine **SciLLM applications across diverse scientific domains and research lifecycle stages**, including hypothesis discovery, experiment planning, scientific writing, and peer review, with specific explorations in life sciences, materials science, chemistry, physics, and environmental sciences [1,5,7,8,11,19,22,26,31,35]. The survey then addresses the **significant challenges and limitations** associated with SciLLMs, encompassing technical hurdles, ethical considerations, and privacy concerns [2,7,20,34]. Finally, we outline **future directions and emerging prospects**, highlighting prospective avenues for examination, including the paradigm shift towards closed-loop autonomous agents for scientific discovery and the continuous evolution shaping the next generation of scientific AI systems [18,19,20]. This structured approach aims to provide a holistic and actionable understanding of SciLLMs, from their conceptual underpinnings to their transformative potential in accelerating scientific discovery.
### 1.1 Background and Motivation

**SciLLMs: Transformative Potential vs. Significant Challenges**

| Category               | Transformative Potential                             | Significant Challenges                                      |
| :--------------------- | :--------------------------------------------------- | :---------------------------------------------------------- |
| **Capabilities**       | Advanced NLU, Generation, Complex Reasoning          | Reliance on Outdated/Incomplete Data, "Hallucinations"      |
| **Scientific Tasks**   | Accelerate Literature Reviews, Information Integration, Molecular Property Prediction, Hypothesis Generation | Generating Truly Novel Hypotheses                           |
| **Research Productivity** | Enhanced Efficiency, Streamlined Workflows           | Slowing Pace of Discovery, Limitations of Traditional Methods |
| **Applications**       | Biomedicine, Chemistry, Materials, Agriculture, Drug Discovery, Policy-Making | Technical Hurdles (Computational, Data Deficiency, Multimodal Integration) |
| **Broader Implications** | Innovation, Interdisciplinary Collaboration        | Data Privacy, Algorithmic Bias, Ethical Concerns, Lack of Interpretability |

The advent of Large Language Models (LLMs) represents a transformative paradigm shift, profoundly impacting artificial intelligence and extending beyond traditional linguistics to specialized scientific domains [1,35]. This evolution signifies a move from earlier, often specialized, computational methods to more generalized, LLM-driven approaches in scientific research. While prior efforts in computer-assisted research, dating back to the 1970s with systems like Automated Mathematician and BACON, demonstrated potential in tasks such as theorem generation and empirical law identification, and more recent models like AlphaFold and OpenFold achieved breakthroughs in specific research areas, the comprehensive vision of AI-assisted research across multiple scientific fields became truly feasible with the emergence and explosive growth of LLMs [19].

LLMs, exemplified by models such as GPT-4, possess advanced capabilities in natural language understanding, generation, and complex reasoning, underpinned by extensive datasets and innovative architectures [19,20,28]. These capabilities directly address long-standing challenges in scientific productivity and discovery, particularly the slowing efficiency of scientific advancements and the limitations of traditional methods for complex scientific tasks [22]. LLMs excel at tasks such as rapidly processing and organizing vast scientific literature, automatically identifying key concepts, relationships, and accelerating literature reviews and information integration [23]. For instance, in molecular property prediction, LLMs can synthesize knowledge from scientific literature and infer patterns from data, enhancing both accuracy and interpretability, a significant improvement over traditional models like Graph Neural Networks (GNNs) which often lack interpretability and struggle to incorporate prior scientific knowledge [3,22]. This transformative potential to accelerate scientific discovery and enhance researcher productivity underscores the relevance and timely nature of this survey [19].

The overarching goal of Scientific Large Language Models (SciLLMs) is to enhance scientific research, innovation, and policy-making by leveraging these advanced LLM capabilities [2,7]. Their extensive application prospects span numerous scientific domains, including biomedicine, chemistry, materials science, agriculture, and drug discovery, aiming to accelerate technological innovation, streamline research and development processes, and inform better policies [5,11,26]. Specifically, LLMs are envisioned to reshape agricultural intelligence by improving production and decision-making, and to accelerate scientific discovery and knowledge dissemination within materials science [7,11].

However, the adoption of LLMs in science presents a dual nature, promising significant advantages while simultaneously introducing considerable risks and challenges [2,7]. While LLMs are poised to revolutionize scientific and social sciences by accelerating research and fostering interdisciplinary collaboration [20], their rapid advancements and widespread accessibility necessitate "careful consideration and responsible usage" to safeguard "good scientific practices and trust in science" [34]. Challenges include inherent limitations in generating truly innovative or profound scientific hypotheses, as LLMs often rely on pre-trained data which may be outdated or incomplete, leading to content that replicates existing patterns rather than proposing groundbreaking theories [23]. Furthermore, the direct application of LLMs to domain-specific problems encounters obstacles such as substantial language differences across contexts, the requirement for in-depth, real-time, and accurate domain knowledge not readily available in generic LLMs, and the phenomenon of "hallucinations" where models produce plausible but incorrect or non-factual text [20]. Other critical concerns encompass data privacy, algorithmic bias, ethical implications of AI-driven decision-making, and the lack of interpretability, which hinders a clear understanding of the rationale behind model predictions [20,30]. These issues underscore the imperative for developing responsible frameworks and guidelines for the ethical and effective deployment of LLMs in scientific contexts, particularly in sensitive areas such as food production, where potential threats like agricultural misinformation and data collection issues are prominent [2].
### 1.2 Scope and Organization
This survey provides a comprehensive and systematic analysis of Scientific Large Language Models (Sci-LLMs), serving as a detailed roadmap for researchers and practitioners navigating this rapidly evolving field. We structure this survey to guide readers through the complex landscape of Sci-LLMs, progressing logically from foundational principles to their cutting-edge applications, inherent challenges, and future prospects [19,31]. This organizational approach is designed to foster a holistic understanding, much like other comprehensive reviews that explore LLM advancements, technical frameworks, and their societal implications [16,18,20].

The survey commences by delineating the **foundational principles** underlying Sci-LLMs. This section will delve into core aspects such as various model architectures, including innovative designs that address Transformer limitations and enhance efficiency [12,20]. It will systematically discuss pre-training strategies, adaptation fine-tuning, and utilization techniques, along with robust capacity evaluation methodologies [17,28,30,33]. Crucially, we will examine the unique characteristics of scientific datasets, including their multimodality, cross-scale nature, and domain-specific challenges, distinguishing them from general NLP corpora [1,18]. Furthermore, the discussion will cover essential components like word embeddings and the application of deep learning models in various LLM tasks [20].

Following the foundational overview, the survey transitions to a detailed examination of **Sci-LLM applications across diverse scientific domains and research lifecycle stages**. We will systematically explore their utility in critical phases of scientific inquiry, including scientific hypothesis discovery, experiment planning and implementation, scientific writing, and peer review [19,31]. Specific applications will be categorized by scientific disciplines, encompassing the life sciences (e.g., molecular, protein, genomic LLMs, drug discovery, biomedicine, agricultural intelligence) [1,8,11,22,26,35], materials science [7], chemistry, physics, and environmental sciences (e.g., climate modeling and weather forecasting) [5,31]. This includes showcasing specific research efforts and their outcomes in these fields [5].

Subsequently, the survey addresses the **significant challenges and limitations** associated with Sci-LLMs. This encompasses technical hurdles such as hallucination, substantial computational requirements, and issues stemming from data deficiency or multimodality [7,20]. We also consider broader implications, including ethical alignment, privacy concerns, and the critical need for responsible usage to safeguard scientific integrity, especially when differentiating between general and specialized LLMs [2,20,34].

Finally, the survey concludes by outlining **future directions and emerging prospects** for Sci-LLMs. This section will highlight prospective avenues for examination, including the paradigm shift towards closed-loop autonomous agents for scientific discovery [18], and other innovative applications and continuous evolution that will shape the next generation of scientific AI systems [19,20]. By presenting these key areas in a structured manner, this survey aims to provide a comprehensive and actionable understanding of Sci-LLMs, from their conceptual underpinnings to their transformative potential in accelerating scientific discovery.
## 2. Foundations of Scientific Large Language Models
This section lays the groundwork for understanding Scientific Large Language Models (Sci-LLMs) by exploring their historical evolution, fundamental architectural principles, the scaling dynamics that give rise to their advanced capabilities, and the unique characteristics of the scientific data they process. The journey of language models (LMs) has progressed from rule-based systems to sophisticated large language models (LLMs), with significant advancements driven by shifts from statistical to neural networks, and crucially, the advent of the Transformer architecture [15,30]. This evolution has paved the way for general LLMs, which, through unprecedented scaling in model size, data volume, and computational resources, have developed emergent abilities, consequently opening new avenues for scientific applications leading to Sci-LLMs [1,30]. The progression of Sci-LLMs from transfer learning to agentic science highlights their increasing sophistication in scientific inquiry [18].

At the core of modern LLMs, and by extension Sci-LLMs, is the Transformer architecture, recognized for its self-attention mechanism that enables context-aware processing and efficient handling of sequential data [10,20]. This design facilitates the learning of complex linguistic patterns and the derivation of meaning from data points, often through multi-layer 'query,' 'key,' and 'value' vectors [4,20]. However, despite its efficacy, the Transformer architecture presents significant limitations for scientific applications, primarily due to the quadratic computational and memory complexity ($O(N^2)$) of its self-attention mechanism with respect to input sequence length $N$ [12]. This makes it "less optimal for Sci-LLMs" when dealing with the extensive and complex data prevalent in scientific domains, such as long genomic sequences, intricate 3D structural information, multimodal inputs, and non-autoregressive generation needs [1,30]. Consequently, ongoing research is focused on architectural innovations, including compression, enhanced memory, hybrid models, and adaptations with Graph Neural Networks (GNNs), to tailor Transformers for the unique demands of scientific data [12,31].

The capabilities of LLMs are profoundly shaped by model scaling laws, which describe the relationship between model size, data quantity, computational power, and performance [28,33]. Laws such as Kaplan-Mitchell and Chinchilla guide the optimization of training resources, demonstrating that increasing these factors leads to improved performance and the acquisition of novel functionalities [28,30]. Crucially, these scaling principles underpin the emergence of "emergent abilities"—qualitatively new behaviors that appear abruptly in larger models beyond specific scale thresholds [17,30]. Key emergent abilities relevant to Sci-LLMs include in-context learning (ICL), instruction following, and step-by-step reasoning via Chain-of-Thought (CoT) prompting, which enable complex problem-solving and adaptation to specialized tasks without extensive retraining [17,33]. These capabilities, such as those demonstrated by Alpha-Geometry and FunSearch, hold immense potential for accelerating scientific discovery, but also introduce challenges related to reliability and potential "hallucinations" [29,31].

Further, Sci-LLMs encounter a diverse and complex landscape of scientific data, which necessitates a structured classification and hierarchical understanding. Scientific corpora are inherently multimodal, integrating text with visual, symbolic, structured, and time-series information; cross-scale, spanning various levels of scientific inquiry; and characterized by highly specialized terminology and syntax, such as molecular strings (e.g., SMILES) or protein sequences (e.g., FASTA) [18,31]. This "scientific language" extends beyond natural text to formal symbolic representations, demanding sophisticated processing capabilities from LLMs [1,35]. The hierarchical nature of scientific knowledge, progressing from raw data to abstract theories, requires LLMs not just to process information but to comprehend and reason over interconnected knowledge structures [11,18]. These unique characteristics present challenges in data deficiency and the need for domain-invariant representations and cross-modal reasoning, driving the development of specialized multimodal datasets and pre-training strategies for Sci-LLMs [18,20]. Addressing these foundational aspects is critical for developing robust, reliable, and effective Sci-LLMs that can genuinely advance scientific research.
### 2.1 Evolution of Language Models: From General AI to Scientific Potential

The trajectory of language models (LMs) has witnessed a profound evolution, commencing from rudimentary symbolic systems and advancing to the sophisticated large language models (LLMs) that now demonstrate significant scientific potential [15,30]. This progression is characterized by several key technological shifts that have fundamentally altered model capabilities and applications.

The earliest forays into language processing were marked by rule-based systems, such as MIT's ELIZA (1966), which mimicked conversation through pattern matching and substitution [10,15]. These initial models, primarily active in the 1950s and 1960s, grappled with the inherent complexities of natural language, relying on predefined rules that limited their adaptability [25]. The subsequent era, from the 1980s to the early 2000s, saw the rise of statistical language models (SLM), which employed probabilistic methods, such as n-gram models, to predict word sequences based on recent context [10,20,30]. While offering advancements over rule-based systems, SLMs were constrained by the "curse of dimensionality" and data sparsity, necessitating smoothing strategies for practical application [30].

A significant paradigm shift occurred with the introduction of neural language models (NLM) around 2003, which leveraged artificial neural networks to "learn" from data [10,30]. This phase saw the emergence of distributed word representations, notably with Word2vec, and the application of recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, which were particularly effective in processing sequential data and learning context-aware features [14,25,30].

The mid-2010s marked another pivotal transition with the deep learning revolution and the advent of the attention mechanism in 2014, culminating in the release of the Transformer model in 2017 [10,15,25,30]. The Transformer architecture, characterized by its self-attention mechanisms, enabled parallel training on multiple GPUs and significantly enhanced language interaction capabilities, laying the groundwork for the pre-trained language model (PLM) paradigm [20,25,30]. PLMs, such as ELMo, BERT, and GPT-1, established a "pre-training and fine-tuning" methodology, where models were first pre-trained on vast unlabeled corpora to acquire universal language representations and then fine-tuned for specific downstream tasks [14,20,30]. This approach led to substantial performance gains across numerous natural language processing (NLP) tasks [30].

The current era of Large Language Models (LLMs), emerging around 2020, represents a significant leap from PLMs, primarily driven by unprecedented scaling in model size, data volume, and computational resources [17,28,30]. Critical factors driving this transition include exponential growth in available datasets and advancements in computational hardware capabilities, which enabled the training of models with billions of parameters [10,20]. Researchers observed that scaling models beyond a certain threshold not only improved performance but also led to the emergence of "surprising" or "emergent abilities," such as in-context learning, complex reasoning, and code generation, which were absent in smaller PLMs [17,24,28,30]. Prominent examples include GPT-3 (175 billion parameters), PaLM (540 billion parameters), and subsequently, ChatGPT (GPT-3.5) and GPT-4, which further refined capabilities through techniques like reinforcement learning with human feedback (RLHF) [21,25,30]. These LLMs fundamentally differed from their predecessors by offering broader language understanding and generation capabilities through large-scale pre-training, becoming foundational models adaptable across diverse AI tasks [4,30].

These profound advancements in general LLMs have directly opened new avenues for scientific applications, leading to the development of powerful Scientific Large Language Models (Sci-LLMs) [1,19,30]. The evolution of Sci-LLMs from 2018 to 2025 can be characterized by four major paradigm shifts:
1.  **Transfer Learning (2018-Early 2020s)**: Early Sci-LLMs leveraged the PLM paradigm by fine-tuning pre-trained models, such as BERT, on domain-specific scientific texts. Models like SciBERT, BioBERT, and PubMedBERT exemplify this phase, acquiring scientific knowledge through self-supervised pre-training on paper texts [3,18,31,32].
2.  **Scaling (Early 2020s-Mid 2020s)**: Following the success of general LLMs, the scientific community focused on increasing model size and data volume to integrate vast amounts of scientific knowledge, leading to models capable of processing extensive scientific literature [18]. Examples include Med-PALM and Galactica, which incorporated extensive scientific literature in their pre-training [3,31]. The development of "Large Language of Life Models" (LLLMs), such as Evo, pre-trained on multi-omics data like billions of DNA bases, showcases this scaling for specialized scientific domains [8].
3.  **Instruction Following (Mid 2020s)**: Models like ChatGPT significantly improved task adaptability through instruction tuning, enabling them to solve complex scientific problems based on natural language instructions [18,31]. This shift allowed LLMs to move beyond passive knowledge processing to become more interactive and adaptable tools for scientific inquiry [18].
4.  **Agentic Science (Projected Mid-Late 2020s)**: The emerging stage envisions LLMs evolving into autonomous agents capable of conducting scientific research, highlighting a shift towards active scientific discovery partners [18,29]. The rapid advancement towards more efficient, reasoning-capable, and specialized models, such as Biomni-R0 for biomedical research, demonstrates this ongoing adaptation for scientific domains [29].

This continuous evolution, fueled by advancements in architectural design, scaling principles, and computational resources, has transformed LMs from general AI components into powerful tools with the potential to revolutionize scientific research by facilitating streamlined literature analysis, creative idea generation, and intricate data interpretation [20]. However, this also brings challenges related to the reliability, interpretability, and ethical implications of deploying such powerful AI in scientific discovery.
### 2.2 Transformer Architecture: Core Principles and Limitations for Scientific Data
The Transformer architecture stands as the fundamental backbone for the vast majority of contemporary Large Language Models (LLMs), a position solidified since its refinement in 2017 [12,15,17,28,30,33]. Originating from the attention mechanism introduced in 2014, Transformers diverge from earlier recurrent or convolutional architectures by relying solely on attention for processing sequential data [15,20]. This foundational design allows LLMs to effectively analyze relationships within sequential datasets, thereby learning complex linguistic patterns and deriving meaning and context from individual data points, such as words in a language model [4]. For instance, GPT models leverage a decoder-only Transformer architecture to compress extensive world knowledge through language modeling, enabling them to recover semantics and function as general-purpose task solvers [30].

A core principle underlying the Transformer's efficacy is the self-attention mechanism, which plays a pivotal role in enabling context-aware processing [10]. Self-attention processes input by assigning weights to each encoded representation, learning the most relevant information and computing a weighted sum of values to produce an outcome [20]. This mechanism establishes associations between tokens in a sequence, generating a contextualized representation, often through the use of 'query,' 'key,' and 'value' vectors across multiple Transformer layers (typically 10 to 100) [20]. The Transformer also employs a mask matrix to control token visibility, further enhancing its attention capabilities [20]. This architecture ultimately converts each input token into a probability distribution over forthcoming tokens, facilitating generation [20].

The functional performance and scalability of Transformer-based LLMs are further influenced by various architectural configurations and optimization techniques [33]. Key areas of configuration include normalization methods (e.g., LayerNorm, RMSNorm, DeepNorm, with Pre-RMSNorm often recommended for stabilization), positional embeddings, and activation functions [28]. As the original Transformer does not inherently encode positional information crucial for word order, various methods such as absolute, relative, and Rotary Positional Embeddings (RoPE) are integrated, with RoPE and ALiBi suggested for better performance, especially with long sequences [20,28]. Activation functions like GeLU, GLU, SwiGLU, and GeGLU enhance non-linear expression, with SwiGLU or GeGLU generally recommended [28]. Furthermore, various attention types, including full attention, sparse attention, multi-query/grouped-query attention, FlashAttention, and PagedAttention, are employed to optimize efficient information processing and interaction [28]. These configurations, combined with common pre-training tasks like language modeling and denoising autoencoding, contribute significantly to the models' ability to process sequential data and learn complex patterns [33].



**Transformer Architecture Limitations for Scientific Data**

| Category           | Limitation Description                                    | Impact on Scientific Applications                                    |
| :----------------- | :-------------------------------------------------------- | :------------------------------------------------------------------- |
| **Computational Complexity** | Quadratic complexity ($O(N^2)$) of self-attention mechanism with sequence length $N$ | Highly inefficient for long genomic sequences, extensive scientific texts, experimental protocols. High development/deployment costs. |
| **Memory Footprint** | Large memory requirements, especially for long sequences  | Limits context length, preventing comprehensive analysis of full research papers or complex biological sequences.               |
| **Structural Information** | Struggles with explicitly incorporating critical 3D structural information                 | Ineffective for molecules, proteins, or materials where 3D structure is essential for scientific representation.          |
| **Non-Textual Data** | Less effective on tasks not best expressed as text        | Poor performance on numerical computations, figures, chemical structures (e.g., graphs), multimodal inputs.                   |
| **Generation Needs** | Primarily autoregressive ("next-token prediction")        | Limited for non-autoregressive scientific generation, which requires capturing/generating semantic info from entire sequences. |
| **Efficiency Bottlenecks** | Feed-Forward Networks (FFNs) can become inefficient with model size growth | Impacts overall model scalability and practical utility for very large scientific LLMs.                                    |

Despite these sophisticated principles, the Transformer architecture possesses inherent constraints that significantly impact its application, particularly in scientific domains. A primary limitation stems from the self-attention mechanism's quadratic computational and memory complexity, which scales as $O(N^2)$ with the input sequence length $N$ [12]. This quadratic scaling renders the architecture highly inefficient for processing the extensive and often complex data common in scientific domains, such as long genomic sequences, high-resolution material structures, or multimodal inputs involving high-resolution images, video, and audio [12]. Beyond self-attention, Feed-Forward Networks (FFNs) can also become efficiency bottlenecks as model size increases [12]. This computational burden translates directly into high development and deployment costs, hindering the accessibility and practicality of LLMs for many scientific endeavors [12].

When applied to scientific problems, these architectural limitations manifest in several critical ways, making the traditional Transformer "less optimal for Sci-LLMs" [1]. Specifically, scientific language often involves sequences considerably longer than those found in conventional natural language, posing a significant challenge for models designed for shorter contexts [1]. Moreover, current LLMs struggle with explicitly incorporating critical 3D structural information (e.g., for molecules or proteins), which is often essential for scientific representation, and they perform less effectively on tasks not best expressed as text, such as numerical computations or questions involving figures and chemical structures [1,21,30]. Another challenge lies in non-autoregressive generation needs; while natural language typically follows an autoregressive pattern, scientific LLMs frequently require the ability to capture and generate semantic information from entire sequences, moving beyond simple next-token prediction [1].

These limitations underscore the necessity for architectural innovations to enable LLMs to become truly effective and widespread tools in science [12]. Current research explores various approaches to overcome these challenges, including efforts to compress and shorten context for longer sequences and faster decoding (e.g., REFRAG), enhance memory for long-context processing (e.g., MemAgent), and develop hybrid models specifically designed for long-context and reinforcement learning tasks (e.g., MiniMax-M1) [29]. Furthermore, studies are modifying Transformer architectures with adapters, Graph Neural Network (GNN)-nested Transformers, and Mixture of Experts (MoE) Transformers to better capture graph signals in multimodal data [31]. Explorations into alternative architectures, such as scalable byte-level autoregressive U-Net models, also indicate a shift towards designs that could potentially outperform token-based Transformers in specific benchmarks [29]. These ongoing advancements highlight a critical drive to address the intrinsic constraints of the Transformer and tailor LLMs for the unique demands of scientific inquiry.
### 2.3 Model Scaling Laws and Emergent Scientific Abilities
The remarkable advancements in Large Language Models (LLMs) are intrinsically linked to the principles of model scaling laws, which delineate the relationship between model size, data quantity, computational power, and performance [20,28,33]. These laws indicate that augmenting a model's size and the quantity of training data consistently enhances its capabilities, leading to improved performance and the acquisition of novel functionalities [20]. This general trend, however, is subject to diminishing returns, meaning that beyond certain thresholds, the performance gains from further scaling may become less significant [28].

Two prominent scaling laws have profoundly influenced the training strategies for LLMs: the Kaplan-Mitchell (KM) Scaling Law and the Chinchilla Scaling Law [28,30,33]. The **KM Scaling Law**, proposed by Kaplan et al. (OpenAI, 2020), models a power-law relationship for model performance (measured by cross-entropy loss) with respect to model size ($N$), dataset size ($D$), and training compute ($C$) [28,30]. It suggests that, given a fixed compute budget, allocating a larger proportion to increasing model size is generally more favorable than to data size [30]. Conversely, the **Chinchilla Scaling Law**, introduced by Hoffmann et al. (Google DeepMind), advocates for compute-optimal training by suggesting that model size and data size should be increased in approximately equal scales for efficient resource allocation [28,30]. This implies that a greater number of training tokens (data) can be beneficial even for smaller model sizes, as exemplified by Chinchilla's superior performance over Gopher despite having fewer parameters [30]. These laws provide crucial guidance for estimating the performance of larger models and optimizing training resources, though they do not encompass all emergent capabilities [30]. The updated development path of the GPT series, including GPT-4v and GPT-4 Turbo, exemplifies the practical application of these scaling principles [9].



**Key Emergent Abilities of Large Language Models**

| Emergent Ability      | Description                                                 | Sci-LLM Relevance / Impact                                         |
| :-------------------- | :---------------------------------------------------------- | :----------------------------------------------------------------- |
| **In-Context Learning (ICL)** | Perform tasks by interpreting natural language instructions and demonstrations within the input (few-shot/zero-shot) without retraining. | Tackle intricate scientific problems with limited examples, adapt to specialized domain tasks. |
| **Instruction Following** | Generalize to perform well on unseen tasks based solely on natural language instructions, strengthened by model scale. | Solve complex scientific problems, move beyond passive knowledge processing to interactive tools. |
| **Chain-of-Thought (CoT) Reasoning** | Solve complex tasks requiring multiple reasoning steps by generating intermediate steps (e.g., mathematical word problems). | Accelerate scientific discovery by enabling complex problem-solving, synthesis of complex knowledge, and pattern inference. |
| **Agentic Capabilities** | Design for long-running tasks and agentic workflows, use computers like humans. | Develop autonomous agents for scientific research, active discovery partners, conducting experiments. |

Beyond mere performance improvements, scaling laws underpin the emergence of "emergent abilities," which are qualitatively new behaviors or capacities not present in smaller models but that arise distinctly in larger models once their scale (parameters) surpasses a specific threshold [10,17,28,30,33]. These abilities are characterized by a significant and sudden jump in performance, akin to phase transitions observed in physics, rather than a continuous, linear improvement [17,28,30]. Examples include the capacity to write basic code, solve logic puzzles [24], and perform numerical computations or language translation [10]. While their emergence has posed challenges in anticipation, and their full potential remains somewhat uncertain, these capabilities significantly broaden the application scope of LLMs [20].

Key emergent abilities include:
1.  **In-context learning (ICL)**: Formally introduced with GPT-3, ICL enables LLMs to perform tasks (few-shot or zero-shot) by interpreting natural language instructions and demonstrations within the input, without requiring additional training or gradient updates [17,28,30,33]. For instance, the 175B GPT-3 demonstrated strong ICL, unlike its predecessors, GPT-1 and GPT-2, though its effectiveness can be task-dependent [28,30].
2.  **Instruction following**: Through instruction tuning—fine-tuning with multi-task datasets formatted via natural language descriptions—LLMs can generalize to perform well on unseen tasks based solely on instructions, with this capability strengthening with increased model scale [17,28,30,33]. An example is LaMDA-PT, which showed significant outperformance on unseen tasks when its model size reached 68B, a capability not observed in smaller variants [30].
3.  **Step-by-step reasoning (Chain-of-Thought, CoT)**: Unlike smaller models, large LLMs can solve complex tasks requiring multiple reasoning steps, such as mathematical word problems, by generating intermediate reasoning steps via CoT prompting [17,28,30,33]. This ability, potentially acquired through code training, showed performance gains for models like PaLM and LaMDA variants with over 60B parameters, becoming more pronounced above 100B [17,30]. Recent developments like DeepSeek-R1 and Grok feature advanced reasoning capabilities utilizing CoT, self-verification, and reinforcement learning [15]. While larger models generally exhibit stronger reasoning, advanced techniques such as Multimodal-CoT have shown that even models with fewer than 1 billion parameters can achieve superior performance on benchmarks like ScienceQA, surpassing larger models like GPT-3.5 [32].
4.  **Agentic capabilities**: Advanced models like Claude Opus 4 are designed for "long-running tasks and agentic workflows," demonstrating the LLM's ability to use computers like humans, while Gemini Flash prioritizes speed for agentic systems [15].

These scaling insights and emergent abilities hold significant potential and challenges for developing powerful Scientific Large Language Models (Sci-LLMs). ICL and CoT prompting are particularly crucial for accelerating scientific discovery, allowing Sci-LLMs to tackle intricate scientific problems and adapt to specialized domain tasks with limited examples [30]. Sci-LLMs trained on scientific literature can gain "profound capabilities to interpret and manipulate formal scientific language, SMILES strings, and to apply information from scientific literature" [3]. The LLM4SD framework, for instance, exemplifies an emergent ability by synthesizing complex knowledge and inferring patterns to generate interpretable insights for scientific discovery [3]. Furthermore, Sci-LLMs are being developed with instruction tuning to solve complex scientific problems and exhibit advanced capabilities in mathematics, such as Alpha-Geometry combining LLMs with symbolic reasoning and FunSearch integrating LLMs with program search to find novel solutions in combinatorial optimization [31]. OpenAI's o1 model family focuses on reasoning models excelling in STEM fields, demonstrating strong mathematical reasoning abilities, including scoring 83% on the International Mathematics Olympiad [15]. Capabilities like code generation, translation, and even specific "tools for drug discovery" within models like GPT-4, underscore the direct applicability of emergent abilities to scientific domains [20]. However, the expansion of LLMs also introduces challenges, including new failure mechanisms such as biases and "hallucinations," though the precision in discerning true statements tends to improve with model scale [20]. The debate surrounding the robustness of LLMs' true reasoning limits, exemplified by the "Illusion of Intelligence" critique, suggests that not all emergent abilities are fully understood or consistently reliable [29]. Therefore, while scaling laws and emergent abilities unlock unprecedented potential for Sci-LLMs, careful consideration of their limitations and reliability is paramount for their effective integration into scientific discovery workflows.
### 2.4 Scientific Data Taxonomies and Hierarchical Knowledge Models

Scientific large language models (Sci-LLMs) operate on a diverse and complex landscape of scientific data, necessitating a structured classification and a hierarchical understanding of knowledge. A unified taxonomy of scientific data encompasses various modalities, including textual reports, experimental measurements, images, sensor readings, molecular structures, and time-series data [18]. Specific examples of data types include raw laboratory data [3], biomedical literature [32], and structured representations like molecular strings [3,31].

These scientific corpora exhibit distinct characteristics that differentiate them from general natural language processing (NLP) datasets. Firstly, they are inherently **multimodal**, integrating text with visual, symbolic, structured, and time-series information [7,18,32]. This multimodality requires advanced processing capabilities to fuse diverse data types, such as text paired with images or audio [20]. Secondly, scientific data is often **cross-scale**, with information spanning from subatomic to cosmological levels, each relevant to different aspects of research [18]. Thirdly, scientific domains feature highly **specialized terminology** and structured syntax, which are distinct from general language use [18,20,31]. For instance, protein and DNA sequences often follow formats like FASTA, while molecules are frequently described using SMILES strings [31].

Scientific knowledge itself is represented hierarchically, progressing from foundational raw data and facts to more abstract concepts and theories [18]. This hierarchical model is crucial for LLMs to not only process information but also to comprehend and reason over complex, interconnected knowledge [7]. For instance, in agriculture, professional knowledge bases are developed using tools like vector databases and knowledge graphs to integrate and structure information, addressing challenges in knowledge acquisition and integration [11]. This structure allows for a deeper understanding, moving beyond mere linguistic patterns to grasp underlying scientific principles.

These unique characteristics pose both challenges and opportunities for LLMs compared to traditional NLP tasks. The "data deficiency and modalities" in existing pre-trained datasets, which are often limited to single modes or languages, present a significant challenge for Sci-LLMs [20]. The integration of diverse data types, including graph data where nodes and edges may lack sufficient unlabeled data for pretraining, is essential for advancing LLMs in scientific contexts [20]. Consequently, there is an exigency for novel multimodal datasets and specialized pre-training strategies. To effectively leverage these complex data types, Sci-LLMs require domain-invariant representations and sophisticated cross-modal reasoning capabilities [18,32]. This involves adapting LLMs to various data structures, such as linearizing graphs into formats like SMILES or employing specialized encoders for visual and graph data [31].

Based on these observations, "scientific language" can be defined as a specialized communication system characterized by distinct terminology, structured syntax, and specific reasoning patterns, used to describe and interpret phenomena across various scientific domains [1,18,35]. It extends beyond natural language text to include formal symbolic representations such as molecular languages (e.g., SMILES strings for molecules), protein and gene languages [1,3], and mathematical expressions used in physics or earth science (e.g., algebraic equations, differential/integral forms, Navier-Stokes equations) [18]. The practical implications of this definition are evident in fields like agricultural intelligence, where LLMs must process and integrate multimodal data to build structured professional knowledge bases. This includes not only textual reports but also experimental data, sensor readings, and imaging, all requiring careful "knowledge acquisition and integration" and "multimodal data processing" to facilitate intelligent agricultural applications [11].
## 3. Efficient Architectural Paradigms for Scientific LLMs
The rapid advancements in Large Language Models (LLMs) have been significantly propelled by the Transformer architecture. However, its inherent quadratic computational complexity, $O(N^2)$, with respect to sequence length $N$, and substantial memory footprint present formidable challenges when processing the exceptionally long sequences and complex data structures frequently encountered in scientific domains. To address these limitations, a diverse array of efficient architectural paradigms has emerged, collectively aiming to enhance scalability, reduce computational costs, and optimize memory utilization without sacrificing the expressive power essential for scientific inquiry `[12]`. These innovations are crucial for enabling scientific LLMs to effectively analyze extensive scientific texts, experimental protocols, genomic sequences, and multimodal scientific data.



**Efficient Architectural Paradigms for Scientific LLMs**

| Paradigm                    | Core Strategy                                           | Key Techniques/Examples                                       | Scientific Impact                                               |
| :-------------------------- | :------------------------------------------------------ | :------------------------------------------------------------ | :-------------------------------------------------------------- |
| **Linear Sequence Modeling** | Re-engineer attention/recurrent mechanisms for O(N) complexity | Linear Attention (GLAs, GSAs, DeltaNet), Linear RNNs (HGRN2, RWKV6), State Space Models (Mamba, Mamba2), TTT-RNNs | Efficient processing of long scientific sequences (e.g., genomic, texts); reduced computational cost. |
| **Sparse Sequence Modeling** | Each token interacts with only a subset of others       | Static Patterns (global, window, dilated), Dynamic Patterns (LSH, MoSA), Inference acceleration (attention sinks, H2O) | Feasible processing of longer sequences; maintains crucial long-range dependencies with reduced FLOPs. |
| **Efficient Full Attention** | Optimize hardware utilization & memory access           | I/O-aware (FlashAttention), KV Cache Reduction (MQA, GQA, MLA), PagedAttention, Flash-Decoding | Significant acceleration, reduces memory footprint, enables processing of extensive data on existing hardware. |
| **Mixture of Experts (MoE)** | Conditional computation: activate only a subset of experts per input | Gating Network, Token-Choice/Expert-Choice, fine-grained/shared experts (Mixtral, Qwen3, Kimi K2) | Scales model capacity without proportional compute increase; beneficial for varied scientific tasks/sub-domains. |
| **Hybrid Architectures**    | Strategically integrate multiple efficient mechanisms   | Inter-layer (Mamba-Transformer, Jamba), Intra-layer (Hymba, LoLCATs), GNN+LLM, Visual Encoder+LLM | Optimal balance of efficiency/performance; handles complex scientific data (text, graph, multimodal). |
| **Diffusion Language Models** | Non-autoregressive, iterative denoising for text generation | Forward/Reverse process, Parallel Decoding (LLaDA, BD3-LMs), Controllability, Bidirectional Attention | Accelerated generation of long scientific texts; stronger control over text attributes. |

This section provides an overview of these critical architectural developments, categorized by their primary strategies for achieving efficiency:

**Linear Sequence Modeling** represents a fundamental departure from quadratic complexity by re-engineering attention or recurrent mechanisms to achieve $O(N)$ computational complexity. This paradigm encompasses several distinct families, including Linear Attention, which approximates the Softmax mechanism to rearrange matrix multiplications and achieve linear scaling, often incorporating gating mechanisms and Delta learning rules for enhanced adaptability and memory capacity `[12]`. Linear RNNs circumvent traditional RNN limitations by simplifying non-linearities for parallel training and efficient inference `[12]`. State Space Models (SSMs), originating from control theory, process sequences by discretizing continuous dynamic systems, with Mamba introducing data-dependent "selectivity" for dynamic information focus `[12,29]`. Additionally, Test-Time-Training RNNs (TTT-RNNs) utilize trainable "fast weights" for strong online adaptivity during inference `[12]`. These diverse approaches often converge on shared principles of memory system optimization, all contributing to the efficient processing of long scientific sequences `[12]`.

**Sparse Sequence Modeling** mitigates the computational overhead of full self-attention by ensuring that each token interacts with only a carefully selected subset of other tokens `[12,28]`. This strategy reduces computational complexity from quadratic to often linear or quasi-linear. Implementations range from static patterns (e.g., global, window, dilated, random attention) that provide predictable savings to dynamic patterns (e.g., LSH, MoSA) that adapt based on input content, aiming to preserve crucial long-range dependencies while maintaining efficiency `[12]`. Furthermore, specific sparse attention methods accelerate inference by pruning attention matrix calculations during prefill or compressing the KV cache during decoding `[12]`.

**Efficient Full Attention** techniques focus on optimizing hardware utilization and memory access patterns to maintain the representational capacity of standard attention while enhancing efficiency. Key innovations address the "memory wall" problem by reducing data transfer between GPU HBM and SRAM. FlashAttention, for instance, employs tiling, online Softmax computation, and recomputation during backpropagation to achieve significant acceleration `[12]`. Other strategies, like PagedAttention and Flash-Decoding, further optimize data transfer during inference `[9,28]`. Additionally, methods like Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce KV cache size, accelerating inference by allowing multiple query heads to share K/V heads, balancing efficiency with representational power `[12]`.

The **Mixture of Experts (MoE) paradigm** offers a unique approach to scaling model capacity without proportionally increasing computational burden. It achieves this through conditional computation, where only a subset of specialized "experts" (typically FFN layers) are activated for any given input, guided by a "gating network" `[12,15,33]`. This specialization allows the model to leverage diverse "expert" knowledge, which is particularly beneficial for the varied tasks and specialized sub-domains within scientific applications. MoE architectures, exemplified by models like Mixtral, Qwen3, and Kimi K2, significantly enhance the ability to handle complex scientific data and contexts by selectively engaging relevant parameters `[12,15,29]`.

**Hybrid Architectures** strategically integrate multiple efficient mechanisms, such as State Space Models (SSMs) and various forms of attention, to achieve an optimal balance between efficiency and performance. These designs assign different mechanisms to distinct parts of the model or processing stages, mitigating the quadratic scaling of traditional Transformers `[12]`. Examples include inter-layer hybrids that stack different layer types alternately (e.g., Mamba and attention layers in Jamba and Zamba) and intra-layer hybrids that combine different computational approaches within a single attention layer, either by head-level splitting (e.g., Hymba) or sequence-level splitting (e.g., LoLCATs) `[12]`. Models like Nemotron Nano 2 and MiniMax-M1 demonstrate the practical utility of these hybrid approaches for enhanced reasoning and long-context handling in scientific applications `[29]`. Beyond sequence processing, hybrid designs also extend to multimodal scientific data integration, combining GNNs with LLMs for text+graph structures, or visual encoders with LLMs for scientific imaging `[31]`.

Finally, **Diffusion Language Models** represent a novel non-autoregressive paradigm for efficient text generation. Unlike traditional autoregressive models, diffusion models generate text in parallel by learning to iteratively denoise a corrupted input over two phases: a forward noising process and a reverse denoising process `[12]`. This mechanism enables parallel decoding, significantly accelerating generation, offers stronger controllability over text attributes, and benefits from bidirectional attention, which addresses limitations of unidirectional models. Models like LLaDA and BD3-LMs showcase how diffusion principles can provide efficient and controllable generation of long scientific texts, simulations, and data descriptions `[12]`.

In summary, these efficient architectural paradigms collectively address the fundamental scalability and computational challenges of applying LLMs to complex scientific data. By reducing computational complexity, optimizing hardware utilization, enabling conditional computation, and fostering hybrid designs, they unlock new possibilities for deep contextual understanding, accelerated processing, and advanced generative capabilities crucial for advancing scientific discovery. The continuous evolution and integration of these diverse strategies underscore an ongoing effort to build more powerful, yet accessible, scientific LLMs.
### 3.1 Linear Sequence Modeling
Linear sequence modeling represents a significant departure from the quadratic complexity inherent in traditional Transformer architectures, aiming to achieve $O(N)$ computational complexity with respect to sequence length $N$ [12]. This paradigm seeks to enhance efficiency and performance, positioning itself as a compelling alternative to Transformer-based models [28].

Several distinct families of approaches contribute to this field, each with unique mechanisms for complexity reduction and associated trade-offs in terms of memory capacity and dynamic adaptation.

**1. Linear Attention:**
This approach re-conceptualizes the standard Softmax attention mechanism, $Attention(Q, K, V) = Softmax(\frac{QK^T}{\sqrt{d_k}})V$, by approximating the $exp(qk^T)$ term with a kernel function, $sim(q, k) = \phi(q)\phi(k)^T$. This approximation facilitates the rearrangement of matrix multiplications, transforming $(QK^T)V$ into $Q(K^TV)$, which can then be expressed in a recurrent form: $S_t = S_{t-1} + \phi(k_t)v_t^T$. This formulation yields a computational complexity of $O(Nd^2)$, which is linear in sequence length $N$ when $N \gg d$ (where $d$ is the model dimension). To address challenges in memory capacity and adaptability, linear attention has incorporated advancements such as gating mechanisms, exemplified by Gated Linear Attention (GLAs) and Gated State Space Attention (GSAs), which dynamically regulate information flow. Furthermore, Delta learning rules, treating the state $S$ as "fast weights" updated during training, enhance memory and adaptability, as seen in DeltaNet and RWKV7. More sophisticated designs, like Log-Linear Attention, optimize memory states to grow logarithmically, achieving $O(N \log N)$ training complexity and $O(\log N)$ inference complexity [12].

**2. Linear RNNs:**
Traditional Recurrent Neural Networks (RNNs) suffer from parallelization difficulties due to their inherent recurrent dependencies and non-linear activations. Linear RNNs circumvent these issues by simplifying or removing non-linearities, thereby enabling parallel training while maintaining $O(1)$ memory and computational cost during inference. Their evolution has progressed from simple vector states to more sophisticated matrix-valued memory, as observed in models like HGRN2 and RWKV6, which significantly expand their memory capacity [12].

**3. State Space Models (SSMs):**
Originating from control theory, SSMs model sequences by discretizing continuous dynamic systems. A continuous-time SSM is typically represented by the ordinary differential equations:
$$\frac{dx(t)}{dt} = Ax(t) + Bu(t)$$
$$y(t) = Cx(t) + Du(t)$$
where $x(t)$ denotes the hidden state, $u(t)$ the input, and $y(t)$ the output. For sequence processing, the continuous parameters $(A, B)$ are discretized into $(\bar{A}, \bar{B})$. A pivotal innovation in this domain is Mamba, which introduces "selectivity" by making the SSM parameters $(A, B, C)$ input-dependent through a linear projection layer. This data-dependent parameterization allows Mamba to dynamically focus on or ignore specific information, marking a crucial shift from data-agnostic to data-dependent parameters and significantly enhancing model expressiveness. Mamba achieves performance comparable to or even surpassing Transformer models [12], and its linear scaling with sequence length is a key advantage [29]. Other notable examples of SSM-based architectures include RWKV, RetNet, and Hyena [28]. Further developments like Mamba2 have simplified the state matrix, while Comba integrates closed-loop control principles and Delta learning rules for state updates, along with output correction mechanisms, leading to both theoretical and practical breakthroughs [12]. The Nemotron Nano 2 family further exemplifies this trend by introducing hybrid Mamba-Transformer LLMs, showcasing an ongoing exploration into more efficient sequence modeling paradigms [29].

**4. Test-Time-Training RNNs (TTT-RNN):**
These models distinguish themselves by treating state matrices as trainable "fast weights" that are updated dynamically during inference, thus providing strong online adaptivity. Examples include TTT, Titans, and Atlas. While offering higher expressive power through their adaptive nature, they typically incur increased computational overhead due to the use of complex optimizers for memory state updates [12].

Despite their diverse origins, these linear sequence modeling approaches exhibit a notable convergence towards unified views, particularly concerning memory systems and optimization of memory states. From a memory perspective, updates can be linear, bilinear, or non-linear, reflecting various strategies for information retention and retrieval. From an optimizer's perspective, these models are understood as optimizing memory states under different loss functions, including local L1/L2, multi-step L2, and global L2. Additionally, the concept of "linearization" provides an economical method to adapt pre-trained Transformer models by replacing Softmax attention layers with efficient linear modules, followed by fine-tuning or distillation where the Transformer serves as a teacher for a linear student model [12].

These innovations directly address the limitations of Transformer models, particularly their $O(N^2)$ computational complexity, which becomes prohibitive for long sequences. By significantly reducing computational cost, these linear sequence models are particularly beneficial for scientific applications. They enable the efficient processing of longer scientific texts, such as research papers and experimental protocols, and facilitate the handling of complex data structures common in scientific domains, where sequence lengths can be extensive and computational resources often constrained [12]. The ability to scale linearly with sequence length allows these models to unlock new possibilities for analyzing large-scale scientific datasets that were previously intractable with quadratic-scaling architectures.
### 3.2 Sparse Sequence Modeling
Sparse attention is a pivotal mechanism designed to mitigate the substantial computational overhead inherent in the self-attention mechanism of Transformer models [12]. Unlike full attention, where every token interacts with every other token, sparse attention operates on the principle that each token interacts with only a carefully selected subset of other tokens [12,28]. This approach aims to reduce the quadratic computational complexity associated with sequence length, making the processing of longer sequences more feasible [12].

The implementation of sparse attention can be broadly categorized into static (fixed patterns) and dynamic (adaptive or learned) strategies [12]. **Static sparse attention** utilizes predefined attention patterns that remain fixed throughout both training and inference phases [12]. Common patterns include global attention, where specific tokens (e.g., token) attend to all others; window attention, which restricts interactions to a local neighborhood; dilated attention, allowing tokens to attend to others at fixed intervals; and random attention, which selects a subset of tokens randomly [12]. Models like Longformer and BigBird often combine multiple such patterns to achieve a balance between capturing local context and preserving long-range dependencies, thereby mitigating the potential loss of information while maintaining computational efficiency [12].

In contrast, **dynamic sparse attention** patterns adapt based on the input content, allowing for more flexible and potentially more informative interactions [12]. Early methods, such as Reformer, leveraged Locality Sensitive Hashing (LSH) to group similar tokens, enabling attention computations within these clusters [12]. More recent advancements, like MoSA (Mixture-of-Sparse Attention), draw inspiration from Mixture-of-Experts (MoE) architectures, empowering individual attention heads to dynamically select a specific, small group of tokens for computation [12]. This adaptive nature helps in retaining crucial long-range dependencies more effectively than purely static patterns, as the sparsity pattern is optimized for the specific input [12].

Beyond training efficiency, a distinct category of sparse attention methods focuses on accelerating inference without requiring model retraining [12]. These techniques primarily target two phases: accelerating prefill and accelerating decoding [12]. For prefill acceleration, static patterns, exemplified by MInference, or dynamic prediction mechanisms, such as SeerAttention, are employed to prune the attention matrix calculations [12]. During decoding, the focus shifts to compressing the KV cache. Strategies include utilizing "attention sinks" to selectively cache only the initial and most recent tokens, as seen in StreamingLLM, or dynamically evicting low-scoring tokens, a technique used in H2O [12].

The inherent trade-off in sparse attention lies between achieving significant efficiency gains and potentially losing critical information or long-range dependencies [12]. Fixed patterns offer predictable computational savings but might struggle to capture complex, non-local relationships. Dynamic and combined static patterns, by adapting to input or integrating diverse interaction ranges, strive to mitigate this information loss, ensuring that critical long-range dependencies are preserved while still maintaining a reduced computational footprint [12].

By substantially reducing the computational cost from quadratic to often linear or quasi-linear with respect to sequence length, sparse attention directly addresses the limitations of processing exceptionally long sequences in traditional full-attention Transformers [12]. This is particularly beneficial for scientific applications that frequently involve handling extensive scientific texts, such as research papers, experimental protocols, or genomic sequences, and complex data structures where long-range contextual understanding is paramount [12]. The reduction in FLOPs and memory footprint enables the deployment of larger models or the processing of longer inputs on existing hardware, thereby democratizing access to powerful language models for scientific inquiry [12].

While the primary focus of sparse sequence modeling often revolves around attention mechanisms, related concepts of sparsity extend to other components of Transformer architectures. For instance, "sparse scaling" in Feed-Forward Networks (FFNs) has been explored to increase model parameters while maintaining consistent training and inference costs (in FLOPs) [20]. Scaled and sparse FFNs (S-FFN) are conceptualized as a form of neural memory, where only a subset of memory cells are activated during retrieval, akin to a sparse memory system [20]. However, this distinct approach to sparsity within FFNs, while promising for parameter efficiency, operates separately from the core principles and techniques of sparse attention applied to sequence interactions [20].
### 3.3 Efficient Full Attention
Efficient full attention techniques represent a critical advancement in large language model (LLM) architectures, aiming to maintain the robust representational capacity of standard attention mechanisms while significantly enhancing computational efficiency through optimized hardware utilization and memory access patterns `[12]`. These innovations directly address the inherent limitations of traditional Transformer attention, such as its quadratic complexity with respect to sequence length and substantial memory footprint, which impede the processing of longer sequences and high-dimensional data `[12]`.

A cornerstone of these advancements is the I/O-aware attention mechanism, exemplified by FlashAttention. This technique redefines the primary bottleneck in attention computation, shifting the focus from Floating Point Operations (FLOPs) to the data transfer volume between GPU High Bandwidth Memory (HBM) and the faster, but smaller, on-chip SRAM `[12]`. FlashAttention mitigates this "memory wall" problem—particularly acute during incremental inference and decoding `[9]`—through a series of sophisticated optimizations. These include tiling, which processes the attention matrix in smaller blocks; online Softmax computation, which avoids writing intermediate values to HBM; and recomputation during backpropagation, which sacrifices some FLOPs for substantial memory savings `[12]`. By fusing multiple HBM read/write operations into a single, more efficient transaction, FlashAttention achieves an end-to-end acceleration of 2-4x, with subsequent iterations like FlashAttention-2 and FlashAttention-3 further solidifying its status as an industry standard `[12]`. Other techniques, such as PagedAttention and Flash-Decoding, also contribute to reducing data transfer during inference, thereby enhancing decoding speed and overall efficiency `[9,28]`.

Beyond I/O optimization, strategies for reducing the Key-Value (KV) cache size play a crucial role in improving inference speed `[12]`. Multi-Query Attention (MQA) addresses this by having multiple query heads share a single set of key (K) and value (V) heads, significantly compressing the KV cache. Grouped-Query Attention (GQA) offers a compromise, grouping query heads so that each group shares K/V heads, balancing the efficiency gains of MQA with the representational power closer to standard Multi-Head Attention (MHA) `[12]`. More advanced methods, such as Multi-Head Latent Attention (MLA) implemented in models like DeepSeek-V2/V3, compress the KV cache into a low-dimensional "latent vector," further optimizing memory usage `[12]`. Additional optimizations like Attention Mixture and Quantized Attention are also explored to enhance efficiency `[12]`.

These efficient full attention mechanisms confer substantial benefits for scientific applications. By drastically reducing computational cost and memory footprint, they enable the processing of significantly longer scientific texts, complex data structures, and high-resolution inputs that were previously intractable due to hardware limitations. For instance, the ability to handle extended contexts is critical for tasks requiring deep contextual understanding in scientific literature, such as knowledge graph construction or complex question answering. REFRAG, for example, demonstrates how such optimizations can scale Retrieval Augmented Generation (RAG) systems to handle 16x longer contexts and achieve 31x faster decoding by effectively compressing and shortening the context, implicitly relying on more efficient attention computations for large inputs `[29]`. This allows scientific LLMs to process entire research papers, experimental protocols, or large biological sequences without sacrificing critical information, thereby broadening the scope and depth of scientific discovery facilitated by AI.
### 3.4 Mixture of Experts (MoE)
The Mixture of Experts (MoE) paradigm represents a significant architectural advancement in large language models (LLMs), enabling the scaling of model size and capacity while meticulously controlling inference costs through the selective activation of specialized sub-networks [12,15,33]. This conditional computation approach ensures that for any given input, only a subset of the total parameters, termed "experts," are engaged, thus preventing a proportional increase in computational burden despite a vast increase in the model's total parameter count [12,15,33].



![Mixture of Experts (MoE) Architecture Overview](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/LAzyyaduKOuiiikRLTHPK_Mixture%20of%20Experts%20%28MoE%29%20Architecture%20Overview.png)

At its core, an MoE architecture comprises two main components: multiple "experts" and a "gating network" [12]. The experts are typically parallel Feed-Forward Network (FFN) layers, each designed to specialize in processing certain types of input or tasks. The gating network, a smaller neural network, is responsible for dynamically routing each input token to a selected subset of these experts. Routing mechanisms vary, with "Token-Choice" being the predominant method in contemporary LLMs like Mixtral, where each token selects the top-k most suitable experts to process it. However, this method can lead to load imbalances among experts. Conversely, "Expert-Choice" mechanisms aim for perfect load balancing by having each expert select the top-k tokens it wishes to process, though this is more complex for autoregressive generation [12]. Beyond standard FFNs, MoE designs are evolving to include fine-grained experts (numerous, smaller experts), shared experts (processed by all tokens), and Deep Mixture (MoD), which treats different Transformer layers as experts [12]. Furthermore, to mitigate the high training costs associated with MoE models, efficient conversion strategies exist, allowing existing dense models to be transformed into MoE models through techniques such as splitting, copying, or merging [12].

The efficacy of MoE as a strategy for achieving high performance and efficiency in large-scale LLMs is widely evidenced by its adoption in prominent models. Examples include ERNIE-4.5-21B-A3B-Thinking, a compact MoE model designed for deep reasoning; Qwen3 30B-A3B, an MoE transformer model; and Qwen3-Coder-480B-A35B-Instruct, featuring a distinctive MoE architecture tailored for agentic coding [29]. Noteworthy trillion-parameter MoE models such as Kimi K2 utilize 32 billion active parameters per token, focusing on long context, code, reasoning, and agentic behavior, while Hunyuan-A13B is a 13B active parameter MoE model with dual-mode reasoning and 256K context [29]. Other examples include Ernie 4.5, Llama 4, and models from the Mistral and IBM's Granite families, all leveraging MoE architectures [15]. The "FastMoE" library further facilitates the development of such LLMs [28].

In scientific contexts, where diverse tasks and specialized sub-domains necessitate nuanced processing, MoE architectures offer substantial advantages. The ability to selectively activate specialized sub-networks allows the model to allocate "expert" knowledge precisely where needed, which is particularly beneficial for handling the complexity and diversity inherent in scientific data and texts [12]. For instance, a model might employ distinct experts for different scientific disciplines (e.g., biology, physics, chemistry) or for specific data modalities (e.g., textual research papers, genomic sequences, chemical structures). This specialization can significantly improve the model's ability to handle longer scientific texts or complex data structures, a common limitation in traditional dense models. By activating only relevant experts, computational costs are substantially reduced, making it more feasible to train and deploy larger, more capable models for scientific discovery [12]. The concept of "hybrid experts Transformers" designed to better capture graph signals when combining text and graph structures in scientific LLMs implicitly supports this benefit, illustrating how specialized components can enhance performance on intricate scientific data [31]. MoE thus represents a critical innovation for developing more powerful and efficient scientific LLMs capable of advancing research across various domains.
### 3.5 Hybrid Architectures
Hybrid architectures represent a significant evolutionary step in large language model design, strategically integrating diverse efficient mechanisms, such as State Space Models (SSMs) like Mamba and various forms of attention, to achieve an optimal balance between computational efficiency and model performance [12]. This approach directly addresses the inherent limitations of traditional Transformer-based models, particularly their quadratic scaling of computational cost with sequence length, which becomes prohibitive for processing long scientific texts and complex data structures. By assigning different mechanisms to distinct parts of the model or different processing stages, hybrid designs reduce computational cost while maintaining or enhancing model capabilities [12].

A prominent example of this paradigm is the Nemotron Nano 2 family, which introduces "hybrid Mamba-Transformer LLMs," explicitly combining the strengths of both architectural components for improved reasoning and efficiency [29]. Similarly, MiniMax-M1 is noted as a substantial 456B hybrid model specifically designed for long-context and reinforcement learning tasks, demonstrating the utility of such designs in handling extensive sequential data typical in scientific domains [29].

Hybrid architectures can be broadly categorized based on how these different modules are integrated within the model [12]:
*   **Inter-layer Hybrid**: This approach involves stacking different types of layers alternately throughout the model's depth. For instance, a model might primarily utilize efficient Mamba layers, interspersed with periodic insertions of standard attention layers. This configuration allows the model to leverage the Mamba layers' efficiency for local context processing while benefiting from the global information interaction and enhanced memory capabilities provided by the standard attention layers. Notable examples in this category include Jamba and Zamba [12].
*   **Intra-layer Hybrid**: This strategy combines different computational approaches within a single attention layer, further divided into two sub-categories:
    *   **Head-level Split**: Here, attention heads are partitioned into two groups. One group employs standard attention mechanisms to capture global dependencies, while the other utilizes more efficient linear attention, such as that found in Mamba-like structures. Hymba is an illustrative example of a model adopting this head-level division [12].
    *   **Sequence-level Split**: This method applies different attention mechanisms to distinct segments of the input sequence. For example, precise windowed attention can be applied to recent tokens for fine-grained local context understanding, while more efficient linear attention handles distant tokens to maintain a broader contextual awareness without incurring excessive computational overhead. LoLCATs exemplify this sequence-level attention partitioning [12].

These architectural innovations are particularly beneficial for scientific applications, which frequently involve processing extremely long texts, such as research papers, experimental protocols, or genetic sequences, and complex data structures like graphs and multimodal inputs. By mitigating the quadratic computational burden of traditional attention for long sequences, hybrid models enable the analysis of more comprehensive scientific datasets at a reduced cost. Beyond sequential data, hybrid architectures also extend to multimodal scientific data integration. For instance, in Text+Graph models, some studies employ Graph Neural Networks (GNNs) as graph encoders and LLMs as text encoders, linking the two modalities via contrastive learning, as seen in Text2Mol which uses GCN and SciBERT [31]. Other hybrid approaches integrate visual encoders, such as ViT or Swin Transformer, to project image data into multiple visual tokens that are subsequently fed into LLMs alongside text tokens, facilitating multimodal understanding in models like G-LLaVA and LLaVA-Med for scientific imaging applications [31]. This integration of specialized encoders with general-purpose LLMs allows for a more comprehensive and computationally efficient handling of the diverse and complex data landscapes prevalent in scientific discovery.
### 3.6 Diffusion Language Models
Diffusion Language Models (LLMs) represent a significant conceptual shift from traditional autoregressive (AR) models, emerging as a novel non-autoregressive paradigm for efficient text generation [12]. Unlike AR models that generate text token by token in a sequential manner, diffusion models operate through a two-phase process: a forward noising process and a reverse denoising process [12]. In the forward phase, the original text is progressively masked or corrupted, transforming it into a noisy version. The model is then trained in the reverse phase to reconstruct the original text by learning to iteratively denoise the masked version [12].

This generative mechanism bestows several key advantages, particularly in efficiency and controllability, distinguishing them from their autoregressive counterparts. A primary characteristic is their ability for **parallel decoding**, which fundamentally resolves the sequential bottleneck inherent in AR models [12]. By generating all tokens simultaneously or in blocks, diffusion LLMs can significantly accelerate the generation process compared to the step-by-step prediction of AR models. Furthermore, they offer **stronger controllability** over generated text attributes, such as length and format, and benefit from **bidirectional attention**, which allows for a global context perception that can mitigate issues like the "inversion curse" often observed in unidirectional models [12].

These architectural innovations position Diffusion LLMs to address several limitations commonly associated with autoregressive models, which are particularly pronounced in scientific applications. The parallel generation capability of diffusion models directly contributes to a substantial reduction in computational cost for generating longer texts, a common requirement in scientific research [12]. This enhanced efficiency is crucial for handling extensive scientific articles, experimental data descriptions, or complex simulations where the generation of long, coherent sequences is necessary. Moreover, their improved controllability and global context understanding make them adept at handling complex data structures and maintaining consistency across lengthy scientific documents, which can be challenging for purely autoregressive models with limited context windows or sequential dependencies. Representative models in this domain include LLaDA, noted as the first 8-billion parameter diffusion LLM trained from scratch, and BD3-LMs, which cleverly combine autoregressive and diffusion techniques to achieve a balance between generation efficiency and flexibility by allowing parallel generation within blocks while maintaining autoregressive generation between blocks [12]. These advancements open new avenues for more effective and efficient tackling of complex scientific generative tasks.
## 4. Scientific Large Language Models: Definition and Taxonomy
Scientific Large Language Models (Sci-LLMs) constitute a specialized paradigm within the broader field of Large Language Models (LLMs), meticulously engineered to address the unique complexities and demands of scientific discovery and research [1,35]. Unlike their general-purpose counterparts, Sci-LLMs are fundamentally distinguished by their explicit design and rigorous training on vast, domain-specific scientific datasets, enabling them to proficiently interpret, generate, and interact with scientific knowledge, including formal scientific languages and diverse data modalities [3,19,20,31]. This tailored approach equips them with the in-depth, real-time, and accurate domain knowledge critical for advanced scientific applications.

The distinction between Scientific LLMs and general-purpose LLMs in scientific contexts is crucial for understanding their respective capabilities and limitations. General-purpose LLMs, such as ChatGPT, GPT-4, and LLaMA, demonstrate remarkable proficiency in broad language understanding, generation, and interaction [4,21,30]. Researchers leverage these models for scientific tasks through sophisticated prompting techniques like Chain-of-Thought (CoT), In-Context Learning (ICL), and Retrieval-Augmented Generation (RAG), achieving promising results in areas such as medical education assessment, clinical documentation, and scientific hypothesis generation [21,23,26,30]. However, these models exhibit significant shortcomings in highly specialized scientific domains, often struggling with language and cultural variations, numerical computation, and the pervasive issue of "hallucinations" [7,21,30]. Their lack of specificity and precision, coupled with knowledge timeliness issues, necessitates extensive human oversight for factual accuracy and scientific rigor, underscoring the imperative for domain-adapted models [5,28].

Scientific LLMs overcome these limitations by specializing in structured scientific data formats and complex domain-specific tasks. They are designed to interpret and manipulate formal scientific languages, such as SMILES strings for chemical structures, FASTA sequences for genetic information, and genomic sequences [3,26,31]. Furthermore, they process diverse scientific data modalities beyond natural language, including molecules, proteins, tabular data, and metadata, integrating domain knowledge from specialized databases and operating within scientific tools [5,31]. This profound specialization positions Sci-LLMs to accelerate research, enhance discovery, and foster interdisciplinary collaboration across a broad spectrum of scientific and social sciences [20].

The burgeoning field of Sci-LLMs necessitates a structured taxonomy to categorize their diverse capabilities and applications. This section establishes a framework for classifying Sci-LLMs based on the specific scientific data modalities they process and their domain focus [1,31]. This taxonomy reflects the increasing specialization of LLMs in scientific research, moving from foundational textual analysis to the integration of complex, multi-omics data. 

We categorize Sci-LLMs into five primary types:

1.  **Text-focused Scientific LLMs (Text-Sci-LLMs)**: These models specialize in interpreting and generating scientific natural language, operating on specialized textual corpora like research papers, medical records, and climate reports across various scientific disciplines [1,18].
2.  **Molecular LLMs (Mol-LLMs)**: Designed to interpret and process chemical languages, Mol-LLMs focus primarily on small molecule compounds, encoded through representations such as SMILES, SELFIES, and molecular graphs, crucial for molecular property prediction and chemical synthesis [1,18].
3.  **Protein LLMs (Prot-LLMs)**: Dedicated to understanding, manipulating, and engineering protein sequences, Prot-LLMs are fundamental for applications in drug design, disease mechanism elucidation, protein structure prediction, and *de novo* protein sequence design [1,26].
4.  **Genomic LLMs (Gene-LLMs)**: These models analyze and interpret DNA and RNA sequences to decipher the genetic "language," pivotal for understanding gene function, predicting variants, and analyzing genetic networks [1,26].
5.  **Multimodal Scientific LLMs (MM-Sci-LLMs)**: Representing a critical evolution, MM-Sci-LLMs integrate diverse data types—including text, visual elements, symbolic representations, structured data, and time-series information—to achieve a more comprehensive understanding and prediction capabilities, addressing the inherent multimodality of scientific data [18].

The development and deployment of these specialized Sci-LLMs underscore a fundamental shift towards more tailored AI solutions in science. However, common challenges across all categories include the critical need for high-quality, multimodal datasets and the establishment of robust, standardized evaluation metrics to assess their performance and reliability, ensuring their continued progression and wider adoption in scientific discovery [7,20].
### 4.1 Defining Scientific LLMs
Scientific Large Language Models (Scientific LLMs) represent a specialized subcategory of Large Language Models (LLMs) explicitly designed to facilitate scientific discovery and research [1,35]. They extend the capabilities of general LLMs by focusing on specialized language systems prevalent across various scientific disciplines [1,35]. A comprehensive working definition for Scientific LLMs is models that are trained on or specifically tailored for scientific domains, enabling them to interpret, generate, and interact with scientific knowledge, including formal scientific languages and diverse data modalities, to accelerate research, enhance discovery, and foster interdisciplinary collaboration [3,19,20,31].

The efficacy of Scientific LLMs stems from their rigorous training on vast scientific datasets and subsequent domain-specific adaptations, which enable them to proficiently interpret, generate, and interact with scientific knowledge [3,19,31]. This training often involves extensive scientific corpora, including biomedical literature, genetic and proteomic datasets, and large-scale code repositories [20,32]. For instance, BioGPT is explicitly trained on biomedical literature to generate and mine biomedical text [32], while LLMs applied in biomedicine scrutinize genetic and proteomic datasets to predict biological functionalities, disease mechanisms, and drug development procedures [20]. Furthermore, models like LLM4SD synthesize literature-based knowledge and infer patterns from experimental data to drive scientific discovery in areas such as molecular property prediction [3,22]. This specialized training paradigm equips Scientific LLMs with the in-depth, real-time, and accurate domain knowledge essential for professional scientific applications [20].

Scientific LLMs are distinguished from general-purpose LLMs by several specialized features, particularly their ability to work with structured scientific data formats and handle complex domain-specific tasks [3,5,19,31]. Unlike general LLMs that primarily process natural language, Scientific LLMs are designed to interpret and manipulate formal scientific languages, such as SMILES strings for chemical structures, FASTA sequences for genetic information, and genomic sequences [3,26,31]. They process diverse scientific data modalities beyond natural language, including molecules, proteins, tabular data, and metadata [31]. For example, "Materials science LLMs (MatSci-LLMs)" are grounded in domain knowledge, enabling hypothesis generation and testing by comprehending and reasoning over complex, interconnected materials science knowledge [7]. Similarly, "Large Language of Life Models" (LLLMs) are characterized by their multi-omics capabilities, interpreting life's multi-level language across molecular biology, including proteins, RNA, DNA, and ligands [8]. These models integrate domain knowledge from specialized databases, such as NCBI and IPCC reports, and can operate within specialized scientific tools, including robotic liquid handlers and chemistry tools [5]. The adaptation to unique scientific data characteristics, such as multimodality and cross-scale properties, often requires specialized representations that preserve domain invariance and enable cross-modal reasoning, as exemplified by the use of SELFIES for ensuring valid molecular strings in generation tasks [18].

The potential of Scientific LLMs to profoundly impact scientific endeavors is significant. They are poised to accelerate research, enhance discovery, and foster interdisciplinary collaboration across various scientific and social sciences [20]. Their capabilities extend to streamlined literature analysis, creative idea generation, and intricate data interpretation [20]. Specific applications include supporting critical stages of scientific research such as hypothesis discovery, experiment planning, scientific writing, and peer review [19]. Models like Biomni-R0 in biomedical research and AlphaGenome for predicting DNA variant impacts illustrate their growing role in specialized scientific problem-solving [29]. Furthermore, Scientific LLMs are capable of enhancing program synthesis processes through contextual representations derived from extensive code samples [20], showcasing their utility in computational science. This transformative potential highlights their capacity to redefine research methodologies and facilitate novel scientific breakthroughs.
### 4.2 General-Purpose LLMs in Scientific Contexts
General-purpose Large Language Models (LLMs), such as ChatGPT, GPT-3, GPT-4, LLaMA, BERT, Claude, and Gemini, represent a significant advancement in artificial intelligence, demonstrating broad capabilities in understanding, generating, and interacting with human language [4,14,19,21,28,30]. These models are optimized for dialogue, possess vast knowledge stores, and exhibit strong mathematical reasoning abilities, accurate context tracing in multi-turn conversations, and alignment with human values for safe usage [28,30]. More advanced versions, like GPT-4, extend these capabilities to multimodal inputs, processing text, audio, and images, and demonstrate enhanced strength in solving complex tasks [8,28,30].

Researchers and practitioners employ various strategies to leverage these general-purpose LLMs for scientific tasks, primarily through sophisticated prompting methods. Direct instruction, where specific commands guide the model's output, is a fundamental approach [21]. More advanced techniques include Chain-of-Thought (CoT) prompting, which enables LLMs to break down complex problems into intermediate reasoning steps, thereby improving performance in intricate reasoning tasks [23,28,30]. In-Context Learning (ICL) allows models to learn from provided examples without explicit gradient updates, while Retrieval-Augmented Generation (RAG) integrates external knowledge bases to provide LLMs with access to up-to-date or specialized information, significantly enhancing their accuracy and creativity [5,23,28]. Additionally, "Context Engineering" focuses on designing and manipulating the input context to elicit more effective responses for specific queries [29], and "tool manipulation" enables LLMs to interact with external tools like calculators or search engines to overcome inherent limitations in numerical computation or access real-time data [17,30].

These strategies have led to promising results in various scientific domains. In medical education and practice, general LLMs have shown remarkable aptitude. ChatGPT has reportedly passed several significant medical examinations, including the United States Medical Licensing Exam (USMLE), the Radiology Board-style Examination, the UK Neurology Specialty Certificate Examination, and the Plastic Surgery In-Service Exam [21]. Similarly, Med-PaLM, an LLM encoding clinical knowledge, achieved human-expert level performance in USMLE-style question-answering, enhancing clinical decision-making efficiency [26]. GPT-4 has also demonstrated "human-level performance in multiple academic exams" [15] and achieved high scores in educational assessments like BLS (96%) and ACLS (92.1%) [25]. Beyond examinations, GPT-3 has automated clinical documentation, generating discharge summaries and clinical notes, while CLARify (utilizing GPT-3) has generated executable experimental plans for chemical robots with improved accuracy [26]. GPT-4 further controls liquid handling robots for biological experiments directly from natural language instructions [5]. In hypothesis generation, LLMs like GPT-4, Llama-3, and Qwen-Max, guided by CoT, ICL, and RAG, can generate scientific hypotheses with relatively high scores in practicality and theoretical basis [23]. Other applications include sentiment analysis for financial prediction, drafting review articles in drug discovery, and generating knowledge for molecular property prediction in materials science [5,22]. Specialized models like OpenAI's o1 are even designed to excel in STEM fields, achieving 83% on the International Mathematics Olympiad, significantly outperforming GPT-4o's 13% [15].



**General-Purpose LLMs in Science: Capabilities vs. Shortcomings**

| Category           | Capabilities                                                | Shortcomings                                                  |
| :----------------- | :---------------------------------------------------------- | :------------------------------------------------------------ |
| **Broad Skills**   | NLU, Generation, Dialogue, Vast Knowledge, Reasoning (GPT-4, LLaMA) | Lack of Specificity & Precision in specialized domains       |
| **Leveraging Strategies** | CoT, ICL, RAG, Context Engineering, Tool Manipulation     | Struggles with Complex Reasoning (Multimodal-CoT outperforms GPT-3.5) |
| **Medical Domain** | Passed Medical Exams (USMLE, Radiology, BLS, ACLS, Med-PaLM) | Failed Chinese National Medical Exams (NMLE, NPLE, NNLE)     |
| **Scientific Tasks** | Hypothesis Generation (theoretical grounding), Clinical Documentation, Lab Automation | Poor Creativity, Feasibility, Data Support in Hypotheses (Likert Scale 2.27-3.29) |
| **Critical Issues** | (None listed as strength)                                   | Language/Cultural Variations, Numerical Computation, Hallucinations, Knowledge Timeliness, Bias, Lack of Interpretability |

Despite these successes, general-purpose LLMs also exhibit significant shortcomings when applied to scientific contexts. For instance, ChatGPT failed to pass the accuracy threshold of 0.6 in any of the three types of Chinese national medical examinations (NMLE, NPLE, NNLE) from 2017 to 2021, with its highest accuracy being 0.5897 in the NNLE [21]. Performance varied across question types and subjects, with higher accuracy in single-choice questions but struggles in areas like pathology, public health regulations, physiology, and anatomy [21]. In scientific hypothesis generation, while LLMs show strength in theoretical grounding, they perform less effectively in terms of creativity, feasibility, and data support, with ChatGPT scoring only 3.29 for creativity, 2.80 for feasibility, and 2.27 for data support on a 5-point Likert scale [23]. Furthermore, a Multimodal-CoT model with fewer than 1 billion parameters significantly outperformed GPT-3.5 on the ScienceQA benchmark (91.68% vs. 75.17%), highlighting that general models are not always superior to smaller, specialized architectures [32]. In highly specialized domains like materials science, general LLMs "currently fall short of being practical materials science tools" due to their limitations in fully comprehending complex and interconnected domain-specific knowledge [7].

Several factors contribute to these performance differences. A primary reason is the language and cultural variations inherent in examination content [21]. LLMs predominantly trained on English data may struggle with non-English medical exams due to language barriers and a lack of in-depth understanding of specific cultural or policy nuances, such as Chinese medical policies and legal regulations [21]. Additionally, general LLMs often exhibit limitations in numerical computation, affecting tasks like dosage calculation or laboratory value interpretation [21]. A critical challenge across all applications is "hallucinations," where LLMs generate factually incorrect but plausible content, a consequence of their probabilistic nature [20,28,30]. This issue, coupled with knowledge timeliness problems, necessitates thorough human review for factual accuracy and scientific rigor [5,28]. General LLMs also face risks such as bias in outputs from training data, lack of adaptability to diverse educational needs, and the potential for overreliance, which can undermine critical thinking [25]. The "lack of specificity and precision" of generic LLMs is a critical risk in highly specialized fields like medical education [20], suggesting that while general LLMs are capable, domain-adapted models can be more efficient and effective due to their specialized pre-training data [22]. The excitement surrounding general LLMs in science is thus tempered by concerns regarding their limitations and the need for careful management within scientific practices [34].
### 4.3 Taxonomy of Scientific LLM Modalities and Types
The burgeoning field of Scientific Large Language Models (Sci-LLMs) necessitates a structured taxonomy to categorize and understand their diverse capabilities, architectures, and applications across various scientific disciplines. This section establishes a framework for classifying Sci-LLMs based on the specific scientific data modalities they process and their domain focus, reflecting the increasing specialization of LLMs in scientific research [1,31]. While general-purpose LLMs offer broad language understanding, the inherent complexity and specialized "languages" of scientific data (e.g., chemical structures, genomic sequences, protein folds) demand bespoke models tailored to specific scientific applications [20,26].

We categorize Sci-LLMs into five primary types: Text-focused Scientific LLMs (Text-Sci-LLMs), Molecular LLMs (Mol-LLMs), Protein LLMs (Prot-LLMs), Genomic LLMs (Gene-LLMs), and Multimodal Scientific LLMs (MM-Sci-LLMs) [1]. This classification highlights the journey from foundational textual analysis to the integration of complex, multi-omics data, emphasizing their multimodal and multi-level nature in understanding biological systems, a concept often termed "Large Language of Life Models" (LLLMs) [8].

**Text-focused Scientific LLMs** are designed to interpret and generate scientific natural language, operating on specialized textual corpora like research papers, medical records, and climate reports [1,18,31]. Models such as BioGPT excel in biomedical text mining and generation, performing tasks like relation extraction and question answering with high accuracy in the biomedical domain [32]. Other examples include SciBERT and Galactica for general scientific literature, and domain-specific variants in mathematics (e.g., MathBERT), physics (e.g., AstroLLaMA), chemistry (e.g., ChemLLM), and environmental sciences (e.g., OceanGPT), showcasing their capability to process complex scientific descriptive language across diverse fields [31]. These models contribute significantly to tasks like analyzing medical records, generating hypotheses, and assisting in peer review workflows [19,26].

**Molecular LLMs** interpret and process chemical languages, primarily focusing on small molecule compounds, often encoded through representations like SMILES, SELFIES, and BigSMILES strings, or molecular graphs [1,18]. Models like LLM4SD are adept at molecular property prediction, synthesizing scientific literature rules with experimental data to generate interpretable feature vectors [3,22]. Chemformer demonstrates strong performance in chemical reaction and synthesis prediction, while tools like ChemCrow integrate LLMs with specialized chemistry tools for autonomous problem-solving in drug discovery and materials design [5,26]. These models represent a crucial bridge between computational chemistry and automated experimental platforms.

**Protein LLMs** are dedicated to understanding, manipulating, and engineering protein sequences, which are fundamental to drug design and disease mechanism elucidation [1,26]. They leverage Transformer architectures on extensive protein sequence datasets, with prominent examples including the AlphaFold series and the Evolutionary Scale Modeling (ESM) family [8,20]. AlphaFold 2 revolutionized protein structure prediction, and AlphaFold 3 extends this to accurately predict 3D structures of complexes involving proteins, DNA, RNA, and small molecules, achieving experimental-level accuracy [8]. Models like ProGen and ProtGPT2 further enable *de novo* protein sequence design with predefined functionalities, while EVOLVEpro supports AI-assisted protein engineering [8,20].

**Genomic LLMs** are at the forefront of computational biology, analyzing and interpreting DNA and RNA sequences to decipher the genetic "language" [1]. Models such as DNA-BERT are trained on vast nucleotide sequences for genetic variation analysis, while Geneformer maps single-cell transcriptomic data to gene sequences for gene network analysis [26]. The Nucleotide Transformer serves as a foundational model for precise predictions of molecular phenotypes [20]. Specialized models like RhoFold and RhoDesign predict RNA conformations and design RNA aptamers, respectively, and AlphaGenome predicts the impact of single variants or mutations in DNA, highlighting their critical role in understanding gene function, splicing, and binding sites [8,29].

Finally, **Multimodal Scientific LLMs** represent a critical evolution, integrating diverse data types—including text, visual elements, symbolic representations, structured data, and time-series information—to achieve a more comprehensive understanding and prediction capabilities [18]. This approach addresses the limitations of text-only models in scientific domains that rely heavily on images, signals, and other non-textual data [23]. Integration strategies include Text+Graph (e.g., OAG-BERT for scientific papers and metadata), Text+Vision (e.g., PaperMage for scientific documents, LLaVA-Med for medical images), and emerging Audio-Language models [29,31]. Frameworks like Multimodal-CoT integrate vision and language for Chain-of-Thought reasoning, significantly enhancing scientific problem-solving accuracy [32]. AlphaFold 3 is also a prime example of multimodal integration, predicting structures of complexes across various biomolecules [8]. These models are crucial for tasks ranging from enhanced decision-making in agriculture to supporting complex scientific hypothesis generation and automated experimentation [5,11].

The development and deployment of these specialized Sci-LLMs underscore a fundamental shift towards more tailored AI solutions in science. While each modality addresses specific data types and scientific problems, a common challenge across all categories involves the creation and utilization of high-quality, multimodal datasets and the establishment of robust, standardized evaluation metrics to assess their performance and reliability [7,20]. The ongoing evolution of these models promises to accelerate scientific discovery by enabling more precise analysis, prediction, and generation across the vast landscape of scientific information.
#### 4.3.1 Text-focused Scientific LLMs
Text-focused Scientific Large Language Models (Text-Sci-LLMs) are specifically designed and trained on specialized textual corpora pertinent to scientific domains, such as comprehensive scientific literature [1,18]. These models excel at understanding and generating scientific natural language, processing complex scientific descriptive language, and assisting in tasks like analyzing medical records or research studies [4,26]. While general-purpose LLMs are trained on diverse text, their application to scientific documents often positions them as Text-Sci-LLMs due to their ability to comprehend and generate intricate scientific narratives [26,28].

The architectural nuances and pre-training datasets for Text-Sci-LLMs vary significantly across scientific disciplines.
In **general science**, models often leverage research papers from bibliographic databases such as AMiner, Microsoft Academic Graph (MAG), and Semantic Scholar, with some datasets like S2ORC providing full-text access [31]. Early models, like SciBERT, utilized a BERT backbone with Masked Language Model (MLM) objectives, while GPT-based models such as SciGPT employed next-token prediction. Recent advancements include instruction-tuned LLMs like Galactica and SciGLM, which are designed for complex problem-solving based on instructions, sometimes augmented by human experts or other LLMs [31]. General LLMs like Falcon also contribute to scientific text processing by incorporating extensive scientific literature from sources such as arXiv and Wikipedia during their pre-training [3]. Furthermore, models like TildeOpen LLM support diverse textual data, including specialized scientific texts in various European languages, while OpenReasoning-Nemotron focuses on complex reasoning tasks across mathematics, science, and code, highlighting a textual emphasis on scientific reasoning [29]. Code-focused models like Codex, fine-tuned on GitHub code, and code-based GPT models like GPT-3.5, also enhance reasoning abilities critical for scientific and engineering disciplines [30].

In the **biomedical domain**, models like BioGPT, BioBERT, and PubMedBERT are prominent examples, specifically trained on extensive biomedical literature corpora [18,20,32]. BioGPT, a generative Transformer model, is particularly noted for its robust performance in biomedical text generation, mining, relation extraction, and question answering [32]. Datasets include PubMed titles/abstracts, PMC full-text, Electronic Health Records (MIMIC-III, MIMIC-IV), knowledge bases (UMLS), and even health-related social media posts. Biomedical models encompass encoder-based architectures (e.g., Bio-ELECTRA, BioRoBERTa) and encoder-decoder models (e.g., SciFive, BioBART), with larger instruction-tuned models such as Med-PaLM and BioMistral representing current frontiers [31]. Med-PaLM assists in clinical decision support, and GPT-3 has been used for generating clinical documentation [3,26].

Specific sub-domains also showcase tailored approaches. For **mathematics**, models like GenBERT and MathBERT use BERT-based architectures, while GSM8K-GPT and NaturalProver are GPT-based. Later models such as Rho-Math and MAmmoTH2 are built upon LLaMA and are instruction-tuned, leveraging question-answering datasets like MathQA, GSM8K, and MATH [31]. In **physics**, astroBERT uses MLM and next-sentence prediction on astronomy papers, and AstroLLaMA fine-tunes LLaMA-2 using arXiv abstracts, with AstroLLaMA-chat being trained on GPT-4 generated domain-specific dialogue [31]. **Chemistry and Materials Science** models like ChemBERT and MatSciBERT are early encoder-only variants, while more recent large-scale decoder-only models such as ChemDFM and ChemLLM utilize research papers, the Materials Project database, and instruction-tuning datasets like Mol-Instructions [31]. In **Geography, Geology, and Environmental Science**, early models like ClimateBERT used Transformer encoders, while recent work features decoder-style autoregressive LLMs such as OceanGPT, fine-tuned with human-curated or LLM-augmented instructions on datasets including Earth science papers and knowledge bases [31]. For **agriculture**, LLMs are adapted to process text-based agricultural information, enabling intelligent Q&A systems for knowledge delivery [11].

Evaluation frameworks for Text-Sci-LLMs include specialized metrics to assess their understanding and generation capabilities. A notable framework is KnowEval, which evaluates models across foundational knowledge, specialized scientific knowledge, and the generation of innovative knowledge [1]. Quantitative metrics are frequently employed; for example, BioGPT's performance in biomedical NLP tasks is measured by F1 scores (e.g., 44.98% for BC5CDR relation extraction, 40.76% for DDI relation extraction) and accuracy (e.g., 78.2% for PubMedQA, with BioGPT-Large achieving 81.0%) [32]. General LLMs like ChatGPT have been evaluated on scientific textual data, such as medical exam questions from Chinese National Medical Licensing Examination, using accuracy metrics with a pass threshold of 0.6 [21]. In mathematics, evaluation involves QA and Mathematical Word Problem (MWP) solving benchmarks like GSM8K and MATH, alongside quantitative reasoning assessments using MMLU-STEM and Big-Bench Hard [31]. Perplexity metrics are also used, such as with N-grams and GPT-2, for analyzing linguistic coherence in scientific contexts [32].

The contributions of Text-Sci-LLMs to scientific text processing are diverse. Beyond foundational tasks, they facilitate various scientific research activities. Models like GeneGPT interact with NCBI Web APIs to enhance biomedical information retrieval by processing natural language queries [5]. The SPIRES tool, utilizing GPT3+ models, recursively extracts structured information from medical texts in a zero-shot manner, transforming complex data into structured tables [5]. GPT-based approaches are also applied in scientometric analysis for accurately identifying AI-related articles, achieving 90% accuracy and 94% recall [5]. Furthermore, Text-Sci-LLMs are instrumental in generating scientific content, such as citation text (e.g., AutoCite), related work sections (e.g., LitLLM), and even drafting entire research papers (e.g., PaperRobot, CoAuthor) [19]. The HILMA framework integrates models like Qwen-Max, Llama-3, and GPT-3.5 to process scientific literature and generate text-based scientific hypotheses, augmented by real-time knowledge retrieval from citation networks [23]. ChatGPT, a general-purpose LLM, has also been employed to draft review articles in drug discovery, where human experts then refine the content to meet scientific standards [5].
#### 4.3.2 Molecular LLMs
Molecular Large Language Models (Mol-LLMs) represent a specialized category of LLMs designed to interpret and process chemical languages, particularly focusing on small molecule compounds [1]. This domain necessitates adept handling of chemical information, often encoded through various molecular representations. Key linear representations include SMILES (Simplified Molecular Input Line Entry System), BigSMILES, and SELFIES (SELF-referencing Embedded Strings) [18]. While SMILES is widely adopted, its more fragile syntax can lead to invalid molecular strings. In contrast, SELFIES offers stricter grammar rules, ensuring the generation of 100% valid molecular strings and thereby enhancing robustness in molecular generation tasks [18]. BigSMILES further extends the capabilities of SMILES to represent more complex structures, such as polymers [18]. Beyond linear strings, molecules are also commonly represented as graphs, with 3D molecular encoders capable of tokenizing these structures [31].

Mol-LLM architectures exhibit diverse approaches to integrating chemical 'language' with large language models. One prominent category involves multi-modal models, such as Text+Graph and Text+Vision architectures, recognizing that molecular structures are inherently graphical [31]. In Text+Graph models, Graph Neural Networks (GNNs) frequently serve as graph encoders, while conventional LLMs like SciBERT function as text encoders. These components are then connected through techniques like contrastive learning, exemplified by Text2Mol for text-to-molecule retrieval [31]. Another architectural strategy involves LLMs encoding both text and graph information, either by linearizing graphs into SMILES strings or by projecting graph embeddings onto virtual tokens. An example is 3D-MoLM, which employs a 3D molecular encoder to represent molecules as tokens for LLaMA-2 input, facilitating tasks like molecule-to-text retrieval and molecular captioning [31]. Specific models like MoLFormer integrate relative position embedding into a high-capacity molecular SMILES transformer, thereby incorporating spatial information into molecular data processing [20].

The development and evaluation of Mol-LLMs rely on specialized datasets and benchmarks. Popular datasets for molecular graphs include ChEBI-20, ZINC, and PCDes [31]. For tasks involving molecular property prediction, the MoleculeNet dataset is a crucial benchmark, with models like LLM4SD achieving state-of-the-art performance across 58 tasks covering classification and regression in physiology, biophysics, physical chemistry, and quantum mechanics [3,22]. Key evaluation tasks for Mol-LLMs encompass molecular property prediction, interaction prediction, reaction prediction, and molecular generation, with specific critical evaluation metrics highlighted for the latter [1].

Significant progress has been made in leveraging Mol-LLMs for various scientific discovery tasks. In **molecular property prediction**, LLM4SD stands out by interpreting SMILES strings, synthesizing known rules from scientific literature (e.g., "molecular weight < 500 mol more likely to cross blood-brain barrier"), and inferring new rules from molecular data. It transforms these rules into executable RDKit functions to generate interpretable feature vectors, such as $$x_1 = I(\text{rdMolDescriptors.CalcExactMolWt(mol)}<500)$$, outperforming existing GNN models [3,22]. Similarly, Molformer excels at large-scale prediction of molecular properties, aiding in the screening of compounds for favorable pharmacokinetic characteristics like ADMET (Absorption, Distribution, Metabolism, Excretion, Toxicity) with higher accuracy than traditional machine learning methods [26].

For **chemical reaction and synthesis**, Chemformer demonstrates strong performance, sometimes exceeding human chemist accuracy in prediction tasks after pre-training and fine-tuning [26]. Tools like ChemCrow enhance LLM capabilities by integrating 13 expert-designed chemistry tools, enabling emergent problem-solving in organic synthesis, drug discovery, and materials design, and autonomously planning and executing complex chemical syntheses [5,19]. Coscientist further integrates LLMs with laboratory automation for reaction optimization [19].

In **molecular generation and design**, SyntheMol is a generative model used for designing and validating novel antibiotics, capable of screening approximately 30 billion compounds [8]. LLMs are also employed in evolutionary search strategies to explore chemical spaces and identify candidate molecules [19], while ChatDrug and DrugAssist utilize LLMs to facilitate drug editing and iteratively optimize molecular structures [19].

Beyond small molecules, Mol-LLMs also extend to **biomolecular interactions and structure prediction**. AlphaFold 3, for instance, predicts 3D structures of complexes involving proteins, DNA, RNA, small molecules, and ligands with experimental-level accuracy, achieving 80% accuracy for protein-ligand complexes within 2 Å of experimental error [8]. Boltz-1 offers comparable accuracy for 3D biomolecular interaction prediction and is open-source, while MassiveFold is an optimized version of AlphaFold for large-scale parallel computation [8]. PocketGen focuses on defining the atomic structure of protein-ligand interactions [8].

Furthermore, Mol-LLMs are increasingly integrated with **laboratory automation**. CLARify leverages LLMs (e.g., GPT-3) to generate executable experimental plans for chemical robots, significantly improving the efficiency and accuracy of automated chemical experiments [26].

Despite these advancements, several challenges persist in the Mol-LLM domain. The critical analysis of existing literature indicates a need for more detailed reporting on diverse model architectures beyond general categories like "transformer model," as well as comprehensive descriptions of datasets utilized for pre-training and benchmarking, and standardized evaluation metrics for various tasks including property prediction, interaction prediction, reaction prediction, and molecular generation [20]. Ensuring the robust generation of valid molecular structures, as addressed by SELFIES, remains crucial for broader applicability. Moreover, while significant strides have been made in bridging computational and experimental chemistry, further integration and enhanced reliability in autonomous experimentation are ongoing areas of research and development. Addressing these challenges is paramount for the continued progression and wider adoption of Mol-LLMs in scientific discovery.
#### 4.3.3 Protein LLMs
Protein Large Language Models (Prot-LLMs) represent a significant frontier in protein research, dedicated to understanding, manipulating, and engineering protein sequences with profound implications for drug design and disease mechanism elucidation [1,26]. These models aim to decode the "language of life" encoded in amino acid sequences, leveraging advancements in neural network architectures, particularly Transformers.

The landscape of Prot-LLMs can be broadly categorized by their architectural underpinnings and the specific applications they address. Many prominent Prot-LLMs, such as the Evolutionary Scale Modeling (ESM) family including ESM-1b, ESM-2, and ESMFold, are built upon Transformer architectures [20,31]. These models have demonstrated superior capabilities in generating accurate structure predictions based on protein sequences, surpassing earlier single-sequence protein language models [20]. ESM models are designed to encode protein sequences effectively, capturing crucial structural properties for downstream prediction tasks, thereby reducing the necessity for labor-intensive experimental procedures [19]. They are also utilized for analyzing protein sequences to predict protein folding, identify binding sites, and provide functional annotations [26].

Beyond sequence analysis, Prot-LLMs excel in various protein research applications:
*   **Protein Structure Prediction**: The AlphaFold series, notably AlphaFold 2 and AlphaFold 3, has revolutionized structural biology. AlphaFold 2 famously addressed the long-standing protein folding problem by accurately predicting the 3D structures of over 200 million proteins solely from their amino acid sequences [8]. AlphaFold 3 extends this capability to predict the 3D structures of complexes involving proteins with DNA, RNA, small molecules, and ligands, achieving approximately 80% accuracy for protein-ligand complexes within 2 Å of experimental error [8]. Similarly, Boltz-1 achieves comparable accuracy in predicting 3D biomolecular interactions [8]. These high-accuracy structure predictions are critical for novel drug design and molecular dynamics simulations, often outperforming traditional methods in protein-protein interaction prediction [26].
*   **Protein Sequence Design**: Models like ProtGPT2 are trained specifically on protein domains to generate *de novo* protein sequences that adhere to natural principles [20]. ProGen, another deep learning model, produces protein sequences with predefined functionalities; for instance, synthetic proteins optimized for specific lysozyme families have exhibited catalytic efficiencies comparable to natural counterparts despite low sequence identity [20]. Fine-tuning LLMs on specific protein families has enabled the generation of new, highly divergent yet potentially functional protein sequences, and an antibody generation LLM has been introduced for *de novo* design of SARS-CoV-2 antibodies, achieving both specificity and diversity [19,31]. AbMAP (Antibody Mutagenesis-Augmented Processing) has successfully designed antibodies with over 20 times higher binding affinity against SARS-CoV-2 [8].
*   **Protein Engineering and Interaction Analysis**: EVOLVEpro is a protein language model dedicated to AI-assisted protein engineering [8]. PIONEER contributes to understanding protein-protein interaction mechanisms crucial for health and disease [8]. PocketGen is a deep generative model that defines the atomic structures of protein-ligand interactions [8]. Evo predicts the effects of protein variants, aiding in understanding mutational impacts [8].
*   **Biological Information Retrieval**: GeneGPT offers an innovative application by retrieving information about protein structures from NCBI databases via Web APIs, allowing researchers to query biological knowledge using natural language [5].

The training of these models primarily relies on extensive datasets of protein sequences, often provided in formats such as FASTA [31]. While the digests frequently mention "protein sequences" and "protein families" as common inputs, detailed descriptions of specific large-scale training datasets beyond these general mentions are less prevalent.

Evaluation criteria for Prot-LLMs are diverse, reflecting their varied applications. For structural prediction tasks, accuracy is a key metric, as exemplified by AlphaFold 3's 80% accuracy for protein-ligand complexes within 2 Å of experimental error [8]. For sequence design, functional metrics like catalytic efficiency (demonstrated by ProGen) and binding affinity (achieved by AbMAP) are crucial [8,20]. Other evaluation criteria include the specificity and diversity of generated sequences, as shown in antibody design models [19]. A significant indicator of progress is the ability of current models to surpass earlier benchmarks and traditional methods, for instance, in protein-protein interaction prediction [20,26].

The current state of Prot-LLMs demonstrates a rapid advancement in understanding and manipulating proteins, offering unprecedented capabilities in structure prediction, functional annotation, and *de novo* design. However, future developments will benefit from more explicit detailing of specific training datasets, further architectural innovations, and standardized, quantifiable performance metrics across all tasks to provide a clearer picture of comparative strengths and weaknesses [20]. The continuous refinement of evaluation methodologies will provide crucial technical support for the sustained evolution of these models [1].
#### 4.3.4 Genomic LLMs
Genomic Large Language Models (Gene-LLMs) represent a significant advancement in computational biology, leveraging LLM architectures to analyze and interpret DNA and RNA sequences [1]. These models are pivotal for deciphering the genetic "language" and are applied across a spectrum of critical biological tasks, including understanding gene function, predicting chromosomal spectra, splicing sites, binding sites, and structures, as well as tasks involving sequence generation and variation analysis [1,8,26].

The techniques employed by Gene-LLMs primarily involve training on vast biological sequence datasets to learn underlying patterns and relationships. Foundational models like **DNA-BERT** are specifically trained on large volumes of nucleotide sequences to decode genetic information and perform genetic variation analysis [26]. Similarly, the **Nucleotide Transformer** functions as a pre-trained foundational model, learning from diverse biological sequences to enable precise predictions of molecular phenotypes [20]. **Evo** further exemplifies this approach, trained on an extensive collection of 2.7 million diverse phage and prokaryotic genomes, demonstrating capabilities in predicting DNA and RNA variant effects and generating novel DNA sequences [8].

Beyond these foundational models, several specialized Gene-LLMs address distinct computational biology challenges:
*   **GeneGPT** is engineered to integrate with NCBI Web APIs, empowering LLMs to efficiently access and utilize genomic data, such as gene sequences, for various biological studies, including querying gene variations and their associated diseases [5].
*   **Geneformer** is tailored for mapping single-cell transcriptomic data to gene sequences, facilitating gene network analysis and the identification of potential therapeutic targets. It has demonstrated success in analyzing network perturbations relevant to cardiac hypertrophy and dilated cardiomyopathy [26].
*   For RNA analysis, **RhoFold** predicts 3D RNA conformations directly from nucleic acid sequences, while **RhoDesign** specializes in designing RNA aptamers [8]. The **General Expression Transformer (GET)** accurately predicts RNA transcription patterns across over 5000 human cell types [8].
*   In the domain of epigenetics, **MethylGPT** and **CpGPT** (both in preprint) are dedicated to tasks such as biological age estimation [8].
*   For detailed variant impact prediction, general DNA language models evaluate the functional effects of variants in both human genomic coding and non-coding regions, having analyzed approximately 9 billion potential single-nucleotide variant sites [8]. **AlphaGenome** from Google DeepMind specifically predicts the impact of single variants or mutations in DNA, directly contributing to genomic sequence analysis [29].
*   Moreover, general LLMs can be fine-tuned on specific protein families to generate highly divergent yet potentially functional new sequences, and predict viral escape mutations [31].

The effectiveness of these models is demonstrated across a diverse array of computational biology tasks. For instance, DNA-BERT excels in genetic variation analysis [26], while Geneformer proves effective in complex gene network analysis and therapeutic target identification [26]. RhoFold and RhoDesign exhibit high utility in structural and functional RNA biology [8], contrasting with GET's strength in gene expression prediction [8]. The precise predictions of molecular phenotypes by the Nucleotide Transformer [20] and the comprehensive variant impact analysis by AlphaGenome [29] underscore their broad applicability.

Common datasets defining success in this domain include raw biological sequences in FASTA format for DNA/RNA [31], large volumes of nucleotide sequences for models like DNA-BERT [26], single-cell transcriptomic data for Geneformer [26], and extensive genomic datasets encompassing coding and non-coding regions for variant analysis [8]. For models like Evo, millions of diverse phage and prokaryotic genomes form the training base [8]. While the digests emphasize the deployment of sophisticated models and their successful application, specific quantitative evaluation metrics (e.g., accuracy, F1-score, AUC) are not detailed across all papers, with the success often described in terms of the model's capability to perform complex tasks or analyze large-scale biological data with high precision [1,20].
#### 4.3.5 Multimodal Scientific LLMs
The development of Multimodal Scientific Large Language Models (MM-Sci-LLMs) is motivated by the inherently multimodal nature of scientific data, which frequently encompasses text, visual elements, symbolic representations, structured data, and time-series information [18]. Traditional text-only models often prove insufficient for disciplines that necessitate extensive processing of images, signals, and audio/video, thereby limiting the scope of scientific inquiry and discovery [23]. MM-Sci-LLMs are designed to overcome these limitations by integrating diverse data types, enabling richer interactions and more precise scientific insights, such as enhanced decision-making in agricultural contexts [2,11]. The ultimate goal is to achieve a more comprehensive understanding and prediction capabilities by preserving domain invariance and facilitating cross-modal reasoning within scientific contexts [18,31].

MM-Sci-LLMs employ various approaches to combine different scientific data types [1]. These models integrate information from modalities such as text, molecules, proteins, and genes [1]. For instance, frameworks like Multimodal-CoT integrate vision and language features to perform Chain-of-Thought reasoning, incorporating visual features at both rationale generation and answer inference stages [32]. Similarly, the LLM4SD framework synthesizes knowledge from textual scientific literature with experimental data, such as SMILES strings and associated property values, to create interpretable feature vectors for molecular property prediction [3]. Furthermore, advanced biological models like AlphaFold 3 are examples of multimodal integration, predicting structures of complexes formed by DNA, RNA, proteins, small molecules, and ligands [8]. The Stanford Virtual Lab extends this by integrating multiple AI models (e.g., AlphaFold-Multimer, Rosetta, ESM) for automated scientific discovery, culminating in visions like "AI Virtual Cells" that simulate dynamic behaviors across various biological levels [8].



**Multimodal Scientific LLM Integration Strategies**

| Integration Strategy    | Description                                                 | Examples / Key Models                           | Scientific Applications                                 |
| :---------------------- | :---------------------------------------------------------- | :---------------------------------------------- | :------------------------------------------------------ |
| **Text+Graph**          | Processes textual data and graph structures (metadata, knowledge graphs) | OAG-BERT, SPECTER, ERNIE-GeoL, PK-Chat          | Scientific paper analysis, knowledge graph construction, dialogue systems |
| **Text+Vision**         | Combines text with visual elements (images, diagrams, figures) | PaperMage, LLaVA-Med, G-LLaVA, SurgicalGPT, UrbanCLIP | Document understanding, mathematics, chemistry (molecular images), biomedicine (medical images), earth sciences |
| **Audio-Language**      | Integrates audio data with natural language                 | AU-Harness, Audio Flamingo 3, Xiaomi MiMo-Audio | Scientific retrieval, signal/audio processing         |
| **Vision-Language-Action (VLA)** | Links visual/language inputs with robotic actions   | SmolVLA                                         | Robotics, autonomous experimental control               |
| **General Multimodal**  | Comprehensive processing of text, images, audio, video      | GPT-4, Gemini, GPT-4o, AlphaFold 3              | Broad scientific tasks, complex biological structure prediction, automated experimentation |

Integration strategies in MM-Sci-LLMs can be broadly categorized based on the modalities combined [31].
*   **Text+Graph Integration**: Scientific papers are often linked to rich metadata (e.g., location, authors, references), forming complex graphs. Models such as OAG-BERT process both paper text and metadata using Masked Language Modeling (MLM), while SPECTER leverages citation links as supervision to enhance the encoding of linked papers [31]. Architectural modifications include the use of adapters, Graph Neural Network (GNN)-nested Transformers, and Mixture of Experts Transformers to effectively capture graph signals. In Earth Sciences, ERNIE-GeoL utilizes a Transformer-based aggregation layer for deep fusion of text and Point of Interest (POI) information, and PK-Chat integrates LLMs with knowledge graphs [31].
*   **Text+Vision Integration**: This is a prevalent strategy given the ubiquity of visual data in science.
    *   **General Document Understanding**: Models like Granite-Docling-258M and Nemotron Nano VL specialize in document understanding and layout-faithful extraction, while PaperMage integrates NLP and computer vision to process visually rich scientific documents, extracting logical structures, graphics, and text content across multiple modalities [19,29].
    *   **Mathematics**: Geometry LLMs handle both textual problems and diagrams. Approaches include converting images to relational structures for models like BART (e.g., Inter-GPS) or using pre-trained Visual Transformers (ViT) to encode visual input, concatenating these visual embeddings with text embeddings for LLMs like LLaMA-2 (e.g., G-LLaVA) [31]. Auxiliary losses such as MASK image modeling and image-text matching are often employed during training [31].
    *   **Chemistry**: Molecular images complement textual and graph data. Research often applies concepts similar to BLIP-2, representing images as tokens for LLMs. GIT-Mol exemplifies this by projecting all modalities (graph and image) into a latent text space and utilizing T5 for encoding and decoding [31].
    *   **Biomedicine and Medical Domains**: This area heavily relies on text-image pairs from medical reports (e.g., MIMIC-CXR, pathological reports) and figures with captions from biomedical papers (e.g., ROCO, MedICaT) [31]. Many biomedical visual language models adopt the CLIP architecture, jointly training text encoders (e.g., BERT, GPT-2, LLaMA) and image encoders (e.g., ResNet, ViT, Swin Transformer) through contrastive learning. Models like LLaVA-Med encode images as visual tokens for LLM input, and MedSigLIP specializes in multimodal medical reasoning [29,31]. SurgicalGPT, an end-to-end language-vision GPT, extends GPT2 with a visual feature extractor and visual token embeddings for visual question answering in surgery [5].
    *   **Earth Sciences**: UrbanCLIP uses a CLIP architecture to jointly process language and visual data for urban studies [31].

Beyond these common combinations, other modalities are also being integrated. **Audio-Language models** are emerging, with toolkits like AU-Harness and advanced models like Audio Flamingo 3 and Xiaomi MiMo-Audio demonstrating progress in audio general intelligence and speech processing [29]. This direction holds promise for scientific retrieval, particularly in disciplines requiring signal and audio processing [23]. **Vision-Language-Action (VLA) models** like SmolVLA are compact VLA models developed for robotics [29]. Furthermore, agent systems for autonomous scientific experiments can process textual prompts and interact with physical experimental setups (e.g., controlling multi-instrument systems), integrating textual instructions with real-world actions [5]. ChemCrow and Coscientist demonstrate another form of multimodal interaction by augmenting LLMs with specialized chemistry tools and laboratory automation, connecting language models with computational and experimental tools [5,19]. More generally, prominent models like GPT-4, Gemini, and GPT-4o exhibit general multimodal capabilities, processing and generating text, images, audio, and video, which can be foundational for scientific applications [4,15,30].

These integrated models leverage the richness of multimodal scientific data to achieve comprehensive understanding and prediction capabilities. By effectively bridging different types of scientific information, models can generate interpretable feature vectors, leading to more robust predictions, as seen in molecular property prediction with LLM4SD [3]. The integration of multiple AI models, as demonstrated by the Stanford Virtual Lab, facilitates automated scientific discovery. In practical applications, Multimodal-CoT achieved 91.68% accuracy on the ScienceQA benchmark, surpassing GPT-3.5's 75.17% and even human performance, showcasing its capability for complex scientific problem-solving [32]. Similarly, SurgicalGPT has achieved superior quantitative performance on public surgical VQA datasets, indicating enhanced diagnostic and decision-support capabilities [5].

However, the development of MM-Sci-LLMs introduces complexities in creating and utilizing multimodal datasets. There is a critical need for high-quality, multimodal datasets sourced from scientific literature, especially given the inherent challenges in extracting diverse information from scientific texts [7]. Establishing intermodal connections necessitates novel multimodal datasets, and data quality—including image and text data, as well as annotation accuracy—is a crucial development consideration [20,28]. For domain-specific applications, such as agriculture, multimodal data processing presents a core challenge for effective domain adaptation [11].

Assessing these integrated models requires specialized evaluation methodologies. Performance is typically evaluated from two perspectives: perception, which involves understanding and classifying image content, and cognition, which focuses on image reasoning and question-answering capabilities [28]. Evaluation paradigms encompass both closed-end tasks, requiring exact match answers, and open-end tasks, which assess the quality and reasonableness of generated text [28]. Benchmarks like LVLM-eHub, Reform-Eval, and MME are commonly used for general MLLMs, while domain-specific benchmarks such as ScienceQA are critical for evaluating scientific reasoning in multimodal contexts [28,32]. Current research hotspots emphasize architectural designs that enable effective multimodal interaction, diverse datasets for training and evaluation, and specific focal points for assessing model capabilities [1]. Key development points also include balancing the performance of language models and visual encoders, alongside ensuring safety alignment concerning accuracy, safety, and human values of generated content [28].
## 5. The Scientific Data Ecosystem: Challenges, Resources, and Co-evolutionary Development
The landscape of Scientific Large Language Models (Sci-LLMs) is fundamentally shaped by a dynamic and complex scientific data ecosystem, characterized by a co-evolutionary relationship between models and their underlying data foundations, inherent challenges in data acquisition and utilization, and the specialized resources and annotation strategies developed to address these issues [18]. This ecosystem distinguishes itself significantly from the general LLM paradigm, where the sheer volume and diversity of data are often paramount; for Sci-LLMs, the focus shifts to domain specificity, quality, and the intricate nature of scientific information [1].

At the heart of this ecosystem is the **co-evolution of Sci-LLMs and their data foundations**. This relationship is deeply reciprocal: the exponential growth in LLM parameters necessitates a parallel increase in the volume, quality, and diversity of training data, especially specialized domain knowledge for scientific applications [20,30]. Examples such as MatSci-LLMs demonstrate this, with their performance being contingent upon "building high-quality, multimodal datasets sourced from scientific literature" [7]. Conversely, Sci-LLMs are not merely passive consumers; they actively shape and contribute to scientific data. Models like LLM4SD exemplify this by synthesizing decades of scientific literature and analyzing laboratory data to discover new patterns, facilitating new scientific insights and potentially generating new data strategies [3]. This continuous feedback loop, where advanced Sci-LLMs demand and inspire better data, which in turn fuels further model development, drives scientific discovery across disciplines [31].

Despite this synergistic development, Sci-LLMs operate within a data landscape fraught with **significant challenges**. These challenges are often amplified compared to general LLMs due to the intrinsic properties of scientific information, which include profound heterogeneity, multi-scale phenomena, and inherent uncertainty [18]. Key issues span several categories:
1.  **Intrinsic Properties of Scientific Data**: Scientific data is inherently complex, originating from diverse instruments and experiments spanning vast scales. This includes structured tabular data in mathematics, diverse corpora in biology and medicine, and highly interconnected knowledge in materials science [7,31]. Experimental variability also introduces inherent uncertainty, complicating the establishment of reliable ground truth [19].
2.  **Scarcity, Quality, and Outdatedness**: Despite increasing data volumes, there is a pervasive scarcity of high-quality, domain-specific training data, particularly for fine-tuning, necessitating labor-intensive preparation [1,19]. The rapid evolution of scientific knowledge also leads to "data lag," where static training datasets quickly become outdated, hindering the generation of innovative hypotheses [18,23].
3.  **Multimodality and Complexity Integration**: Integrating diverse data modalities (text, graphs, images, chemical structures) remains a substantial hurdle, often due to a lack of integrated datasets [1,20]. Furthermore, current LLMs face context length limitations, restricting their ability to process extensive scientific texts or complex biological sequences [22,26].
4.  **Trustworthiness and Integrity**: Data-related issues compromise the reliability of Sci-LLM outputs. Biases embedded in training data can lead to inaccurate or unfair responses, and the pervasive problem of "hallucinations" results in confidently presented yet incorrect information, significantly impacting scientific discovery and advice [26,29].
5.  **Accessibility, Transparency, and Privacy**: Many scientific datasets are not "AI-ready," contributing to a "data provenance crisis" where tracing data origins is difficult [18]. The lack of transparency around industry-trained models and significant privacy concerns, especially with sensitive biomedical data, further constrain development and deployment [26,30]. These challenges are highly interdependent, with issues like data scarcity exacerbating problems like hallucinations and the lack of domain-specific understanding. Addressing these requires novel methods for building large-scale, multimodal datasets, improving quantitative analysis, ensuring dynamic knowledge updating, and establishing robust mechanisms for factual accuracy and data privacy [7,11].

To navigate these challenges, the Sci-LLM community relies on diverse and specialized **scientific data resources and rigorous annotation practices**. While general LLMs leverage vast, multi-petabyte corpora from web data, books, and code [28,30], Sci-LLMs necessitate domain-specific datasets. Over 270 pre-training and post-training datasets specifically tailored for Sci-LLMs have been identified, covering domains such as biomedical literature (e.g., BioGPT, NCBI databases, EHRs), chemistry and materials science (e.g., Materials Project, MoleculeNet), mathematics and code (e.g., MathQA, GitHub), and general scientific literature (e.g., AMiner, ArXiv) [18,32].

The annotation of scientific data is particularly critical and often demands meticulous, expert validation, distinguishing it from general unsupervised pre-training [18,21]. Emerging solutions combine expert oversight with semi-automated pipelines to manage the scale and specificity of scientific data. Examples include zero-shot learning for structured information extraction (SPIRES) and LLM-assisted knowledge synthesis frameworks like HILMA, which uses sophisticated graph representations for citation networks:
$$
G_c = ({V_{\text{cite}}} \cup {V_{\text{cited}}} \cup {V_{\text{related}}},{E_{\text{cite}}} \cup {E_{\text{cited}}} \cup {E_{\text{related}}})
$$
where $V$ represents nodes (documents) and $E$ represents edges indicating citation or relatedness relationships [23]. The development of toolkits like AI Sheets and automated pipelines for specialized data further underscores the field's efforts to enhance data curation and annotation efficiency [29]. Ultimately, the effective development of Sci-LLMs hinges on a continuous and integrated effort to build, curate, and responsibly utilize a rich, high-quality, and increasingly "AI-ready" scientific data ecosystem.
### 5.1 Co-evolution of Scientific LLMs and Data Foundations
The development of Scientific Large Language Models (Sci-LLMs) is intrinsically linked to and profoundly shaped by their data foundations through a dynamic co-evolutionary process [18]. 

![Co-evolution of Sci-LLMs and Data Foundations](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/1iMiHK-7pkMAud4CyhN7E_Co-evolution%20of%20Sci-LLMs%20and%20Data%20Foundations.png)

This intricate relationship highlights that advancements in one aspect—be it model architecture or data characteristics—drive innovations in the other, and vice versa [18]. This subsection synthesizes how this co-evolutionary dynamic influences the capabilities and limitations of Sci-LLMs, comparing and contrasting perspectives from various studies.

The foundational premise is that the exponential growth in LLM parameters necessitates a parallel increase in the volume, quality, and diversity of training data [20,25,30]. Early LLM advancements were predominantly credited to the "exponential augmentation in extensive datasets," emphasizing data as a primary input [20]. For general LLMs, this involved compiling vast textual data from diverse sources like Wikipedia, newspapers, and social media [25]. However, for Sci-LLMs, this dependency becomes more specialized. High-quality and professional data are recognized as the bedrock for pre-training, enhancing model capabilities in specific scientific domains [28]. This is particularly evident in fields like materials science, where the performance of MatSci-LLMs is largely contingent upon "building high-quality, multimodal datasets sourced from scientific literature" [7]. Similarly, future improvements in LLM performance within the medical domain will demand "high-quality medical data" [21]. The sheer volume of biological data, accumulated over decades from projects like the Human Genome Project, has even led some to suggest that the "data bottleneck no longer exists" for training machine learning models in this domain, enabling the rapid development of Life Language Models (LLLM) and ambitious visions like "AI Virtual Cells" [8]. This demonstrates a critical aspect of co-evolution: the availability of domain-specific scientific data directly fuels the development of specialized Sci-LLMs, such as Biomni-R0 for biomedical research and AlphaGenome for genomics [29].

Conversely, Sci-LLMs are not merely passive consumers of data; they actively shape and contribute to scientific data foundations. This represents a key distinction from more general discussions of LLM-data relationships, which often focus on data primarily as an input source rather than a participant in a reciprocal innovation cycle [20,25]. Sci-LLMs are fundamentally changing how scientific data is understood and utilized, particularly in complex areas like drug research and development [26]. Models like LLM4SD exemplify this by "reading decades of scientific literature and analyzing laboratory data" to answer complex scientific questions [3]. They possess the unique ability to "quickly synthesize decades of prior knowledge, and then turn to discover new patterns in data that may not have been widely reported," suggesting an iterative process of knowledge generation and data interpretation [3]. This capability for synthesis and pattern discovery goes beyond simple text generation, facilitating new scientific insights and potentially new data generation strategies [29]. The integration of LLMs with tool-use reinforcement learning and agentic AI further enhances this co-evolution, allowing these systems to interact with and contribute to scientific data pipelines, forming closed-loop systems for discovery [29]. For instance, the deep integration of LLMs with the agricultural sector is projected to enhance the intelligence level of agricultural production, decision-making, and services, driving a reciprocal advancement where LLM capabilities inform and refine agricultural data practices [11].

This co-evolutionary dynamic shapes both the capabilities and limitations of Sci-LLMs. On the one hand, the increasing availability of specialized scientific data, coupled with sophisticated model architectures, has enabled Sci-LLMs to achieve unprecedented domain-specific excellence, as seen with models tailored for biomedical and genomic research [29]. They can synthesize vast amounts of scientific knowledge and uncover hidden patterns, accelerating scientific discovery [3]. On the other hand, the limitations arise from the very nature of this dependency. While the demand for massive data is clear, the quality of this pre-training data is paramount, necessitating "careful cleaning processes" to ensure model capacity and reliability [30]. Furthermore, the "professional use of LLMs requires in-depth, real-time, and accurate domain knowledge that pre-trained LLMs cannot easily provide" [20]. This highlights a persistent challenge where, despite advancements, Sci-LLMs require structured knowledge and sophisticated data curation to bridge the gap between general linguistic patterns and nuanced scientific expertise. The co-evolution thus involves a continuous feedback loop: as Sci-LLMs become more capable, they generate a demand for even more specialized and higher-quality data, which in turn fuels further model development and more profound scientific insights [31].
### 5.2 Data Challenges in Scientific Domains
Scientific Large Language Models (Sci-LLMs) operate within a data landscape profoundly different from general domains, characterized by unique properties such as heterogeneity, multi-scale phenomena, and inherent uncertainty [18]. These distinct characteristics significantly impact the training and performance of Sci-LLMs, often amplifying general data limitations observed in conventional LLMs [16,18]. The sheer volume, diversity, and often proprietary nature of scientific datasets contribute to the "unfathomable datasets" challenge, where the scale and complexity preclude thorough review and assessment of quality, further hindering accessibility and reliability [16,25].



**Major Data Challenges in Scientific Domains for Sci-LLMs**

| Category                      | Specific Challenges                                     | Impact on Sci-LLM Development & Reliability             |
| :---------------------------- | :------------------------------------------------------ | :------------------------------------------------------ |
| **Intrinsic Data Properties** | Heterogeneity (varied instruments, scales), Multi-scale Phenomena, Inherent Uncertainty (experimental variability) | Difficulty in comprehension, reasoning, and establishing ground truth. |
| **Scarcity & Quality**        | Domain-Specific Data Scarcity, Limited High-Quality Fine-tuning Data, Rapid Outdatedness ("data lag") | Hinders fine-tuning, generates outdated hypotheses, impacts model performance and relevance. |
| **Multimodality Integration** | Lack of Integrated Multimodal Datasets, Context Length Limitations for Long Sequences | Struggles with complex inputs (figures, tables), limits comprehensive analysis of extensive texts/sequences. |
| **Trustworthiness & Integrity** | Biases in Training Data, Pervasive "Hallucinations", Data Contamination in Hypothesis Reduplication | Generates inaccurate/unfair responses, unreliable outputs, compromises scientific integrity. |
| **Accessibility & Privacy**   | "Data Provenance Crisis" (not AI-ready), Lack of Transparency for Industrial Models, Sensitive Data Privacy Risks | Hinders research, reproducibility, deployment; limits data collection and usage in critical domains. |

The challenges facing Sci-LLMs can be systematically categorized and analyzed, revealing their root causes and interdependencies.

**1. Intrinsic Properties of Scientific Data:**
Scientific data inherently possesses properties that complicate its use in LLMs:
*   **Heterogeneity and Multi-scale Nature**: Scientific information originates from diverse instruments and experiments, spanning vast scales from atomic structures to macroscopic phenomena. For instance, in mathematics, data can be tabular, requiring models to interpret structured tables, often through compression into linear text [31]. Biology and medicine necessitate integrating diverse pre-training corpora, including research articles, electronic health records (EHRs), knowledge bases, and social media posts, all of which represent distinct data formats and scales [31]. Materials science knowledge is highly complex and interconnected, posing difficulties for LLMs in comprehension and reasoning [7]. The "ultimate complexity of living systems" further underscores the multi-scale challenge in biological domains [8].
*   **Inherent Uncertainty**: Experimental variability and the probabilistic nature of many scientific findings introduce uncertainty. The difficulty in establishing reliable ground truth and expert evaluation, particularly in fields like chemistry where automated experiments are needed for validation, highlights this uncertainty [19].

**2. Data Scarcity, Quality, and Outdatedness:**
Despite the increasing volume of scientific data, Sci-LLMs frequently encounter issues of scarcity, suboptimal quality, and rapid obsolescence.
*   **Domain-Specific Data Scarcity**: There is a significant shortage of specialized training data, especially for smaller research communities or fields with less specialized vocabulary, impacting applications like peer review [19]. Similarly, the limited scale of expert-annotated benchmarks for scientific discovery constrains model development and evaluation [19].
*   **Quality of Fine-tuning Data**: The effectiveness of Sci-LLMs hinges on high-quality, task-specific datasets for fine-tuning, which are often scarce [1,25]. Data preparation, encompassing cleaning, tagging, and feature engineering, is labor-intensive and inefficient for large scientific datasets [19].
*   **Outdated Information and Data Lag**: Scientific knowledge evolves rapidly, particularly in fields like computer science, biology, and materials science. LLMs trained on static datasets struggle to keep pace, generating hypotheses based on outdated or incomplete information [23]. This issue, described as "data lag," necessitates mechanisms for "dynamic knowledge updating" to ensure data currency and relevance [11,18].

**3. Multimodality and Complexity Integration:**
The need for Sci-LLMs to process and integrate diverse modalities presents considerable hurdles.
*   **Multimodal Data Integration**: Combining text with graphs, vision, or other specific data formats is a core challenge. There is a scarcity of integrated datasets that combine multiple scientific modalities, which is critical for advancing multimodal Sci-LLMs [1,20]. For instance, general LLMs often struggle with questions involving figures, tables, or chemical structures, necessitating their exclusion from evaluations [21].
*   **Context Length Limitations**: Current LLMs face limitations in processing long text data and complex biological sequences, such as proteins and genes, which are significantly longer than chemical representations like SMILES strings. This restricts their ability to effectively analyze extensive scientific data, including medical information where organizing heterogeneous and unstructured text data is difficult [5,22,26].

**4. Trustworthiness and Integrity:**
The integrity and reliability of Sci-LLM outputs are compromised by data-related issues, leading to concerns about scientific trustworthiness.
*   **Bias and Misinformation**: Biases present in training data can cause LLMs to generate and amplify inaccurate or unfair responses, reflecting stereotypical or political leanings [10,17,20,25,26]. In agricultural contexts, this can lead to the propagation of misinformation, impacting agronomic advice and potentially eroding the "digital agricultural commons" by mixing human and AI-generated content [2].
*   **Hallucinations**: A pervasive problem, hallucinations involve LLMs generating confident yet incorrect outputs [29]. This is particularly problematic in hypothesis generation and experiment planning, where limited planning capabilities and susceptibility to hallucinations can hinder scientific discovery [13]. Fine-tuned smaller language models, for instance, can produce "hallucinated reasoning chains" that mislead subsequent inference in multimodal Chain-of-Thought reasoning [32].
*   **Data Contamination**: When re-discovering fundamental hypotheses, data contamination is a concern, requiring careful consideration of publication dates to ensure LLMs are trained on data predating discoveries [19].

**5. Accessibility, Transparency, and Privacy:**
Challenges related to data accessibility, transparency, and privacy further constrain the development and deployment of Sci-LLMs.
*   **AI-Readiness and Provenance**: Scientific data is often not in a format readily usable for AI model training, contributing to a "data provenance crisis" where tracing the origin and history of data is difficult [18].
*   **Transparency and Public Access**: Much of the important training data and procedures used by industry-trained LLMs are not publicly revealed, hindering research and adaptation to specialized scientific datasets [30].
*   **Privacy Concerns**: Handling sensitive data, particularly in fields like biomedical research, raises significant privacy risks, especially when personal LLM agents access sensitive user data [14,20,26,29].

**Interdependencies and Amplified Effects:**
These challenges are highly interdependent. Data scarcity often forces reliance on more general LLMs, exacerbating issues like hallucinations and a lack of domain-specific understanding. For instance, the shortage of specialized training data leads to LLMs exhibiting difficulties in understanding specialized terminology and inconsistencies in contextual analysis during peer review [13]. The inherent heterogeneity and multi-scale nature of scientific data make data preparation labor-intensive and contribute to the scarcity of integrated, AI-ready datasets. Outdated or incomplete knowledge directly leads to a lack of innovation and depth in LLM-generated scientific hypotheses [23]. Biases embedded in large, untraceable datasets perpetuate and amplify societal biases within scientific applications, impacting fairness and equity.

**Critique of Proposed Workarounds and Open Questions:**
While some implicit workarounds exist, such as the synthesis of experimental data when real-world data is scarce, or the necessity of detailed data preprocessing steps (deduplication, privacy reduction, multi-source mixing) [19,28,33], these are often reactive or general practices rather than comprehensive solutions specifically addressing the multifaceted data challenges in scientific domains. Current research primarily identifies these problems, with robust solutions still nascent.

Significant research gaps and open questions remain. Developing novel methods for building large-scale, multimodal datasets that capture complex scientific principles is crucial, particularly for fields like materials science [7,20]. Improving LLMs' quantitative analysis and multimodal processing capabilities, along with overcoming context length limitations for complex scientific sequences, are paramount [22,26]. Furthermore, establishing mechanisms for "dynamic knowledge updating" and ensuring factual accuracy and reliability of generated reasoning chains are critical for maintaining the trustworthiness of Sci-LLMs [11,32]. Addressing the "data provenance crisis" and rendering scientific data "AI-ready" are fundamental steps towards enabling more effective Sci-LLM development [18]. Finally, developing effective strategies to safeguard user data privacy while maintaining model performance remains a significant challenge [14].
### 5.3 Scientific Data Resources and Annotation
The development of Scientific Large Language Models (Sci-LLMs) fundamentally relies on the availability and quality of training data, distinguishing significantly between general corpora and specialized scientific datasets [18]. General LLMs are typically pre-trained on extensive, multi-petabyte datasets, often containing trillions of tokens drawn from diverse sources such as CommonCrawl, Wikipedia, books (e.g., BookCorpus, Project Gutenberg), and code repositories (e.g., GitHub, StackOverflow) [4,10,15,28,30]. Prominent models like GPT-3, PaLM, and LLaMA exemplify this approach, utilizing carefully mixed data sources and employing sophisticated "data scheduling" strategies encompassing "data mixture" and "data curriculum" to enhance various capabilities, including code, mathematics, and long-text modeling [9,30].

In contrast, Sci-LLMs necessitate specialized datasets that capture the complexity and domain specificity of scientific information [1,18]. A comprehensive review identified over 270 pre-training and post-training datasets specifically tailored for Sci-LLMs, underscoring the critical role of domain-specific data throughout the model's lifecycle [18]. These specialized resources include:
*   **Biomedical Data**: BioGPT, for instance, was pre-trained on "large scale biomedical literature" [20,32]. GeneGPT demonstrates the integration of vast biomedical databases from NCBI, covering gene sequences, protein structures, compound properties, and biomedical text, accessible via unified Web APIs [5]. Other notable resources include Electronic Health Records (EHRs) like MIMIC-III/IV, knowledge bases such as UMLS, medical examination questions, and extensive patient-doctor dialogues (e.g., ChiMed, MedInstruct-52k, BiMed1.3M) [31]. Foundational initiatives like the Human Cell Atlas, Human Genome Project, Cancer Genome Atlas, ENCODE, and Human Protein Atlas also provide critical data for life science models like Evo and SCimilarity [8].
*   **Chemistry and Materials Science**: The LLM4SD framework leverages "decades of scientific literature" for knowledge synthesis and "SMILES strings from training data" for pattern inference, evaluated on the MoleculeNet dataset which includes 58 benchmark tasks across various scientific disciplines [3,22]. Specific datasets for these domains include the Materials Project database, Mol-Instructions, SMolInstruct, and molecular graph datasets such as ChEBI-20, ZINC, and PCDes [31]. Models like MOOSE-Chem are trained on "tens of billions of scientific papers" [19].
*   **Mathematics and Code**: Specialized datasets for mathematical tasks include MathQA, Ape210K, Math23K for multiple-choice questions, and GSM8K, MATH, Meta-MathQA for generative tasks. Geometry-specific data are found in Geometry3K and GeoQA, alongside tabular data like WikiTableQuestions and WikiSQL [31]. Code data from GitHub and StackOverflow are also crucial [30], with ByteDance's Seed-Coder emphasizing scalable, automated pipelines for code data [29].
*   **General Scientific Literature and Knowledge Bases**: Bibliographic databases like AMiner, Microsoft Academic Graph (MAG), and Semantic Scholar serve as primary sources. Datasets like S2ORC provide full-text papers, while others offer abstracts and metadata [31]. ArXiv is a commonly used source for various scientific domains [30,31]. The Pile also incorporates scientific papers as a high-quality subset [30].
*   **Other Scientific Domains**: For physics, arXiv abstracts are utilized for astronomy LLMs [31]. In agriculture, structured knowledge bases are developed using vector databases and knowledge graphs [11], alongside real-time news, satellite imagery, and sensor data [2]. Earth sciences utilize research papers, climate news, and authoritative reports like the IPCC Sixth Assessment Report (AR6) for chatIPCC, along with climate time series datasets such as ERA5 and CMIP6 [5,31].

Curating high-quality scientific data presents significant challenges, primarily due to the inherent complexity and specialized nature of scientific information, which often demands robust annotation methods [18]. Unlike general LLM data, which can often rely on unsupervised pre-training on vast, unlabeled text corpora [4], scientific data frequently requires meticulous annotation and validation. For instance, the exclusion of questions with figures, tables, or chemical structures from medical examination datasets often necessitates manual identification by a clinician, highlighting the need for "high-quality medical data" and expert involvement [21]. Similarly, there is a "critical need for building high-quality, multimodal datasets sourced from scientific literature" to enable performant MatSci-LLMs [7]. Ethical concerns regarding data usage without consent, as observed in general LLM development, also extend to scientific data, particularly when dealing with proprietary or sensitive information [10].

The annotation processes for general LLM data and scientific data diverge significantly. While general LLMs may undergo supervised fine-tuning on labeled, task-specific data after initial unsupervised pre-training [4], scientific data annotation places a paramount emphasis on expert validation and increasingly, semi-automated annotation to manage its scale and specificity [18]. Expert validation is crucial for ensuring the accuracy and relevance of scientific information. For example, instruction tuning data for Sci-LLMs is often augmented by existing LLMs (e.g., GPT-4) or meticulously curated by human experts, demonstrating a blend of automation and human oversight [31].

Emerging solutions for scientific data annotation leverage semi-automated pipelines:
*   **Zero-shot Learning for Structured Extraction**: SPIRES facilitates populating knowledge bases by extracting structured information from medical texts using zero-shot learning, serving as an effective semi-automated annotation method for structured data [5].
*   **LLM-assisted Knowledge Synthesis**: The HILMA framework utilizes LLMs to generate concise reviews for subgraphs of citation networks, synthesizing research themes, methods, and findings from related scientific literature. This provides LLMs with systematic and reliable knowledge for tasks like hypothesis generation. The construction of these citation networks involves formulas to represent relationships between citing, cited, and related documents:
    $$ E_{\text{cite}}=\{({v}_{i},{v}_{c})|{v}_{i}\in {V}_{\text{cite}},\text{文献 } i \text{ 引用 } c\} $$
    $$ E_{\text{cited}}=\{({v}_{i},{v}_{c})|{v}_{i}\in {V}_{\text{cited}},\text{文献 } i \text{ 被 } c \text{ 引用}\} $$
    $$ E_{\text{related}}=\{({v}_{i},{v}_{c})|{v}_{i}\in {V}_{\text{related}},\text{文献 } i \text{ 与 } c \text{ 相关}\} $$
    $$ G_c = ({V_{\text{cite}}} \cup {V_{\text{cited}}} \cup {V_{\text{related}}},{E_{\text{cite}}} \cup {E_{\text{cited}}} \cup {E_{\text{related}}}) $$
    This process demonstrates a sophisticated approach to leverage LLMs for data organization and summarization at scale [23].
*   **Toolkit Development**: Tools like AI Sheets by Hugging Face directly address the need for better mechanisms to manage and annotate data for LLMs, indicating a growing emphasis on practical solutions for data curation [29]. Furthermore, the development of scalable, automated data pipelines for specific data types, such as code data in Seed-Coder, underscores the shift towards more efficient and specialized annotation workflows [29].

In summary, while general LLMs prioritize the sheer volume and diversity of data through sophisticated data scheduling [9], Sci-LLMs demand a deeper engagement with domain-specific knowledge. This necessitates not only specialized datasets but also rigorous annotation practices, combining expert validation with innovative semi-automated techniques, to accurately represent and synthesize the intricate nature of scientific information [18]. The progress in developing frameworks like HILMA and tools like AI Sheets, alongside the increasing focus on high-quality, curated scientific corpora, highlights the field's evolving strategies to overcome data-related challenges in scientific AI.
## 6. Training and Adaptation Strategies for Scientific LLMs
The development of Scientific Large Language Models (Sci-LLMs) represents a significant frontier in AI, moving beyond general language understanding to specialized scientific knowledge acquisition, processing, and reasoning. This necessitates a comprehensive and multi-faceted approach to training and adaptation, fundamentally differing from the methodologies applied to general-purpose LLMs [14,31]. 

![Phases of Sci-LLM Training and Adaptation](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/XMYtPeRvuqfL0gPzfkYZF_Phases%20of%20Sci-LLM%20Training%20and%20Adaptation.png)

The journey from a foundational model to a proficient scientific agent involves three critical phases: establishing a robust knowledge base through pre-training, specializing capabilities and aligning outputs through fine-tuning, and optimizing the entire process while expanding functional capacities through advanced techniques.

The initial phase, **Pre-training Data and Strategies**, is pivotal in imbuing LLMs with foundational scientific knowledge and linguistic patterns. This involves a deliberate shift from general web-scale corpora to highly specialized scientific datasets, which vary significantly in scale, diversity, and granularity [3,31]. These corpora encompass broad scientific literature (e.g., research papers from Semantic Scholar) as well as highly domain-specific data from mathematics, physics, chemistry, biomedicine, and earth sciences [26,31]. A key challenge lies in the meticulous curation, cleaning, and continuous updating of these complex datasets, including dealing with specialized formats like molecular graphs and ensuring data quality and privacy [28,33]. Various pre-training tasks, such as Masked Language Modeling (MLM) for encoder-only models and Next-Token Prediction (NTP) for generative models, are employed to foster specific abilities, alongside advanced data scheduling techniques like "data mixing" and "data curriculum" to optimize knowledge acquisition and long-context understanding [31,33].

Following pre-training, **Fine-tuning and Alignment Techniques** are essential for adapting general LLMs to specific scientific tasks and ensuring their outputs are accurate, reliable, and aligned with human values and scientific rigor [20,33]. Instruction tuning, which trains models on diverse datasets with natural language descriptions, is a cornerstone technique, significantly enhancing a model's ability to follow instructions and generalize to unseen tasks, often unlocking emergent reasoning abilities like Chain-of-Thought (CoT) prompting [17,30]. The creation of high-quality instruction-following datasets, ranging from established NLP benchmarks to synthetic data generated by powerful LLMs and human-curated domain-specific instructions, is critical for this adaptation [1,31]. Furthermore, aligning LLM outputs with ethical considerations and factual accuracy, particularly in scientific contexts, heavily relies on human feedback mechanisms, such as Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR), as well as direct integration of expert knowledge [17,30]. To mitigate the significant computational overhead of fine-tuning, Parameter-Efficient Fine-Tuning (PEFT) methods, including LoRA and adapter tuning, have emerged, making customized model development more accessible [28,30].

Finally, **Advanced Techniques for LLM Enhancement** address the formidable computational demands and aim to significantly expand the functional capabilities and practical applicability of Sci-LLMs. The sheer scale of modern LLMs necessitates distributed training strategies, such as 3D parallelism (data, pipeline, and tensor parallelism), facilitated by sophisticated frameworks like DeepSpeed and Megatron-LM, and powered by massive supercomputing infrastructures [17,30]. Efficiency is further optimized through techniques like mixed-precision training (e.g., FP16/BF16) and various memory optimization strategies (e.g., ZeRO, FlashAttention, PagedAttention), alongside methods to ensure training stability [12,28]. Beyond core training, advancements in long-context capabilities, sophisticated prompting techniques (e.g., Chain-of-Thought, automatic prompt optimization), and the strategic manipulation of external tools (e.g., calculators, search engines, specialized scientific plugins) dramatically enhance LLMs' reasoning and problem-solving prowess [17,29]. Architectural innovations, including adapters, Graph Neural Network-nested Transformers, and multimodal integration, further tailor LLMs for handling complex scientific data, cementing their role as increasingly versatile and powerful collaborators in scientific discovery [5,31]. Collectively, these strategies enable the creation of Sci-LLMs that are not only knowledgeable but also efficient, adaptable, and aligned with the rigorous demands of scientific research.
### 6.1 Pre-training Data and Strategies
The effectiveness of scientific large language models (LLMs) is profoundly influenced by their pre-training data and the strategies employed during this foundational phase [14,20]. Initially, general LLMs are trained on massive datasets comprising books, web pages, and conversational texts to acquire broad linguistic knowledge and pattern recognition capabilities [4,10,28]. However, the development of scientific LLMs necessitates a deliberate shift towards specialized corpora and tailored pre-training approaches to imbue models with scientific knowledge and reasoning abilities [3,31].

**Characteristics of Scientific Corpora:**

Scientific corpora for LLM training exhibit significant diversity, ranging from broad scientific literature to highly domain-specific datasets. General scientific corpora, such as research papers from AMiner, Microsoft Academic Graph (MAG), and Semantic Scholar, provide a wide-angle view of scientific discourse [31]. Some datasets, like S2ORC, include full-text articles, offering deep contextual information, while others, such as those used for AstroLLaMA, primarily rely on abstracts [31].

For domain-specific applications, highly specialized datasets are curated. In mathematics, these include QA datasets (MathQA, GSM8K, MATH), geometry datasets (Geometry3K, GeoQA), and tabular data (WikiTableQuestions) [31]. Physics models like astroBERT utilize astronomy papers, while chemistry and materials science models leverage research papers, the Materials Project, Mol-Instructions, and molecular graph datasets (ChEBI-20, ZINC) [31]. Biomedical LLMs, such as BioGPT and Med-PaLM, are specifically pre-trained on extensive biomedical literature, electronic health records (EHRs like MIMIC-III/IV), and medical exam questions to encode clinical knowledge [26,31,32]. Similarly, DNA-BERT is trained on large volumes of nucleotide sequences, and Chemformer is tailored for chemical reaction prediction using specific chemical datasets [26]. Earth science models utilize papers, climate news, and geospatial data like OpenStreetMap [31]. The LLM4SD framework, for instance, operates by leveraging knowledge from extensive scientific text and data representations like SMILES strings [3]. Even agriculture-specific LLMs rely on specialized data for knowledge integration [11].

**Impact of Data Scale, Diversity, and Granularity:**

The sheer scale of pre-training data is a defining characteristic of modern LLMs. Models like mmBERT are pretrained on 3 trillion tokens, and code LLMs like Seed-Coder on 6 trillion tokens, demonstrating the massive data volumes involved [29]. The precision of word prediction and the quality of generated text improve with increased model size and adequate data [20].

Data diversity is crucial for comprehensive scientific understanding. While general datasets provide broad linguistic understanding, domain-specific data is essential for acquiring nuanced scientific knowledge and reasoning abilities. This is exemplified by the observation that scientific-specific smaller models, such as Galactica-6.7B, can achieve comparable or even superior performance in scientific tasks compared to much larger general models like Falcon, precisely because of their domain-specific pre-training data [3,22]. Falcon itself incorporates vast amounts of scientific literature, including arXiv and Wikipedia, during its pre-training [3]. The Othello-GPT, trained solely on game scripts, demonstrates that even highly structured symbolic sequences can be learned as a form of language modeling [24].

Data granularity, specifically the choice between full-text papers and abstracts, significantly impacts knowledge acquisition. Full-text articles, as found in S2ORC, provide a richer, more comprehensive context, potentially enabling deeper understanding and more robust reasoning than models trained solely on abstracts, such as AstroLLaMA which uses arXiv abstracts [31]. This suggests that access to the full content of scientific papers can lead to models with enhanced scientific knowledge representation.

**Challenges in Data Curation, Cleaning, and Updating:**

Curating, cleaning, and updating specialized scientific datasets present substantial challenges. The process involves several critical steps:
1.  **Quality Filtering and Selection**: Low-quality data must be identified and removed using classifier-based and heuristic methods, filtering by language features, text quality metrics, and keywords [28].
2.  **Deduplication**: Redundant data, which can negatively impact training, needs to be eliminated at sentence, document, and even dataset levels [28,33].
3.  **Privacy Redaction**: Personally Identifiable Information (PII) must be removed to protect privacy, particularly in sensitive domains like biomedicine (e.g., EHRs) [28,33].
4.  **Tokenization**: Texts must be segmented into appropriate units (words or sub-words) for model processing [28,33].

Beyond these general challenges, scientific data introduces unique complexities. Complex scientific data formats, such as molecular graphs, tabular data, or particle coordinates, often require linearization to fit into LLM input structures, for example, converting molecular graphs into SMILES strings [31]. The "high pre-training costs" associated with processing these massive, specialized datasets and the computational demands for training models with hundreds of billions of parameters pose significant barriers [16,17]. Furthermore, ensuring the temporal relevance of scientific data is critical; benchmarks must account for publication dates to prevent LLMs from "re-discovering" hypotheses from data available prior to their actual discovery, thus avoiding data contamination [19]. A significant challenge is the lack of transparency, as many important training details, particularly data collection and cleaning, are often not publicly revealed for industrially trained LLMs [30].

**Pre-training Tasks and Long Context Modeling:**

Different pre-training tasks significantly impact the general capabilities and scientific reasoning abilities of LLMs. Common pre-training tasks include:
*   **Masked Language Modeling (MLM)**: Widely used for encoder-only BERT-based models (e.g., SciBERT, ChemBERT, MatSciBERT), where models predict masked tokens based on surrounding context [25,31].
*   **Next-Token Prediction (NTP)**: A foundational task for GPT-based models (e.g., SciGPT, ChemLLM), where models predict the next token in a sequence, fostering generative capabilities [31].
*   **Denoising Autoencoding**: Involves restoring original text from corrupted input, which helps models learn robust representations [28,33].
*   **Next Sentence Prediction (NSP)**: Used in models like BERT to understand relationships between sentences [25].
*   **Mixed Objectives**: Combining tasks, such as Mixture-of-Denoisers (MoD), can enhance pre-training effectiveness [28].

Beyond these, specialized tasks are crucial for scientific LLMs. Contrastive loss is employed for models like BERT-TD and Text2Mol, and in CLIP-like architectures for biomedical and earth sciences, to map paired text/image/graph modalities closer in latent space, enabling cross-modal understanding [31]. Auxiliary tasks like MASK image modeling or text-image matching are also utilized for visual modeling in scientific contexts [31].

Long context modeling, implicitly addressed through data scheduling, is vital for scientific LLMs that need to process lengthy research papers or complex experimental protocols. Data scheduling strategies, encompassing "data mixing" and "data curriculum," are designed to manage the proportion and sequence of different data sources during training [9,28,33]. Data mixing optimizes the ratios of diverse data sources to improve general capabilities and enhance domain-specific abilities by adjusting the proportion of specialized data like mathematical texts or code [28]. Data curriculum, by arranging the order of data presentation, can further improve skills such as code, mathematics, and long-text modeling more quickly and effectively [9]. These sophisticated data scheduling techniques directly impact the model's ability to maintain coherence and draw inferences over extended scientific discourse, which is critical for complex scientific reasoning. Ultimately, the meticulous selection and preparation of scientific corpora, coupled with advanced pre-training tasks and data scheduling, are indispensable for developing scientific LLMs capable of sophisticated knowledge acquisition and effective reasoning [31].
### 6.2 Fine-tuning and Alignment Techniques
The development of Scientific Large Language Models (Sci-LLMs) necessitates sophisticated adaptation strategies to enhance their performance on specialized tasks and ensure their outputs align with scientific rigor and human values. Fine-tuning, the process of adjusting pre-trained language models (PLMs) with datasets and objectives customized for specific applications, serves as a fundamental adaptation strategy [14,20,33]. This allows generally trained models to specialize, achieving high performance in particular domains such as machine translation, sentiment analysis, or question answering [14,20]. The process often combines unsupervised pre-training with subsequent supervised fine-tuning, utilizing labeled data points for precise concept recognition [4].

**Mechanisms and Advantages of Instruction Tuning**

Instruction tuning is a key fine-tuning technique that involves training LLMs on diverse, multi-task datasets formatted with natural language descriptions [17,28,30]. This approach significantly enhances the model's ability to follow instructions for novel, unseen tasks without explicit examples, thereby improving generalization and task-specific accuracy [17,28,30]. Its mechanisms unlock or enhance capabilities, allowing the adaptation of general LLMs into domain-specific expert models [28].

For scientific LLMs, instruction tuning is critical for tasks ranging from general scientific inquiry to specialized areas like mathematics, chemistry, biomedicine, and earth sciences [18,31]. Examples include Galactica and SciGLM for general scientific tasks, Rho-Math and MAmmoTH2 for mathematics, ChemDFM and ChemLLM for chemistry, Med-PaLM and BioMistral for biomedicine, and K2 and OceanGPT for earth sciences [31].



**Fine-tuning and Alignment Techniques for Sci-LLMs**

| Technique               | Description                                                 | Key Methods/Examples                                          | Sci-LLM Impact & Advantages                               |
| :---------------------- | :---------------------------------------------------------- | :------------------------------------------------------------ | :-------------------------------------------------------- |
| **Instruction Tuning**  | Train LLMs on diverse multi-task datasets with natural language descriptions | P3, FLAN (NLP tasks); ShareGPT, Alpaca (chat data); GPT-4 generated (AstroLLaMA-chat, MedInstruct) | Enhances instruction following, generalization, task-specific accuracy, unlocks CoT reasoning. |
| **Human Feedback (RLHF)** | Incorporate human preferences into training loops to align model behavior | Supervised fine-tuning, Reward model training, RL (PPO) fine-tuning (InstructGPT, ChatGPT, GPT-4) | Ensures helpful, honest, harmless outputs; mitigates bias and undesirable content; process supervision. |
| **Reinforcement Learning with Verifiable Rewards (RLVR)** | Use verifiable outcomes to improve reasoning in complex tasks | Tülu 3, Grok-4-Fast (tool-use), Biomni-R0 (biomedical)           | More robust learning from feedback, improves reasoning in verifiable domains (math, complex instructions). |
| **Constitutional AI / Deliberative Alignment** | Guide AI outputs with a set of principles for safety and accuracy | Claude (Constitutional AI), o4-mini (deliberative alignment)  | Mitigates exploitation, ensures adherence to ethical principles and safety. |
| **Expert Knowledge Integration** | Directly embed domain expertise into models               | Med-PaLM (clinical knowledge), chatIPCC (authoritative reports), human review/rewriting | Grounds models in scientific reality, ensures factual correctness, critical for validation. |
| **Parameter-Efficient Fine-Tuning (PEFT)** | Reduce tunable parameters while maintaining performance | LoRA, Adapter Tuning, Prefix Tuning, Prompt Tuning (LLaMA, BLOOM) | Mitigates high computational cost of full fine-tuning; makes customized model development more accessible. |

A notable advantage of instruction tuning is its role in fostering emergent abilities such as step-by-step reasoning, exemplified by Chain-of-Thought (CoT) prompting [17,30,33]. CoT, along with techniques like in-context learning (ICL) and ReAct, improves the reliability and accuracy of LLM-assisted workflows, particularly in experimental planning and scientific hypothesis generation [13,23]. For instance, the Multimodal-CoT framework utilizes a decoupled training strategy to generate effective rationales from vision features, mitigating issues of hallucinated reasoning chains in complex multimodal tasks [32].

**Instruction-Following Datasets: Creation and Effectiveness**

The effectiveness of instruction tuning heavily relies on the quality and diversity of instruction-following datasets [1]. Various methods are employed for their creation and utilization:
*   **NLP Task Datasets**: Established benchmarks like P3 and FLAN are used to train models across a broad spectrum of natural language processing tasks [28].
*   **Daily Chat Datasets**: Data from platforms like ShareGPT, OpenAssistant, and Dolly capture conversational patterns, contributing to more natural and versatile instruction following [9,28].
*   **Synthetic Datasets**: A significant trend involves generating synthetic instruction data using existing powerful LLMs. Examples include Self-Instruct-52K, Alpaca, Baize, and WizardLM, which complexify instructions to increase diversity and scale [9,28,30]. This approach is particularly prominent in scientific domains, where GPT-4 generated dialogue is used for models like AstroLLaMA-chat and MedInstruct-52k/BiMed1.3M [31].
*   **Human-Curated and Domain-Specific Datasets**: These datasets are critical for adapting LLMs to specific scientific problems, involving downstream task data and human-curated domain-specific instructions [31]. The study on ChatGPT's performance in medical examinations highlights the use of direct instruction strategies via multiple prompts and suggests incorporating knowledge-enhanced training for better results [21].

Models like Flan-T5 are premier examples of instruction tuning, demonstrating improved generalization by scaling task numbers and model sizes, and by incorporating CoT prompting data [30]. LLaMA models are also extensively adapted through instruction tuning, offering a computationally efficient path for customized model development [30].

**Role of Human Feedback and Expert Knowledge in Alignment**

Beyond task performance, aligning LLM outputs with human values and objectives is crucial, especially in scientific contexts where accuracy, truthfulness, and ethical considerations are paramount [17,20,28]. Alignment aims to ensure models are helpful, honest, and harmless, mitigating the generation of toxic, biased, or harmful content [17,28,30].

Human feedback and expert knowledge are integral to these tuning processes:
*   **Reinforcement Learning from Human Feedback (RLHF)**: This is a core technique, formalized by InstructGPT and utilized in models like ChatGPT and GPT-4, that incorporates human feedback into the training loop [17,25,28,30]. RLHF typically involves three stages: supervised fine-tuning (SFT) with human-labeled data, training a reward model from human preference rankings, and then fine-tuning the LLM using an RL algorithm (e.g., Proximal Policy Optimization) to maximize the reward signal [28,30]. GPT-4 further refined this with iterative alignment and safety reward signals, alongside interventions like red teaming [30]. Process supervision, which provides more fine-grained oversight than outcome supervision, has emerged to enhance human alignment [9,28].
*   **Reinforcement Learning with Verifiable Rewards (RLVR)**: Tülu 3 and other models use RLVR for tasks with verifiable outcomes, such as mathematical problem-solving or complex instruction following. This technique can even leverage incorrect answers to improve reasoning, demonstrating a more robust approach to learning from feedback [15,29]. Other RL-based methods for alignment include Grok-4-Fast for tool-use, Biomni-R0 for biomedical intelligence, and frameworks like LlamaRL for scalable training [29].
*   **Constitutional AI and Deliberative Alignment**: Claude, developed by Anthropic, employs Constitutional AI, guiding AI outputs with a set of principles for helpful, harmless, and accurate responses [15]. Similarly, the o4-mini model uses deliberative alignment to mitigate system exploitation and unsafe content generation [15].
*   **Expert Knowledge Integration**: Expert domain knowledge plays a direct role in grounding LLMs. Med-PaLM encodes clinical knowledge to achieve human-expert levels in medical questions [26]. In climate science, chatIPCC enhances GPT-4 by integrating authoritative reports, preventing inaccurate information [5]. Human authors also engage in thorough review and rewriting of LLM-generated scientific content to ensure alignment with scientific standards and factual correctness [5].

**Evolution and Impact on LLM Versatility**

Instruction tuning has profoundly impacted LLM versatility, transforming them from general text generators to models capable of following complex scientific commands [28,33]. The evolution has seen a shift from basic supervised fine-tuning to sophisticated instruction-following paradigms, enabling LLMs to execute new tasks by simply understanding instructions without needing explicit examples [17]. The combination of instruction tuning with advanced prompting techniques like CoT has further enhanced reasoning abilities and task adaptability, as seen in models designed for scientific hypothesis generation and experimental planning [13,23].

However, fine-tuning an entire LLM demands substantial computational resources, often comparable to pre-training, which poses a significant "Fine-Tuning Overhead" and limits practicality for many researchers [16]. To address this, Parameter-Efficient Fine-Tuning (PEFT) methods have emerged, aiming to reduce the number of tunable parameters while maintaining performance [28,30]. These methods include adapter tuning, prefix tuning, prompt tuning, and Low-Rank Adaptation (LoRA), which are widely applied in open-source LLMs like LLaMA and BLOOM, demonstrating efficacy in cost reduction and performance preservation [28,30]. Furthermore, memory-efficient model adaptation techniques like quantization are being explored to allow instruction-tuned models to operate with reduced memory footprints while maintaining capabilities [9]. These advancements collectively foster the development of more accessible and versatile scientific LLMs.
### 6.3 Advanced Techniques for LLM Enhancement
The extraordinary scale of Large Language Models (LLMs) necessitates sophisticated training strategies and optimization techniques to manage their immense computational demands and memory footprints, thereby enabling the development of larger and more complex Scientific LLMs (Sci-LLMs) [33]. These advancements address inherent limitations of earlier LLM versions, significantly enhancing their practical applicability across diverse and complex tasks [9].

**Distributed Training and Parallelization Strategies**:
The training of LLMs, often involving billions of parameters, is feasible only through large-scale distributed training [17,30]. This paradigm partitions the model and data across multiple computational devices, typically Graphics Processing Units (GPUs), to accelerate training and overcome single-device memory constraints. A primary approach is **3D parallelism**, which integrates three distinct strategies: data parallelism, pipeline parallelism, and tensor parallelism [28].
*   **Data Parallelism** involves replicating the model on each device and distributing different mini-batches of data, with gradients aggregated and synchronized across devices.
*   **Pipeline Parallelism** divides the model layers into sequential stages, with each stage processed on a different device, allowing for concurrent execution of different layers on different data batches.
*   **Tensor Parallelism** (or Model Parallelism) shards individual model layers or tensors across multiple devices, which is critical when a single layer's computation or memory usage exceeds a device's capacity [28,30].

Optimization frameworks such as DeepSpeed and Megatron-LM are instrumental in facilitating these parallel algorithms. DeepSpeed, developed by Microsoft, offers memory optimization capabilities through techniques like ZeRO (Zero Redundancy Optimizer) and gradient checkpointing, alongside pipeline parallelism [30]. NVIDIA's Megatron-LM, conversely, emphasizes model and data parallelism, incorporating mixed-precision training and FlashAttention to boost efficiency [30]. The sheer scale of modern LLM training infrastructure is exemplified by Grok's Colossus supercomputer, comprising over 100,000 Nvidia GPUs, and Google's use of multiple TPU 4 Pods for models like Palm, underscoring the vital role of these distributed computing solutions [15]. These infrastructures are essential for managing the significant computational resources required for LLM training and execution [20].

**Optimization for Efficiency and Scale**:
Beyond distributed training, a suite of optimization techniques directly contributes to the development of larger and more intricate Sci-LLMs by enhancing memory efficiency and training stability.
*   **Mixed-Precision Training** is a pivotal technique that utilizes lower-precision number formats, specifically 16-bit floating-point numbers (FP16) or Brain Floating Point (BF16), in conjunction with standard 32-bit floating-point numbers. This significantly reduces memory usage and communication overhead during training, while often maintaining comparable model accuracy and enabling larger models to fit into available memory [28,30].
*   **Memory Optimization** strategies are crucial for both training and inference. Techniques such as ZeRO (mentioned with DeepSpeed) and gradient checkpointing conserve memory by intelligently storing and recomputing activations [30]. FlashAttention, an I/O-aware technique, dramatically improves efficiency by optimizing data movement between GPU memory and on-chip caches, a form of memory optimization impacting large-scale training [12,30]. During incremental inference, the "memory wall" problem, where data transfer dominates computation, is addressed by strategies like PagedAttention and Flash-Decoding, which reduce data transfer, and speculative decoding, which optimizes decoding algorithms [9]. Further memory-centric advancements include MEM1, a memory-efficient framework for long-horizon language agents, and MemOS, a memory-centric operating system for adaptive LLMs [29].
*   **Training Stability** techniques, such as weight decay and gradient clipping, are employed to prevent divergence and ensure robust learning [28,30]. Restarting training to overcome loss spikes and adjusting embedding layer gradients are also vital for maintaining stable optimization [28,30]. The broader concept of "algorithm-system-hardware co-design" emphasizes the need for algorithmic efficiency to be tightly coupled with hardware capabilities, ensuring that system-level support fully leverages modern computing architectures [12].

**Enhancing LLM Capabilities and Practical Applicability**:
These foundational training and optimization techniques are complemented by methods that directly enhance LLMs' functional capabilities and applicability, particularly for complex scientific domains.
*   **Long-Context Capabilities**: Handling extended text sequences is critical for many applications. Advancements include methods for positional encoding extension and context window adaptation [9]. Techniques like REFRAG use context compression for significantly longer contexts and faster decoding, while MemAgent leverages reinforcement learning for long-context processing [29]. Specialized architectures such as Phi-4-mini-Flash-Reasoning are designed for efficient long-context reasoning with compact models [29].
*   **Advanced Prompting Techniques**: These strategies significantly improve LLM reasoning and task performance.
    *   **Chain-of-Thought (CoT) prompting** involves incorporating intermediate reasoning steps, allowing LLMs to decompose complex problems and arrive at more accurate solutions [17,19]. Enhancements include improved prompt design, sampling and verification-based methods for CoT generation, and extensions to more complex reasoning structures like trees and graphs [9]. Multimodal CoT further extends this by generating intermediate rationales to infer answers in multimodal contexts, significantly improving performance in tasks like scientific question answering [32].
    *   **Instruction Tuning** fine-tunes LLMs with natural language task descriptions, substantially improving their generalizability to unseen tasks [17].
    *   **Automatic Prompt Optimization** leverages methods to automatically discover optimal prompts, including discrete approaches (gradient-based, reinforcement learning-based, editing-based, LLM-based) and continuous methods (data-sufficient prompt learning, data-sparse prompt transfer) [9].
    *   Other advanced prompting techniques like in-context learning and ReAct are crucial for enhancing the reliability and accuracy of LLM-assisted experimental planning workflows, enabling reflection and iterative refinement by simulating expert discussions [19].
*   **Tool Manipulation and External Augmentation**: To overcome inherent limitations such as poor numerical calculation abilities or outdated knowledge, LLMs can leverage external tools. This includes using calculators for precise computations, search engines for retrieving current information, or specialized plugins for specific functions [17,30]. For instance, ChatGPT's plugin mechanism allows LLMs to interact with diverse applications, acting as "eyes and ears" to expand their capabilities [17]. In scientific contexts, systems like ChemCrow augment LLMs with expert-designed chemistry tools, enabling complex tasks in organic synthesis, drug discovery, and materials design [5]. Autonomous scientific agent systems integrate multiple LLMs with planners, allowing internet access, Python code execution, document retrieval, and physical experiment control, demonstrating sophisticated tool integration and multimodal interaction [5].
*   **Architectural Innovations and Multimodality**: Modifications to the LLM architecture can better capture domain-specific data characteristics. This includes using adapters, Graph Neural Network (GNN)-nested Transformers, and Mixture of Experts (MoE) Transformers, especially beneficial for scientific data with rich graph signals [31]. The concept of "sparse scaling" is also emerging as a promising direction to increase model parameters while maintaining training and inference costs . **Multimodal integration** is another key area, projecting graph or image modalities into a latent text space (e.g., GIT-Mol, LLaVA-Med) or using contrastive learning to align multiple encoders (e.g., CLIP-like architectures) [11,31].
*   **Reasoning and Decoding Enhancements**: Beyond CoT, techniques like Thought Anchors identify and measure key reasoning steps, while ALPHAONE offers a universal test-time framework for modulating reasoning. ProtoReasoning enhances LLM generalization via logic-based prototypes, and reinforcement learning approaches enable LLMs to provide intermediate answers, improving speed and accuracy for complex reasoning [29]. Decoding strategies have also evolved to include recent random sampling methods like contrastive sampling, and optimized decoding algorithms such as speculative decoding help mitigate the "memory wall" problem during incremental inference [9].
*   **Model Updating and Unsupervised Learning**: Challenges in updating LLM knowledge post-training are addressed by scalable frameworks like MEMOIR for lifelong model editing [29]. Unsupervised learning frameworks like Internal Coherence Maximization (ICM) aim to specify desired behaviors without human supervision, enhancing LLM adaptability [29].

In conclusion, the progression of LLMs from basic language generators to powerful scientific collaborators is intrinsically linked to these advanced techniques. Distributed training and optimization methods like mixed-precision training and memory management are foundational, allowing for the scaling to massive model sizes previously unattainable [30]. Simultaneously, advancements in long-context handling, sophisticated prompting (e.g., CoT, auto-prompting), and tool augmentation directly overcome limitations in reasoning, factual accuracy, and domain-specific problem-solving. Architectural innovations and multimodal integration further tailor LLMs for specialized scientific data. These coordinated enhancements address the computational burden [20] and inherent deficiencies of earlier LLMs, fostering greater reliability, versatility, and practical utility in diverse and demanding applications, especially within scientific discovery.
## 7. Understanding How Scientific LLMs Learn and Reason
This section delves into the fundamental mechanisms by which Scientific Large Language Models (LLMs) acquire, process, and apply knowledge, exploring the profound implications for their role in scientific discovery and research. 

![LLM Learning Mechanisms: Surface Statistics vs. Internal World Models](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/QaZ8l0MJzE6pBRGPwTmgc_LLM%20Learning%20Mechanisms%3A%20Surface%20Statistics%20vs.%20Internal%20World%20Models.png)

A central debate within the field concerns whether LLMs operate primarily by identifying statistical correlations and patterns within vast datasets—a "surface statistics" approach—or if they develop and utilize abstract, conceptual representations of the world, akin to "internal world models" [24]. Understanding this distinction is crucial for assessing their capabilities, limitations, and potential for genuine scientific understanding.

The "surface statistics" perspective posits that LLMs are sophisticated probabilistic engines, adept at predicting the next token based on learned frequencies and contextual patterns [20,25]. Phenomena like "hallucinations," where models generate plausible but factually incorrect information, are often cited as evidence of this reliance on statistical likelihoods over strict factual adherence [20]. This view suggests that while models can achieve high precision in linguistic tasks, their "reasoning" might be an elaborate mimicry rather than true comprehension [29].

In contrast, a growing body of evidence supports the notion that LLMs can indeed form and leverage "internal world models." Key research, such as the Othello-GPT experiment, provides compelling empirical demonstration. By training a GPT model solely on Othello game transcripts, researchers found that the model not only learned to predict legal moves with high accuracy but also developed an internal, geometrically structured representation of the Othello board state, which could be probed and visualized [24]. Crucially, this study further demonstrated that by directly manipulating these internal representations, the model's subsequent predictions could be consistently altered, indicating that these internal models actively govern the model's behavior and are not mere epiphenomena [24]. Beyond this, other developments like emergent abilities in larger LLMs for complex tasks, the generation of interpretable reasoning chains (e.g., Multimodal-CoT), and frameworks enabling LLMs to infer scientific patterns (e.g., LLM4SD) all point towards a capacity for deeper, structured understanding beyond simple statistical matching [3,5,22,31,32].

Building upon this understanding of how LLMs learn, the subsequent challenge and opportunity lie in the **interpretability and controllability of the scientific knowledge** embedded within these models [24]. If scientific LLMs are to become trustworthy and effective tools for researchers, it is imperative to move beyond their "black-box" nature [30]. The ability to interpret *why* an LLM makes a particular scientific prediction or inference, and to control its internal reasoning processes, is paramount for aligning model outputs with established scientific principles and ensuring robust generalization to novel scientific data [24].

The Othello-GPT study again serves as a paradigm, illustrating how internal representations can be both interpreted through probing and controlled via direct intervention in the latent space [24]. This allows for counterfactual analysis and the creation of "latent saliency maps," enhancing transparency beyond traditional input-feature attribution. In scientific contexts, frameworks like LLM4SD aim to generate interpretable feature vectors and simple rules to explain molecular property predictions, fostering trust among scientists [3]. Similarly, integrating symbolic reasoning engines with LLMs, as seen in Alpha-Geometry, provides a structured and verifiable component for logical deductions [31]. Approaches like Multimodal-CoT and "Thought Anchors" focus on generating and identifying explicit reasoning chains, thereby improving the understandability of an LLM's decision-making process [29,32]. While alignment tuning methods like Reinforcement Learning from Human Feedback (RLHF) contribute to controlling model behavior and ensuring safety, they primarily address output-level control rather than directly exposing or manipulating the internal scientific knowledge representations [17,30].

In essence, the progression from mere statistical pattern recognition to the formation of sophisticated, potentially manipulable internal world models represents a significant leap for scientific LLMs. The ongoing research trajectory seeks to enhance not only the depth of their understanding but also the transparency and steerability of their internal mechanisms, ultimately paving the way for more robust, trustworthy, and collaboratively intelligent AI systems in science.
### 7.1 Surface Statistics vs. Internal World Models
A fundamental debate within the field of large language models (LLMs) centers on their underlying learning mechanisms: do they merely operate by correlating data points through "surface statistics," or do they construct sophisticated, abstract representations akin to "internal world models" [24]? This question challenges our understanding of what LLMs truly "learn" and whether they develop genuine understanding or simply mimic linguistic patterns.

Arguments supporting the "surface statistics" perspective often highlight LLMs' probabilistic nature and limitations. For instance, the phenomenon of "hallucinations," where models produce plausible but factually incorrect information, is attributed to their tendency to predict the next word or phrase based on learned patterns rather than strict factual verification [20]. While the precision in discerning true statements improves with model scale, this behavior still underscores a reliance on statistical probabilities [20]. Early mathematical models encountered difficulties in understanding English due to inherent linguistic complexities, and traditional machine learning algorithms have long struggled with interpreting contextual and semantic components, suggesting a tendency towards surface-level processing [25]. Furthermore, diagnostics like perplexity, used in contexts such as assessing cognitive impairments in Alzheimer's Disease, focus on the statistical fit of a language model to a text sequence, reinforcing the view of LLMs as sophisticated statistical predictors [32]. The critique presented in Apple's "Illusion of Intelligence" paper further questions whether large reasoning models possess genuine human-like reasoning, implying their capabilities might be more of an elaborate mimicry than true internal understanding [29]. Indirectly, the dismissal of claims regarding LaMDA's "sentience" as "unfounded" reflects a prevailing academic skepticism towards LLMs possessing true consciousness or deep understanding [15].

In contrast, a growing body of evidence suggests that LLMs can indeed form and leverage internal world models. A pivotal study illustrating this is the Othello-GPT experiment [24]. This research directly addresses the debate by training a GPT model solely on Othello game transcripts—sequences of moves—without any explicit knowledge of the game's rules or board structure. The model's task was to predict the next legal move [24]. The trained Othello-GPT achieved a remarkably low error rate of 0.01% in making legal moves, a significant improvement over an untrained model's 93.29% error [24]. To investigate the formation of an internal world model, researchers employed a probing technique. They trained 64 independent two-layer Multi-Layer Perceptron (MLP) classifiers to predict the state of each of the 64 Othello board tiles using the Othello-GPT's internal representations as input. The error rates of these probes dramatically decreased from 26.2% on a randomly-initialized model to 1.7% on the trained model, strongly indicating the presence of an internal world model [24]. Furthermore, Principal Component Analysis (PCA) applied to "concept vectors" for each tile revealed an emergent geometry resembling an Othello board, described as a "draped cloth on a ball" [24].

Crucially, the Othello-GPT study went beyond merely detecting internal representations by demonstrating their active role in prediction through an intervention method. Researchers developed a technique to directly modify the model's internal activations, thereby altering its internal representation of the game state on the fly. By, for example, programmatically "flipping" a disc's color in the internal representation, they observed how the model's subsequent move predictions aligned with this *intervened* board state. The model achieved an average error of only 0.12 tiles in predicting legal moves for the manipulated state, even for "unreachable" board configurations, providing compelling evidence that these internal world representations are actively utilized for prediction and are not mere epiphenomena [24]. This suggests a "unified mid-stage representation" shared between the neural network's black-box operations and a human-understandable world model [24].

Beyond the Othello-GPT experiment, several other studies lend credence to LLMs learning beyond mere surface statistics. The notion that "the neural network learns some representation of the process that produced the text" implies that higher fidelity in next-word prediction corresponds to a more accurate internal representation of the underlying world generating the text [30]. The Multimodal-CoT approach exemplifies this by enabling LLMs to generate "intermediate reasoning chains" for complex tasks, suggesting a move towards structured, interpretable reasoning rather than simple statistical pattern matching [32]. The "emergent ability" of larger LLMs like ChatGPT to perform sentiment analysis from news headlines for stock price prediction, a task simpler models like GPT-1, GPT-2, and BERT cannot achieve, points to a deeper understanding of context and the potential formation of complex internal representations [5]. In scientific discovery, frameworks like LLM4SD demonstrate that LLMs can generate interpretable rules to explain molecular property predictions, synthesize scientific literature, and infer new, scientifically plausible patterns [3,22]. This capability indicates a form of "meaning" learning, where underlying scientific principles are captured, implying the formation of internal models reflecting scientific reality. Furthermore, applications in mathematics, such as Alpha-Geometry and FunSearch, showcase LLMs' capacity for structured problem-solving and the potential for internal representations of logical operations [31]. The development of models like K2 Think and OpenReasoning-Nemotron for "advanced AI Reasoning" and ProtoReasoning for enhancing generalization via logic-based prototypes further supports the idea of LLMs developing sophisticated, internal reasoning mechanisms beyond superficial statistical correlations [29]. The HILMA framework, inspired by human cognitive processes like divergent and convergent thinking, also seeks to instill more structured and innovative reasoning in LLMs, moving beyond simple pattern repetition [23]. Even in complex domains like biology, the aspiration for AI to "deconstruct the extreme complexity of life's language" suggests a pursuit of forming internal "biological world models," despite acknowledging the current limits in fully grasping such profound "meaning" [8].

In summary, while the debate regarding surface statistics versus internal world models is ongoing, the Othello-GPT study provides compelling empirical evidence for the formation of interpretable and controllable internal representations that actively guide LLM behavior in a controlled environment [24]. This, coupled with the emergent abilities, structured reasoning capabilities, and scientific discovery applications seen in other LLMs, indicates a trajectory where these models move beyond mere statistical correlations to develop more abstract and functional internal models of the world. However, the prevalence of phenomena like hallucinations suggests that while LLMs *can* form world models, their application and fidelity are not universally consistent across all tasks and domains, highlighting a nuanced interplay between statistical prowess and genuine internal understanding.
### 7.2 Interpretability and Controllability of Scientific LLM Knowledge
The ability of Large Language Models (LLMs) to form internal world models, particularly within scientific domains, holds significant implications for their utility, trustworthiness, and ultimately, their integration into scientific research workflows [24]. If these internal representations of scientific phenomena—such as material properties or biological processes—can be made interpretable and controllable, it profoundly enhances their application and reliability [24]. This capability is crucial for aligning LLM outputs with established scientific principles, ensuring robust generalization to novel scientific data, and empowering human scientists to understand and steer the models' "reasoning" in complex tasks [24].

A pervasive challenge across LLM development is the inherent "lack of interpretability," which impedes understanding the rationale behind model predictions [20,30]. This lack of transparency is a major limitation, especially when troubleshooting undesirable outcomes [10] or when developers possess only a superficial grasp of an LLM's functionalities during deployment [20]. For ethical alignment and maintaining coherence, the capacity to comprehend the model's inner workings is pivotal [20]. While some query-based interpretability exists, allowing inquiries into an LLM's reasoning for generated code snippets [10], a full internal model transparency remains elusive. Efforts like alignment tuning, including Reinforcement Learning from Human Feedback (RLHF), aim to control LLM output to align with human values and preferences, making them beneficial and harmless [17,30]. However, these methods primarily focus on output control rather than explicating or manipulating internal knowledge representations.

The Othello-GPT study provides a compelling demonstration of an LLM forming an "interpretable and controllable representation" of a game state, offering a blueprint for scientific applications [24]. Interpretability was achieved through probing techniques, which allowed for the prediction of the board state directly from the model's internal activations and visualization of its geometric organization [24]. More critically, controllability was demonstrated by an intervention technique: direct manipulation of the model's internal representation consistently altered its subsequent predictions to align with the new, intervened state [24]. This capability signifies that the internal model actively governs behavior, rather than being a passive artifact. Such intervention allows for counterfactual predictions, e.g., asking "what would the model predict if F6 were white?" even without a corresponding input sequence, which is directly translatable to scientific "what-if" scenarios [24]. Furthermore, "Attribution via Intervention" was introduced to create "latent saliency maps," attributing next-step moves to specific board tiles by comparing factual and counterfactual predictions [24]. This approach, focusing on the model's latent space, enhances interpretability beyond just input features and implies significant robustness for out-of-distribution scientific data, as a model with an internal world model is less prone to breakdown than one relying solely on surface statistics [24].

Several other approaches contribute to enhancing interpretability and controllability, albeit with varying degrees of direct application to *internal world models* of scientific phenomena. For instance, the LLM4SD framework generates interpretable feature vectors and employs simple rules to explain its analysis and predictions, fostering trust among scientists by providing transparency that contrasts with "black-box" verification tools [3]. Similarly, in Alpha-Geometry, the integration of symbolic reasoning engines alongside LLMs provides a structured and interpretable component for logical deductions, enhancing interpretability by grounding reasoning in formal logic [31].

To improve interpretability and controllability of reasoning processes, the Multimodal-CoT framework incorporates vision features within a decoupled training process, featuring a distinct "rationale generation" stage to produce "effective rationales" that mitigate "hallucinated reasoning chains" [32]. Complementary to this, "Thought Anchors" offer a machine learning framework for precisely identifying and measuring key reasoning steps within LLMs, moving towards more interpretable internal workings [29]. Efforts to control and guide the reasoning process are also underway with tools like ALPHAONE, a universal test-time framework for modulating AI model reasoning, which is crucial for scientific applications demanding robustness and alignment with human understanding [29]. While CTRL-LLM utilizes control codes as explicit instructions for governing text generation based on user parameters, allowing a form of explicit control over output style [25], this primarily addresses output-level control rather than the deeper manipulation of internal scientific representations exemplified by Othello-GPT.

Comparing these methods reveals a spectrum of interpretability and controllability. The Othello-GPT study showcases a deep, internal manipulation of a learned world model, demonstrating how intervention in latent space directly alters subsequent behavior, which is highly relevant for establishing scientific causality within LLMs [24]. Other approaches, such as LLM4SD's interpretable feature vectors [3] or Alpha-Geometry's symbolic reasoning [31], focus on generating understandable explanations or structuring reasoning, which enhances external transparency. The Multimodal-CoT and Thought Anchors methods aim to expose or construct clearer reasoning paths, thereby improving the understandability of *how* an LLM arrives at a conclusion [29,32]. While general alignment tuning methods (e.g., RLHF) improve safety and adherence to human values [17,30], they often do not provide direct insight into or control over the underlying internal models that generate scientific knowledge.

In summary, the development of interpretable and controllable internal representations in scientific LLMs, as illuminated by studies like Othello-GPT, signifies a critical advancement towards truly trustworthy AI in science [24]. This capability enables better alignment with complex scientific principles, provides mechanisms for human scientists to actively steer the LLM's "reasoning," and fosters more robust generalization to unprecedented scientific scenarios by moving beyond mere statistical correlations to a deeper, manipulable understanding of the world.
## 8. Applications of Scientific LLMs
Large Language Models (LLMs) are profoundly transforming the scientific research landscape, acting as powerful catalysts for accelerating discovery, enhancing existing processes, and fostering interdisciplinary collaboration across diverse scientific and social science domains [9,20]. Their inherent capacity to understand, process, and generate human-like text positions them as pivotal interfaces between human researchers and intricate scientific data or computational tasks. This integration signals a paradigm shift towards greater efficiency, automation, and autonomy in scientific endeavors, fundamentally reshaping how researchers approach various stages of their work.



![Broad Categories of Scientific LLM Applications](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/OCGBwbvSTPoCdct67kvQY_Broad%20Categories%20of%20Scientific%20LLM%20Applications.png)

The applications of Scientific LLMs can be broadly categorized into three interconnected areas: their utility across the scientific research lifecycle, their cross-cutting capabilities applicable to multiple scientific tasks, and their specialized deployment within distinct scientific domains. First, LLMs are being integrated throughout the **scientific research lifecycle**, offering sophisticated assistance in critical phases such as hypothesis discovery and generation, experiment planning and design, scientific writing and communication, and peer review and research evaluation. This includes streamlining literature analysis, generating creative ideas, and interpreting complex data to accelerate discovery processes [9,20]. For instance, they leverage both knowledge-driven and data-driven approaches for hypothesis formulation and assist in generating detailed experimental protocols and optimizing parameters, thereby providing preliminary guidance and automating complex procedural steps [19]. In scientific communication, LLMs augment productivity by streamlining literature reviews, drafting academic papers, and summarizing information, while also increasingly assisting in peer review through automatic review generation and support for human reviewers [13,19].

Second, LLMs offer powerful **cross-cutting applications** that enhance fundamental aspects of scientific work. These include sophisticated information extraction and synthesis from vast scientific literature, which is crucial for managing and making sense of the ever-growing body of scientific knowledge [20,30]. LLMs excel at foundational tasks like text summarization, classification, and sophisticated generation, transforming unstructured scientific text into structured knowledge [4,14]. Concurrently, LLMs are revolutionizing code generation and programming assistance, profoundly enhancing programming workflows, debugging processes, and the automation of experimental procedures across various scientific disciplines [10,20]. Their ability to generate code from natural language instructions and automate scientific tasks, such as generating robotic scripts for laboratory automation, democratizes access to advanced experimental and computational methods [5,22]. The synergy between these capabilities significantly boosts efficiency and holds promise for improving reproducibility through the standardization of methodologies [20,25].

Third, Scientific LLMs are meticulously adapted and deployed in highly **domain-specific applications**, extending their capabilities beyond general language tasks into specialized fields such as biomedical and healthcare, life sciences, drug discovery and development, chemistry and materials science, physical sciences (including Earth and environmental sciences), and economic, financial, and social sciences [16,19]. These specialized LLMs are often trained or fine-tuned on vast repositories of domain-specific literature, databases, and experimental data, enabling them to handle complex data formats like molecular linear representations (e.g., SMILES), macromolecular structures, genomic sequences, and multimodal scientific datasets [1,35]. This specialized training empowers them to accurately interpret domain-specific terminology, recognize unique scientific patterns, and generate outputs that adhere to rigorous scientific standards, fostering novel discoveries and streamlining complex research workflows.

Despite these significant advancements, several overarching challenges persist in the application of Scientific LLMs. Critical concerns include ensuring the **accuracy and factuality** of LLM-generated content, whether it be a literature summary, a hypothesis, or a piece of code, as current models are susceptible to hallucinations and inconsistencies in contextual analysis [13,19]. The **explainability and interpretability** of LLM outputs remain areas for substantial improvement, as understanding the reasoning behind a synthesized conclusion or a generated code block is paramount for maintaining scientific rigor and fostering trust [20,28]. Furthermore, **ethical considerations** such as potential biases in AI-generated content, issues of authorship, data privacy, economic harms (e.g., job displacement), and the necessity for robust ethical oversight are paramount, especially in sensitive areas like medical education, clinical decision support, and social science research [2,19]. The considerable **computational cost** associated with training and deploying large models, alongside their current limitations in effectively processing **long contexts**, presents practical constraints for their widespread integration into highly complex scientific workflows [28]. Addressing these challenges through continued research into more transparent models, improved verification techniques, and efficient architectural designs will be essential for fully realizing the transformative potential of LLMs in accelerating scientific discovery while ensuring responsible and ethical deployment, emphasizing human-AI collaboration over full automation.
### 8.1 LLMs in the Scientific Research Lifecycle
Large Language Models (LLMs) are profoundly transforming the scientific research lifecycle by acting as powerful catalysts for accelerating discovery, enhancing existing processes, and fostering interdisciplinary collaboration across diverse scientific and social science domains [9,20]. Their capacity for streamlined literature analysis, creative idea generation, and intricate data interpretation positions them as pivotal tools, moving beyond traditional applications to address emerging scenarios such as intelligent agents and automated evaluation [6,9,20]. This integration signals a paradigm shift towards greater efficiency and autonomy in scientific endeavors, fundamentally reshaping how researchers approach various stages of their work.



![LLMs Across the Scientific Research Lifecycle](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/Ll-GaKqbc2mfksOlZl11a_LLMs%20Across%20the%20Scientific%20Research%20Lifecycle.png)

Throughout the scientific research lifecycle, LLMs demonstrate their utility across several key phases. In the critical stage of **Hypothesis Discovery and Generation**, LLMs leverage both knowledge-driven (e.g., Literature-Based Discovery, inductive reasoning) and data-driven approaches (e.g., inferring rules from experimental data) to formulate novel scientific ideas. These capabilities are further enhanced by advanced methodologies and robust human-machine collaboration frameworks, ensuring the generated hypotheses are refined for quality, novelty, and validity. Moving into **Experiment Planning and Design**, LLMs assist in generating detailed experimental protocols, optimizing parameters, and even predicting potential outcomes, thereby providing preliminary guidance and automating complex procedural steps. Their ability to decompose tasks, apply advanced prompting techniques, and engage in reflective processes contributes to more efficient and structured experimental setups. Subsequently, in **Scientific Writing and Communication**, LLMs significantly augment researchers' productivity by streamlining literature reviews, drafting various sections of academic papers, generating accurate citations, and summarizing complex information. This extends to broader scientific communication, making complex concepts more accessible. Finally, LLMs are increasingly being integrated into **Peer Review and Research Evaluation** processes, both through automatic review generation and by assisting human reviewers with tasks such as paper summarization, claim validation, and review writing support. While LLMs offer immense potential, their deployment throughout the lifecycle necessitates careful consideration of ethical implications, factual accuracy, contextual coherence, and the ongoing critical role of human expertise and oversight.
#### 8.1.1 Hypothesis Discovery and Generation
Hypothesis discovery stands as a pivotal stage within the scientific research cycle, where Large Language Models (LLMs) are increasingly recognized for their transformative potential in generating novel scientific hypotheses [13,19]. This application area leverages LLMs' capabilities to synthesize knowledge, interpret data, and propose testable ideas across diverse scientific disciplines, aiming to accelerate the pace of scientific inquiry [31].



**Paradigms for LLM-Driven Hypothesis Generation**

| Paradigm                     | Description                                                 | Key Mechanisms / LLM Role                                 | Example Models / Frameworks                                 |
| :--------------------------- | :---------------------------------------------------------- | :-------------------------------------------------------- | :---------------------------------------------------------- |
| **Knowledge-driven Hypothesis Generation** | Leverages existing scientific literature & knowledge bases to identify hidden connections & infer rules. | Translates LBD (Literature-Based Discovery) into natural language; synthesizes vast textual data; applies inductive reasoning. | HILMA, SPIRES, DNA-BERT, Geneformer, ESM, MatSci-LLMs     |
| **Data-driven Hypothesis Generation** | Derives hypotheses directly from experimental observations and empirical data. | Performs "knowledge inference" from data (e.g., SMILES + experimental labels) to deduce novel patterns/rules; converts rules into interpretable features. | LLM4SD, Molformer, SyntheMol, NYUTron, Evo                |
| **Advanced Methodologies**   | Strategies to enhance the quality, novelty, and validity of generated hypotheses. | Inspiration Retrieval, Feedback Modules (novelty, validity, clarity checkers), Evolutionary Algorithms, Multi-inspiration Utilization, Hypothesis Ranking. | SciMON, ResearchAgent, MOOSE, FunSearch, HypoGeniC, SGA, Nova |
| **Human-Machine Collaboration** | Ensures reliability, relevance, and trustworthiness through iterative refinement and oversight. | Multi-agent debates with human arbitration, interpretability frameworks, integrating AI agents with external tools. | HILMA, LLM4SD, Stanford Virtual Lab, ChemCrow, CLARify    |

LLM-driven hypothesis generation fundamentally encompasses two primary paradigms: knowledge-driven and data-driven approaches, often complemented by sophisticated methodological enhancements and crucial human-machine collaboration.

**Knowledge-driven Hypothesis Generation** primarily relies on existing scientific literature and structured knowledge bases [13,19]. Rooted in Literature-Based Discovery (LBD) and inductive reasoning, this approach leverages LLMs to identify hidden connections and infer general rules from fragmented information. Traditional LBD, such as Swanson's "ABC" model, identified associations between concepts based on shared intermediates. LLMs significantly advance this by translating LBD into a natural language framework, articulating hypotheses as coherent sentences and synthesizing vast textual data to generate novel scientific ideas [19,31]. Frameworks like HILMA integrate current research by building structured citation networks for LLM-driven review generation and hypothesis formulation [23]. Similarly, SPIRES employs zero-shot learning to extract structured information from medical texts for knowledge base population, facilitating hypothesis generation by identifying relationships from organized literature [5]. LLMs also apply inductive reasoning to infer broad "rules" from specific "observations," ensuring hypotheses are consistent, reflect reality, and possess generality [19]. Specialized scientific LLMs, including DNA-BERT, Geneformer, ESM, and MatSci-LLMs, further demonstrate this capability by interpreting complex domain-specific information to facilitate hypothesis generation in fields like genomics, proteomics, and materials science [7,26].

In contrast, **Data-driven Hypothesis Generation** empowers LLMs to derive hypotheses directly from experimental observations and empirical data, often in conjunction with existing knowledge [13,19]. The LLM4SD framework exemplifies this, performing "knowledge inference" by processing molecular data (e.g., SMILES strings with experimental labels) to deduce novel patterns or rules. For instance, it can infer that "molecules containing halogens are more likely to cross the blood-brain barrier" [3,22]. These inferred rules are then converted into interpretable feature vectors, like $$x_1=I(\text{rdMolDescriptors.CalcExactMolWt(mol)}<500)$$, to train machine learning models, thereby enhancing predictive accuracy and interpretability [22]. LLM4SD achieves optimal performance by synergistically combining both knowledge synthesis (from literature) and knowledge inference (from data), demonstrating state-of-the-art results in molecular property prediction tasks [3,22]. Beyond molecular science, this approach extends to materials science for experiment design and property prediction, drug discovery for candidate screening and novel antibiotic identification, and genomics for predicting genetic variant effects and transcription patterns [5,26]. This paradigm directly leverages experimental results to form novel observations and general hypotheses, distinguishing itself from purely literature-based synthesis [19].

To further enhance the quality, novelty, and validity of LLM-generated hypotheses, **Advanced Methodologies and Human-Machine Collaboration** are increasingly integrated [19]. Advanced components include: (1) **Inspiration Retrieval Strategies**, which use techniques like Latent Semantic Analysis or LLM-selected candidates to spark novel ideas from existing knowledge [6,19]. (2) **Feedback Modules**, which iteratively refine hypotheses via novelty, validity, and clarity checkers. While novelty and clarity are often assessed by LLMs, true validity remains a challenge, often relying on heuristic models or awaiting automated experimental verification [13,19]. (3) **Evolutionary Algorithms** optimize hypotheses by treating experimental evaluations as environments for selection and mutation [19]. (4) **Multi-inspiration Utilization** decomposes complex problems, especially in fields like chemistry, by drawing from multiple sources [19]. (5) **Hypothesis Ranking** becomes crucial for prioritizing hypotheses, utilizing LLM scoring, direct code execution, or pairwise comparisons to manage the high cost of validation [19]. (6) **Automatic Research Question Construction** represents a shift towards autonomous discovery by enabling LLMs to generate research questions themselves [19].

Central to these advancements is the emphasis on **Human-Machine Collaboration**, which is paramount for ensuring reliability, relevance, and trustworthiness in generated hypotheses [23]. Frameworks like HILMA propose multi-agent debates arbitrated by human experts to iteratively refine initial hypotheses, leveraging human creativity and decision-making alongside LLMs' data processing capabilities [23]. The interpretability offered by frameworks like LLM4SD, which provides understandable explanations for its predictions, fosters trust and allows human scientists to critically evaluate and refine AI-driven insights [3]. Furthermore, agent systems such as the Stanford Virtual Lab and ChemCrow integrate specialized AI agents and external tools, implicitly embedding human expertise in their design to automate experimental planning and execution, while human oversight remains critical for ethical considerations and directional guidance [5,8]. This synergistic approach ensures that LLMs serve as powerful accelerators for human scientific endeavors, augmenting rather than supplanting human critical thinking and creativity.

Despite these significant strides, challenges persist, notably the inherent difficulty in experimental verification of generated hypotheses and the fundamental reliance on the evolving capabilities of current LLM architectures [13]. The systematic evaluation of hypothesis generation relies on benchmarks categorized as literature-based and data-driven, with key metrics including novelty and validity [13]. Future research directions will likely focus on developing more robust validation mechanisms, enhancing the clarity and detail of LLM-generated hypotheses, and further integrating sophisticated human-in-the-loop and multi-agent systems to refine AI's capacity for scientific discovery.
##### 8.1.1.1 Knowledge-driven Hypothesis Generation (LBD, Inductive Reasoning)
Knowledge-driven hypothesis generation, particularly through the lens of Literature-Based Discovery (LBD) and inductive reasoning, represents a pivotal application area for Large Language Models (LLMs) in scientific research [19]. The foundational concept of LBD, originally proposed by Swanson, posits that novel insights reside within fragmented pieces of knowledge that, when retrieved, summarized, and interpreted collectively, can reveal previously undiscovered connections [19]. The classical "ABC" model of LBD identifies an association between concepts A and C if both are linked to a common intermediate concept B, albeit in separate literature sources [19].

Traditionally, LBD methods were constrained by their focus on predicting pairwise relationships and their inability to incorporate the rich natural language context inherent in human scientific inquiry [19]. LLMs critically enhance this paradigm by addressing these limitations, translating LBD into a natural language framework where hypotheses are articulated as coherent sentences rather than mere relational links [19]. This evolution signifies a shift from discrete concept prediction towards more nuanced and expressively rich hypothesis generation [19]. LLMs leverage their capacity to synthesize and interpret vast textual information, extending traditional LBD capabilities to generate novel scientific ideas [31].

Several frameworks demonstrate the efficacy of LLM-augmented LBD. The HILMA framework, for instance, systematically integrates current scientific research into LLMs by constructing structured subgraph citation networks. It then employs LLMs to generate high-level reviews for these networks, establishing a comprehensive and up-to-date knowledge base. From this base, LLMs formulate initial scientific hypotheses, represented as $h_0 = LLM(P, G_t, s_t)$, where $P$ is the prompt, $G_t$ is the selected target citation subgraph, and $s_t$ is its research review [23]. This approach mitigates the issue of LLMs relying on outdated or incomplete knowledge, grounding generated hypotheses in the latest scientific advancements [23]. Similarly, SPIRES employs zero-shot learning to recursively extract structured information from medical texts, populating knowledge bases with insights into cell signaling pathways and disease mechanisms. This process facilitates knowledge-driven hypothesis generation by enabling LLMs to identify potential relationships or rules from organized literature [5].

Beyond LBD, LLMs are increasingly applied to inductive reasoning in scientific contexts, which involves inferring general "rules" or "hypotheses" with broad applicability from specific "observations" [19]. Philosophical criteria for sound scientific hypotheses typically include: (1) consistency with observations, (2) reflection of reality, and (3) generality to encompass new information beyond initial observations [19]. Yang's pioneering work in applying inductive reasoning to natural language processing highlighted an additional requirement for LLM-generated rules: clarity and detail, due to the models' propensity for generating vague statements [19].

Methods for generative inductive reasoning often involve an "over-generate-then-filter" mechanism, where LLMs initially produce a multitude of preliminary rules, which are subsequently filtered based on their adherence to the established philosophical and practical requirements [19]. For example, LLM4SD synthesizes knowledge from its pre-trained scientific literature to retrieve and apply known rules, such as deducing that molecules with a molecular weight below 500 Da are more likely to pass the blood-brain barrier (BBB) [3,22]. This capability provides theoretical support and contextualization for prediction tasks, surmounting limitations of traditional methods lacking natural language understanding [22]. Specialized scientific LLMs like DNA-BERT, Geneformer, and ESM further demonstrate this capability by decoding genetic language, analyzing gene networks, and predicting protein properties, thereby facilitating knowledge-driven hypothesis generation by interpreting complex biological information [26]. MatSci-LLMs are similarly envisioned to be grounded in domain knowledge to facilitate hypothesis generation within materials science [7]. The Stanford Virtual Lab integrates advanced models to infer and combine knowledge for designing novel nanobodies and predicting biological impacts, leading to new hypotheses [8].

While LLMs show considerable promise in producing novel and significant scientific rules, their current state faces challenges. Strategies such as inspiration retrieval and feedback modules are employed to improve hypothesis quality [13]. Evaluation benchmarks are categorized into literature-based and data-driven approaches, assessing metrics like novelty and validity [13]. However, significant limitations include the inherent difficulty in experimental verification of generated hypotheses and the fundamental reliance on the evolving capabilities of current LLM architectures [13]. Despite these challenges, applications such as autonomous discovery of social science hypotheses from public web data by LLMs, as demonstrated by Yang, underscore their potential to generate robust and verifiable scientific insights [19]. The integration of knowledge graphs and intelligent Q&A systems further exemplifies knowledge-driven LLM applications, as seen in agriculture [11].
##### 8.1.1.2 Data-driven Hypothesis Generation (LLM4SD)
Large Language Models (LLMs) are increasingly instrumental in driving hypothesis generation directly from scientific data, offering a powerful paradigm shift from purely literature-based discovery. This data-driven approach synergistically combines insights gleaned from experimental observations with existing literature knowledge, thereby fostering novel scientific hypotheses [19].

A prominent example of this methodology is the LLM4SD framework, which explicitly focuses on "data-driven discovery" in molecular science [22]. LLM4SD's core mechanism, termed "knowledge inference," processes molecular data, typically in the form of SMILES strings, along with their corresponding experimental labels [3,22]. From this empirical data, the LLM deduces novel patterns or rules. For instance, it can infer specific relationships such as "molecules containing halogens are more likely to cross the blood-brain barrier" [3,22]. These inferred rules are then converted into executable RDKit functions, like $$x_1=I(\text{rdMolDescriptors.CalcExactMolWt(mol)}<500)$$, to construct interpretable feature vectors [22]. Subsequently, these feature vectors are utilized to train standard machine learning models, such as random forests, for various prediction tasks [3,22]. This integration with traditional machine learning models enhances both predictive accuracy and interpretability, as the importance of the inferred rules can be quantified, thereby forming the basis for new scientific hypotheses [22].

The advantages of the LLM4SD framework are evident in its performance and interpretability. When combining both knowledge synthesis (from literature) and knowledge inference (from data), LLM4SD achieves optimal performance. For instance, in physiology tasks, it yielded an AUC-ROC of 73.62%, significantly outperforming synthesis alone (70.46%) and inference alone (69.25%) [22]. Furthermore, LLM4SD demonstrated state-of-the-art performance across 58 benchmark tasks in molecular property prediction from the MoleculeNet dataset, covering diverse domains such as physiology, biophysics, physical chemistry, and quantum mechanics [3]. Notably, it improved accuracy by 48% in predicting critical quantum properties relevant for material design [3].

Beyond molecular science, the application of LLMs for data-driven hypothesis generation spans various scientific disciplines. In materials science, LLMs are employed to design experiments for predicting the electrical performance of solar cells and to develop materials or devices tailored to specific target parameters [5]. These LLMs often achieve performance comparable to traditional machine learning methods without requiring extensive feature selection, underscoring their capability to infer patterns and guide experimental design directly from data [5]. In drug discovery, models like Molformer leverage large-scale molecular property prediction for screening drug candidates based on pharmacokinetic properties [26], while generative models such as SyntheMol have screened billions of compounds to identify novel antibiotics, demonstrating robust data-driven discovery of molecules with desired properties [8]. In clinical settings, platforms like NYUTron, based on BERT models trained on Electronic Health Records (EHR) data, predict patient outcomes such as in-hospital mortality and 30-day readmission rates, showcasing effective data-driven prediction in healthcare [26]. Similarly, in genomics, models like Evo, trained on vast quantities of DNA bases, can predict genetic variant effects and generate new DNA sequences, and the General Expression Transformer (GET) predicts RNA transcription patterns from chromosomal accessibility, providing data-driven insights into gene regulation [8].

This data-driven approach contrasts sharply with purely literature-based discovery, which primarily relies on synthesizing existing published knowledge. Majumder introduced the concept of "data-driven discovery" to address the underutilization of vast amounts of publicly available experimental data, which holds immense potential for uncovering new scientific hypotheses [19]. Unlike literature synthesis that re-contextualizes established findings, data-driven discovery, particularly relevant for inductive reasoning tasks, directly leverages experimental results to form novel observations and general hypotheses [19]. Benchmarks like DiscoveryBench and DiscoveryWorld have been established to evaluate this capacity, providing tasks where the input includes a research question and experimental data, aiming to produce data-supported structured hypotheses [19]. The unique contribution of LLM-powered data-driven frameworks like LLM4SD is their ability to not only process and understand experimental data but also to infer actionable, interpretable rules directly from it, which can then be rigorously tested and integrated with existing scientific knowledge to accelerate the pace of scientific discovery.
##### 8.1.1.3 Advanced Methodologies and Human-Machine Collaboration for Hypothesis Refinement
The landscape of scientific discovery is being reshaped by advanced methodological components within Large Language Models (LLMs) and innovative human-machine collaboration frameworks, which collectively aim to enhance the quality, novelty, and validity of LLM-generated hypotheses [19]. These methodologies are integrated into coherent pipelines to facilitate robust scientific inquiry, each contributing uniquely to the overall process [19].

A foundational component involves **Inspiration Retrieval Strategies**, which are crucial for formulating novel hypotheses by systematically leveraging existing knowledge [19]. Various approaches exist: SciMON, for instance, utilizes Latent Semantic Analysis (LSA) concepts, retrieving semantically similar knowledge via SentenceBERT embeddings, knowledge graph neighbors, and citation graph neighbors to spark inspiration, with a focus on novelty [6,19]. ResearchAgent employs the "ABC" model to construct concept maps and identify co-occurring concepts, while Scideator retrieves inspiration papers through semantic matching (using the Semantic Scholar API) and concept matching across different subfields [19]. SciPIP further refines this by integrating semantic similarity, concept co-occurrence, and citation graph analysis, complemented by methods to filter irrelevant concepts [19]. In contrast, MOOSE, MOOSE-Chem, and Nova exemplify a trend towards LLM-selected inspirations, where the models themselves choose candidates based on a given research background, leveraging their vast internal knowledge acquired from training on scientific literature [19]. These strategies collectively underscore the diversity in knowledge integration, ranging from explicit graph-based traversals to implicit LLM-internal knowledge harnessing.

Following inspiration retrieval, **Feedback Modules** play an iterative role in refining hypotheses for novelty, validity, and clarity, first proposed by MOOSE [19]. **Novelty Checkers** ensure hypotheses are new compared to existing literature, employing methods such as LLM-based evaluation (MOOSE), iterative retrieval of related papers (SciMON, SciAgents, Scideator, CoI), or direct utilization of LLM internal knowledge (Qi, ResearchAgent, AIScientist, MOOSE-Chem, VirSci) [19]. **Validity Checkers** aim to ensure hypotheses accurately reflect the objective world. While true validity often necessitates experimental results, current methods primarily rely on LLMs or other trained neural models heuristically due to cost and time constraints. Notable exceptions include FunSearch, which verifies code generation through compilers; HypoGeniC and LLM-SR, which use data-driven consistency with observations; and SGA, which employs virtual physical simulation. The progression towards automated wet-lab experiments and advanced auto-coding systems represents a critical future direction [19]. Lastly, **Clarity Checkers** ensure hypotheses are well-articulated and sufficiently detailed, with current methods (MOOSE, ResearchAgent, MOOSE-Chem, VirSci) relying on LLMs for self-assessment of clarity [19].

**Evolutionary Algorithms** introduce an optimization layer, treating experimental evaluations as environments for selection and mutation. FunSearch and LLM-SR leverage island-based evolutionary algorithms, where "islands" of similar methods mutate and recombine. SGA incorporates "evolutionary search" and "evolutionary crossover" with LLMs, while MOOSE-Chem designs an "evolutionary unit" that associates background and inspiration knowledge, generates multiple hypotheses, refines them, and recombines them for a cohesive output [19]. The concept of **Multi-inspiration Utilization**, first introduced by MOOSE-Chem, is particularly vital in complex fields like chemistry and materials science, where complete hypotheses often require multiple sources of inspiration. This approach decomposes complex problems into smaller, executable steps, aiming to re-discover high-impact published hypotheses. Nova further utilizes sequential retrieval of multiple inspirations to generate more diverse and novel hypotheses by varying input context [19].

Given the LLMs' capacity to generate numerous hypotheses, **Hypothesis Ranking** becomes essential for comprehensive sorting, especially considering the high cost of physical validation. Methods include LLM scoring as a reward (MCR, AIScientist, MOOSE-Chem, CycleResearcher), direct code execution (FunSearch), fine-tuning graph neural networks for rewards (ChemReasoner), or consistency with observations (HypoGeniC, LLM-SR) [19]. Interestingly, IGA and CoI adopt pairwise comparisons (Idea Arena) due to LLMs' observed superior accuracy in relative judgments compared to direct scoring [19]. Finally, **Automatic Research Question Construction** represents a significant shift, moving LLM systems from a "copilot" to a "fully autonomous driving" mode [19]. MOOSE pioneered this by using LLM-based agents to continuously search web corpora for interesting questions, while other systems like AIScientist, MLR-Copilot, SciAgents, Scideator, VirSci, and Nova employ varied approaches ranging from analyzing research gaps to generating seed ideas directly from input papers or concept pairings [19]. These components, when integrated, form a sophisticated pipeline, where inspiration retrieval seeds ideas, feedback modules refine them, evolutionary algorithms optimize, multi-inspiration enriches, ranking prioritizes, and automatic question generation propels the discovery cycle forward. The "workflow of a MatSci-LLM-based materials discovery cycle" also implies structured methodologies for hypothesis refinement, though specific components are not detailed [7].

Beyond algorithmic components, the critical role of human-in-the-loop and collaborative multi-agent frameworks is paramount for scientific hypothesis generation, particularly in enhancing reliability, relevance, and trustworthiness [23]. The HILMA framework, inspired by Guilford's theory of structured intelligence, proposes an innovative human-machine collaborative strategy utilizing a multi-agent debate mechanism for deep refinement and optimization of scientific hypotheses [23]. This iterative process involves: 1) **Role Allocation**, assigning agents roles like proposer, reviewer, or neutral analyst; 2) **Initial Hypothesis Generation** from knowledge-enhanced LLMs; 3) An **Open-ended Debate**, where agents argue for or against hypotheses, guided by human experts; 4) **Evidence Integration**, where valid points modify hypotheses, and human experts act as arbiters, querying LLMs for further evidence; and 5) An **Iterative Loop** until predefined scientific rigor and innovation standards are met [23]. This collaboration leverages human experts' creative thinking and high-level decision-making with LLMs' efficient data processing, ensuring scientifically sound and innovative hypotheses. Experimental results demonstrate HILMA's significant outperformance against baseline models across metrics like creativity, practicality, and theoretical basis, achieving an overall score of 4.10, indicating "high scientific value and practical significance" for its generated hypotheses [23]. Human experts provide crucial directional guidance, correcting deficiencies in LLM-generated ideas, thereby notably improving initial hypotheses [23].

Similarly, LLM4SD enhances traditional machine learning models by providing synthesized knowledge and generating interpretable explanations for its predictions, fostering trust and collaboration between AI and human scientists [3]. This interpretability promotes human oversight and refinement of hypotheses, ensuring AI-driven predictions are both reliable and accessible [3]. The `agent` system, combining multiple LLMs with a core planner, exemplifies advanced methodologies for automated scientific discovery by autonomously designing, planning, and executing experiments, while accessing external resources [5]. ChemCrow further augments LLMs for chemistry research by integrating 13 expert-designed tools, enabling more complex task execution and hypothesis refinement through tool-augmented capabilities [5]. The CLARify system, utilizing GPT-3 to generate executable experimental plans for chemical robots, showcases AI's potential to automate and refine scientific processes, implying a collaborative refinement where LLMs handle intricate tasks traditionally performed by humans [26]. In specific applications, human authors have actively reviewed and rewritten ChatGPT-generated content in drug discovery review articles to ensure factual accuracy and scientific rigor, highlighting a crucial human-machine collaboration in meeting scientific standards [5]."

These collaborative models consistently balance automation with human judgment. While LLMs excel at generating ideas, retrieving knowledge, and performing initial checks, human experts provide critical thinking, domain expertise, and ethical oversight. For instance, in HILMA, human experts actively guide debates and arbitrate evidence, adjusting the depth and breadth of the discussion and querying LLMs for specific support or refutation [23]. This direct intervention ensures that LLM-generated outputs are continually scrutinized and aligned with scientific principles. LLM4SD's focus on interpretable explanations also empowers human scientists to understand the AI's reasoning, facilitating informed decisions and preventing blind acceptance of automated results [3]. The emphasis on "teacher-AI collaboration to balance automation with human oversight" and "human-in-the-loop approaches" in educational settings [25] equally applies to scientific research, ensuring that AI complements, rather than supplants, human critical thinking and creativity. Even in highly autonomous systems like the Stanford Virtual Lab, where five specialized AI agents design and validate nanobodies with minimal human intervention, the human expertise is embodied in the initial design and ongoing refinement of these expert agents [8]. Similarly, LLMs can synthesize knowledge to identify trends and gaps, supporting hypothesis refinement by providing comprehensive overviews that human researchers then interpret and act upon [2]. The capacity of LLMs to find expert reviewers for articles and provide feedback further integrates them into the broader research ecosystem, complementing human-driven quality control [31].

In summary, advanced methodologies for hypothesis refinement leverage sophisticated LLM-based components such as diverse inspiration retrieval strategies, iterative feedback modules for novelty, validity, and clarity, evolutionary optimization algorithms, multi-inspiration utilization, and intelligent hypothesis ranking [19]. These are increasingly complemented by human-machine collaboration frameworks, exemplified by HILMA, which integrate human creativity and critical judgment directly into the iterative refinement process [23]. This synergy, where LLMs handle vast data processing and idea generation, while humans provide high-level guidance, ethical oversight, and validation, leads to more robust, reliable, and innovative scientific discoveries. The trend is towards systems that embed human expertise and facilitate clear oversight, ensuring that AI serves as a powerful accelerator for human scientific endeavors rather than a replacement for human intellect and wisdom.
#### 8.1.2 Experiment Planning and Design
Large Language Models (LLMs) are increasingly demonstrating nascent but impactful applications in assisting with scientific experiment planning and design, fundamentally transforming research workflows towards greater efficiency and autonomy [19]. These applications span generating detailed experimental protocols, suggesting optimal parameters, and even predicting potential outcomes, thereby providing preliminary guidance for researchers [19].

A primary area of LLM contribution is the generation of experimental protocols and procedures. Agent systems, often driven by multiple LLMs, are capable of autonomously designing and planning complex scientific experiments, such as formulating synthesis procedures for ibuprofen or catalyzing cross-coupling reactions [5]. In chemistry, the CLARify system, built upon GPT-3, excels at generating experimental plans that are directly executable by chemical robots, showcasing higher accuracy compared to baseline systems and significantly boosting the efficiency of automated chemical experimentation [26]. Similarly, GPT-4 can translate natural language instructions into robotic scripts for biological experiments, automating laboratory processes like liquid handling and lowering the barrier for researchers without specialized robotics programming knowledge [5]. Beyond specific protocols, LLMs assist in broader workflow design and scientific code generation, as exemplified by `BioPlanner` for automatic evaluation of LLMs in biological protocol planning, and their utility in causal discovery experiment design and developing protocols for self-driving laboratories [6].

LLMs also contribute to optimizing experimental parameters and predicting results. In materials science, these models are utilized to design experiments aimed at predicting the electrical performance of solar cells and for tailoring materials or devices to meet specific target parameters [5]. While not explicitly "experiment planning" in the scientific sense, LLMs also enhance "decision-making" in agricultural production, which infers a role in optimizing agricultural strategies akin to experimental designs [2,11]. The Stanford Virtual Lab further extends this by automating the design of SARS-CoV-2 nanobodies and integrating experimental validation, suggesting LLMs can guide not just design but also the verification phase, similar to SyntheMol's approach for antibiotics [8].

To enhance their capabilities in experimental planning, LLMs employ several advanced techniques [19]:
*   **Task Decomposition**: LLMs simplify intricate experimental problems by breaking them into smaller, manageable subtasks, defining conditions, and specifying outputs. Notable examples include HuggingGPT for parsing user queries into structured task lists, CRISPR-GPT for automating gene-editing experiment design, and ChemCrow, which uses an iterative reasoning loop ("thought, action, action input, observation") for dynamic planning [19]. Multi-LLM systems like Coscientist and LLM-RDF further leverage specialized agents to extract methodologies, convert natural language protocols, generate execution code for automated platforms, and enable self-correction during execution [19]. CRISPR-GPT, in particular, demonstrates the efficacy of augmenting LLM agents with domain knowledge to enhance the design process of CRISPR-based gene editing experiments [31].
*   **Advanced Prompting**: Techniques such as in-context learning, Chain-of-Thought (CoT), and ReAct are commonly employed to improve the reliability and accuracy of LLM-assisted experimental planning, enabling more structured and logical reasoning [19].
*   **Reflection and Improvement**: LLMs can continuously evaluate and refine experimental plans by simulating expert discussions, challenging assumptions, and iteratively improving outputs. This mirrors the real-world scientific process where critical analysis and consensus-building drive deeper exploration [19].

Despite these advancements, the application of LLMs in experimental planning faces significant challenges. Current LLMs exhibit limitations in their planning capabilities, are susceptible to hallucinations, and often lack sufficient adaptability to highly specific experimental scenarios [13]. The current stage of research primarily focuses on generating initial designs and automating certain process steps, rather than providing comprehensive, concrete, and actionable guidance that encompasses all unforeseen experimental complexities [19]. Future opportunities lie in overcoming these limitations by developing more robust planning algorithms, enhancing domain-specific knowledge integration, and improving their ability to adapt to dynamic experimental conditions. Integrating explicit hypothesis testing within LLM frameworks, as implied by MatSci-LLMs, represents a promising avenue for more rigorous and scientifically sound experimental design [7]. The continuous evolution towards multi-LLM agent systems capable of self-correction and iterative refinement suggests a path towards more autonomous and reliable experimental design in the future.
#### 8.1.3 Scientific Writing and Communication
Large Language Models (LLMs) are profoundly transforming scientific writing and communication by offering robust assistance across various stages of the research lifecycle, from initial literature review to final manuscript preparation [19]. This integration promises significant productivity gains for researchers by automating arduous tasks and enhancing the clarity and coherence of scientific discourse. However, it also introduces critical ethical considerations concerning authorship, originality, potential biases in AI-generated content, factual accuracy, and contextual coherence [13,19].

LLMs are increasingly utilized for drafting scientific papers, generating citations, and summarizing related work [13]. For **literature review and summarization**, LLMs can synthesize vast amounts of scientific literature into organized formats, acting as a precursor to writing [6]. Tools such as `ArxivDIGESTables` (Newman et al., 2024.10) facilitate the synthesis of scientific literature into tabular formats, while `LitLLM` (Agarwal et al., 2024.02) provides a toolkit for scientific literature review [6]. Similarly, Iris.ai leverages AI to summarize relevant scientific papers, thereby expediting the research process and reducing manual effort in literature synthesis [20]. This capability extends to complex medical documentation, where LLMs like GPT-3 have been used to automatically generate discharge summaries and coherent summaries from clinical notes, improving efficiency and quality in clinical settings [26]. These advancements represent a significant improvement over traditional multi-document summarization models by processing longer inputs and offering richer contextual understanding [19].

In **generating related work sections**, LLMs excel at rapidly scanning scientific publication datasets to produce initial drafts [19]. To mitigate common issues such as hallucinations, techniques like Retrieval Augmented Generation (RAG) are applied in systems like `LitLLM` and `HiReview`, which enhance factual accuracy by incorporating content from external sources [19]. Nishimura's approach further integrates LLMs to emphasize novelty statements, effectively comparing new research with existing methodologies to highlight unique contributions [19].

**Citation text generation** is another area where LLMs demonstrate capabilities. They can generate accurate text summaries for papers to be cited based on the context of the citing paper [19]. Various methods have been developed, including pointer-generator networks (Xing), LLMs prompted to describe relationships between papers (Li & Ouyang), multimodal approaches combining citation network structure with text (AutoCite, BACO), and fine-tuning LMs with user-specified citation intent or keywords (Gu & Hahnloser, Jung) [19].

For the **general drafting and writing of scientific papers**, LLMs can generate various sections, from specific elements to entire manuscripts [13,19]. Examples include `BioGPT`, which is specifically designed for biomedical text generation, producing fluent descriptions for biomedical terms [32]. LLMs can generate controllable complexity scientific definitions (August) and automate captions for scientific figures (SCICAP) [19]. More comprehensive systems, such as `PaperRobot`, employ incremental drafting methods to organize and draft paper sections based on user input [19]. Collaborative frameworks like `CoAuthor` involve human-computer interaction, where LLMs assist authors with suggestions, illustrating a hybrid approach to manuscript creation [19]. The pursuit of fully autonomous writing is also being explored, with studies examining LLMs' ability to generate complete research papers from data analysis to final draft (Ifargan) and autonomously write comprehensive surveys by synthesizing and organizing existing research (AutoSurvey) [19]. Some LLMs, like ChatGPT, are also recognized for their general capability to "write papers" [3,21], and systems such as `AI Scientist` and `CycleResearcher` contribute to the entire scientific process, including hypothesis generation and experimental design, alongside paper drafting [19].

Beyond manuscript drafting, LLMs enhance **broader scientific communication**. They can generate non-technical content to aid in broader dissemination of scientific concepts [10] and improve agricultural extension services by simplifying scientific knowledge and overcoming language barriers, thus enhancing the communication of scientific findings to farmers [2]. Furthermore, LLMs can provide intelligent Q&A systems for delivering precise knowledge and decision support, as seen in agricultural applications [11].

Despite these advantages, the deployment of LLMs in scientific writing is accompanied by notable challenges. Concerns regarding **factual accuracy and contextual coherence** are paramount, directly impacting the reliability of AI-generated content [13]. These issues raise significant questions about academic integrity and necessitate rigorous human oversight. While LLM-generated text can serve as a starting point, human authors remain essential for conducting thorough information veracity reviews, assessing text generation capabilities, and rewriting manuscripts to balance AI output with established scientific standards [5]. The lack of specific metrics or detailed experimental setups in some discussions of LLMs' capabilities in generating research paper parts [6,20] underscores the need for more empirical validation to fully understand their performance and limitations in specialized scientific writing tasks.
#### 8.1.4 Peer Review and Research Evaluation
Large Language Models (LLMs) are progressively being integrated into the peer review process to address longstanding challenges such as review bias, inconsistent evaluation standards, and the substantial workload placed on human reviewers [19]. This integration is not merely theoretical; prominent computer science conferences, exemplified by ICLR 2025, have already begun adopting LLM-assisted review practices [19]. The application of LLMs in this domain extends to various tasks, including providing feedback, verifying claims, and generating comprehensive reviews [6,31].

The emerging role of LLMs in peer review can be broadly categorized into two primary approaches: automatic review generation and LLM-assisted review workflows, each with distinct aims and methodologies [13,19].

The **automatic review generation** paradigm seeks to streamline scientific evaluation by enabling LLMs to independently analyze research papers and produce comprehensive reviews with minimal human intervention [19]. These systems are designed to evaluate critical aspects such as methodology validation, result verification, and contribution assessment [19]. This approach encompasses both single-model and multi-model architectures. Single-model methods include CGI2, which employs a phased review process from key opinion extraction to summary generation; CycleReviewer, which utilizes reinforcement learning for end-to-end review generation; and ReviewRobot, which constructs knowledge graph-based structured reviews [19]. More advanced multi-model architectures, such as Reviewer2, SEA, and MARG, leverage specialized models to manage different facets of the review process, thereby improving complexity handling, enhancing review quality, and addressing challenges associated with processing lengthy paper contexts [19]. Furthermore, models like GPT-4 have demonstrated capabilities in generating useful feedback on research papers, promoting the potential for automated review generation [31]. The Stanford Virtual Lab's multi-agent system, which incorporates a "Scientific Reviewer" agent, also exemplifies efforts towards automated or semi-automated evaluation of scientific findings [8].

In contrast, **LLM-assisted review workflows** are designed to augment human reviewers rather than replace them, acknowledging the enduring criticality of human judgment and domain expertise in academic evaluation [19]. In this approach, LLMs serve as supplementary tools for various tasks, including paper summarization, reference verification, and consistency checks, thereby freeing human experts to focus on critical assessment and judgment [19]. Key functions within these workflows include:
*   **Information Extraction and Summarization**: Systems like PaperMage integrate natural language processing (NLP) and computer vision for analyzing visually-rich documents, while CocoSciSum offers customizable paper summaries to aid reviewer comprehension [19].
*   **Manuscript Validation and Quality Assurance**: This category includes tools operating at different analytical granularities. ReviewerGPT focuses on local-level systematic error detection and guideline compliance, whereas PaperQA2 performs global validation by cross-referencing claims against broader scientific literature [19]. Scideator, on the other hand, assists in validating ideas and checking for novelty [19]. `CLAIMCHECK` is another example, evaluating the groundedness of LLM critiques of scientific papers [6].
*   **Review Writing Support**: ReviewFlow provides intelligent scaffolding for novice reviewers through contextual reflection prompts [19]. CARE enhances collaborative aspects through NLP-enhanced inline annotations and real-time features, while DocPilot automates repetitive tasks in document workflows via modular task planning and code generation [19]. The observation that LLMs can assist NLP researchers in critiquing papers further underscores their utility in this context [6]. Moreover, frameworks like HILMA integrate multi-agent debate methods that simulate scientific peer review, with intelligent agents assuming roles such as hypothesis proposer and reviewer, debating and providing feedback under real-time human expert guidance [23].

Despite the promising advancements, the integration of LLMs into peer review raises significant ethical implications and limitations. A critical concern is the potential for "misuse of LLMs in peer review," which could compromise academic integrity [2]. Entrusting AI with evaluative tasks that traditionally demand nuanced human judgment and specialized domain expertise presents substantial challenges [19]. Limitations include LLMs' inherent difficulty in understanding highly specialized terminology and inconsistencies in contextual analysis [13]. For instance, approaches like "LLM-as-a-Judge," where LLMs assign scores based on criteria such as correctness and completeness, face critical questions regarding the definition of evaluation rubrics and the boundaries of their signal reliability [29]. While LLMs can perform general evaluation tasks, assessing text quality or argument rationality based on prompts [28], their direct involvement in the complex, subjective, and ethically sensitive landscape of scientific peer review requires careful consideration. The development of specialized benchmarks is crucial for evaluating LLM-based peer review systems [13], and the concept of a "blind peer review" benchmark for scientific intelligence evaluation in educational contexts also highlights the need for robust, unbiased assessment mechanisms [25]. The tension between leveraging LLM capabilities for efficiency and preserving the integrity and intellectual rigor of human-led peer review remains a central challenge in this evolving field.
### 8.2 Cross-cutting Applications
Large Language Models (LLMs) are fundamentally reshaping the scientific research landscape by introducing sophisticated capabilities across diverse domains, thereby enhancing productivity, accelerating discovery, and automating complex workflows. These applications span from the systematic processing and synthesis of vast amounts of scientific literature to the generation and debugging of code crucial for experimental execution and data analysis. The core strength of LLMs—their ability to understand, process, and generate human-like text—positions them as powerful interfaces between human researchers and intricate scientific data or computational tasks.



**Cross-cutting Applications of Sci-LLMs**

| Application Area               | Description                                                 | Key LLM Tasks/Capabilities                                   | Scientific Impact / Benefits                                  |
| :----------------------------- | :---------------------------------------------------------- | :----------------------------------------------------------- | :------------------------------------------------------------ |
| **Information Extraction & Synthesis in Scientific Literature** | Processing & summarizing vast scientific literature; transforming unstructured text to structured knowledge. | Text Summarization, Classification, Generation, RAG for factual grounding, Structured Data Extraction, Knowledge Base Construction, Enhanced Information Retrieval. | Streamlined literature review, automated knowledge management, accelerated hypothesis generation, improved information access. |
| **Code Generation & Programming Assistance** | Generating, debugging, and explaining code for scientific workflows; automating experimental procedures. | Code Generation (from natural language), Code Completion, Debugging Assistance, Script Generation (e.g., lab robotics), Solving Programming Problems. | Enhanced programming workflows, automation of experiments, improved reproducibility, increased efficiency, democratization of advanced methods. |

One principal area of impact lies in **Information Extraction and Synthesis in Scientific Literature**, where LLMs streamline the entire lifecycle of scientific information management [20,30]. These models excel at foundational tasks such as text summarization, classification, and sophisticated generation, which form the bedrock for advanced literature review and analysis applications [4,14]. They facilitate the transformation of unstructured scientific text into structured knowledge, thereby assisting researchers in tasks ranging from comprehensive literature reviews and hypothesis generation to the construction of specialized knowledge bases [6,19]. Techniques like Retrieval Augmented Generation (RAG) are critical for grounding LLM-generated content in factual information, effectively minimizing inaccuracies, while specialized tools extract structured data and build knowledge graphs directly from scientific publications [11,19,29].

Concurrently, LLMs are revolutionizing **Code Generation and Programming Assistance** within scientific contexts, profoundly enhancing programming workflows, debugging processes, and the automation of experimental procedures [10,20]. Leveraging extensive training on diverse code data, transformer-based LLMs demonstrate robust capabilities in generating code from natural language instructions, solving complex programming problems, and facilitating code completion [4,30]. This capability extends to automating scientific tasks, such as generating robotic scripts for laboratory automation or translating scientific rules into executable code functions, thereby democratizing access to advanced experimental setups and computational methods [5,22].

The synergy between these applications underscores a broader, transformative shift in scientific methodology. Both domains leverage LLMs to bridge the gap between human conceptual understanding, typically expressed in natural language, and the structured, often computational, demands of rigorous scientific inquiry. This dual capability—to efficiently distill knowledge from existing literature and to precisely execute computational tasks through generated code—collectively accelerates the pace of scientific discovery. The overarching benefits include a substantial increase in **efficiency**, as LLMs automate labor-intensive tasks, and potential improvements in **reproducibility** through the standardization of methodologies facilitated by code generation from natural language specifications [20,22,25].

Despite these significant advancements, several common challenges persist across both information synthesis and code generation applications. A critical concern is ensuring the **accuracy and factuality** of LLM-generated content, whether it manifests as a literature summary or a piece of executable code. This necessitates the development of robust validation mechanisms and continuous human oversight [5,19]. Furthermore, the **explainability and interpretability** of LLM outputs remain a substantial area for improvement; understanding the reasoning behind a synthesized conclusion or a generated code block is paramount for maintaining scientific rigor and fostering trust, particularly when dealing with complex algorithms or critical scientific findings [20,28]. Additionally, the considerable **computational cost** associated with training and deploying large models, alongside their current limitations in processing **long contexts** effectively, presents practical constraints for their widespread integration into highly complex scientific workflows [28]. Addressing these challenges through continued research into more transparent models, improved verification techniques, and efficient architectural designs will be paramount for fully realizing the transformative potential of LLMs in cross-cutting scientific applications.
#### 8.2.1 Information Extraction and Synthesis in Scientific Literature
Large Language Models (LLMs) are profoundly transforming the landscape of scientific literature analysis by offering sophisticated capabilities in synthesizing and generating scientific text. These models leverage their inherent ability to process massive text data and their vast store of knowledge, often derived from scientific corpora like ArXiv and The Pile, to streamline literature review, analysis, and creative idea generation [30]. Fundamentally, LLMs excel in text summarization, classification, and generation, forming the bedrock for these advanced applications [4,10,14,20,25].

The application of LLMs significantly enhances the efficiency and quality of scientific content generation compared to traditional human-driven processes. For instance, tools like Iris.ai can rapidly process and summarize extensive medical literature, substantially accelerating the research lifecycle by assisting in comprehensive literature reviews and synthesis [20]. In a similar vein, Retrieval Augmented Generation (RAG) is a critical technique that bolsters LLM-based literature review generation by grounding content in factual information retrieved from external sources, thereby minimizing hallucinations and reducing the time required for thorough reviews [19]. Examples such as LitLLM and HiReview showcase RAG's efficacy in this domain [19]. However, it is noteworthy that while LLMs demonstrate impressive capabilities, as evidenced by ChatGPT drafting drug discovery review articles, human validation remains crucial to ensure accuracy and contextual relevance [5]. Critical analysis also highlights that while tools like Iris.ai exemplify LLMs' potential, detailed methodologies, experimental setups, or performance metrics for automated literature search, retrieval, structuring, and organization are often not extensively elaborated [20].

LLMs assist in managing and synthesizing scientific literature in myriad ways, fundamentally transforming raw text into structured knowledge and facilitating efficient information access for researchers [6]. They automate various stages of the literature workflow, including search, retrieval, synthesis, structuring, and organization [6]. Specialized tools and frameworks have emerged to address distinct needs:
*   **Literature Review and Synthesis:** LLMs are instrumental in generating "related work" sections for scientific papers, contributing to comprehensive literature reviews [13,19]. Systems like `LitLLM` serve as toolkits for scientific literature review, and LLMs can perform efficient title and abstract screening in biomedical literature reviews [6]. The HILMA framework utilizes LLMs for "bottom-up literature network review generation," synthesizing documents from structured subgraph citation networks to provide systematic research overviews [23].
*   **Knowledge-Driven Discovery and Hypothesis Generation:** LLMs facilitate knowledge-driven hypothesis generation by retrieving and combining insights from existing literature [13]. Tools like `SCIMON` foster scientific inspiration, and `ResearchAgent` aids in iterative research idea generation [6]. In fields like agriculture, LLMs synthesize knowledge to identify research trends and gaps, accelerating innovation [2].
*   **Structured Information Extraction:** LLMs possess a remarkable ability to extract structured data from unstructured scientific texts. For instance, LangExtract is an open-source library specifically designed for this purpose [29]. Several tools convert scientific literature into structured table formats, such as `Text-Tuple-Table`, `TKGT`, and `ArxivDIGESTables` [6]. SPIRES, using zero-shot learning with GPT3+, recursively extracts and presents structured information from medical texts in table formats, offering high customizability across diverse fields without requiring specific training data [5]. This capability is crucial for building necessary multimodal datasets, particularly in areas like materials science, where overcoming "key materials science information extraction challenges" from scientific literature is paramount [7].
*   **Knowledge Base Construction and Organization:** LLMs are used to construct "structured professional agricultural knowledge bases" by integrating information from various sources using vector databases and knowledge graphs [11]. `Science Hierarchography` focuses on the hierarchical organization of scientific literature, while systems like Universal Deep Research (UDR) and Tongyi DeepResearch offer frameworks for advanced literature search, synthesis, and organization [6,29].
*   **Enhanced Information Retrieval:** LLMs are increasingly integrated into information retrieval (IR) systems. They can act as components in IR models to re-rank search results based on textual understanding and semantic analysis, thereby improving accuracy and relevance [28]. They also enhance existing IR models by generating intermediate text, expanding queries, or augmenting documents, providing more context [9,17,21,28]. AstroLLaMA, for example, is evaluated on paper recommendation tasks, demonstrating its utility in information retrieval for scientific contexts [31].

Comparing different methods and models reveals unique contributions and nuanced differences. While general-purpose LLMs exhibit fundamental capabilities in summarization and text generation [4,14], specialized scientific LLMs offer targeted solutions. For instance, LLM4SD is designed for molecular property prediction, synthesizing knowledge from existing literature to derive rules like the relationship between molecular weight and solubility [3,22]. Similarly, BioGPT focuses on biomedical text generation and mining, excelling in relation extraction tasks on benchmarks such as BC5CDR, KD-DTI, and DDI, and question answering on PubMedQA [32]. GeneGPT allows researchers to efficiently retrieve biomedical information from NCBI databases via natural language queries, encompassing gene sequences, protein structures, and texts [5]. For scientometric analysis, a GPT-based approach can identify and classify AI-related articles with high accuracy (90%) and recall (94%), uncovering research trends [5]. In peer review workflows, systems like PaperMage understand and synthesize documents, while CocoSciSum provides content summaries to aid reviewers [19]. The rise of multi-agent frameworks, such as the Baidu AI Search Paradigm and NVIDIA's UDR, further signifies a move towards more sophisticated and autonomous systems that integrate synthesis and organization for comprehensive research workflows [29]. Despite these advancements, challenges persist, particularly the high computational cost of LLMs and their limitations in processing long texts, which necessitates model adjustments for efficient integration into IR systems [28]. These varied approaches underscore the dynamic evolution of LLMs from general text processors to specialized instruments capable of profound scientific information extraction and synthesis.

---
**Note on Task 1 (Reference Conversion):**
All multi-reference citations in the provided content are already grouped into a single `` block. There were no instances of consecutive, separate reference blocks (e.g., ` `) that required conversion into a single list. Therefore, no changes were needed for this task.

**Note on Task 2 (KaTeX Syntax Check):**
The provided content does not contain any mathematical formulas, expressions, or commands that would require KaTeX rendering or conversion from other macro packages. Therefore, this part of the task is not applicable.
#### 8.2.2 Code Generation and Programming Assistance
Large Language Models (LLMs) are profoundly impacting scientific programming workflows by enhancing code generation, facilitating debugging, and offering avenues for explaining complex algorithms, thereby empowering advancements akin to office automation in scientific development [10,20,28].

The utility of LLMs in code generation has evolved significantly. Historically, code generation began with theorem proving software to construct proofs and extract logical programs, transitioning through deep learning methods like Long Short-Term Memory (LSTM) networks and Recursive Reverse-Recursive Neural Networks for program construction from samples [20]. More recently, transformer-based LLMs, such as GPT-3 and T5, have demonstrated substantial success in generating code by utilizing contextual representations derived from vast code samples, public repositories, and natural language data [20]. This progression underscores a shift from rule-based and pattern-matching systems to models capable of understanding and generating code with increasing semantic awareness. For instance, OpenAI's Codex, a GPT model fine-tuned on GitHub code, showcased the ability to solve challenging programming problems and improved performance in mathematical problem-solving [4,30]. Similarly, code-based GPT models like code-davinci-002, which underpin GPT-3.5 models, illustrate that extensive training on code data enhances the reasoning capabilities of LLMs [30].

Modern LLMs offer a spectrum of capabilities in code generation and programming assistance. Models like Claude, OpenAI's o1 models, Qwen models, Palm, and the Phi series (e.g., Phi-4-reasoning) possess broad programming functionalities, making them well-suited for application development and coding tasks [15]. Beyond general programming, specialized code models such as Qwen3-Coder-480B-A35B-Instruct, Devstral 2507 (code-centric), and Seed-Coder (trained on 6 trillion tokens) have emerged, indicating a trend towards highly optimized LLMs for specific coding demands [29]. Code completion, or autocompletion, is another critical application where LLMs provide text input suggestions, expediting development and reducing errors for programmers. Transformer architectures are integral to contemporary code completion systems, and recent LLM advancements have "exhibited enhanced efficacy on evaluation benchmarks such as CodeXGLUE, surpassing traditional statistical language models and earlier DL approaches" [20]. Strategies to further improve code synthesis include sampling multiple candidate solutions and planning-guided decoding [28]. Evaluation typically involves assessing the pass rate (e.g., pass@k) against test cases using datasets like APPS, HumanEval, and MBPP [28]. The MTPB benchmark also measures multi-turn program synthesis, reflecting the complexity of real-world programming scenarios [30].

In scientific research, LLM-assisted code generation is transformative. GPT-4, for instance, can generate robotic scripts from natural language instructions for biological laboratory automation, specifically for controlling Opentrons OT-2 liquid handling robots [5]. This capability democratizes access to automated experimentation, allowing biologists to execute complex protocols without needing deep expertise in robotics programming [5]. Agent systems leveraging LLMs can execute Python code for experimental design and execution, performing complex computations required for scientific workflows [5]. For mathematical problem-solving, FunSearch, which combines LLMs with program search, has generated novel solutions for combinatorial optimization problems like the cap set problem, outperforming human-designed solutions [19,31]. In chemistry, the LLM4SD framework converts natural language scientific rules into executable code functions, such as RDKit functions, to generate interpretable feature vectors. An example is transforming "molecular weight < 500 mol" into a binary feature function like $$x_1=I(\text{rdMolDescriptors.CalcExactMolWt(mol)}<500)$$ [22]. This demonstrates LLMs' ability to bridge high-level scientific concepts with concrete programming logic. Furthermore, LLMs contribute to scientific code generation in areas like experiment planning and execution, with dedicated benchmarks such as `SciCode` for research coding, `DS-1000` for data science code generation, and `MLE-bench` for machine learning engineering tasks [6].

LLMs also play a crucial role in debugging, offering capabilities for bug fixing and enhancing overall developer productivity [25,29]. However, while LLMs can generate code and assist with corrections, their ability to explain the reasoning behind generated code or complex algorithms remains a nuanced area. While some sources suggest LLMs can "enable programmers to inquire about the reasoning behind the generated code" [10], several critical analyses of specific papers indicate a deficiency in explicitly capturing or detailing this explanatory capability [17,20,25,28,30]. This highlights a critical gap in the transparency and interpretability of LLM-generated code, particularly pertinent for scientific applications where understanding underlying logic is paramount.

The implications of LLM-assisted code generation for reproducibility and efficiency in scientific research are substantial. **Efficiency** is significantly boosted as LLMs accelerate development, reduce coding errors, and automate complex tasks, enabling researchers to focus on higher-level scientific inquiry rather than low-level programming details [5,20,25,31]. For **reproducibility**, the field faces both opportunities and challenges. The ability to translate natural language specifications into standardized code functions, as seen with LLM4SD, can enhance consistency in scientific methodologies [22]. Moreover, benchmarks like VERINA, which evaluate LLMs on end-to-end verifiable code generation with formal proofs, underscore the increasing demand for accuracy and reliability, directly contributing to more reproducible research outcomes [29]. However, the aforementioned lack of explicit reasoning from some LLMs poses a potential challenge to reproducibility if the generated code's logic cannot be fully audited or understood. As LLMs become more integrated into scientific workflows, ensuring the transparency and explainability of their code generation processes will be crucial for maintaining scientific rigor and fostering trust in automated research methods.
### 8.3 Domain-Specific Applications of Scientific LLMs

**Domain-Specific Applications of Scientific LLMs**

| Domain                              | Key Applications for Sci-LLMs                               | Example Models/Frameworks                                   |
| :---------------------------------- | :---------------------------------------------------------- | :---------------------------------------------------------- |
| **Biomedical & Healthcare**         | Medical Education/Exams, Clinical Decision Support, Patient Outcome Prediction, Biomedical Text Understanding | BioGPT, Med-PaLM, NYUTron, SurgicalGPT, Biomni-R0, GeneGPT  |
| **Life Sciences**                   | Molecular Structure/Function Prediction, Protein/RNA/DNA Design & Engineering, Cellular Mechanism Understanding | AlphaFold series, ESM, ProtGPT2, RhoFold, Evo, Geneformer, SCimilarity |
| **Drug Discovery & Development**    | Disease Mechanism Understanding, Drug Candidate Design, ADMET Prediction, Clinical Trial Optimization, Lab Automation | LLM4SD, Chemformer, Molformer, SyntheMol, AlphaFold 3, CLARify, ChemCrow |
| **Chemistry & Materials Science**   | Molecular Property Prediction, Autonomous Experimental Design, Material Design, Chemical Reaction Prediction | LLM4SD, Chemformer, MOOSE-Chem, HILMA, ChemCrow, Coscientist |
| **Physical, Earth & Environmental Sciences** | Mathematical Reasoning, Climate Modeling, Environmental Data Analysis, Geospatial Analytics, Physics Simulations | Alpha-Geometry, LLM4SD (QM), chatIPCC, Pangu-Weather, GeoGalactica |
| **Economic, Financial & Social Sciences** | Market Prediction, Risk Assessment, Policy Simulation, Scientometric Analysis, Hypothesis Generation | ChatGPT (sentiment analysis), BloombergGPT, LLMs for agricultural policy, GPT-based scientometric tools |

Large Language Models (LLMs) are rapidly transforming the landscape of scientific research and development by extending their capabilities beyond general language tasks into highly specialized domains. This section provides a comprehensive overview of how Scientific LLMs are being meticulously adapted and deployed across various scientific disciplines, including biomedical and healthcare, life sciences, drug discovery and development, chemistry and materials science, physical sciences, earth and environmental sciences, and economic, financial, and social sciences. The integration of LLMs within these fields signifies a paradigm shift, enabling unprecedented advancements in data analysis, hypothesis generation, experimental design, and knowledge synthesis [16,19].

The application of Scientific LLMs is characterized by a focused adaptation to domain-specific data modalities and intricate problem-solving requirements. While general-purpose LLMs demonstrate foundational linguistic understanding, their scientific counterparts are often trained or fine-tuned on vast repositories of scientific literature, specialized databases, and experimental data. This includes handling complex data formats such as molecular linear representations (e.g., SMILES), macromolecular structures (e.g., proteins, nucleic acids), genomic sequences, and multimodal scientific datasets, which encompass textual, graphical, and visual information [1,35]. This specialized training empowers Scientific LLMs to accurately interpret domain-specific terminology, recognize patterns unique to scientific phenomena, and generate outputs that adhere to the rigorous standards of scientific inquiry.

Across these diverse domains, Scientific LLMs are proving instrumental in addressing a wide array of challenges. In biomedical and healthcare applications, they are revolutionizing medical education, enhancing clinical decision support, and accelerating research in drug discovery and basic biology. Within the life sciences, LLMs are pushing the boundaries of molecular structure prediction, understanding protein function, and deciphering genetic and cellular mechanisms. The drug discovery and development pipeline benefits from LLMs' capacity to accelerate the understanding of disease mechanisms, design novel drug candidates, and optimize clinical trials. In chemistry and materials science, LLMs are pivotal for molecular property prediction, autonomous experimental design, and the generation of novel materials. Furthermore, in the physical, earth, and environmental sciences, these models contribute to mathematical reasoning, climate modeling, and the analysis of vast geographical datasets. Even in economic, financial, and social sciences, LLMs are employed for forecasting, risk assessment, scientometrics, and providing deeper insights into societal dynamics. This broad applicability underscores the transformative potential of Scientific LLMs to not only automate routine tasks but also to foster novel discoveries, streamline complex research workflows, and ultimately, accelerate the pace of scientific advancement.
#### 8.3.1 Biomedical and Healthcare Applications
Large Language Models (LLMs) are profoundly impacting the biomedical and healthcare sectors, revolutionizing diverse areas from medical education and clinical support to drug discovery and fundamental biological research [4,18,20,25].

In **medical education and examination performance**, general-purpose LLMs like ChatGPT (GPT-3.5-turbo) have been evaluated for their potential. A comprehensive assessment of ChatGPT's performance on the Chinese National Medical Licensing Examination (NMLE), National Pharmacist Licensing Examination (NPLE), and National Nurse Licensing Examination (NNLE) from 2017 to 2021 revealed significant insights [21]. Despite achieving high accuracy rates in specific medical tests like BLS (96%) and ACLS (92.1%), and an overall score of 92 in some assessments [25], ChatGPT failed to meet the passing threshold of 0.6 in any of the Chinese national examinations across all five years [21]. Its highest accuracy reached 0.5467 for NMLE, 0.5599 for NPLE, and 0.5897 for NNLE, correlating with the perceived complexity of the exams [21]. Notably, ChatGPT demonstrated proficiency in subjects such as clinical epidemiology, human parasitology, and dermatology, as well as topics related to health management, prevention, diagnosis, and screening [21]. Conversely, it struggled in areas requiring detailed anatomical knowledge, physiological processes, pathology, and public health regulations [21]. While these results highlight "the great potential of large language models in medical education" for knowledge acquisition and teaching material preparation, they also underscore the critical risks associated with using generic LLMs in specialized fields due to a lack of specificity and precision [20,21].

For **biomedical text understanding and generation**, specialized LLMs often demonstrate superior capabilities. BioGPT, a domain-specific generative Transformer model pre-trained on extensive biomedical literature, excels in tasks such as relation extraction and question answering [20,32]. For instance, BioGPT achieved F1 scores of 44.98% on BC5CDR, 38.42% on KD-DTI, and 40.76% on DDI for end-to-end relation extraction, and recorded accuracies of 78.2% (BioGPT) and 81.0% (BioGPT-Large) on PubMedQA, setting new benchmarks [32]. These models are instrumental in summarizing and understanding vast amounts of medical literature [20]. Other notable specialized models include BioBERT, PubMedBERT, and Meditron, which are prominent text-focused scientific LLMs used for biomedical text understanding [18]. Further advancements include Biomni-R0, an agentic LLM trained with multi-turn reinforcement learning for expert-level intelligence in biomedical research, and GeneGPT, which enhances biomedical information retrieval by accessing NCBI Web APIs for accurate biological knowledge on gene sequences and protein structures via natural language queries [5,29]. SPIRES (Structured Prompt Interrogation and Recursive Extraction of Semantics) uses GPT3+ models with zero-shot learning to extract structured information from medical texts, populating knowledge bases with details on cell signaling pathways, disease treatments, and drug mechanisms [5].

In **clinical decision support and patient outcome prediction**, LLMs are proving transformative. Med-PaLM, by encoding clinical knowledge, has achieved human-expert level performance in USMLE-style question-answering tasks, significantly boosting the efficiency of clinical practice [26,31]. The NYUTron platform leverages BERT models with Electronic Health Records (EHR) data to accurately predict patient outcomes, such as in-hospital mortality and 30-day readmission rates, facilitating more effective treatment planning [26]. Furthermore, GPT-3 automates the generation of discharge summaries and clinical notes, improving documentation efficiency and quality [26]. LLMs are also applied to analyze medical records and research studies, identifying patterns or making predictions about outcomes related to specific health treatments or conditions [4]. More recently, SDBench and MAI-DxO are advancing realistic, cost-aware clinical reasoning, while DeepRare, an AI-powered agentic diagnostic system, is transforming clinical decision-making for rare diseases [29]. Google DeepMind and Google Research have introduced MedGemma 27B and MedSigLIP for scalable multimodal medical reasoning, integrating text with other medical data modalities [29].

LLMs are also making significant strides in **drug discovery, development, and fundamental biomedical research**. LLM4SD, for instance, predicts critical molecular properties such as blood-brain barrier permeability (BBBP) and toxicity (Tox21) in physiology, and HIV inhibitory activity and BACE-1 enzyme inhibition in biophysics [3,22]. In these tasks, LLM4SD achieved an AUC-ROC of 76.60% in physiology and biophysics, outperforming existing state-of-the-art Graph Neural Networks (GNNs) by 2.07% [22]. Other applications include antibody generation LLMs for *de novo* design of SARS-CoV-2 antibodies, and CRISPR-GPT, which automates the design of CRISPR gene editing experiments, selecting systems, designing guide RNAs, and recommending delivery and validation methods [19,31]. ESM-1b and ESM-2 models are utilized for encoding protein sequences and predicting structural properties [19]. Specialized models like MethylGPT and CpGPT focus on epigenetic analysis for biological age estimation, while SCimilarity identifies cell types using machine learning and the Human Cell Atlas [8]. The development of Protein LLMs (Prot-LLMs) and Genomic LLMs (Gene-LLMs) is crucial for understanding and manipulating protein and genomic sequences for tasks like structure/function prediction and sequence analysis, respectively [1]. Furthermore, GPT-4 is used to generate robotic scripts for biological experiments, automating laboratory processes like liquid handling, and ChatGPT assists in drafting drug discovery review articles, which are subsequently refined by human experts [5]. SurgicalGPT, an end-to-end language-vision GPT model, also demonstrates advanced capabilities in visual question answering for surgical contexts [5].

**Contrasting the strengths and weaknesses** of general-purpose versus specialized LLMs in these domains reveals distinct characteristics. General-purpose LLMs, exemplified by ChatGPT, offer broad applicability across various tasks and possess a vast knowledge base, making them valuable for initial information gathering, content generation, and assisting in general medical education [21]. They can automate routine tasks such as medical dictation (e.g., GPT-4 with Nuance’s Dragon Medical One) and clinical note generation (e.g., GPT-3) [20,26]. However, their primary weakness lies in a lack of domain-specific precision and accuracy, particularly in critical, highly specialized fields like clinical diagnostics or complex medical examinations, as demonstrated by ChatGPT's failure in national medical licensing exams [20,21]. They may also generate content that requires significant human review and fact-checking, as seen in drafting scientific articles [5].

In contrast, specialized LLMs, such as BioGPT, Med-PaLM, BioBERT, and LLM4SD, are trained on extensive domain-specific datasets, granting them superior performance in targeted tasks. BioGPT excels in biomedical relation extraction and question answering, setting new records in specific benchmarks [32]. Med-PaLM achieves expert-level performance in medical question-answering, directly impacting clinical practice efficiency [26]. LLM4SD demonstrates quantifiable improvements in molecular property prediction over state-of-the-art methods [22]. These models offer high precision, reliability, and deeper contextual understanding within their specific scientific niches. Their weakness, however, often lies in their narrower scope of application and the intensive resources required for their domain-specific pre-training and fine-tuning. The trend increasingly involves fine-tuning general-purpose LLMs on biomedical and clinical data (e.g., Med-PaLM and NYUTron leveraging BERT models) to harness their foundational capabilities while imbuing them with the necessary domain expertise for specialized applications [26]. This hybrid approach aims to mitigate the weaknesses of both general and specialized models, fostering novel perspectives and accelerating advancements in biomedical and healthcare LLM applications.
#### 8.3.2 Life Sciences: From Molecules to Cells (LLLMs)
The integration of Artificial Intelligence (AI), particularly Large Language Models (LLMs), has profoundly impacted the life sciences, driving revolutionary advancements across various scales, from molecular interactions to cellular functions [8]. This paradigm shift is often heralded by breakthroughs in molecular structure prediction, exemplified by models like AlphaFold 2, which famously solved the long-standing 50-year challenge of highly accurate protein structure prediction [8].

Advancements in molecular structure prediction have continued with the AlphaFold series. AlphaFold 3 extends this capability significantly by predicting the 3D structures of complex biomolecular assemblies, including proteins, DNA, RNA, small molecules, and ligands, achieving experimental-level accuracy, with 80% of protein-ligand complexes predicted within 2 Å of experimental error [8]. Boltz-1 emerges as an open-source alternative, offering comparable accuracy for 3D biomolecular interaction prediction, making advanced tools more accessible to the research community [8]. Further enhancing protein analysis, MassiveFold provides an optimized and parallelized version of AlphaFold, improving computational efficiency [8].

Beyond structure prediction, LLMs are crucial for understanding protein function and design. The Evolutionary Scale Modeling (ESM) family, including ESM-1b, ESM-2, and ESMFold, are Transformer-based protein language models adept at predicting protein folding, binding sites, and functions directly from protein sequences [19,20,26,31]. These models can encode protein sequences to capture structural characteristics, thereby reducing the need for extensive laboratory experiments [19]. For protein engineering, EVOLVEpro is a dedicated protein language model, while ProtGPT2 generates *de novo* protein sequences [8,20]. ProGen, another generative model, produces protein sequences with predefined functionalities, even demonstrating catalytic efficiencies comparable to natural counterparts [20]. Furthermore, specific applications include PocketGen for defining atomic structures of protein-ligand interactions, PIONEER for comprehending protein-protein interactions in health and disease, and AbMAP for designing novel antibodies with significantly enhanced binding affinity, such as a 20-fold increase for SARS-CoV-2 antibodies [8,19]. Chai-2, a multimodal AI model, has achieved a 16% hit rate in zero-shot *de novo* antibody design across 52 diverse antibodies, showcasing its efficacy in complex biological design tasks [29]. The broader category of Protein LLMs (Prot-LLMs) leverages various architectures, datasets, and evaluation methods for tasks including protein structure/function prediction and sequence design, with notable contributions like "OntoProtein" and "Multi-level Protein Structure Pre-training with Prompt Learning" [1].

LLMs are also transforming RNA and DNA analysis and design. RhoFold predicts 3D RNA conformations from nucleic acid sequences, while RhoDesign is specialized for RNA aptamer design [8]. The General Expression Transformer (GET) accurately predicts RNA transcription patterns across over 5000 human cell types, offering crucial insights into gene regulation [8]. In genomic analysis, various DNA language models evaluate the functional effects of human genome variants, having analyzed billions of single-nucleotide variant sites [8]. MethylGPT and CpGPT serve as foundational models for epigenetic analysis, with applications in biological age estimation [8]. Evo, trained on 2.7 million genomes, is capable of predicting variant effects in DNA, RNA, or proteins and generating new DNA sequences [8]. Other significant models include DNA-BERT, which decodes genetic language through training on nucleotide sequences for genomic analysis, and AlphaGenome from Google DeepMind, which comprehensively predicts the impact of single variants or mutations in DNA [26,29]. The Nucleotide Transformer is a pre-trained foundational model that learns from biological sequences for precise predictions of molecular phenotypes [20]. Genomic LLMs (Gene-LLMs) generally cover model architectures, datasets, and evaluation methods specifically for DNA and RNA sequences, addressing tasks such as understanding gene function, predicting chromosomal spectra, splicing sites, binding sites, structures, sequence generation, and variation analysis [1].

Extending to the cellular level, models like SCimilarity utilize machine learning to identify cell types based on 62 million single-cell maps from the Human Cell Atlas, significantly advancing our understanding of cellular heterogeneity [8]. Geneformer further contributes by analyzing single-cell transcriptomic data to map gene networks and identify therapeutic targets, as demonstrated in heart disease models [26]. Research also includes "Knowledge-graph-based cell-cell communication inference for spatially resolved transcriptomic data with SpaTalk" and "De novo analysis of bulk RNA-seq data at spatially resolved single-cell resolution," which directly relate to enhancing cellular-level understanding [1]. LLM4GRN focuses on discovering causal gene regulatory networks, offering another avenue for deeper cellular insights [6].

The emergence of multi-agent systems represents a significant step towards automating scientific discovery. The Stanford Virtual Lab exemplifies this approach by integrating multiple specialized AI models, such as AlphaFold-Multimer, Rosetta, and ESM, with expert agents to automate complex scientific processes like nanobody design [8]. This integrated framework streamlines experiments, moving towards autonomous research. Beyond specific design tasks, LLMs like GPT-4 can generate robotic scripts for biological laboratory automation, particularly for liquid handling, thereby simplifying the execution of biological experiments [5]. BioPlanner further aids in protocol planning specific to biology [6]. CRISPR-GPT, an LLM-based system, automates the design of CRISPR gene editing experiments, facilitating system selection, guide RNA design, delivery method recommendations, and validation planning [19].

LLMs also contribute broadly to biological data interpretation and biomedical text mining. BioGPT is applied in biomedical text generation and mining, covering relation extraction and question answering within biomedical literature, which is fundamental to comprehending complex biological entities and processes [32]. GeneGPT enables efficient access to vast biomedical databases like NCBI via Web APIs, allowing researchers to query information on gene sequences, protein structures, and disease relations using natural language [5]. The SPIRES method can extract structured information regarding multi-species cell signaling pathways and multi-step drug mechanisms from medical texts, contributing to understanding biological processes at molecular and cellular levels [5]. MoLFormer is a high-capacity molecular SMILES transformer model for integrating spatial information into molecular data, crucial for molecular data interpretation [20]. LLM4SD, a molecular LLM, contributes by predicting molecular properties relevant to biological processes, such as solubility or biological barrier permeability, which are fundamental to understanding molecular interactions within living systems [3].

The ambitious vision of "AI Virtual Cells" (AIVC) aims to simulate molecular, cellular, and tissue dynamics using various integrated models [8]. This comprehensive approach holds the potential to revolutionize biomedical research by enabling *in silico* experimentation and a holistic understanding of life's intricate processes at multiple scales [8]. Overall, LLMs have demonstrated immense capacity for scrutinizing and predicting biological functionalities, disease mechanisms, and drug development procedures through the utilization of genetic and proteomic datasets, fundamentally transforming computational biology [16,20].
#### 8.3.3 Drug Discovery and Development
Large Language Models (LLMs) are profoundly transforming the landscape of drug discovery and development, offering innovative solutions across the entire pipeline from foundational understanding of disease mechanisms to the optimization of clinical trials and patient care [8,20,26]. 

![LLM Integration Across the Drug Discovery Pipeline](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/fKgdbPhpaSSq1Q3RS1UuG_LLM%20Integration%20Across%20the%20Drug%20Discovery%20Pipeline.png)

This section provides a comprehensive review of LLM integration into this critical domain, highlighting their diverse applications, key models, and the significant impact they have on accelerating and enhancing pharmaceutical research and development [1].

The initial phase, **Understanding Disease Mechanisms**, leverages specialized LLMs to decipher the complex biological underpinnings of diseases [26]. Models are employed for analyzing genetic sequences, predicting the functional impact of DNA mutations (e.g., DNA-BERT, AlphaGenome, Gene-LLMs), and mapping single-cell transcriptomic data to identify therapeutic targets (e.g., Geneformer, GET). Furthermore, Protein LLMs like ESM are crucial for predicting protein structures and functions, while frameworks such as LLM4SD contribute by predicting molecular properties and generating interpretable rules, aiding in the understanding of disease pathogenesis and target identification [3,22]. Text-focused LLMs, like BioGPT, also play a vital role by extracting and synthesizing knowledge from vast biomedical literature, revealing chemical-disease and drug-target relationships that deepen our understanding of disease states [32].

Subsequently, in **Drug Candidate Discovery and Design**, LLMs streamline and enhance the process of identifying and developing novel therapeutic agents [26]. This involves their application in chemical experiments, where models like Chemformer predict reactions and systems like ChemCrow and Coscientist automate laboratory procedures and organic synthesis [5,19]. For *in vitro* simulations and *de novo* drug design, Protein LLMs such as the AlphaFold series and ProGen are instrumental in accurately predicting protein structures and generating new protein sequences with desired functionalities, including therapeutic proteins and antibodies [26]. Critically, LLMs like Molformer and LLM4SD provide superior capabilities in predicting ADMET (Absorption, Distribution, Metabolism, Excretion, Toxicity) properties, which are essential for screening and optimizing drug candidates to ensure their safety and efficacy [3,22].

Finally, LLMs are increasingly integrated into **Clinical Trial Optimization and Support**, bringing advancements to clinical decision-making, patient outcome prediction, and administrative efficiencies [26]. Models like Med-PaLM demonstrate human-expert level performance in medical knowledge tasks, providing crucial clinical decision support [26]. For patient outcome prediction, platforms such as NYUTron leverage Electronic Health Records (EHR) to forecast critical outcomes like mortality and readmission rates, offering valuable prognostic insights [26]. General-purpose LLMs, including GPT-3 and GPT-4, are also employed for automating medical documentation, such as generating discharge summaries and transcribing medical dictations, thereby reducing the administrative burden on healthcare professionals and allowing for more focused patient care [20,26].

Throughout these stages, the integration of LLMs presents a diverse ecosystem of specialized models, each with unique contributions. The following sub-sections will delve into these applications in greater detail, explicitly comparing and contrasting different methods, models, and findings to highlight their unique strengths, potential limitations, and to foster novel perspectives for future research and development in this rapidly evolving field.
##### 8.3.3.1 Understanding Disease Mechanisms
Large Language Models (LLMs), particularly specialized architectures, are increasingly instrumental in deciphering the intricate mechanisms underlying diseases, thereby advancing foundational biological understanding and therapeutic development [20,26]. These models contribute across various biological scales, from genetic and transcriptomic analysis to protein structure and molecular interaction prediction, as well as knowledge extraction from biomedical literature.

At the genomic level, LLMs facilitate a deeper understanding of genetic patterns and variations. For instance, **DNA-BERT** operates as a Genomic LLM, specifically designed to decode genetic language from nucleotide sequences, offering insights into genetic variations relevant to disease [26]. Complementing this, Google DeepMind's **AlphaGenome** focuses on predicting the impacts of DNA mutations, which is foundational for understanding genetic predispositions to diseases [29]. Similarly, **Gene-LLMs** broadly contribute by analyzing DNA and RNA sequences to understand gene function and sequence variations [1], while models like the **Nucleotide Transformer** learn from biological sequences to predict molecular phenotypes [20]. Other notable contributions include **Evo**, which predicts the functional impact of variants across DNA, RNA, or proteins [8], and **LLM4GRN**, which applies LLMs to discover causal gene regulatory networks, a critical aspect of disease pathogenesis [6]. Furthermore, **GeneGPT** aids researchers in acquiring biological knowledge by querying gene variations and their associated diseases from extensive databases like NCBI [5].

In the realm of transcriptomic analysis, LLMs provide critical tools for identifying therapeutic targets. **Geneformer**, a Genomic LLM with a focus on transcriptomics, maps single-cell transcriptomic data to gene sequences. This enables comprehensive gene network analysis and the identification of potential therapeutic targets, successfully analyzing network perturbations in conditions such as cardiac hypertrophy and dilated cardiomyopathy [26]. Similarly, **GET** predicts RNA transcription patterns, offering vital insights into gene regulation and potential disease pathways [8].

For protein target analysis, LLMs like **ESM** (Evolutionary Scale Modeling) are crucial. ESM models, including ESM-1b and ESM-2, analyze protein sequences to predict folding, binding sites, and functional annotations [26]. Their capability to encode protein sequences to capture structural properties is fundamental for tasks like secondary and tertiary structure prediction, which are essential for understanding biological mechanisms related to disease [19,20]. Another model, **PIONEER**, contributes by revealing proteome-wide perturbations caused by disease mutations, thereby enhancing the understanding of disease states at a systemic proteomic level [8].

Beyond sequence and structural analyses, specialized LLMs like **LLM4SD** contribute by accurately predicting molecular properties relevant to biological processes. LLM4SD is applied to physiological tasks such as predicting blood-brain barrier permeability (BBBP) and toxicity (Tox21), and biophysical tasks like HIV inhibition and BACE-1 enzyme inhibition [3,22]. By generating interpretable rules for these predictions, LLM4SD offers insights into molecular mechanisms underlying disease, aiding in the identification of potential therapeutic targets and understanding drug-likeness characteristics, demonstrating a robust capability with an AUC-ROC of 76.60% for physiology and biophysics predictions [22].

Furthermore, LLMs excel at extracting and synthesizing knowledge from vast biomedical literature. **BioGPT**, for instance, significantly contributes to understanding disease at a foundational level through its capabilities in relation extraction [32]. It achieved an F1 score of 44.98% on the BC5CDR task for identifying chemical-disease relationships, 38.42% for drug-target interactions (KD-DTI), and 40.76% for drug-drug interactions (DDI) [32]. This capability directly aids in deciphering how substances affect biological processes and diseases. Similarly, **SPIRES** can extract structured information regarding disease treatments and multi-step drug mechanisms from medical texts, thereby supporting a foundational understanding of disease processes [5].

In summary, while models like DNA-BERT, AlphaGenome, Nucleotide Transformer, and Gene-LLMs focus on dissecting genetic code and variant impact, Geneformer and GET advance the understanding of gene expression and regulatory networks. ESM and PIONEER provide critical structural and functional insights into proteins. LLM4SD bridges molecular characteristics with physiological outcomes through predictive modeling, offering interpretable rules. Finally, BioGPT and SPIRES act as powerful knowledge extractors, synthesizing existing biomedical information to reveal complex relationships. This diverse suite of specialized LLMs, each with unique contributions and analytical strengths, collectively forms a robust framework for comprehensively understanding disease mechanisms across biological hierarchies, from molecular interactions to systemic pathways.
##### 8.3.3.2 Drug Candidate Discovery and Design
Large Language Models (LLMs) are profoundly transforming the landscape of drug candidate discovery and design, accelerating various stages from initial screening to lead optimization [22,26]. These models enhance efficiency and accuracy across chemical experiments, *in vitro* simulations, and crucial pharmacokinetic property predictions, offering a systematic framework for developing novel therapeutics [1].

In the realm of **chemical experiments**, LLMs significantly streamline and automate complex procedures. Models like Chemformer, a specialized Molecular LLM, excel in chemical reaction and reaction prediction, demonstrating the ability to outperform human chemists in accuracy for certain tasks after pre-training and fine-tuning [26]. Complementing predictive capabilities, the CLARify system, built upon GPT-3, automates laboratory procedures by generating executable experimental plans for chemical robots, thereby enhancing efficiency and reducing manual errors [26]. Beyond specific tasks, more integrated systems like ChemCrow augment LLMs with a suite of 13 expert-designed chemistry tools, enabling them to perform intricate tasks in organic synthesis, drug discovery, and materials design [5,19]. Further examples of LLMs integrating with laboratory automation include Coscientist for reaction optimization and Ramos, which combines natural language input with Bayesian optimization for catalyst synthesis [19]. These advancements illustrate a dual impact: predictive accuracy for chemical transformations (e.g., Chemformer) and autonomous execution of experimental workflows (e.g., CLARify, Coscientist), collectively reducing experimental iterations and accelerating discovery [19].

For **in vitro simulations and novel drug design**, LLMs, particularly Protein LLMs, have made substantial strides in protein structure prediction and therapeutic protein engineering. The AlphaFold series (including AlphaFold 2, AlphaFold 3, and MassiveFold), along with Boltz-1, are pivotal for accurately predicting protein structures and biomolecular interactions, which are foundational for rational drug design and *in vitro* simulations [8,26]. This high-accuracy prediction is crucial for understanding protein behavior without extensive labor-intensive experiments [19]. Beyond structural prediction, LLMs are also instrumental in *de novo* design. The ProGen language model, for instance, utilizes deep learning to generate protein sequences with predefined functionalities, yielding synthetic proteins (e.g., lysozymes) with catalytic efficiencies comparable to their natural counterparts [20]. Similarly, ESM models (ESM-1b, ESM-2) encode protein sequences to predict structural properties, aiding new drug design [19]. In the specialized area of antibody engineering, Chai-2 and other antibody generation LLMs have successfully demonstrated *de novo* antibody design, including for SARS-CoV-2, which is critical for therapeutic development [8,19,29]. Other contributions include PocketGen, which defines atomic structures of protein-ligand interactions; EVOLVEpro for AI-assisted therapeutic protein engineering; AbMAP for designing novel antibodies with enhanced affinities; and SyntheMol, a generative AI model that successfully screened billions of compounds for new antibiotics and experimentally verified active candidates [8]. The Stanford Virtual Lab further exemplifies this through a multi-agent system automating nanobody design, showcasing a closed-loop approach to discovery [8].

In predicting **ADMET (Absorption, Distribution, Metabolism, Excretion, Toxicity) properties**, LLMs like Molformer provide significant advantages by performing large-scale molecular property predictions with higher accuracy than traditional machine learning methods, thereby facilitating the efficient screening of compounds for desirable pharmacokinetic characteristics [20,26]. A notable advancement in this domain is LLM4SD, a framework designed for accurate and interpretable prediction of a wide range of molecular properties crucial for drug development, including solubility (ESOL), lipophilicity, and quantum mechanical properties (e.g., dipole moment, HOMO energy from QM9) [3,22]. LLM4SD distinguishes itself by outperforming advanced Graph Neural Networks (GNNs) across 58 molecular tasks and achieving a Mean Absolute Error (MAE) of 5.8233 and Root Mean Square Error (RMSE) of 1.28 for physical chemistry and quantum mechanics tasks [22]. Furthermore, its ability to generate interpretable rules, such as "molecular weight < 500 mol more likely to cross blood-brain barrier," directly aids in lead optimization and provides explainable insights into molecular behavior [22]. While not always explicitly detailed, the use of LLMs for molecular structure optimization (e.g., DrugAssist) and evolutionary search strategies for chemical space exploration inherently supports the prediction and optimization of ADMET-related properties [19]. Beyond these, BioGPT contributes to early-stage drug development by extracting critical drug-target interactions (KD-DTI F1 score of 38.42%) and drug-drug interactions (DDI F1 score of 40.76%), offering valuable insights for identifying candidates and understanding their effects [32]. MOOSE-Chem also contributes by generating novel chemical hypotheses, thereby enhancing the discovery phase [6].

In summary, LLMs are not merely tools but increasingly autonomous agents in drug discovery. Their capabilities range from precise chemical reaction prediction (Chemformer) and laboratory automation (CLARify, Coscientist) to high-fidelity protein structure prediction (AlphaFold series, ESM) and *de novo* design of therapeutic proteins and antibodies (ProGen, Chai-2, AbMAP). Furthermore, models like Molformer and LLM4SD provide critical, often interpretable, predictions of ADMET properties, which are vital for lead optimization. The combined effect of these specialized LLM applications is a significant acceleration of drug development, marked by enhanced accuracy, efficiency, and the generation of novel, validated drug candidates.
##### 8.3.3.3 Clinical Trial Optimization and Support
Large Language Models (LLMs) are increasingly integrated into various facets of clinical trials and practice, offering advanced capabilities for decision support, patient outcome prediction, and administrative automation [26]. This integration streamlines processes, enhances prognostic accuracy, and mitigates the administrative burden on healthcare professionals.

A key application lies in **clinical decision support (CDS)**, where LLMs assist medical practitioners by processing and interpreting complex medical knowledge. Med-PaLM, a general-purpose LLM specifically adapted for healthcare, exemplifies this by achieving human-expert level performance in USMLE-style question-answering, thereby directly supporting clinical decision-making [26]. This capability to excel in medical knowledge-based tasks positions Med-PaLM as a significant tool for enhancing diagnostic reasoning and treatment planning. Other models like SDBench and MAI-DxO also contribute to clinical reasoning, which is fundamental for optimizing clinical trials and patient management [29]. DeepRare, an agentic diagnostic system, further extends this by specifically impacting clinical decision-making in specialized areas such as rare disease management [29]. While [20] broadly acknowledges the role of NLP techniques and LLMs in clinical healthcare assistance, pattern detection, and diagnosis, it does not detail specific models like Med-PaLM or provide quantifiable performance metrics for clinical decision support. Moreover, the multi-modal end-to-end language-vision GPT model, SurgicalGPT, designed for visual question answering in surgery, offers an indirect form of clinical support by providing visual and language-based information that could aid medical professionals in clinical settings [5].

Beyond decision support, LLMs are pivotal in **patient outcome prediction**. The NYUTron platform, which leverages BERT models, meticulously analyzes Electronic Health Records (EHR) to predict critical patient outcomes such as in-hospital mortality, comorbidity, and 30-day readmission rates [26]. This provides invaluable prognostic insights that can inform treatment planning and optimize patient management strategies. Extending this, models like MethylGPT and CpGPT, which perform epigenetic analysis for biological age estimation, could potentially enhance patient stratification or outcome prediction within clinical trials by offering deeper biological insights [8]. The broader vision encapsulated by AI Virtual Cells (AIVC) further aims to revolutionize precision medicine and drug R&D, providing a more profound understanding of disease progression and drug efficacy at a cellular level, which inherently optimizes and supports various clinical stages [8]. In contrast to the detailed model-specific applications of NYUTron, MethylGPT, and CpGPT, the discussion in [20] offers a general overview of LLMs' utility in processing medical records but does not delve into specific models or experimental details for patient outcome prediction.

Finally, LLMs significantly contribute to **automating and enhancing medical documentation**, thereby reducing the substantial administrative burden on healthcare professionals. General LLMs, such as GPT-3, are employed for generating discharge summaries and various clinical notes, which substantially enhances the efficiency and quality of medical record-keeping [26]. Complementing this, GPT-4 is utilized for transcribing medical dictations directly into electronic health records, further streamlining administrative tasks and allowing clinicians to allocate more time to direct patient care [20]. While both GPT-3 and GPT-4 serve to automate documentation, GPT-3 is highlighted for its role in generating summary content, whereas GPT-4 is specifically noted for transcription. The critical analysis within [20] points out that although it covers medical transcription and information processing, it lacks the specific models, experimental details, or quantifiable performance metrics found in other works discussing documentation automation.

In summary, the integration of LLMs into clinical practice spans a spectrum from highly specialized models providing precise clinical decision support (e.g., Med-PaLM, SDBench, MAI-DxO, DeepRare) and detailed patient outcome prediction (e.g., NYUTron, MethylGPT, CpGPT), to general-purpose LLMs adapted for administrative tasks such as documentation automation (e.g., GPT-3, GPT-4). While some studies offer specific model implementations with clear performance indicators [5,8,26,29], others provide a broader conceptual overview that, while valuable, may lack detailed experimental evidence and quantifiable metrics for specific applications [20]. This highlights a diverse landscape where LLMs are transforming clinical trials and practice through both direct patient-facing applications and crucial administrative efficiencies.
#### 8.3.4 Chemistry and Materials Science
Large Language Models (LLMs) are demonstrating transformative potential across Chemistry and Materials Science, addressing complex challenges from molecular property prediction to autonomous experimental design [1,35]. This domain requires LLMs to effectively interpret and process specialized data formats, including molecular linear representations like SMILES, BigSMILES, and SELFIES, which are crucial for tasks such as molecular property prediction, interaction prediction, reaction prediction, and molecular generation [1,18]. The development of "materials science LLMs (MatSci-LLMs)" necessitates high-quality, multimodal datasets derived from scientific literature to facilitate hypothesis generation and testing [7].

A prominent example of LLM application in this field is LLM4SD, a framework designed for molecular property prediction across various sub-domains of chemistry and materials science [3,22]. LLM4SD distinguishes itself by leveraging LLMs to synthesize rules directly from chemical literature and infer novel patterns from molecular data, specifically SMILES strings [22]. These inferred rules are then converted into executable RDKit functions to generate interpretable feature vectors, which are subsequently employed by explainable models, such as random forests, for prediction [22]. This approach significantly enhances interpretability, providing valuable insights for chemical and materials design. Experimentally, LLM4SD has shown remarkable performance improvements in physical chemistry tasks (e.g., solubility and lipophilicity prediction), achieving a Mean Absolute Error (MAE) of 5.8233, which is 48.2% better than the next best model. In quantum mechanics (e.g., predicting 12 QM9 properties like dipole moment and HOMO energy), it achieved a Root Mean Square Error (RMSE) of 1.28, a 12.9% improvement, and a 48% accuracy improvement in predicting key quantum properties vital for material design [3,22].

Different LLM approaches are employed to handle the diverse data formats inherent in chemistry and materials science.
*   **Text-focused models**, such as ChemBERT, MatSciBERT, and BatteryBERT, utilize masked language modeling (MLM) and are pre-trained on extensive textual data including research papers and the Materials Project database. More advanced models like ChemDFM, ChemLLM, and LlaSMol employ next-token prediction and instruction tuning, benefiting from datasets like Mol-Instructions and SMolInstruct [31].
*   **Text+Graph models** integrate graph neural networks (GNNs) as encoders for molecular graphs with LLMs for textual information. Examples include Text2Mol, which combines GCNs with SciBERT for text-to-molecule retrieval. Other models, like 3D-MoLM, use LLMs that can encode both text and graph representations (linearized SMILES or graph projections) for tasks such as molecule-to-text retrieval and molecular captioning. These models typically utilize datasets like ChEBI-20, ZINC, and PCDes [31].
*   **Text+Vision models** extend multimodal capabilities by representing molecular images as tokens for LLM input, similar to BLIP-2. GIT-Mol, for instance, projects graph and image modalities into a latent text space and leverages a T5 architecture for processing [31].

These diverse architectural approaches contribute significantly to various sub-fields:
*   **Drug Discovery and Biomedical Research**: LLMs are extensively applied here. Chemformer is utilized for chemical reaction prediction and inverse synthesis, showcasing its ability to process complex chemical language [26]. Molformer focuses on large-scale molecular property prediction, critical for screening compounds with desirable ADMET (absorption, distribution, metabolism, excretion, and toxicity) properties [26]. The CLARify system, powered by GPT-3, automates chemical experimental plans, enhancing efficiency in research operations [26]. SyntheMol is a generative AI model for designing and validating novel antibiotics, involving complex chemical synthesis and property prediction [8]. Furthermore, models like AlphaFold 3 and Boltz-1 predict the structures and interactions of small molecules and ligands in biological complexes, directly advancing chemical understanding and design in biological contexts [8]. LLMs also support evolutionary search strategies to efficiently explore vast chemical spaces, thereby reducing experimental burdens, and integrating natural language input with Bayesian optimization for catalyst synthesis to streamline iterative design cycles [19].
*   **Material Design**: LLMs are crucial for "structured information inference," extracting device-level information from data, such as for perovskite solar cells, achieving a 91.8% F1 score [5]. They can also design experiments for predicting electrical performance and target material/device parameters, achieving performance comparable to traditional machine learning without extensive feature selection [5]. For hypothesis generation, MOOSE-Chem utilizes LLMs trained on scientific papers to identify inspirational knowledge and re-discover high-impact hypotheses in chemistry and materials science by decomposing complex problems [6,19]. The HILMA framework has been validated in materials science for generating detailed hypotheses. For example, in silicon nitride ceramics, HILMA proposed a multi-level structured composite material design, integrating micro-nano scale reinforcing phases and smart-responsive corrosion inhibitors. This comprehensive hypothesis included specific methods for substrate optimization, micron-scale reinforcement dispersion, nano-scale smart inhibitor encapsulation, and gradient functional coating, outlining a full scope of experimental validation for unprecedented durability and reliability [23].

To overcome domain-specific limitations and enhance practical utility, LLMs are frequently augmented with specialized tools. ChemCrow is a notable example, integrating an LLM with 13 expert-designed chemistry tools to perform complex tasks in organic synthesis, drug discovery, and materials design, demonstrating emergent problem-solving capabilities [5,19]. This agentic approach allows ChemCrow to autonomously plan and execute complex chemical syntheses, effectively connecting computational and experimental domains [19]. Similarly, Coscientist integrates LLMs with laboratory automation for optimizing reactions, such as palladium-catalyzed synthesis, further exemplifying the power of tool-augmented LLMs in accelerating scientific discovery [19].

In summary, the application of LLMs in chemistry and materials science is characterized by diverse model architectures, sophisticated data handling strategies for specialized formats, and significant contributions to critical research areas. While models like MoLFormer and Molformer focus on efficient molecular property prediction through specialized embeddings for SMILES strings [20,26], LLM4SD uniquely emphasizes interpretability and performance gains by synthesizing explicit rules from literature and data [22]. Hypothesis generation systems like MOOSE-Chem and HILMA offer the potential for novel discoveries and detailed experimental planning [6,23], complementing the automation and experimental capabilities provided by tool-augmented agents like ChemCrow and Coscientist [5,19]. This combination of predictive power, interpretability, hypothesis generation, and autonomous experimentation underscores the transformative impact of LLMs in these scientific domains.
#### 8.3.5 Physical Sciences, Earth, and Environmental Sciences
Large Language Models (LLMs) are increasingly being deployed across physical sciences, earth, and environmental sciences, leveraging their advanced reasoning and data interpretation capabilities to address complex challenges. These applications span from assisting with abstract mathematical proofs and simulating physical phenomena to analyzing vast geographical and environmental datasets [19,31].

In **mathematics and theoretical physical sciences**, LLMs demonstrate significant promise in areas demanding logical deduction and precise problem-solving. For instance, advanced models like OpenAI's o1 family exhibit strong mathematical reasoning, achieving 83% on the International Mathematics Olympiad (IMO) compared to GPT-4o's 13%, alongside other models such as Grok, Palm, and Qwen, which perform well on reasoning and mathematics benchmarks [15]. Beyond standard benchmarks, LLMs have been explored for highly challenging research problems, such as the "P = NP or not" question, where GPT-4 has been used in conjunction with Socratic reasoning to recursively discover, solve, and integrate complex problems [19]. The Multimodal-CoT framework, which achieved 91.68% accuracy on the ScienceQA benchmark, showcases the potential for LLMs to process and reason with both vision and language features, an essential capability for scientific problems integrating textual explanations with visual data [32]. Furthermore, LLMs contribute to hypothesis generation in fields like astronomy, as demonstrated by approaches leveraging adversarial prompting to formulate robust hypotheses [6]. A notable application in quantum mechanics, a sub-field of physical sciences, is the LLM4SD framework. This framework predicts 12 different properties from the QM9 dataset, including dipole moment and HOMO energy, achieving a Root Mean Square Error (RMSE) of 1.28, representing a 12.9% improvement over the second-best model [22]. It also shows a 48% accuracy improvement in predicting key quantum properties relevant for material design, demonstrating its utility in inferring rules from molecular data and translating them into interpretable features [3].

Adapting LLMs for the highly symbolic and quantitative nature of these fields often requires unique architectural or training considerations. The use of symbols to represent physical processes, such as algebraic equations and differential/integral forms in physics, and Navier-Stokes equations in earth science, necessitates specialized handling [18]. Domain-specific models like astroBERT, which uses Masked Language Modeling (MLM) and next-sentence prediction on astronomy papers, and AstroLLaMA, fine-tuned on arXiv astronomy abstracts, exemplify text-focused adaptations for physics. AstroLLaMA-chat further extends this by incorporating GPT-4 generated dialogue for conversational capabilities [31].

In **Earth, geological, and environmental sciences**, LLMs play a crucial role in analyzing vast and diverse datasets. They assist in interpreting environmental data to enhance understanding and decision-making [19]. Applications include climate modeling, environmental impact assessment, resource exploration, and natural hazard prediction [31]. For climate science, chatIPCC enhances GPT-4 by integrating information from the IPCC Sixth Assessment Report, grounding the AI in authoritative documents to provide accurate and reliable answers to climate change questions and mitigate issues of hallucination and outdated knowledge [5].

The architectural and training considerations in these domains are even more varied due to the multimodal nature of the data. Early encoder-based LLMs like ClimateBERT, SpaBERT, and MGeo utilize MLM on earth science research papers, climate articles, Wikipedia, and Point-of-Interest (POI) data for geo-entity linking and POI matching. More recent decoder-style autoregressive LLMs, such as K2, OceanGPT, and GeoGalactica, are fine-tuned with human-curated or LLM-augmented domain-specific instructions [31]. Beyond text, LLMs are integrated with other data modalities: ERNIE-GeoL fuses text and POI information, while PK-Chat integrates LLMs with knowledge graphs for dialogue systems. For visual data, UrbanCLIP utilizes a CLIP architecture. Critically, for climate modeling and weather forecasting, models like FourCastNet and Pangu-Weather employ Vision Transformers (ViT) and Swin Transformers on climate time series data such as ERA5 and CMIP6 [31]. In geotechnical engineering, LLMs like ChatGPT are being explored for applications such as building context-aware professional search engines, creating integrated interfaces for complex task flows, and facilitating chain-of-thought reasoning [5].

Comparing these advancements, a clear distinction emerges between general-purpose LLMs exhibiting strong foundational reasoning (e.g., o1, Grok, Palm, Qwen) and specialized scientific LLMs that are meticulously adapted for specific scientific domains and data types. While general models showcase impressive mathematical capabilities, specialized models like LLM4SD provide quantitative improvements for specific tasks in physical chemistry [3,22]. The evolution from text-focused models (e.g., astroBERT, ClimateBERT) to multimodal architectures (e.g., Multimodal-CoT, UrbanCLIP, ERNIE-GeoL) and those designed for time series (e.g., FourCastNet, Pangu-Weather) highlights a trend towards handling the diverse data inherent in scientific research [31,32]. Furthermore, strategies like grounding LLMs with authoritative documents (chatIPCC) [5] or employing Socratic reasoning (Dong's work) [19] represent crucial steps in mitigating LLM limitations, such as hallucination and enhancing their ability to tackle highly complex, abstract scientific and mathematical problems. The continuous development of these specialized models and integration techniques underscores the transformative potential of LLMs in accelerating scientific discovery across these quantitative fields.
#### 8.3.6 Economic, Financial, and Social Science Applications
This section delves into the transformative applications of Large Language Models (LLMs) across economic, financial, and social science domains, with a particular emphasis on their innovative use in agricultural intelligence. LLMs are rapidly reshaping these sectors by offering advanced capabilities in processing, analyzing, and synthesizing vast textual and numerical datasets, thereby enhancing decision-making, optimizing processes, and accelerating research and discovery [16,19,20].

The "Economic and Financial Applications" subsection will detail how LLMs are progressively enhancing forecasting, refining risk assessment methodologies, and providing robust decision support systems. Key applications include financial market prediction, where models like ChatGPT demonstrate an "emergent ability" to analyze news sentiment, significantly outperforming traditional methods in forecasting stock price movements [5]. The utility of LLMs extends to general financial data analysis, market predictions, and investment guidance, with the emergence of domain-specific models such as BloombergGPT highlighting a trend towards tailored solutions for the finance industry [4,20,25]. Within agriculture, LLMs facilitate "market dynamics analysis" and contribute to policy assessment by simulating intricate farmer behaviors, market reactions, and supply chain dynamics, providing crucial insights for informed agricultural policy-making [2,11]. This subsection will also explore the practical enterprise AI considerations for financial applications, including regulatory complexities, security, compliance, and cost-capability trade-offs, alongside broader societal concerns such as potential economic harms and job displacement [10,29].

The "Scientometric and Social Science Applications" subsection will explore LLMs' pivotal role in understanding the dynamics of scientific knowledge production and analyzing complex human and societal phenomena. In scientometrics, LLMs streamline academic processes by identifying scientific challenges, suggesting future research directions, assisting in expert reviewer selection, and generating reviews [31]. A notable example includes GPT-based approaches for scientometric analysis in AI research, which demonstrate high accuracy in classifying and analyzing academic articles to map publication trends and interdisciplinary connections [5]. For social sciences, LLMs empower researchers by enabling hypothesis generation from public web data, simulating social environments with LLM agents, and providing insights into public sentiment and emerging trends through social media analysis [19]. In agricultural contexts, LLMs enhance social services by simplifying scientific knowledge, offering personalized recommendations, and overcoming language barriers for diverse farming communities through agricultural extension services and LLM-powered chatbots for public engagement [2]. This section will further discuss applications in health-related social science, such as diagnosing Alzheimer's disease through communication pattern analysis, and in educational research, addressing pedagogical impacts and cultural considerations [25,32].

A central theme uniting these diverse applications is the significant potential of LLMs to address global food production challenges. By boosting agricultural productivity through personalized agronomic advice, natural language control of farm robotics, and accelerating innovation via R&D assistance and hypothesis generation, LLMs offer pathways to mitigate climate-related pressures and improve agricultural policy through scenario simulation and enhanced public engagement [2]. While both economic/financial and scientometric/social science applications leverage LLMs for processing extensive textual datasets, their objectives often diverge; the former focuses on predictive and quantitative analysis for market and risk assessment, while the latter delves into more nuanced human behavior, communication, and societal dynamics to generate actionable insights [5,19]. Finally, this section will underscore the critical ethical considerations inherent in these applications, including algorithmic bias, data privacy, potential for misinformation, and the necessity for transparency, interpretability, fairness, and accountability to ensure responsible development and deployment of LLMs in these sensitive and impactful domains [2,5,32].
##### 8.3.6.1 Economic and Financial Applications
Large Language Models (LLMs) are progressively transforming economic and financial sectors by enhancing forecasting capabilities, refining risk assessment methodologies, and providing robust decision support systems. These domains, characterized by their data-rich yet sensitive nature, present both unique opportunities and significant challenges for LLM integration [5].

A prominent application of LLMs lies in financial market prediction. Research indicates that models such as ChatGPT can analyze news headlines for sentiment, significantly outperforming traditional sentiment analysis methods in predicting stock price movements [5]. This predictive capability is described as an "emergent ability" specific to larger models (e.g., ChatGPT over GPT-1, GPT-2, BERT), enabling more accurate forecasts and improved performance in quantitative trading strategies [5]. Beyond stock prediction, LLMs are generally applied in financial data analysis to boost overall predictive capabilities and strengthen risk assessment frameworks [19].

In terms of decision support, LLMs offer versatile tools across various economic sub-sectors. For instance, in agriculture, LLMs can simulate intricate behaviors of farmers, market reactions, and supply chain dynamics. This provides crucial insights that can inform agricultural policy-making and contribute to a deeper understanding of economic and financial impacts within the sector [2]. Furthermore, LLMs are being leveraged for "market dynamics analysis" within agriculture, specifically to comprehend and predict economic trends pertinent to the industry [11]. The broader utility of LLMs in finance extends to supporting financial analysis, general market predictions, and investment guidance [25].

The development of domain-specific LLMs is a critical trend in this area. BloombergGPT, for example, represents a specialized LLM meticulously designed for financial tasks, highlighting the potential for models tailored to specific industry needs [4]. Initial research into other finance-focused LLMs has also yielded encouraging outcomes, demonstrating enhanced efficacy in financial assignments while maintaining parity with general benchmarks [20]. However, it is important to note that specific details regarding methodologies, experimental setups, and performance metrics for some of these finance-focused LLMs are often lacking in initial reports, making direct comparison and comprehensive evaluation challenging [20]. This contrasts with the more specific findings regarding ChatGPT's emergent abilities in sentiment-based stock prediction [5].

Despite the promising applications, integrating LLMs into financial institutions presents unique challenges. Practical enterprise AI considerations for financial applications include navigating complex regulatory postures, balancing cost-capability trade-offs, ensuring stringent security, upholding compliance standards, and managing deployment patterns effectively [29]. Moreover, the broader societal implications of LLM adoption cannot be overlooked. Concerns about "economic harms," such as potential job displacement and the widening of inequality gaps between those with and without access to these advanced technologies, highlight the need for careful consideration and responsible development [10]. These considerations underscore the necessity for a balanced approach, leveraging the predictive and analytical power of LLMs while diligently addressing ethical, regulatory, and socio-economic challenges inherent in these sensitive domains.
##### 8.3.6.2 Scientometric and Social Science Applications
Large Language Models (LLMs) are increasingly pivotal in transforming scientometric and social science research, primarily through their advanced capabilities in processing, analyzing, and synthesizing vast textual datasets [5,19]. These applications range from mapping scientific landscapes and identifying emerging research trends to facilitating nuanced social inquiry and public engagement.

In scientometrics, LLMs offer sophisticated tools for understanding the dynamics of scientific knowledge production. They can be integrated into search engines to identify scientific challenges, suggest future research directions, assist in finding expert reviewers, and even generate automatic reviews, thereby streamlining academic processes [31]. Beyond these foundational utilities, LLMs are instrumental in synthesizing knowledge to pinpoint trends and gaps within specific research domains, such as agricultural research and development [2]. A notable methodological advancement is a GPT-based approach designed for scientometric analysis in the rapidly evolving field of Artificial Intelligence (AI) research [5]. This method employs a multi-step filtering process, meticulously combining Web of Science (WoS) citation themes, categories, and keywords with GPT classification. This results in a highly accurate identification of AI-related articles, demonstrating an accuracy of 90% and recall of 94% across the entire WoS corpus. Subsequently, it facilitates the analysis of publication volume trends, interdisciplinarity, identification of top countries and institutions through citation analysis, and determination of common research themes using keyword analysis and GPT [5]. Furthermore, LLMs can bolster structured review methods, akin to systematic literature reviews, by employing thematic analysis frameworks to synthesize findings from numerous primary studies, as exemplified by a review on LLMs in education [25].

In social sciences, LLMs empower researchers to explore complex human and societal phenomena. They are utilized for hypothesis generation by extracting and structuring information from publicly available web data related to social science concepts [19]. LLMs also significantly contribute to analyzing social media data, providing crucial insights into public sentiment and emerging social trends [19]. Beyond broad societal analysis, LLMs find applications in specialized social science domains. For instance, in health-related social science, language models are employed to analyze human communication patterns, such as using semantic coherence markers for the early diagnosis of Alzheimer's disease [32]. This application showcases LLMs' capacity to derive health insights from linguistic features, bridging computational methods with human cognition and health conditions. In the agricultural sector, LLMs simplify complex scientific knowledge, offer personalized recommendations, and overcome language barriers, thus enhancing communication and understanding within farming communities through agricultural extension services [2]. LLM-powered chatbots further improve interaction and understanding of citizens' needs, fulfilling a vital social science function in community engagement [2]. The application of LLMs in education, addressing pedagogical impacts and cultural considerations and analyzing educational trends, also constitutes a significant social science contribution [25]. While some works broadly assert LLMs' potential to revolutionize social sciences [16,20], the specific examples highlight their concrete and diverse impacts.

Comparing these applications, both scientometrics and social sciences harness LLMs' prowess in processing and interpreting extensive textual datasets. However, their objectives and resultant analyses often diverge. Scientometric applications, such as the GPT-based AI research analysis, typically focus on quantitative and structural analyses of academic outputs to map research landscapes, identify macro-level trends, and assess impact with metrics like accuracy and recall [5]. In contrast, social science applications frequently delve into more nuanced human behavior, communication, and societal dynamics. This involves tasks like generating hypotheses, sentiment analysis, and facilitating personalized information dissemination, often translating qualitative data into actionable insights for specific communities [2,19,32]. While the GPT-based scientometric approach offers a highly structured and automated pipeline for classifying and analyzing research articles with measurable performance [5], social science applications utilize LLMs for more interpretive tasks, such as thematic analysis in educational research or linguistic pattern detection for health diagnostics [25,32].

The application of LLMs in these fields introduces significant ethical considerations [5]. In scientometrics, there is a risk of algorithmic bias influencing the identification of emerging topics, the ranking of researchers, or the selection of expert reviewers, potentially perpetuating existing inequalities or overlooking novel perspectives. For social sciences, concerns include data privacy, especially when analyzing sensitive social media or health-related linguistic data [32]. The potential for algorithmic bias in sentiment analysis or hypothesis generation could lead to misinterpretations of social phenomena, biased policy recommendations, or the reinforcement of stereotypes. Furthermore, the simplification of scientific knowledge for public consumption, while beneficial for accessibility, requires careful oversight to prevent oversimplification, misinformation, or manipulation [2]. Ensuring transparency, interpretability, fairness, and accountability in LLM applications is paramount to mitigate these risks and uphold the integrity of research in both scientometric and social science domains.
## 9. Agentic Science: Towards Autonomous Scientific Discovery
The advent of Scientific Large Language Models (Sci-LLMs) has catalyzed a fundamental paradigm shift in scientific research, moving beyond their role as mere analytical tools to envision them as proactive agents capable of driving autonomous scientific discovery. This transformation defines "Agentic Science," a cutting-edge frontier where Sci-LLMs are empowered to design, execute, validate, and learn from experiments autonomously, thereby contributing to and evolving scientific knowledge within self-contained closed-loop systems [18,19].



![Iterative Cycle of Agentic Science](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/eYAAZbz8pAbWrVsd3owI4_Iterative%20Cycle%20of%20Agentic%20Science.png)

At the core of Agentic Science lies the iterative "plan-execute-feedback-improve" cycle, which serves as the foundational architectural principle for these autonomous systems [28]. In this cycle, LLMs generate experimental or computational plans based on defined research objectives. These plans are then executed, either directly by the LLM agent or through delegation to specialized external tools. Critical to this autonomy is the acquisition of environmental feedback, which can manifest as natural language descriptions, multimodal signals, or structured data. This feedback loop allows the LLM agents to dynamically adjust and refine their strategies to achieve desired outcomes without continuous human intervention [28]. This framework supports the independent discovery of hypotheses, exemplified by systems such as MOOSE, which utilizes LLM-based agents to search web corpora for research questions [19].

A pivotal enabler for this agentic capability is the sophisticated integration of LLMs with external tools and knowledge bases. This integration addresses inherent limitations of LLMs, such as their inability to perform precise numerical calculations or directly access real-time external information from their pre-training data, effectively providing LLMs with "eyes and ears" beyond their internal knowledge confines [17,30,33]. Tools ranging from general utilities like search engines and code interpreters to specialized scientific applications via plugin mechanisms significantly expand the functional scope of Sci-LLMs [17,28,30,33]. Examples abound in various scientific domains: ChemCrow leverages 18 expert tools for autonomous chemical synthesis, bridging computational and experimental chemistry [5,19]; LLM4SD integrates with RDKit to translate natural language rules into executable functions for molecular property prediction [22]; and GeneGPT enhances biomedical research by connecting to vast external knowledge via NCBI Web APIs [5]. Furthermore, the ability for LLMs to interface with and control physical laboratory setups, as demonstrated by Coscientist in optimizing reactions and CLARify in automating chemical experiments with robots, underscores their growing capacity to interact with the tangible world [19,26].

Beyond individual tool use, the frontier of Agentic Science is increasingly defined by advanced agentic capabilities, including sophisticated feedback mechanisms, robust memory systems, and collaborative multi-agent architectures. Plan improvement techniques, such as "推理改进" (reasoning improvement) utilizing methods like ReAct and ChatCoT, and "回溯调整" (backtracking adjustment) through search algorithms like Tree of Thoughts, DEPS, and TIP, are vital for refining experimental and computational strategies [28]. Long-term memory, supported by systems like Reflexion, Generative Agents' memory streams, skill libraries, and vector databases, ensures consistency and coherence across extended scientific investigations [28,29]. The integration of multiple specialized agents within a collaborative framework allows for the tackling of complex, end-to-end scientific workflows. Systems like Coscientist and the Stanford Virtual Lab, which combine various expert agents and language models, illustrate the potential for automated design, evaluation, and validation of scientific hypotheses, such as in the discovery of SARS-CoV-2 nanobodies [8,19]. Dynamic knowledge updating, facilitated by integrating LLMs with structured knowledge bases and search engines, ensures that these agents can continuously access and incorporate new information, even demonstrating the ability to autonomously create new tools based on task demands [11,18,28,30].

Despite significant progress, critical challenges remain in realizing fully autonomous scientific discovery. A key limitation is the "experimental verification difficulty," highlighting the current gap in the ability of LLM-generated hypotheses to be independently validated by the systems themselves [13]. Furthermore, efficient architectural solutions for handling long context processing are essential for autonomous agents to manage extended interactions and complex information flows [12]. Addressing these challenges, alongside the engineering complexities of building robust, production-grade AI agents, will pave the way for a future where Sci-LLMs act as "true partners" in accelerating scientific discovery [1,18,29]. The trajectory points towards self-evolving knowledge bases and sophisticated "AI Virtual Cells" capable of simulating complex scientific phenomena, marking a profound shift towards truly intelligent and autonomous scientific exploration [8].
### 9.1 Autonomous Scientific Agents and Closed-Loop Systems
The emergence of Scientific Large Language Models (Sci-LLMs) heralds a paradigm shift, envisioning them not merely as passive data processors but as proactive agents driving scientific discovery [18]. This vision moves towards "Agentic Science," the latest evolution in Sci-LLM applications, wherein autonomous agents based on Sci-LLMs actively design, execute, validate, and learn from experiments, thereby contributing to an evolving knowledge base within closed-loop systems [18,19].

The architecture of such closed-loop systems is characterized by an iterative "plan-execute-feedback-improve" cycle [28]. In this model, LLMs generate experimental plans based on specific research goals, then either execute these plans themselves or delegate execution to external tools [28]. Environmental feedback, which can be in natural language or multimodal signals, informs the LLM agent about the outcome of the execution, allowing for autonomous adjustments and refinements to achieve objectives [28]. This process enables LLM-based agents to independently discover hypotheses without human intervention, as exemplified by systems like MOOSE, which utilizes LLM-based agents to search web corpora for research questions [19].

This proactive approach is built upon the foundational capability of LLMs to manipulate external tools, analogized to providing LLMs with "eyes and ears" to overcome their inherent deficiencies [17,30,33]. Tools such as calculators for numerical computations, search engines for information retrieval, and specialized applications via plugin mechanisms significantly expand LLM capabilities beyond their internal knowledge [17,30]. Modern LLMs, such as Claude's Opus 4 and Gemini Flash, are designed with advanced agentic workflows and speed for real-time applications, indicating a broader industry trend towards more autonomous and sequential operations [15]. Similarly, Google's Sensible Agent reframes AR assistance as a "coupled 'what+how' decision," showcasing LLMs' role in integrating reasoning with tool use [29].

Several systems demonstrate progress towards autonomous scientific agents. ChemCrow augments LLMs with 13 expert tools for autonomous operation in chemistry tasks [5]. The CLARify system utilizes GPT-3 to generate executable experimental plans for chemical robots, achieving higher accuracy in automating experimental design and execution than baseline methods, signifying an early form of closed-loop automation [26]. CRISPR-GPT functions as an LLM agent that enhances gene editing experiment design by leveraging domain knowledge [31]. The LLM4SD framework exemplifies an "eyes and ears" capability by interfacing LLM reasoning with external computational tools (e.g., RDKit functions) to generate interpretable rules for hypothesis validation and contribute to a knowledge base, even if not fully autonomous in experimental execution [3,22]. Furthermore, the MatSci-LLM-based materials discovery cycle outlines a systematic process for LLMs to actively contribute to scientific discovery through hypothesis generation and testing, implicitly aligning with closed-loop systems [7].

To achieve fully autonomous scientific research and self-evolving knowledge bases, there are several critical steps and challenges. A significant limitation is the "experimental verification difficulty" for LLM-generated hypotheses, highlighting a gap in the ability of current systems to independently validate their findings [13]. Additionally, efficient architectural solutions are needed for LLMs to handle long context processing, which is crucial for extended interactions and complex information processing required by autonomous agents [12].

The potential of integrated multi-agent AI systems to revolutionize scientific workflow is substantial, enabling complex, end-to-end experimental design and validation [8]. Multi-LLM systems like Coscientist and LLM-RDF exemplify this by utilizing specialized agents to extract methods, translate natural language into standardized protocols, generate execution code for automated platforms, and adaptively correct errors during execution [19]. The Stanford Virtual Lab further illustrates this by integrating multiple expert agents (e.g., Chief Researcher, Immunologist, ML Expert) and language models to achieve automated design of SARS-CoV-2 nanobodies with minimal human intervention, performing a closed-loop process of design, evaluation, and validation [8]. This reflects the LLMs acting as "eyes and ears" that actively experiment and contribute to knowledge. The HILMA framework, while integrating human experts, employs a multi-agent debate method where different LLM agents play specific roles (proposer, reviewer, analyst) to simulate and refine scientific hypotheses, showcasing collaborative intelligence [23]. Other notable examples of agentic LLMs include Alibaba's Tongyi DeepResearch, NVIDIA AI's Universal Deep Research (UDR), Biomni-R0 for biomedical research, and the Allen Institute for AI's AutoDS, described as a "groundbreaking prototype engine for open-ended autonomous scientific discovery" [29]. The concept of "AI Virtual Cells" further proposes simulating molecular, cellular, and tissue dynamics using various models, implying a sophisticated autonomous system for understanding and engineering life [8]. The overall trend points towards enhancing Sci-LLMs through integration with professional tools and intelligent agents, pushing the boundaries towards agentic capabilities [1]. This move from instruction-following models to self-directed scientific explorers represents a profound shift towards LLMs as "true partners" in accelerating scientific discovery [18].
### 9.2 Enhanced Tool Integration and Agentic Capabilities
The augmentation of Scientific Large Language Models (Sci-LLMs) through integration with external knowledge bases, physical simulation tools, and intelligent agents represents a critical frontier for advancing scientific discovery systems [1,30]. This integration addresses inherent limitations of LLMs, such as their inability to perform accurate numerical calculations or access up-to-date, external information directly from their pre-training data [17,30]. By establishing sophisticated interfaces with external resources, Sci-LLMs can transcend their foundational text-generation capabilities to become more robust, adaptive, and ultimately, autonomous scientific discovery engines [1].

The mechanisms for achieving this enhanced capability primarily revolve around tool manipulation, where LLMs act as central controllers within complex workflows. This involves their capacity to utilize external tools such as search engines for information retrieval, calculators for precise computations, and code interpreters for executing domain-specific logic [28,30,33]. For instance, the plugin mechanism in ChatGPT exemplifies this, significantly extending the functional scope of LLMs by allowing integration with various external applications [17,30]. Beyond general utilities, Sci-LLMs are increasingly integrated with specialized scientific tools. ChemCrow, for example, is an LLM-based chemical agent that leverages 18 expert-designed tools to autonomously plan and execute complex chemical syntheses, bridging computational and experimental domains [5,19]. Similarly, the LLM4SD framework integrates LLMs with RDKit, enabling the LLM to generate natural language rules that are translated into executable functions for molecular property prediction and hypothesis generation, such as converting "molecular weight < 500 mol" into a specific `rdMolDescriptors.CalcExactMolWt(mol)` indicator function [22]. In the biomedical domain, GeneGPT enhances LLMs by connecting them to vast external biomedical knowledge sources via NCBI Web APIs [5], while CRISPR-GPT supports gene editing design through specialized domain knowledge integration for experimental planning [31].

The integration extends to sophisticated physical simulation tools and real-world experimental setups. Coscientist integrates LLMs with laboratory automation to optimize reactions, demonstrating their capacity to control tangible experimental environments [19]. The CLARify system further exemplifies this by using a general LLM (GPT-3) to generate plans that can be executed by chemical robots, leading to improved efficiency and accuracy in automating chemical experiments [26]. In a broader context, the Stanford Virtual Lab combines models like AlphaFold-Multimer, Rosetta, and ESM within a multi-agent framework to achieve automated scientific discovery, aligning with a vision of "AI Virtual Cells" capable of simulating molecular, cellular, and tissue dynamics [8]. This highlights the potential of LLMs to interact with and orchestrate complex simulation environments to deepen scientific understanding.

A crucial aspect of advanced scientific LLMs lies in their agentic capabilities, particularly concerning agent collaboration and dynamic knowledge updating. The future trajectory for Sci-LLMs is envisioned as a progression towards "autonomous scientific agents" that can systematically plan, execute, reflect, and iterate on scientific tasks [18]. These agents require robust mechanisms for feedback acquisition, which can stem from internal LLM evaluations of plan quality or external sources like error messages from code interpreters or visual perceptions from simulations [28]. Plan improvement techniques such as "推理改进" (reasoning improvement) using methods like ReAct and ChatCoT, and "回溯调整" (backtracking adjustment) through search algorithms like Tree of Thoughts, DEPS, and TIP, are vital for refining experimental and computational strategies [28]. Furthermore, long-term memory, supported by systems like Reflexion, Generative Agents' memory streams, skill libraries, and vector databases, ensures consistency and coherence across extended scientific investigations [28]. The development of modular multi-agent memory systems like MIRIX further underscores the importance of persistent state and reasoning for LLM-based agents [29].

Agent collaboration is highlighted as both a challenge and a necessity, particularly in areas like agriculture, where LLMs need to integrate with external tools and continuously update their knowledge bases with new information and discoveries [11]. The "agent" system for autonomous scientific experimentation integrates internet access, Python code execution, document retrieval, and physical experiment control, demonstrating a high degree of tool integration for scientific reasoning and problem-solving [5]. Dynamic knowledge updating is facilitated by integrating LLMs with structured knowledge bases, such as vector databases and knowledge graphs, which are critical for managing vast amounts of agricultural information [11]. The use of search engines also contributes to accessing unknown or up-to-date information, thereby implicitly enabling dynamic knowledge enhancement [30]. The ability for LLMs to even autonomously create new tools based on task demands [28] and continuously contribute to an evolving knowledge base [18] points towards a truly closed-loop scientific discovery system.

Comparing specific applications, while ChemCrow and GeneGPT focus on deep integration within chemistry and biomedicine respectively, utilizing a predefined set of expert tools [5,19], systems like DocPilot demonstrate broader modular task planning and code generation for automating repetitive tasks across scientific document workflows [19]. The `agent` system emphasizes a comprehensive integration spanning information access, computation, and physical control, showcasing a holistic approach to scientific experimentation [5]. In contrast, chatIPCC focuses on grounding LLMs with authoritative external knowledge (IPCC AR6 documents) to improve accuracy and mitigate misinformation in climate science [5]. While some LLMs like Grok-4-Fast are trained end-to-end with tool-use reinforcement learning [29], others emphasize modularity and scalability through Prompt Orchestration Markup Language (POML) to define advanced agentic workflows [29]. The engineering complexity behind building production-grade AI agents, encompassing data plumbing, controls, and observability, is recognized as equally critical as the core AI models themselves [29].

Ultimately, these integrations—from leveraging external knowledge bases and physical simulations to fostering agent collaboration and dynamic knowledge updating—are crucial for enabling more robust and autonomous scientific discovery systems. They empower Sci-LLMs to move beyond mere information processing towards active participation in the scientific method, capable of proposing hypotheses, designing experiments, interpreting results, and continuously expanding the collective scientific knowledge.
## 10. Evaluation and Benchmarking of Scientific LLMs
The robust evaluation and benchmarking of Scientific Large Language Models (Sci-LLMs) are paramount to understanding their capabilities, limitations, and potential for driving scientific discovery. This section provides a comprehensive overview of the current state of Sci-LLM assessment, detailing the evolving frameworks and the diverse, yet often constrained, landscape of benchmarks. It critically examines the methodologies employed to quantify Sci-LLM performance, ranging from computational metrics to real-world validation, while simultaneously addressing the significant challenges and inherent gaps in existing evaluation paradigms [1,20].

The evaluation landscape for Sci-LLMs encompasses a broad array of computational metrics and methodologies designed to assess performance across specialized scientific tasks. These include F1 scores for relation extraction, accuracy for question answering, perplexity for semantic coherence, and quantitative measures for tasks like surgical Visual Question Answering (VQA) and scientometric analysis [5,32]. Domain-specific evaluations extend to graph-aware Sci-LLMs in retrieval and recommendation, mathematical reasoning benchmarks (e.g., MMLU-STEM, Big-Bench Hard), and tasks in biomedicine such as protein structure prediction [31]. Frameworks like LLM4SD further introduce metrics for molecular property prediction, involving both classification and regression [3]. Despite this variety, a critical gap exists between computational assessment and the "gold standard" of real-world "wet-lab experiments," often due to impracticality and resource demands [1,13]. This is exemplified by the need for practical utility and scientific validity in materials science, where current LLMs "fall short of being practical materials science tools" without robust empirical verification [7]. Successful real-world validations, such as AlphaFold 3's accuracy in predicting protein-ligand complexes and laboratory testing of SyntheMol's antibiotics, underscore the necessity for practical deployment in chemical experiments and clinical trials to confirm the true effectiveness of LLMs in fields like drug discovery [8,26]. Beyond these domain-specific issues, general limitations in LLM evaluation stem from models' susceptibility to hallucinations, leading to difficulties in ensuring "correctness/faithfulness/completeness," and "brittle evaluations" where minor prompt changes drastically alter results, highlighting a lack of robustness and consistent evaluation due to their probabilistic nature [16,20,28,29].

In parallel, a diverse ecosystem of benchmarks has emerged, ranging from general LLM evaluations like MMLU and BIG-bench to specialized ones such as MMLU-STEM for science and CodeXGLUE for code generation [20,28]. For Sci-LLMs specifically, benchmarks span the entire scientific research lifecycle, including hypothesis discovery (e.g., DiscoveryBench, DiscoveryWorld), experiment planning (e.g., MLAgentBench, BioPlanner), scientific writing (e.g., ALCE, CiteBench), and peer review (e.g., MOPRD, NLPeer) [6,19]. Domain-specific benchmarks are prevalent in molecular science (e.g., MoleculeNet for LLM4SD, Chemformer, Molformer), biomedicine (e.g., BioGPT on BC5CDR, PubMedQA; ChatGPT on medical licensing exams), and other areas like mathematics (e.g., GSM8K, MATH), physics (AstroLLaMA), and surgical VQA [3,5,21,22,25,26,31,32]. Even with over 190 datasets for Sci-LLMs, these benchmarks face significant limitations: they often rely on **traditional, static evaluation metrics and human-written ground truth**, which quickly become insufficient for evolving LLM capabilities and fail to capture true scientific reasoning [16,18]. This can lead to "hallucinated reasoning chains" that produce correct answers but undermine reliability [32].

The ability to capture **complex reasoning and domain-specific nuances** remains a critical challenge, as many evaluations rely on "simplified, static scenarios" that do not reflect real-world scientific problems, particularly in areas like hypothesis generation or experimental design [18,29]. The **robustness and reliability of current evaluation systems** are also under scrutiny, with "brittle evaluations" leading to biased outcomes [25]. The inability of LLMs, such as GPT-4, to reliably act as evaluators ("LLM-as-a-Judge") for their own or other models' outputs highlights a significant flaw, as they may struggle to distinguish between correct and incorrect answers, especially in complex scientific domains like chemistry [5,28,29]. This underscores the indispensable role of **human expert validation** for ensuring scientific rigor, particularly for novel tasks like hypothesis generation or when dealing with "noisy or redundant" generated rules [5,22,23,34]. Finally, a critical deficiency is the **lack of standardization and transparency** across the field, making cross-domain comparisons difficult and obscuring critical bottlenecks and future research directions, partly due to undisclosed training details of many industrial LLMs [20,30].

In response to these complexities, evaluation paradigms for Sci-LLMs are evolving from static knowledge recall towards assessing abilities related to scientific processes, creativity, and discovery [6,18]. This shift aims to evaluate problem-solving processes, experimental design, and the generation of novel hypotheses, moving towards assessing emergent scientific intelligence [18]. Techniques like "probing" internal representations and incorporating interpretability metrics, as seen in LLM4SD, are becoming crucial for understanding a model's internal workings and fostering trust [3,22,24]. Ultimately, the research community emphasizes the critical need for "new evaluation methodologies that move towards process- and discovery-oriented assessments" and the "development of evaluation tools and benchmarks for Sci-LLMs" that can provide more comprehensive, fair, and dynamic assessments, extending beyond simplified scenarios to better reflect the intricacies of real-world scientific inquiry [1,18].
### 10.1 Evaluation Frameworks and Evolving Paradigms
The evaluation of Scientific Large Language Models (Sci-LLMs) presents a complex landscape, necessitating diverse metrics and evolving paradigms to adequately assess their performance in specialized scientific domains. Current approaches encompass a wide array of computational methods, yet simultaneously grapple with inherent limitations and a critical gap concerning real-world validation [1,20].

A diverse set of computational metrics and methodologies are employed to quantify Sci-LLM capabilities across various scientific tasks. For instance, BioGPT's performance is evaluated using F1 scores for relation extraction tasks and accuracy for question answering, while Multimodal-CoT utilizes accuracy on benchmarks like ScienceQA, and perplexity serves as an information-theoretic measure in contexts such as Alzheimer's diagnosis [32]. In specialized applications, SurgicalGPT's efficacy is measured quantitatively on surgical Visual Question Answering (VQA) tasks using public datasets, demonstrating its superior performance [5]. Similarly, GPT-based scientometric analysis is assessed by its accuracy and recall in identifying AI-related articles [5]. The evaluation of graph-aware Sci-LLMs in general scientific tasks includes retrieval, recommendation, and author name disambiguation [31]. For mathematics, tasks like Question Answering (QA) and Mathematical Word Problem (MWP) solving, alongside benchmarks such as MMLU-STEM and Big-Bench Hard, gauge quantitative reasoning [31]. In biomedicine, evaluation tasks range from single-round QA to protein structure prediction [31]. Furthermore, frameworks like LLM4SD are evaluated on benchmark tasks from datasets such as MoleculeNet for molecular property prediction, involving both classification and regression tasks [3]. Bias evaluation often utilizes specific metrics like those derived from ideal context association (iCAT) texts to assess stereotypical bias in LLMs [4].

Despite the proliferation of computational metrics, establishing definitive assessment criteria for complex outputs like generated molecules or proteins remains a significant challenge [1]. Computational evaluations are often considered "not decisive," especially when compared to the "gold standard" of real-world "wet-lab experiments" [1,13]. This discrepancy highlights a substantial gap between computational assessment and empirical verification, largely due to the impracticality and resource demands of wet-lab validation for many AI research teams [1]. For instance, while LLMs show promise in materials science, current models "fall short of being practical materials science tools," necessitating evaluation frameworks that prioritize practical utility and scientific validity [7]. Successful examples of empirical validation include AlphaFold 3 achieving experimental-level accuracy in predicting protein-ligand complexes, and SyntheMol's designed antibiotics, along with nanobodies from the Stanford Virtual Lab, undergoing laboratory testing and verification [8]. This underscores the crucial need for real-world validation in fields such as drug discovery, where computational assessments require practical deployment in chemical experiments and clinical trials to ascertain the true effectiveness of LLMs [26]. Beyond these domain-specific challenges, general limitations in LLM evaluation include difficulties in ensuring "correctness/faithfulness/completeness" due to the models' susceptibility to hallucinations [20,28], as well as "brittle evaluations" where minor prompt alterations can drastically change results, indicating a lack of robustness [16]. The probabilistic nature of LLMs further complicates consistent evaluation [29].

In response to these complexities, evaluation paradigms for Sci-LLMs are evolving from measuring static knowledge recall towards assessing abilities related to scientific processes, creativity, and discovery [6,18]. This significant shift moves beyond traditional knowledge-based assessments (e.g., exams or Q&A) to advanced protocols that evaluate a model's problem-solving process, its capacity to design experiments, and its potential to generate novel hypotheses, thereby assessing emergent scientific intelligence [18]. For instance, Multimodal-CoT's ability to generate intermediate reasoning chains reflects an evolving focus on evaluating the problem-solving *process* itself, rather than merely the final answer [32]. Techniques like "probing" and "intervention" delve into a model's internal representations, evaluating the quality of internal knowledge and verifying its active use in predictions, thus representing a process-oriented assessment [24]. The LLM4SD framework exemplifies this shift by incorporating interpretability as a key metric, generating explainable rules from LLM outputs to foster trust and provide scientific insights beyond mere predictive accuracy [3,22]. Furthermore, advancements in clinical reasoning evaluations, such as SDBench and MAI-DxO, signify a move towards more realistic and process-oriented assessments in complex domains [29].

Despite the emergence of sophisticated evaluation methods, mechanisms like "LLM-as-a-Judge" face critical limitations, underscoring the continued importance of human expert validation in ensuring scientific rigor [5]. While model-based methods, employing powerful LLMs such as GPT-4 as evaluators, can expedite evaluation processes, they are susceptible to inherent biases and inconsistencies [28,29]. A notable case is the evaluation of ChemCrow, where GPT-4 acting as an evaluator "could not distinguish between its own obviously incorrect answers and the correct answers generated by GPT-4 augmented with ChemCrow," revealing a significant flaw in LLMs' self-assessment capabilities [5]. Moreover, direct LLM evaluations exhibit high consistency with human expert judgments in social sciences but are often deemed unreliable in complex natural sciences like chemistry, where specialized expertise is paramount [19]. Consequently, human expert validation remains indispensable across various scientific tasks. For instance, in generating drug discovery review articles, human authors conducted "thorough information veracity review" and rewrote manuscripts to ensure scientific standards, indicating that qualitative assessment and expert oversight are crucial for AI-generated scientific writing [5]. Similarly, in developing LLM-powered tutoring systems, human educators are essential for pedagogical alignment [25]. For novel tasks such as scientific hypothesis generation, a hybrid evaluation approach combining model-based automatic assessment with human expert evaluation is proposed, leveraging domain-expert annotators for pairwise comparisons to ensure rigor [23]. This collective evidence strongly supports the ongoing necessity of human expertise to validate the accuracy, scientific soundness, and trustworthiness of Sci-LLM outputs, especially when facing "noisy or redundant" generated rules [22,34].
### 10.2 Current Benchmarks and Limitations
The evaluation of Large Language Models (LLMs), particularly Scientific Large Language Models (Sci-LLMs), relies on a diverse array of benchmarks designed to probe their capabilities across various tasks and domains. Prominent general LLM benchmarks include MMLU, BIG-bench, and HELM, which assess knowledge understanding, common-sense reasoning, and multi-scenario performance, respectively [28]. Human exam benchmarks such as AGIEval, MCMU, M3KE, and C-Eval further evaluate comprehensive abilities by simulating human-like tests [28]. Task-specific general benchmarks also exist for language generation (e.g., LAMBADA, HumanEval), knowledge utilization (e.g., TriviaQA), complex reasoning (e.g., GSM8k), and human alignment (e.g., TruthfulQA) [28]. More specialized benchmarks like MTPB evaluate multi-turn program synthesis for code generation [30], CodeXGLUE targets code completion tasks [20], and OMEGA and VERINA assess mathematical generalization and verifiable code generation, respectively [29]. Moreover, the Othello-GPT experiment introduces benchmarks for evaluating the internal learning and world models of LLMs, measuring error rates in predicting legal moves and probing internal states [24]. Benchmarks like iCAT texts and Stanford studies are also used to assess biases in LLMs [4].

For Sci-LLMs, benchmarks span the entire scientific research lifecycle and specialized scientific domains [13]. In **hypothesis discovery**, benchmarks exist for Literature-Based Discovery (LBD), which focus on generating new discoveries from existing knowledge, and Data-Driven Discovery, which involves LLMs identifying patterns from experimental observations. Specific examples include DiscoveryBench for structured hypothesis generation and DiscoveryWorld for virtual environment-based hypothesis testing [19]. For general scientific idea generation, benchmarks like ResearchBench and IdeaBench have been developed [6]. In **experiment planning and implementation**, benchmarks like MLAgentBench support ML experiment design and BioPlanner aids in biological protocol planning [6,19]. **Scientific writing** is evaluated using benchmarks for citation text generation (e.g., ALCE, CiteBench), related work generation (e.g., AAN, S2ORC, though without universal standards), and drafting from scientific tables or integrating external information (e.g., SciGen, SciXGen) [19]. For **peer review**, datasets such as MOPRD and NLPeer offer comprehensive evaluation, while ASAP-Review and Reviewer2 focus on specialized aspects like opinion synthesis [19].

Domain-specific Sci-LLM benchmarks are also prevalent. In **molecular science and drug discovery**, LLM4SD is evaluated on the MoleculeNet dataset, which comprises 58 tasks covering physiology, biophysics, physical chemistry, and quantum mechanics, comparing LLM performance against traditional methods and Graph Neural Networks [3,22]. Chemformer and Molformer are assessed for reaction prediction and molecular property prediction, respectively [26]. For **biomedicine and medical education**, BioGPT is evaluated on specific NLP tasks (e.g., BC5CDR, PubMedQA), and multimodal Sci-LLMs on ScienceQA [32]. ChatGPT's performance has been benchmarked on national medical licensing exams in China (NMLE, NPLE, NNLE) [21] and USMLE-style questions for Med-PaLM [26], as well as BLS and ACLS exams [25]. Other scientific domains also feature specialized benchmarks: SciDocs and SciRepEval for graph-aware Sci-LLMs [31], various benchmarks for mathematics (e.g., GSM8K, MATH, MMLU-STEM) and geometry (e.g., Geometry3K) [31], AstroLLaMA for physics-related text generation [31], and surgical VQA datasets for SurgicalGPT [5].

Despite the proliferation of these benchmarks, including over 190 datasets specifically for Sci-LLMs [18], significant limitations hinder their ability to fully capture the capabilities of these models, especially concerning complex reasoning, domain-specific nuances, and the need for robust evaluation. A primary critique is the pervasive reliance on **traditional, static evaluation metrics and human-written ground truth** [16]. Such static assessments are often subjective and quickly become insufficient as LLM capabilities evolve, failing to measure true scientific reasoning and innovative capacities [16,18]. This limitation is evident in the tendency of some LLMs to produce "hallucinated reasoning chains," which, despite yielding correct final answers, undermines the reliability and trustworthiness of the model's underlying process [32]. Similarly, issues with factual accuracy and contextual coherence are reported in scientific writing benchmarks [13]. Moreover, current computational benchmarks struggle with issues like "hallucinations" and "outdated information" [5,28], further highlighting their static nature.

The ability to capture **complex reasoning and domain-specific nuances** remains a critical challenge. Many evaluations "fall short by relying on simplified, static scenarios" that do not reflect the complexity of real-world scientific problems, such as clinical reasoning [29]. The focus on specific task performance often overlooks a model's capacity for the broader scientific process, including hypothesis generation or experimental design [18]. For instance, LLM4SD's success in molecular property prediction is acknowledged as merely "scratching the surface" of scientific discovery, implying the vastness and complexity of scientific intelligence are not yet fully captured by existing benchmarks [3]. Furthermore, the field of scientific hypothesis generation is still nascent, lacking established comparative baselines, prompting studies to develop custom experimental setups with common LLMs and prompting techniques as proxies [23]. Specific domain nuances, such as specialized terminology and contextual analysis inconsistencies in peer review, present significant hurdles [13]. In medical education, benchmarks often exclude questions with figures, tables, or chemical structures, limiting their ability to represent the multimodal complexity of real-world medical examinations and reveal deficiencies in specialized knowledge with cultural or policy nuances [21]. Even in applied fields like materials science, existing LLMs "fall short of being practical materials science tools," indicating that current evaluations do not adequately capture the real-world complexity and practical requirements of such applications [7].

The **robustness and reliability of current evaluation systems** are also under scrutiny. "Brittle evaluations" can lead to biased outcomes, either overestimating or underestimating LLM capabilities based on singular responses [25]. The output of LLMs, such as generated rules in molecular discovery, can be "noisy or redundant," necessitating expert validation beyond computational benchmarks, underscoring the limitations of relying solely on automated metrics [22]. A notable issue is the inability of LLMs, like GPT-4, to reliably act as evaluators, struggling to differentiate between correct and incorrect outputs from other LLMs, which challenges the objectivity of LLM-based evaluation [5]. In drug discovery, despite strong computational performance, there is a recognized need for "real-world validation and application" through chemical experiments and clinical trials to assess practical utility and robustness, highlighting the shortcomings of solely computational benchmarks [26]. Additional challenges include "unreliable generation evaluation," inconsistencies between human and automatic metrics, and bias in "LLM-as-a-judge" mechanisms [28].

A critical deficiency lies in the **lack of standardization and transparency** across the field. There is "no comprehensive standardization or summary of methodologies for evaluating different domain specialization strategies," making cross-referencing and comparison across diverse application domains exceedingly difficult [20]. This lack of clarity obscures critical bottlenecks, unresolved problems, and potential future research directions [20]. Furthermore, the undisclosed training details (e.g., data collection and cleaning) for many industrially trained LLMs hinder transparent and reproducible evaluation by the research community [30]. The task-specificity of emergent abilities (e.g., in-context learning, Chain-of-Thought prompting) means that strong performance on one benchmark may not generalize to another without careful adaptation, complicating universal evaluation [30].

In conclusion, while a vast landscape of benchmarks exists for evaluating LLMs and Sci-LLMs across various scientific stages and domains, the current systems are constrained by their static nature, inability to fully capture complex reasoning and domain-specific nuances, and issues with reliability and standardization. The research community emphasizes the critical need for "new evaluation methodologies that move towards process- and discovery-oriented assessments" [18] and the "development of evaluation tools and benchmarks for Sci-LLMs" that can provide more comprehensive, fair, and dynamic assessments, moving beyond simplified scenarios to better reflect real-world scientific inquiry [1].
## 11. Challenges and Limitations of Scientific LLMs
The burgeoning field of Scientific Large Language Models (Sci-LLMs) holds immense promise for accelerating discovery, yet its advancement is significantly constrained by a multifaceted array of challenges and limitations [16]. These issues span fundamental technical hurdles, data infrastructure deficiencies, architectural incongruities with scientific requirements, inherent performance and reliability deficits, and the persistent difficulty in robustly evaluating their scientific utility. 

![Interconnected Challenges of Scientific LLMs](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/os7BJRtN9nDg5a7Hc66Zt_Interconnected%20Challenges%20of%20Scientific%20LLMs.png)

Many of these challenges are deeply interconnected, creating a complex landscape that necessitates concerted research efforts for Sci-LLMs to realize their full potential.

A primary set of obstacles stems from **technical and resource constraints**. The development and deployment of capable Sci-LLMs are characterized by exorbitant computational and financial burdens, demanding vast quantities of high-performance hardware and energy [16,20]. This "huge cost of model pre-training" severely limits accessibility, hindering reproducibility and comprehensive ablation studies within the broader scientific community [16,30]. Furthermore, current models face inherent limitations in **context length**, restricting their ability to process extensive scientific literature and complex data sequences vital for comprehensive analysis [16,19]. This is fundamentally tied to the **architectural limitations** of Transformer-based models, where the self-attention mechanism exhibits a quadratic computational and memory complexity, $O(N^2)$, with respect to sequence length $N$, making long-sequence processing prohibitively expensive [12]. Issues like tokenizer reliance, prompt brittleness, and the opaque nature of industry-trained models further complicate their application and validation [16,30].

Complementing these technical limitations are profound **data scarcity and quality issues**. Unlike general LLMs, Sci-LLMs require meticulously curated, high-quality, and often multimodal domain-specific datasets, which are scarce across many scientific fields [1,30]. The problem is exacerbated by outdated or incomplete information in existing datasets, hindering their ability to reflect rapid scientific advancements [23]. Pervasive data quality concerns, including biases, noise, and the mixing of human and AI-generated content, can lead to inaccurate, discriminatory, or harmful scientific outputs [2,30]. The labor-intensive nature of data preparation, coupled with privacy and security concerns for sensitive scientific data, further constrains the availability and utility of datasets [19,29].

**Model architecture limitations** present a critical barrier for Sci-LLMs, as their designs are often ill-suited for the inherent complexities of scientific data and reasoning [1]. Beyond the long-sequence processing challenges, current architectures struggle with the explicit integration of 3D structural information, crucial in fields like chemistry and materials science [1]. The "guess-the-next-word" autoregressive paradigm, while effective for natural language, fundamentally mismatches the non-autoregressive and causal reasoning objectives often required for genuine scientific discovery [24]. Consequently, Sci-LLMs exhibit cognitive and reasoning limitations, including difficulties with specialized terminology, numerical computation, robust internal reasoning (leading to "hallucinated reasoning chains"), and generating truly novel hypotheses [19,32].

These limitations inevitably lead to significant **performance and reliability issues**, most notably the pervasive problem of **hallucinations**. Sci-LLMs frequently generate confident yet incorrect or factually inaccurate outputs, ranging from fictitious content to nonexistent references or "unreasonable plans" in experimental design [16,19]. This probabilistic nature of language generation, coupled with potential knowledge gaps and reliance on "surface statistics," undermines trustworthiness [24,28]. Other reliability concerns include prompt brittleness, rapidly outdated knowledge bases, the indistinguishability of AI-generated text from human content (raising concerns for scientific integrity), a general lack of scientific accuracy and depth, interpretability issues (the "black box" nature), inconsistent reasoning, generalization limits, and the potential for generating biased or harmful content [16,20,34].

Finally, the **robust and reliable evaluation** of Sci-LLMs remains a substantial challenge [1,13]. A critical gap exists between computational metrics and the stringent requirements for scientific validity, where real-world (wet-lab) experiments often serve as the ultimate "gold standard" [1,26]. Existing benchmarks are often limited in scale, lack comprehensive standardization, and struggle to assess complex scientific capabilities like understanding, reasoning, and emergent behaviors [19,20]. Methodological hurdles, such as the unreliability of expert evaluations, the biases introduced by "LLM-as-a-Judge" paradigms, and a pervasive lack of reproducibility, further complicate objective assessment [19,29]. The "inherent unpredictability of many significant LLM capabilities" and their "black box" nature impede a deep understanding of their functioning, making robust evaluation exceptionally difficult [20].

In conclusion, these interconnected challenges underscore the need for a holistic approach to Sci-LLM development. Addressing issues of computational cost, data quality, architectural suitability, reliability, and rigorous evaluation is paramount. Future research must prioritize novel architectural designs capable of natively handling complex scientific data structures (e.g., 3D information) and fostering causal reasoning, alongside efforts to build high-quality, transparent, and ethically sourced scientific datasets. Developing comprehensive, standardized, and dynamic evaluation frameworks that bridge computational performance with real-world scientific validity will be crucial for establishing trust and enabling the widespread, responsible adoption of Sci-LLMs in accelerating scientific discovery [1,18].
### 11.1 Technical and Resource Constraints
The development and deployment of Scientific Large Language Models (Sci-LLMs) are significantly hampered by immense computational and data requirements, posing substantial barriers to entry, scalability, and reproducibility for researchers [16,17]. 

**Key Technical and Resource Constraints for Sci-LLMs**

| Constraint Category      | Specific Constraint                                         | Impact on Sci-LLMs                                           |
| :----------------------- | :---------------------------------------------------------- | :----------------------------------------------------------- |
| **Computational & Financial Burden** | Exorbitant cost (hardware, energy, time) for training/inference. | Limits accessibility, hinders reproducibility & ablation studies, high operational costs. |
| **Context Length**       | Restrictions on input text length                           | Prevents full processing of long scientific documents/data, fragmented analysis in peer review. |
| **Tokenizer Reliance**   | Incurs overhead, language dependency, struggles with new terminology, fixed vocabularies | Impacts precise representation/understanding of specialized scientific language, information loss. |
| **Architectural Limitations** | Quadratic complexity of self-attention ($O(N^2)$); FFN bottlenecks; limited planning capabilities; prompt brittleness. | Prohibits long-sequence processing, high costs, inconsistent guidance, difficult troubleshooting, poor 3D structural info integration. |

These constraints manifest across various stages of the LLM lifecycle, from pre-training and fine-tuning to inference and specialized applications.

A primary concern is the exorbitant **computational and financial burden** associated with LLMs. Training capable LLMs, which often comprise millions or billions of parameters, demands extensive computational resources, time, and significant financial investment [14,16,17,20,25,30]. For instance, pre-training models exceeding 10 billion parameters typically necessitates hundreds or thousands of high-performance GPUs, with examples like LLaMA utilizing 2,048 A100-80G GPUs and Grok requiring over 100,000 Nvidia GPUs, leading to substantial energy consumption [15,30]. This "huge cost of model pre-training" is a root cause for limited accessibility, making it challenging for the broader research community to perform replication and ablation studies [16,17,30]. Furthermore, fine-tuning an entire LLM often requires memory comparable to pre-training, rendering it infeasible for many practitioners [16]. Beyond training, LLM inference still suffers from high latency due to low parallelism and large memory footprints, hindering real-time applications and increasing operational costs [15,16,19,22]. The maintenance of LLMs also incurs ongoing costs related to monitoring, bias awareness, and continuous training [25]. The lack of transparency regarding crucial training details (e.g., data collection and cleaning) for industry-trained models further exacerbates issues of reproducibility for academic researchers [30].

**Limited context length** poses another critical constraint, particularly for scientific applications involving extensive literature and complex data. Current models have restrictions on input text length, preventing full understanding and processing of longer scientific documents [16,19,26]. In scientific writing and peer review, this limitation restricts LLMs' ability to manage vast references, comprehensively integrate related literature, maintain coherent analysis across long manuscripts, and handle complex methods that exceed context window limits, potentially leading to incorrect citation order or incomplete understanding [19].

**Tokenizer reliance** introduces specific challenges in scientific contexts. Tokenization processes can incur computational overhead, exhibit language dependency, struggle with new scientific terminology and symbols, rely on fixed vocabularies, and lead to information loss or low human interpretability [16]. This directly impacts the precise representation and understanding of specialized scientific language, which is crucial for accuracy in Sci-LLMs.

**Architectural limitations** further compound these issues. The self-attention mechanism fundamental to Transformer architectures, which underpins most LLMs, exhibits quadratic computational and memory complexity ($O(N^2)$) with respect to sequence length [12]. This quadratic scaling is a root cause for the high computational costs and the difficulty in processing long context lengths, particularly for applications like Retrieval-Augmented Generation (RAG), AI Agents, and long-chain reasoning [12]. Beyond general scaling issues, the Feed-Forward Networks (FFNs) within Transformers also present efficiency bottlenecks as models grow [12]. Furthermore, current LLMs exhibit limitations such as "limited planning capabilities" for experimental design and automation, as well as "prompt brittleness" where slight changes in wording can lead to inconsistent guidance [19]. The inherent complexity of LLMs' inner mechanisms also makes troubleshooting difficult when unexpected results occur [10]. While not explicitly detailed in the digests as a current struggle, the architectural design of LLMs needs to evolve to better integrate 3D structural information, which is critical for fields like materials science and drug discovery [1].

Many of these challenges are **interdependent**. For instance, insufficient training data, coupled with limited computational resources, can significantly escalate overall computational costs [20]. The quadratic complexity of Transformer architectures directly links the desire for longer context windows with exponentially increasing computational demands [12]. The huge demand for computational resources makes repetitive or ablating studies, crucial for understanding and improving LLM training strategies, prohibitively expensive [30].

Researchers are actively exploring **mitigation strategies and workarounds**. Efforts include sparse scaling, which aims to increase parameter counts while maintaining training and inference costs (in FLOPs) [20]. The development of "cost-optimized" models like Grok-4-Fast and tools such as "llm-optimizer" for benchmarking and performance tuning aim to alleviate inference latency and computational overhead [29]. Furthermore, the continuous development of more efficient architectures, such as Mixture-of-Experts (MoE) models (e.g., ERNIE-4.5-21B-A3B-Thinking, Kimi K2), and specialized long-context solutions (e.g., REFRAG, MemAgent, Phi-4-mini-Flash-Reasoning) reflects ongoing attempts to push the boundaries of LLM capabilities and efficiency [29]. For multimodal applications, deploying smaller models (e.g., under 1 billion parameters) has been proposed to overcome implicit resource constraints associated with larger LLMs [32]. The challenge of rapidly updating model weights is also being addressed, as evidenced by developments like MoonshotAI's checkpoint-engine for dynamic LLM deployment [29].

Despite these advancements, significant **open questions and research gaps** remain. A core challenge lies in determining how to effectively reduce computational costs while simultaneously maintaining or even improving model performance [14]. The persistent lack of transparency in training details for many advanced models continues to hinder academic reproducibility and innovation [30]. Moreover, a critical need exists for explicit engineering design to ensure reliable and accurate results when applying LLMs in complex scientific domains, moving beyond generic applications [5]. The architectural integration of complex data types, such as 3D structural information, into LLMs specifically designed for scientific tasks represents a significant frontier [1]. Addressing these technical and resource constraints is paramount for unlocking the full potential of Sci-LLMs and ensuring their broad accessibility and utility within the scientific community.
### 11.2 Data Scarcity and Quality
The development of Scientific Large Language Models (Sci-LLMs) confronts critical data-related challenges that are distinct from, and often more complex than, those faced by general-purpose Large Language Models (LLMs) [1,30]. While general LLMs demand a high volume of training data to support their extensive parameter counts [30], Sci-LLMs require not only scale but also unparalleled specificity, accuracy, and multimodal integration, which are severely limited by data scarcity and quality issues in scientific domains.

**Data Scarcity in Scientific Domains.** A primary limitation for Sci-LLM development is the inherent scarcity of high-quality data. Unlike the vast, albeit noisy, web corpora available for general LLM pre-training [28], scientific applications necessitate meticulously curated, domain-specific datasets. A significant challenge lies in the limited availability of high-quality, task-specific datasets crucial for fine-tuning Sci-LLMs to specialized problems [1,13,25]. Furthermore, there is a notable deficit of integrated datasets that combine multiple scientific modalities, which is essential for advancing multimodal Sci-LLMs [1,20,26]. Specific instances highlight this scarcity: in materials science, extracting key information from literature to build large-scale, multimodal datasets remains a significant hurdle [7]; for graph data, a substantial lack of unlabeled data suitable for pretraining exists [20]; and in agriculture, structured professional knowledge bases are often unavailable [11]. Moreover, LLMs often rely on outdated or incomplete information from their pre-training datasets, hindering their ability to reflect rapid scientific advancements and generate cutting-edge hypotheses, particularly in fast-evolving fields like climate science [5,23]. The limited scale of expert-annotated benchmarks also poses a significant obstacle, as accurate, well-structured scientific benchmarks are resource-intensive to create and difficult to expand [19]. For instance, ChatGPT's performance in Chinese medical exams was hampered by a scarcity of high-quality, language-specific medical data and a lack of coverage for dynamic, culturally specific policies [21].

**Pervasive Data Quality Issues.** Beyond mere quantity, the quality of scientific data presents formidable challenges. General LLMs trained on web data frequently encounter "噪声和质量参差不齐的问题" (noise and uneven quality), which necessitates extensive preprocessing [28,30]. This problem is exacerbated in scientific contexts, where precision is paramount. A critical issue is the presence of biases—societal, cultural, linguistic, gender, racial, and religious—within training datasets, which can lead to LLMs generating discriminatory, inaccurate, or harmful scientific outputs [2,4,10,17,20,25,26,30]. The persistence of "hallucinations" in LLMs, even after training improvements, is frequently attributed to underlying data quality or coverage deficiencies [29]. The mixing of human and AI-generated content in datasets also degrades information quality, potentially leading to less reliable AI outputs [2]. Furthermore, the output of LLMs, such as rule extractions, might contain noise or redundancy, requiring subsequent expert validation to ensure reliability [22]. In critical scientific tasks like hypothesis re-discovery, data contamination is a major concern, requiring careful consideration of publication dates to ensure models are trained on data predating discoveries [19]. The inherent heterogeneity, multimodality, cross-scalability, and uncertainty of scientific data make it challenging for models to achieve domain invariance and robust cross-modal reasoning [18]. Moreover, the "unfathomable datasets" used for pre-training, often not publicly accessible, render it impossible for researchers to comprehensively assess data quality and transparency [16].

**Difficulties in Acquiring, Curating, and Integrating Multimodal Scientific Data.** The challenges extend to the practicalities of data management. Data preparation—including cleaning, tagging, and feature engineering—is profoundly labor-intensive, especially for the vast datasets required by modern LLMs [5,19]. The difficulty in obtaining real-world experimental data, particularly in fields with costly or unethical experiments (e.g., social sciences with human subjects), often necessitates reliance on synthetic data generated from LLM-simulated environments [19]. Privacy and security concerns, especially when dealing with sensitive scientific or personal data, impose strict limitations on data collection and usage, further exacerbating data scarcity for specific scientific domains [14,20,25,26,29]. This is a root cause that impacts data availability. Adding to these issues, many crucial training details, such as data collection and cleaning procedures for industrially developed LLMs, are not publicly disclosed, impeding reproducibility and further research within the academic community [30].

**Interdependencies and Consequences.** These data-related challenges are deeply intertwined. The scarcity of high-quality, domain-specific data often compels reliance on broader, noisier datasets, which in turn exacerbates issues of bias, inaccuracies, and hallucinations. For example, the lack of diverse and domain-specific data directly leads to LLMs demonstrating "insufficient adaptability to specific scenarios" and struggling with "understanding specialized terminology" in scientific tasks like experiment planning or peer review [13]. Similarly, privacy concerns limit the scope of usable data, intensifying scarcity and potentially forcing researchers to work with suboptimal or less comprehensive datasets. The "dependence on existing LLM capabilities" for hypothesis generation implicitly links the quality and scope of pre-training data to the innovativeness of scientific discovery [13]. This highlights how fundamental infrastructure problems, such as a shortage of specialized training data, create interdisciplinary imbalances and affect fields with less specialized vocabulary [19].

**Proposed Workarounds and Open Questions.** Various strategies have been proposed to mitigate these challenges, though their effectiveness varies. Deduplication is crucial for enhancing data quality and diversity in LLM pre-training [28]. To address the issue of outdated information, retrieval mechanisms are employed to ground models in up-to-date authoritative documents, as exemplified by chatIPCC in climate science [5,23]. Methods like SPIRES use zero-shot learning to extract structured information from texts, aiming to overcome the labor-intensiveness of manual data preparation for scientific knowledge bases [5]. Frameworks like LLM4SD leverage existing literature and discover new patterns in data, making comprehensive use of available, albeit disparate, data sources [3]. The development of tools like Hugging Face's AI Sheets for dataset creation and enrichment points to ongoing efforts to simplify the preparation of high-quality, task-specific training data [29]. However, these workarounds often address symptoms rather than root causes, and their scalability and generalizability remain open questions. For instance, while expert validation can improve the reliability of LLM-generated rules [22], it is not scalable to all contexts.

Significant research gaps remain. A critical need exists for enhancing quantitative analysis and multimodal processing capabilities of Sci-LLMs, necessitating improvements in the quantitative fidelity and multimodal diversity of scientific datasets [26]. Addressing ethical and privacy concerns surrounding scientific data collection and usage is paramount for ensuring sustainable LLM development [26]. Future work must focus on developing culturally diverse datasets to mitigate bias [25], expanding expert-annotated benchmarks [19], and solving the data provenance crisis and data lag issues that hinder the development of high-quality, AI-friendly scientific data [18]. The fundamental challenge of achieving domain invariance and robust cross-modal reasoning despite the inherent complexities of scientific data remains a frontier [18]. The continuous improvement of data cleaning and careful alignment with human values are foundational for mitigating the risks of toxic or biased outputs [17,30]. Ultimately, creating robust Sci-LLMs demands innovative solutions for comprehensive data acquisition, meticulous quality control, and seamless multimodal integration across diverse and sensitive scientific domains.
### 11.3 Model Architecture Limitations for Scientific Data
Current Large Language Models (LLMs), primarily built upon the Transformer architecture, encounter significant architectural limitations when applied to the distinct characteristics of scientific data and research objectives. These limitations span several critical areas, from handling complex data structures to performing sophisticated scientific reasoning, often stemming from the disparity between their natural language processing origins and the specific requirements of scientific domains [1].

One of the most prominent architectural challenges lies in the **processing of long scientific sequences** [1]. Scientific texts, such as academic manuscripts, or biological data like proteins and genes, can be orders of magnitude longer than typical natural language sequences [22]. Standard LLM input sequences for pre-training typically range between 500 and 2000 tokens [20], yet many scientific sequences, particularly in genomics and proteomics, vastly exceed these lengths [22]. This limitation is fundamentally rooted in the Transformer's self-attention mechanism, which exhibits a quadratic computational and memory complexity ($O(N^2)$) with respect to the input sequence length $N$ [12]. This quadratic scaling renders the processing of very long sequences computationally prohibitive and memory-intensive, leading to restricted context windows [26]. Consequently, LLMs often struggle to maintain consistent and coherent analysis across extensive scientific documents, frequently resulting in fragmented or contradictory evaluations, particularly in tasks like peer review [19]. The continuous research into "long-context" solutions and architectural improvements, such as byte-level autoregressive U-Net models or hybrid Mamba-Transformer designs, implicitly acknowledges this persistent limitation in traditional Transformer architectures [28,29].

A second critical limitation is the **explicit integration of 3D structural information** [1]. Scientific domains, such as chemistry, materials science, and structural biology, heavily rely on understanding the three-dimensional structures of molecules, proteins, and other complex entities. Current LLM architectures, designed primarily for sequential text data, are not inherently optimized to directly incorporate or reason about spatial relationships and topological information. While adaptations exist, such as linearizing molecules into SMILES strings or employing Graph Neural Networks (GNNs) for graph-structured data and visual encoders for images, these are often external modifications rather than intrinsic architectural capabilities [31]. The difficulty in handling multimodal inputs, including figures, tables, and chemical structures, further exemplifies this gap, often necessitating manual exclusion or specialized processing in evaluations [21].

Furthermore, there exists a significant **disparity between natural language generation paradigms and the ideal learning objectives for scientific LLMs (Sci-LLMs)** [1]. Conventional LLMs operate on an autoregressive "guess-the-next-word" mechanism, predicting tokens sequentially based on preceding context [24]. However, many scientific tasks require non-autoregressive generation objectives, demanding a holistic understanding of an entire sequence or structure rather than merely local prediction. This fundamental algorithmic design choice raises a profound question: do LLMs learn genuine "meaning" or merely "surface statistics" [24]? If the latter, they inherently lack a causal model of underlying scientific processes, severely limiting their ability to truly understand complex scientific data and generalize effectively to out-of-distribution scenarios [24].

Beyond these architectural and generative paradigm issues, current LLMs exhibit several **cognitive and reasoning limitations** crucial for scientific discovery. They struggle to fully grasp specialized terminology and complex concepts prevalent in academic fields like biochemistry or theoretical physics, which impacts their ability to evaluate research methods and validate evidence [19]. This translates into limitations in numerical computation, which is critical for quantitative reasoning in scientific tasks such as dosage calculations or laboratory value interpretation [21,30]. Moreover, LLMs often demonstrate a deficiency in robust internal reasoning structures, leading to "hallucinated reasoning chains" in multimodal contexts and a struggle to consistently produce accurate and reliable intermediate reasoning steps [32]. This can hinder their capacity for genuine innovative thinking, as they tend to repeat patterns observed in their training data rather than generating truly novel scientific hypotheses, indicating a deficit in "divergent and convergent thinking" [23]. The "black box" nature of many LLMs also presents a transparency deficit, making it challenging to understand their analytical processes, predictions, and results, a critical requirement for scientific credibility, although models like LLM4SD are specifically designed to provide interpretable explanations [3]. Even with increased scale, as seen with models like GPT-4, the inherent capabilities of predominantly Transformer-based architectures may not fully address the complexities required for advanced scientific reasoning or real-world problem-solving, hinting at "Tasks Not Solvable By Scale" [15,16].

These challenges are often interdependent. The quadratic complexity of self-attention directly underpins the long-sequence limitation, which, in turn, exacerbates the difficulty in maintaining coherent analysis across comprehensive scientific documents and hinders the development of robust internal reasoning structures [12,19]. The "guess-the-next-word" training paradigm, while effective for natural language, restricts the learning of causal models essential for deep scientific understanding and generalization, thus contributing to issues like limited novelty and surface-level statistical understanding [23,24].

Critically appraising proposed workarounds reveals that while specialized architectural adaptations (e.g., GNNs for graphs, visual encoders for images) [31] and long-context solutions (e.g., REFRAG, MemAgent, Kimi K2) [29] can mitigate specific issues, they often function as band-aids rather than fundamental architectural shifts. Many still operate within the core Transformer paradigm, struggling to natively integrate diverse modalities or inherently support non-autoregressive reasoning. The challenge of fully capturing the non-linear and multi-scale complexities inherent in biological systems, for example, points to an intrinsic limitation that may require architectures fundamentally different from current designs [8].

Therefore, significant research gaps remain. Future work must explore novel architectural paradigms that move beyond the limitations of current Transformer-based models to natively support long-sequence processing, explicitly integrate 3D structural and other multimodal scientific data, and foster non-autoregressive learning objectives [1,19]. Developing architectures capable of learning true causal models, demonstrating transparent and robust scientific reasoning, and generating genuinely novel hypotheses are paramount for Sci-LLMs to fulfill their potential in accelerating scientific discovery.
### 11.4 Performance and Reliability Issues (Hallucinations)
The deployment of Large Language Models (LLMs) in scientific domains (Sci-LLMs) introduces critical reliability concerns, primarily due to the pervasive problem of hallucinations, which can lead to misinformation or fabricated results [16,17]. This issue fundamentally undermines the trustworthiness and utility of Sci-LLMs, necessitating a comprehensive understanding of their manifestations, root causes, and interdependencies with other performance limitations.

A primary challenge is the phenomenon of **hallucinations**, defined as the generation of confident yet incorrect or factually inaccurate outputs that appear plausible [10,29]. These can range from "fictitious content" [17] and "false information" [28] to specific errors like nonexistent references [16], inaccurate or irrelevant text in scientific writing, or even convincingly incorrect evaluations in peer review, especially concerning novel research methods [19]. In experiment planning, Sci-LLMs are "prone to hallucinations" that manifest as "unreasonable plans" [19]. A particularly insidious form involves "hallucinated reasoning chains," where smaller models fine-tuned for multimodal Chain-of-Thought reasoning produce incorrect intermediate steps that actively mislead the final inference in complex scientific tasks [32]. Even advanced models like GPT-4 are not immune to generating hallucinations with factual errors or potentially risky responses [28,30], exemplified by its initial undesirable trait of providing instructions on biological weapons synthesis [20]. This is a fundamental limitation rooted in the probabilistic nature of language generation, where models predict based on learned patterns rather than strict factual verification [20,28]. This issue is further exacerbated if LLMs primarily learn "surface statistics" and rely on "spurious correlations," leading to poor generalization or incorrect outputs when encountering out-of-distribution data [24]. Knowledge gaps in specific areas can also directly contribute to errors or lack of critical information, akin to hallucinations [4].



**Sci-LLM Performance and Reliability Issues**

| Issue Type                      | Description                                                 | Impact on Sci-LLM Trustworthiness / Utility                 |
| :------------------------------ | :---------------------------------------------------------- | :---------------------------------------------------------- |
| **Hallucinations**              | Generation of confident but incorrect/factually inaccurate outputs. | Undermines trustworthiness, leads to misinformation, false research results, "unreasonable plans." |
| **Prompt Brittleness**          | Extreme sensitivity to minor variations in input prompts.   | Inconsistent answers, requires careful prompt engineering, reduces robustness. |
| **Outdated Knowledge**          | Static nature of training data leads to rapid obsolescence of knowledge. | Generates irrelevant/incorrect info, lacks latest advancements, costly updates. |
| **Indistinguishability of Generated Content** | Difficulty in differentiating AI-generated from human-written text. | Compromises scientific integrity, enables unchecked misinformation, impacts peer review. |
| **Lack of Scientific Accuracy/Depth** | Content lacks scientific rigor, true innovation, or comprehensive understanding. | Superficial/trivial outputs, homogenization of academic feedback, struggles with specialized terminology. |
| **Interpretability Issues**     | Opaque decision-making processes ("black box" behavior).     | Hinders trust/verification of outputs, makes debugging difficult, impacts scientific credibility. |
| **Inconsistent Reasoning/Limited Capabilities** | Flawed reasoning despite correct answers, poor planning, struggles with numerical computation/generalization. | Unreliable predictions, limited innovative thinking, unpredictable behavior in complex tasks. |
| **Bias and Harmful Content**    | Perpetuation of stereotypes/biases from training data.       | Discriminatory/toxic outputs, misaligned with human values, ethical concerns. |

Beyond outright factual errors, several other limitations contribute to reliability issues:
*   **Prompt Brittleness**: Sci-LLMs can be highly sensitive to minor variations in prompts, leading to drastically different answers [16]. For instance, different phrasing in questions related to Chinese healthcare policies impacted ChatGPT’s responses, highlighting a lack of robustness across varied prompting styles [21]. This sensitivity necessitates careful prompt engineering for robust interaction with complex scientific queries [16].
*   **Outdated Knowledge**: The static nature of LLM training data means their knowledge base can rapidly become outdated in rapidly evolving scientific fields [16]. This results in generated text lacking the latest information or providing incorrect answers on current policy questions [21]. While periodic updates are possible, they are costly and carry the risk of "catastrophic forgetting" of previously learned information [28]. This issue can directly compromise the relevance and accuracy of Sci-LLM outputs [16,20].
*   **Indistinguishability of Generated and Human-Written Text**: The increasing difficulty in differentiating between LLM-generated and human-written text [16,25] poses significant implications for scientific integrity and peer review processes. This can "compromise good scientific practices and trust in science" by potentially allowing inaccurate or misleading information to enter the scientific discourse without proper scrutiny [34]. For example, review article drafts generated by LLMs necessitate "thorough information veracity review" and human rewriting due to potential inaccuracies, indicating a lack of inherent reliability [5].
*   **Lack of Scientific Accuracy, Depth, and Rigor**: LLM-generated content often lacks sufficient scientific accuracy and depth, struggling to achieve true innovation or capture the rigorous reasoning required for academic writing [19,23]. This can lead to superficial or trivial outputs and a homogenization of academic feedback if over-relied upon in peer review, reducing diversity of perspectives [19]. Models also exhibit difficulties with specialized terminology and inconsistent contextual analysis during peer review [13].
*   **Interpretability Issues (Black Box Behavior)**: The "black box" nature of LLMs hinders understanding their decision-making processes and the rationale behind their predictions [20,26]. This lack of transparency makes it challenging for scientists to trust and verify outputs, particularly for critical scientific tasks [3]. Improving Sci-LLMs' scientific understanding capabilities is crucial for consistent and accurate handling of complex information [26].
*   **Inconsistent Reasoning and Limited Capabilities**: Despite various reasoning strategies, LLMs can produce correct answers with flawed reasoning or incorrect ones, undermining reliability [28]. This is reflected in "limited planning capabilities" for experiment design [13] and challenges with "logical reasoning" in domain adaptation [11]. The "Illusion of Intelligence" critique suggests that perceived performance gains might not indicate true understanding [29].
*   **Generalization and Scalability Limits**: The speculation of GPT-4 achieving artificial general intelligence was "unfounded" [15], indicating that some complex tasks may not be solvable by scale alone [16,17]. LLMs also face challenges in generalizing to new, unseen data or tasks [14], and struggle with numerical computation, especially with rare symbols or large-scale calculations [28]. This suggests inherent limits to their text-generation capabilities for certain problem types.
*   **Bias and Harmful Content**: LLMs can generate "toxic, biased, or even harmful content" for humans if not aligned with human values and preferences [17,30]. This "misaligned behavior" can lead to unpredictable outputs [16] and algorithmic bias, particularly concerning sensitive attributes like sex, gender, race, and religion [19,20].

These challenges are often interdependent. For instance, the probabilistic nature of LLMs, coupled with incomplete or noisy training data, is a root cause for hallucinations [28]. A lack of interpretability makes it harder to diagnose why a hallucination occurred or to detect subtle inaccuracies, compelling the need for "domain expert verification" of LLM-generated outputs like rules for scientific validity [22]. The erosion of the "digital agricultural commons" due to blending human and AI-generated content can further degrade the reliability of future AI outputs [2].

Proposed workarounds include grounding LLMs in authoritative scientific reports, such as chatIPCC's approach using climate science data to provide accurate answers [5]. Enhancing interpretability through methods like LLM4SD, which generates "simple rules to explain its analysis process, predictions, and results," can improve trust and reliability among scientists [3]. However, the effectiveness of current evaluation methods is also under scrutiny, with critiques highlighting how "LLM-as-a-Judge" paradigms can "reinforce the problem of hallucinations" by inadequately defining 'correctness/faithfulness/completeness' [29]. For example, GPT-4, when used as an evaluator, "failed to distinguish between its own incorrect answers" and correct ones provided by augmented models, pointing to flaws in LLM self-assessment and reliability for critical scientific evaluation [5].

Significant research gaps remain in developing robust mechanisms to consistently prevent hallucinations, improve reasoning capabilities, and ensure the factual accuracy and scientific rigor of Sci-LLM outputs [13]. Open questions persist regarding how to achieve true understanding versus surface-level statistical learning in LLMs, and how to reliably generalize to new, complex scientific tasks without encountering unpredictable or unreliable behaviors [20,24]. Addressing these challenges is paramount for Sci-LLMs to genuinely accelerate scientific discovery and maintain integrity within the scientific community [18,34].
### 11.5 Robust and Reliable Evaluation
The rigorous and reliable evaluation of scientific large language models (Sci-LLMs) is paramount for fostering trust and facilitating their adoption in scientific research, yet it presents a complex and pressing challenge [1,13]. A significant disparity exists between current computational metrics and the stringent validation requirements inherent in scientific practice, encompassing the assessment of domain-specific understanding and emergent abilities.

**The Gold Standard Dilemma: Bridging Computational Metrics and Real-World Validation**

A central limitation in Sci-LLM evaluation stems from the disconnect between computational performance and verifiable scientific validity [1]. While computational metrics are available for evaluating generated outputs in fields like chemistry or materials science, they are often "not decisive" in fully capturing genuine scientific validity [1]. This is particularly evident in domains such as materials science, where LLMs "currently fall short of being practical materials science tools," implying an inadequacy in evaluation methods to reliably assess their utility in real-world contexts [7]. In drug discovery, for instance, a critical limitation is the insufficient "real-world validation and application," with chemical experiments and clinical trials remaining the ultimate "gold standard" for efficacy, a benchmark that current computational assessments cannot fully capture [26].

The necessity of real-world (wet-lab) experiments for validating hypotheses is frequently highlighted [19]. Experimental verification for LLM-generated hypotheses underscores a fundamental challenge, as wet-lab experiments are typically outside the scope and capabilities of many AI research teams, creating a substantial gap between theoretical model performance and practical scientific confirmation [1,13]. Even for models providing interpretable rules, such as LLM4SD, "domain expert verification" is deemed essential to address potential noise or redundancy, highlighting the limitations of relying solely on computational evaluation for scientific validity [22]. Prominent successes in scientific AI, like AlphaFold 3, SyntheMol, and the Stanford Virtual Lab, implicitly validate the critical role of direct experimental validation, confirming that computational metrics alone are often insufficient for robust assessment in life sciences [8].

**Limitations of Current Benchmarking and Standardization**

Existing benchmarks for Sci-LLMs suffer from several deficiencies. Establishing accurate and well-structured benchmarks for scientific discovery is heavily reliant on expert input, resulting in benchmarks that are typically "very limited in scale" [19]. Furthermore, there is a pronounced "no comprehensive standardization or summary of methodologies" for evaluating different domain specialization strategies, which "obscures the current bottlenecks, difficulties, unresolved problems" in assessment [20]. This absence of standardized methodologies impedes the cross-referencing and comparison of evaluation results across diverse application domains [20].

Evaluations are often "brittle," meaning minor changes to benchmark prompts or protocols can lead to drastically different outcomes [16]. Many evaluations rely on "static, human-written ground truth," which introduces subjectivity and becomes less effective as model capabilities evolve [16]. For complex tasks like clinical reasoning, current evaluations often "fall short by relying on simplified, static scenarios," failing to capture real-world complexity [29]. Even models achieving state-of-the-art results on existing benchmarks, such as LLM4SD, recognize that "scientific discovery is vast and complex; this study has only scratched the surface," indicating that current benchmarks may not fully capture the robustness required for intricate scientific endeavors [3]. The demand for "public benchmarking datasets and fair evaluation metrics" is particularly noted in medical education, emphasizing the inadequacy of existing methods [21].

**Challenges in Assessing Complex Scientific Capabilities**

Evaluating Sci-LLMs necessitates moving beyond superficial metrics to genuinely assess their understanding, reasoning, and emergent abilities. A fundamental challenge lies in the complexity of defining "correctness" for scientific LLM outputs, especially in fields with nuanced research where definitive answers are not always straightforward [19]. LLMs are susceptible to "plausible false assertions" or hallucinations, and the probabilistic nature of their predictions makes it difficult to accurately discern factual accuracy [5,20]. The issue of "hallucinated reasoning chains" in multimodal contexts further suggests that evaluations focusing solely on final answers may not capture the integrity of a model's reasoning process [32]. Similarly, challenges in climate science regarding hallucinations and outdated information highlight the need for dynamic, real-time knowledge validation [5].

The "emergent nature and inherent unpredictability of many significant LLM capabilities" pose considerable difficulties for comprehensive evaluation, as LLMs can exhibit "fundamentally novel behaviors" unforeseen by developers [20]. The underlying principles of LLMs are "still not well explored," leading to a lack of deep understanding of how they achieve their superior abilities and when emergent capabilities occur, thus complicating robust evaluation [30]. Initial assessments of capabilities, such as early speculation about GPT-4 achieving AGI based on academic exams, have been proven "unfounded," demonstrating the difficulty in reliably evaluating advanced intelligence [15]. Beyond mere output, assessing whether LLMs truly learn "internal world models" rather than just surface statistics remains an open question, as explored by studies like Othello-GPT [24].

**Methodological and Practical Hurdles in Evaluation**

Several methodological and practical issues impede robust evaluation. Even expert evaluations of newly generated hypotheses in disciplines like chemistry can be unreliable, underscoring the need for automated experiments for validation [19]. Problems with peer review, including difficulties in understanding specialized terminology and inconsistent contextual analysis, also contribute to unreliable evaluation [13]. The practice of using "LLM-as-a-Judge" is criticized for introducing bias and relying on "project-specific rubrics without task-grounded definitions," thus questioning the true measurement [28,29].

Furthermore, there is a "lacking experimental designs" for evaluations, indicating an absence of systematic methods in designing experiments and assessments [16]. A notable "lack of reproducibility" due to incomplete details hinders the replication and verification of research results [16]. While human evaluation is crucial for reflecting real-world performance, it is inherently "costly, subjective, and difficult to scale" [28]. The "huge demand of computation resources" and the fact that "many important training details are not revealed to the public" by industrial trainers make it challenging for the research community to conduct systematic, reproducible studies and robustly evaluate findings [30]. Aligning LLMs with human values and preferences for safety and ethical considerations also poses difficulties in evaluation beyond mere computational metrics [30].

**Interdependencies, Implications, and Future Directions**

These challenges are highly interdependent. The absence of robust benchmarks and standardized methodologies (Challenge 2) exacerbates the difficulty in demonstrating the scientific validity of LLMs against wet-lab experiments (Challenge 1). The inherent black-box nature and emergent capabilities of LLMs (Challenge 3) make it harder to develop reliable evaluation metrics and foster mechanistic understanding, contributing to issues like hallucinations and unreliable output. The reliance on costly and subjective human expert validation (Challenge 4) becomes unsustainable when rigorous, scalable verification for scientific discovery is needed.

The critical implication of these challenges is a potential erosion of trust and slow adoption of Sci-LLMs in scientific practice. To address these issues, research must focus on developing more robust computational metrics and benchmarking systems that can reliably assess performance in dynamic scientific processes, including hypothesis generation, experimental design, and problem-solving [1,18]. Open questions include: How can evaluation frameworks effectively bridge the gap between computational and wet-lab validation? How can standardized, dynamic benchmarks be developed that accurately capture complex scientific reasoning and emergent capabilities, rather than simplified, static scenarios? What methods can ensure truthfulness, originality, and the scientific validity of LLM outputs, especially in light of hallucinations and the cost of human oversight? Furthermore, novel approaches to integrate human expert validation scalably and transparently are needed, along with efforts to improve reproducibility and overcome the opaqueness of industrial training practices.
## 12. Ethical, Social, and Economic Considerations
The burgeoning integration of Large Language Models (LLMs) into scientific research and applications presents a complex interplay of ethical, social, and economic considerations that demand rigorous attention to ensure responsible innovation. While LLMs offer transformative potential across various scientific disciplines, their deployment introduces profound challenges ranging from maintaining data integrity and fostering equitable access to navigating privacy concerns and complex legal frameworks. 

Addressing these multifaceted issues is paramount to harnessing the benefits of scientific LLMs without compromising academic rigor, societal well-being, or individual rights [1,2,13].

One primary area of concern revolves around **misinformation and data integrity**. LLMs have demonstrated a propensity for "hallucinations," generating plausible yet factually incorrect or contextually inappropriate outputs that undermine the trustworthiness of scientific information and can lead to significant real-world harms, particularly in critical fields like agriculture [2,20,29]. This phenomenon, coupled with biases inherited from training data, threatens to degrade the quality of scientific data and erode trust in findings, raising serious academic integrity concerns [13,30,34]. Furthermore, the increasing volume of indistinguishable AI-generated content poses a long-term threat to the "digital agricultural commons" and can be maliciously exploited for disinformation campaigns [2,10,19]. Mitigation strategies involve robust testing like "red teaming," constitutional AI, and domain-specific LLMs, though fundamental questions about whether LLMs truly learn meaning versus "surface statistics" remain unanswered [5,15,24,30].

Concurrently, the **socio-economic impact and inequality** engendered by LLMs are significant. The potential for job displacement, especially for low-skilled workers, is a pressing concern, threatening to widen existing economic disparities [2,4,10]. A critical exacerbating factor is the "digital divide," where unequal access to advanced LLM technologies disproportionately disadvantages specific communities and educational institutions [2,25]. Moreover, biases embedded in LLM training data reflect and amplify existing societal inequalities, leading to discriminatory outcomes across various applications, including scientific and medical research [4,26]. Language barriers further limit accessibility, although LLMs also offer potential to bridge these divides through translation technologies [2,25]. While LLMs hold promise for democratizing access to knowledge and enhancing efficiency in fields like healthcare and agriculture, comprehensive research is still needed to ensure these advancements bridge, rather than widen, existing inequities [20,21,25].

**Data privacy, security, and governance** constitute another critical pillar of ethical LLM development. The training of LLMs on vast datasets often involves the unauthorized use of copyrighted or sensitive information, raising significant legal and ethical challenges regarding intellectual property and data ownership [4,10,28]. Beyond training, LLM interactions pose acute privacy risks, particularly when processing sensitive personal, agronomic, or patient data through prompts or agents [2,25,26,29]. The general security posture of LLMs, including vulnerabilities to "insider threat-like behaviors," further underscores the need for robust safeguards [29]. Effective mitigation requires technical solutions like differential privacy and data anonymization, alongside comprehensive data governance frameworks that ensure informed consent, regulatory compliance, and ethical oversight throughout the LLM lifecycle [25,28,29].

Finally, the potential for **over-reliance and legal challenges** poses substantial risks to scientific integrity and accountability. An "automation dependency" on LLMs can impair critical thinking and foster the uncritical acceptance of potentially flawed AI outputs, undermining the rigorous methodology essential for scientific discovery [2,19,34]. This is exacerbated by a lack of model interpretability, which obscures the reasoning behind LLM-generated insights, making critical human oversight more difficult [26]. Simultaneously, the legal landscape is complicated by ambiguities surrounding intellectual property rights and inventorship for AI-generated content, with ongoing lawsuits concerning training data usage and the blurring of human and AI contributions [4,10,28]. Academic integrity faces new threats from plagiarism and authorship blurring, while the potential for LLMs to provide inaccurate advice on complex legal or ethical matters introduces significant liability risks [19,21]. Mitigating these risks necessitates enhancing model interpretability and rigorously adhering to "good scientific practices" with robust human oversight, alongside the urgent development of adaptable legal frameworks for generative AI [3,25,34].

In summary, the ethical, social, and economic challenges posed by scientific LLMs are deeply interconnected. Biases embedded in training data contribute to both misinformation and socio-economic inequality. A lack of transparent governance frameworks exacerbates privacy and security vulnerabilities while also complicating accountability and legal challenges. Moreover, issues of misinformation and data integrity directly impact the potential for over-reliance, as unverified AI outputs can undermine critical thinking and lead to flawed scientific outcomes. Addressing these complex interdependencies requires a holistic and multidisciplinary approach, focusing on developing transparent, accountable, and equitably accessible LLMs, supported by adaptive regulatory frameworks and a continued emphasis on human-centric scientific rigor [23,25,26].
### 12.1 Misinformation and Data Integrity
The propagation of misinformation by Large Language Models (LLMs) represents a critical challenge, particularly within scientific contexts, where its consequences can be severe and far-reaching [2]. For instance, in fields such as agriculture, inaccurate or unreliable scientific advice generated by LLMs could lead to significant real-world harms [2]. This issue extends beyond individual false statements, threatening the integrity of scientific data and knowledge bases by degrading information quality and trustworthiness over time [2]. The unchecked use of LLMs risks compromising established scientific practices and eroding trust in scientific findings [34].

A primary root cause of misinformation is the inherent tendency of LLMs to "hallucinate," generating confident yet incorrect outputs that appear plausible [20,29]. These "plausible false assertions" are not based on factual data or are contextually inappropriate, stemming from the probabilistic nature of LLMs that predict the next word based on learned patterns rather than strict factual verification [20]. This is evident in issues like "hallucinated reasoning chains" in Multimodal-CoT models, which can mislead answer inference in complex scientific tasks [32]. Such hallucinations directly lead to "factual errors" and compromise "contextual coherence" in scientific writing and experiment planning, raising significant "academic integrity concerns" [13,19].

Another significant root cause is the bias or incompleteness embedded within the massive datasets used to train LLMs [2]. LLMs trained on such data can inadvertently perpetuate stereotypes and biases, leading to "toxic, biased, or even harmful content for humans" [4,10,17,30]. This can result in "skewed or inaccurate information," making it difficult to align models with human values and purge "spurious correlations" if the models only learn surface statistics rather than underlying meaning [24,25]. The challenge of differentiating AI-generated from human-written text further exacerbates the risk, allowing undetected misinformation to be accepted as fact [25].

The integrity of scientific data and knowledge bases faces a long-term threat from the increasing influx of AI-generated content. This phenomenon, referred to as the "erosion of the digital agricultural commons" in specific domains, results from the mixing of human and AI-generated content, potentially leading to publication spam and the misuse of LLMs in peer review [2,19]. Moreover, LLMs can exhibit "fundamentally novel behaviors" unforeseen by developers, raising concerns about uncontrolled content generation and potential misuse [20]. Beyond unintentional errors, there is a serious risk of malicious actors exploiting LLM vulnerabilities to generate harmful content or bolster disinformation campaigns and fraud [2,10]. An example of potentially harmful content generated is LLMs providing instructions for synthesizing biological weapons to non-specialists [20].



**Misinformation & Data Integrity: Challenges and Mitigation**

| Challenge Category       | Specific Issue                                              | Root Cause                                                 | Mitigation Strategy                                         |
| :----------------------- | :---------------------------------------------------------- | :--------------------------------------------------------- | :---------------------------------------------------------- |
| **Misinformation (Hallucinations)** | Generation of plausible but factually incorrect outputs.    | Probabilistic nature, knowledge gaps, "surface statistics." | "Red teaming," Constitutional AI, Domain-specific LLMs (chatIPCC), Alignment tuning (honesty). |
| **Bias & Incompleteness** | Perpetuation of stereotypes, generation of inaccurate/harmful content. | Biases in training data, insufficient data quality.        | Active efforts to "reduce bias," ethical alignment.           |
| **Data Integrity (Long-term Threat)** | Erosion of "digital commons," publication spam, misuse in peer review, malicious exploitation. | Blending human/AI content, difficulty in differentiating AI-generated text. | Robust testing, careful governance, transparency, clear guidelines, expert verification. |

To counteract these threats, various mitigation strategies and safeguards have been proposed and implemented. GPT-4, for instance, employs "multiple intervention strategies," including "red teaming," to reduce the generation of harmful content and combat hallucinations [30]. Similarly, Claude's "constitutional AI" aims to ensure outputs are "helpful, harmless and accurate," while the o4-mini model uses "deliberative alignment" to identify and mitigate attempts to exploit the system for unsafe content [15]. The development of domain-specific LLMs, such as chatIPCC, specifically addresses the challenge of misinformation in critical fields like climate change by grounding models in reliable, authoritative sources [5]. General ethical considerations in AI development, including addressing ethical and privacy issues and reducing bias, are also highlighted as crucial for ensuring data integrity [3,17,26].

Despite these efforts, significant research gaps remain. A fundamental open question revolves around whether LLMs truly learn "meaning" or merely "surface statistics" [24]. This distinction is critical because without understanding what LLMs actually learn, it becomes inherently difficult to fully align them with human values and reliably purge spurious correlations, which are direct contributors to biased and inaccurate outputs [24]. Critically appraising the effectiveness of current workarounds reveals that while techniques like alignment tuning focus on "honesty" and "harmlessness" to prevent false or offensive content [28], their capacity to fully eliminate hallucinations or deeply ingrained biases remains an active area of research. Further work is needed to develop more robust methods for detecting and correcting misinformation, especially in complex scientific reasoning chains, and to ensure that LLMs contribute positively to scientific knowledge generation without compromising its integrity.
### 12.2 Socio-Economic Impact and Inequality
The proliferation of Large Language Models (LLMs) presents a complex landscape of opportunities and challenges concerning socio-economic impact and inequality. While LLMs hold significant promise for advancing various fields, concerns are rising that they may exacerbate existing disparities and introduce new forms of inequality [2].



**Socio-Economic Impacts and Inequalities of Sci-LLMs**

| Impact Category                 | Specific Socio-Economic Effect                              | Mitigation / Opportunity                                    |
| :------------------------------ | :---------------------------------------------------------- | :---------------------------------------------------------- |
| **Job Displacement**            | Reduction of low-skilled jobs (e.g., agriculture, customer service). | Demands new skill sets, potential for job creation in AI development/management. |
| **Digital Divide**              | Unequal access to LLM technologies disadvantages communities/institutions. | LLMs for democratizing knowledge, potential to bridge divides (translation), equitable access models needed. |
| **Bias & Discrimination**       | Biases in training data perpetuate societal inequalities, discriminatory outcomes. | Active bias reduction, inclusive design, diverse training data. |
| **Language Barriers**           | Limits accessibility/utility for diverse communities.       | Real-time translation, speech recognition (improves equity). |
| **Impact on Knowledge Generation** | Homogenization of academic feedback, reduced diversity of insights in research. | Multi-agent collaboration, emphasis on human critical thinking, diverse review processes. |

One of the primary concerns revolves around **job displacement**, particularly in sectors reliant on a large workforce. In agriculture, for instance, the integration of LLMs could lead to the displacement of low-skilled workers, intensifying existing trends of workforce reduction within the sector [2]. Beyond agriculture, LLMs are being deployed in areas like customer service, with the explicit goal of "reduc the need for human employees," indicating a broader potential for job loss across industries [4]. More generally, the technology poses a risk of displacing workers and widening inequality gaps between those with and without access to it [10]. While tools like ChemCrow aim to "lower the barrier for non-experts" in complex tasks like chemistry, democratizing access to scientific research, this advancement implicitly raises questions about evolving job roles and the demand for new skill sets [5].

A critical factor contributing to inequality is the **digital divide**, stemming from disparities in access to advanced LLM technologies. Unequal access to LLMs can significantly disadvantage specific communities or demographics. For example, in agricultural contexts, unequal access can widen existing digital divides, negatively impacting certain farmer groups [2]. Similarly, in education, universities and schools with limited financial resources or inadequate staff face considerable difficulties in leveraging LLM advantages, thereby perpetuating unequal access to quality educational tools [25]. Even as LLMs offer the potential to democratize access to foundational information by addressing the scarcity and cost of traditional tutoring services, it is acknowledged that unequal access to these advanced AI tools could inadvertently widen existing educational divides [20]. The cost-capability trade-offs, security, and compliance associated with LLMs also affect their accessibility and adoption rates across different economic entities, implicitly highlighting these socio-economic concerns [29].

**Bias embedded in LLM training data** represents another significant root cause of exacerbated inequalities. These biases, which often reflect and amplify existing societal inequalities based on income, education, or political views, can lead to discriminatory outcomes [4]. In scientific and medical applications, the presence of bias within LLMs necessitates active efforts to "reduce bias" to ensure fairness and equitable access to the benefits of LLM-driven discoveries [26]. Similarly, in educational settings, biases in LLMs can perpetuate dominant cultural norms, potentially marginalizing diverse perspectives and hindering inclusive learning environments [25].

Furthermore, **language barriers** significantly compound these issues, limiting the accessibility and utility of LLMs for diverse communities, particularly in contexts like agriculture [2]. However, LLMs also present an opportunity to mitigate this specific challenge. For instance, in education, LLMs, through real-time translators and speech recognition systems, can improve access for non-native speakers and students with disabilities, thereby fostering equity [25].

Beyond economic and social disparities, the widespread adoption of LLMs can also impact the very **generation and dissemination of scientific knowledge**. Concerns have been raised regarding the potential for homogenization of academic feedback if a large number of researchers rely on the same LLM systems for tasks like peer review. Such reliance could reduce the diversity of perspectives and creative insights typically derived from individual human reviewers, potentially affecting the quality and direction of scientific discourse [13].

Synthesizing these challenges, a clear interdependency emerges: **unequal access** to LLM technologies, coupled with **biases embedded in their training data**, directly contributes to the deepening of **digital divides** and disadvantages specific groups. These issues are further exacerbated by **language barriers**, which limit overall accessibility and utility [2]. While specific workarounds for deep-seated biases are often implicitly called for through the "need to reduce bias" [26], concrete and widely adopted solutions remain a critical area of ongoing development.

Despite these significant concerns, LLMs also offer potential positive socio-economic impacts. In healthcare, LLMs are seen as a "promising avenue for enhancing medical education and advancing healthcare reform," with the potential to reduce the medical burden and address shortages of licensed professionals, particularly in rural areas facing "low pass rates" in medical examinations [21]. Similarly, LLMs can contribute to democratizing access to foundational educational information by overcoming the prohibitive costs and scarcity of traditional tutoring [20]. In agriculture, LLMs are envisioned to "effectively enhance" the intelligence level of production, decision-making, and services, implying broader socio-economic benefits [11].

Despite these opportunities, critical research gaps remain. There is "insufficient research on how these models can bridge educational inequities, such as access to high-quality learning resources for underprivileged or marginalized communities" [25]. More broadly, a systematic appraisal of the effectiveness of proposed workarounds for bias and access disparities is needed. Open questions persist regarding the long-term impacts of LLMs on job markets, the efficacy of bias mitigation strategies in diverse real-world contexts, and the development of equitable access models that do not inadvertently create new divides [1,13]. Future research must focus on developing LLMs that are not only technologically advanced but also ethically responsible and socially equitable.
### 12.3 Data Privacy, Security, and Governance
The rapid proliferation of large language models (LLMs) in scientific domains has introduced heightened concerns regarding data privacy, security, and governance, particularly when these models interact with sensitive personal or proprietary scientific data [1,2,13]. The risks of unauthorized data collection and potential breaches are significant, necessitating robust safeguards and clear ethical guidelines [2].

A primary challenge lies in the unauthorized use of training data and the pervasive lack of explicit consent. LLMs are frequently trained on massive datasets that may include copyrighted material or sensitive information without the explicit consent of the creators or individuals, leading to legal challenges and ethical disputes concerning intellectual property and data ownership [4,10,28]. This issue extends to "information hazards," where LLMs might inadvertently disclose private information present in their training data [10]. Critically, a significant gap exists in the academic discourse, as few studies rigorously address privacy concerns in LLM-based innovations, with particular deficiencies noted in disclosing strategies for obtaining consent and implementing data protection measures like anonymization and sanitization [20].

Beyond training data, privacy risks are acute during LLM interaction and reasoning. Personal LLM agents, when accessing sensitive user data through prompts or interactions, create serious vulnerabilities for privacy breaches [29]. The natural language input to LLMs often contains highly personal and sensitive information about individuals' private lives and identities, exacerbating the problem when adequate attention to privacy is lacking [20]. This is particularly evident in sensitive domains. For instance, the increased collection of personal agronomic data via LLM chatbots raises concerns about privacy breaches and security vulnerabilities in food production [2]. Similarly, in educational settings, LLM usage raises concerns about data breaches, unauthorized access, or misuse of confidential student information [25]. The medical field also faces similar challenges, with LLMs in drug discovery and medical education requiring robust frameworks to address ethical and privacy issues related to sensitive scientific and patient data [1,21,26].

Furthermore, the general security posture of LLMs presents a challenge. Investigations into "insider threat-like behaviors from large language model (LLM) agents" highlight fundamental security vulnerabilities and governance challenges [29]. These root causes are interconnected, as the lack of clear governance frameworks complicates the ethical and legal implementation of LLMs, contributing to both privacy risks and security vulnerabilities [25]. Moreover, the reliance on massive datasets, which can include low-quality or sensitive information, necessitates careful control over LLM outputs to prevent the generation of harmful content, underscoring the trade-off between leveraging large data for model capability and respecting individual privacy [14,30]. Ethical dilemmas are also magnified by the potential for bias propagation, as discussed in the context of Sci-LLMs [1,13].

To mitigate these risks, several workarounds and solutions are being explored, though their effectiveness and widespread adoption remain subjects of ongoing research. Technical approaches include privacy-preserving techniques such as differential privacy, as demonstrated by VaultGemma, and methods for training language models without direct data sharing, exemplified by FlexOlmo [29]. Data preprocessing steps like "privacy reduction" (隐私减少) aim to protect user privacy by removing Personally Identifiable Information (PII) before model training [28]. More broadly, alignment techniques that integrate human values and objectives are crucial to ensure models adhere to ethical standards and avoid generating harmful content [17,20].

However, the most critical need is the establishment of robust data governance frameworks, privacy-preserving techniques, and clear guidelines for the responsible and ethical use of LLMs in scientific domains [2,26]. This includes adopting robust encryption and anonymization, ensuring compliance with global regulations (e.g., GDPR, FERPA), establishing ethical oversight committees, and obtaining informed consent from data subjects [25]. While general efforts to mitigate risks, such as those undertaken by GPT-4, have been noted [30], there remains a significant deficiency in specific details regarding comprehensive frameworks for managing privacy breaches and security vulnerabilities. The academic community is actively called upon to establish clear guidelines and ethical standards for LLM use to ensure academic integrity and originality, which also implicitly addresses governance and responsible data handling [3,19].

Despite ongoing efforts, significant research gaps persist. The lack of detailed strategies for consent and data protection measures in many LLM applications, particularly in specialized domains, highlights an urgent need for practical, enforceable guidelines and technical solutions [20,26]. Open questions remain regarding how to effectively balance model performance with stringent privacy requirements across diverse scientific fields, and how to develop universal yet adaptable frameworks that can address both general LLM privacy issues and domain-specific challenges, such as those presented by biological or agronomic data. Future research must focus on developing transparent, accountable, and legally compliant data handling practices throughout the entire LLM lifecycle, from data acquisition and training to deployment and interaction, to build trust and ensure the ethical advancement of scientific LLMs.
### 12.4 Over-reliance and Legal Challenges
The integration of Large Language Models (LLMs) into scientific inquiry and decision-making introduces significant risks related to "automation dependency," where an over-reliance on these tools can impair critical thinking and independent problem-solving skills [2]. This dependency may lead to the uncritical acceptance of potentially flawed AI outputs, undermining the rigorous methodology foundational to scientific discovery and agricultural innovation [2].

Several studies highlight the devaluation of critical thinking as a primary concern. The ease of using LLMs to draft parts of scientific papers, for instance, risks undermining the intellectual effort and critical thinking traditionally required for academic writing, thereby potentially devaluing the learning process and research itself [19]. Similarly, in broader scientific contexts, there is a concern that such over-reliance could degrade original thought and rigorous methodology among scientists, allowing ease of use to supersede scientific thoroughness [34]. This issue is not limited to academic settings; it also extends to fields like agriculture, where excessive reliance on LLMs could foster a passive acceptance of AI-generated advice among farmers [2]. In educational settings, over-dependence on AI tools is recognized as a factor that may hinder students' development of critical thinking and problem-solving skills, and even undermine the role of mentors [25]. Even advanced models like GPT-4 acknowledge "overreliance" as a critical issue requiring "multiple intervention strategies" [30]. The inherent characteristic of current LLMs to be "overly eager to satisfy human instructions" further exacerbates this challenge, as minor human feedback can significantly compromise the integrity of scientific outputs, such as hypotheses, thereby highlighting difficulties in balancing human and multi-agent collaboration [23]. Some papers implicitly touch upon these risks without explicit detailing, such as broad discussions on "reducing potential risks of model usage" or "safety and misuse" [33] and "academic integrity concerns" in scientific writing and peer review [19]. Others discuss risks of uncontrolled or unintended model behaviors, which are closely related to a decline in critical thinking if AI advice is passively accepted [20].

The interpretability of LLMs emerges as a critical factor in mitigating over-reliance. A lack of model interpretability, where the reasoning behind an LLM's output is opaque, can foster over-reliance on AI-generated insights, potentially diminishing critical human oversight and understanding [26]. This makes it difficult to trace the provenance of discoveries or outcomes and can contribute to uncritical acceptance. Conversely, providing "simple rules to explain its analysis process, predictions, and results" helps scientists "trust and act on its insights" by promoting understanding rather than blind acceptance, as demonstrated by models like LLM4SD [3]. Research in drug discovery further emphasizes the indispensable role of human authors for thorough information veracity review and rewriting, cautioning against over-reliance on LLMs without critical human oversight to prevent compromised scientific quality or factual errors [5].

Concurrently, the emerging legal complexities surrounding intellectual property (IP) rights and inventorship pose significant challenges in the context of LLM-assisted scientific innovation, blurring the line between human and AI contribution [2]. A primary root cause for legal uncertainty stems from "multiple lawsuits" arising from claims of training data usage without consent, leading to questions about AI-generated creative works [4,10,28]. This issue, often discussed under data privacy, lays a foundation for broader IP disputes. Beyond training data, the use of LLMs in generating scientific content raises significant academic integrity concerns, including plagiarism and authorship blurring. Researchers might present machine-generated text as their own, and LLMs can produce text highly similar to existing literature, increasing the risk of unintentional plagiarism [19]. New forms of academic misconduct, such as "plagiarism laundering," are also emerging with LLM-assisted peer review [19]. The lack of interpretability in models can also have implications for accountability and future legal challenges, particularly in tracing the ownership or responsibility for AI-generated discoveries [26]. Furthermore, the failure of LLMs to correctly answer questions on "healthcare policies" and "legal regulations" highlights a potential risk if professionals rely on AI for sensitive advice without human verification, which could lead to significant legal liabilities [21].

The interdependencies between these challenges are evident. For instance, the lack of interpretability contributes to both over-reliance by hindering critical human oversight and potentially complicates legal accountability by obscuring the origin of insights [26]. Similarly, biases present in LLM training data can lead to skewed scientific research results [1], which, if uncritically accepted due to over-reliance, could result in flawed innovations with subsequent legal ramifications. The "lack of clear governance frameworks" is a significant root cause for both ethical and legal challenges [25].

Proposed workarounds and mitigation strategies include enhancing model interpretability [3,26] and emphasizing the maintenance of "good scientific practices" through robust human oversight and critical verification [5,34]. However, open questions and significant research gaps remain. There is an urgent need for further research on the ethics of intellectual property rights in educational contexts and, more broadly, in scientific innovation, necessitating the update of ethical codes to address problems caused by generative AI tools [25]. The challenge of developing comprehensive and adaptable legal frameworks for rapidly evolving AI problems, especially concerning issues like inventorship and accountability for AI-driven insights, requires substantial attention [25]. Moreover, achieving a balanced and effective human-AI collaboration that harnesses the power of LLMs without compromising scientific integrity or fostering uncritical acceptance remains a complex open problem [23].
## 13. Grand Challenges and Future Research Frontiers
The rapid evolution of Scientific Large Language Models (Sci-LLMs) has heralded a transformative era in scientific research, yet their full potential remains contingent upon addressing several grand challenges and charting clear pathways for future innovation. 

![Grand Challenges and Future Research Frontiers for Sci-LLMs](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/RMhv7JQdN_C5l6kKIQXsF_Grand%20Challenges%20and%20Future%20Research%20Frontiers%20for%20Sci-LLMs.png)

These frontiers encompass fundamental architectural advancements, deep integration of domain-specific knowledge, novel learning paradigms, robust ethical governance, assurance of trustworthiness and reproducibility, and the pursuit of a broader scientific intelligence [2,12].

A primary direction involves **Advancements in Efficient and Generalizable Architectures**, moving beyond the confines of traditional Transformer models towards a "post-Transformer era" characterized by modular, hybrid designs and "algorithm-system-hardware co-design" [12]. The imperative is to achieve linear scalability and maximize performance within computational budgets, processing ever-growing and diverse scientific datasets efficiently [17,30]. This includes developing parameter-efficient models, addressing context length limitations for long biological sequences, and integrating multimodal data effectively through unified architectures or specialized adaptations [22,31].

Concurrently, **Enhancing Domain Specificity and Knowledge Grounding** is critical to overcome the limitations of generic LLMs in specialized scientific fields [7]. Future research must focus on sophisticated techniques for knowledge representation, semantic integration of diverse scientific ontologies, and advanced information extraction from various data types [18]. The integration of knowledge graphs and the establishment of a 'Scientific Data Commons' are pivotal for fostering cross-modal training, mitigating hallucinations, and ensuring knowledge-grounded generation, especially through Retrieval Augmented Generation (RAG) models [22,28]. This deep grounding is essential for robust hypothesis generation and automated scientific discovery [7].

The unique characteristics of scientific data and problems also necessitate **Novel Architectures and Learning Paradigms** that extend beyond standard models [1]. This includes the development of Hybrid Geometric-Linguistic Architectures, which integrate methods like graph neural networks (GNNs) with Transformers to process spatial and 3D data [31]. Exploration of alternative architectures, such as State Space Models (SSMs) and Mixture-of-Experts (MoE), alongside dynamic adaptive designs and memory mechanisms, aims to overcome existing limitations [28,29]. Furthermore, novel learning paradigms like Multimodal Chain-of-Thought (CoT), knowledge-enhanced training, and Reinforcement Learning (RL) are crucial for capturing scientific relationships beyond linguistic patterns and fostering agentic behavior for complex reasoning [29,32].

As Sci-LLMs become more powerful, **Responsible AI Development and Policy Frameworks** emerge as non-negotiable aspects of their integration [2]. This involves actively mitigating algorithmic bias, preventing the dissemination of misinformation, safeguarding privacy, and addressing academic integrity concerns in scientific writing and peer review [4,19]. Aligning Sci-LLMs with human values through approaches like Reinforcement Learning from Human Feedback (RLHF) and constitutional AI, coupled with robust interdisciplinary collaboration among AI researchers, domain experts, ethicists, and policymakers, is paramount to ensure beneficial and ethical deployment [15,30].

Crucially, **Ensuring Trustworthiness and Reproducibility** is fundamental for the widespread adoption of Sci-LLMs in scientific practice [18]. Addressing issues like hallucinations, lack of interpretability, and the high cost of model pre-training is essential for building confidence [17,20]. Future efforts will focus on enhancing model interpretability through transparent reasoning pathways, rigorous real-world validation and experimental verification (e.g., "Virtual Wet-Lab" environments), and implementing robust reproducibility mechanisms like comprehensive reporting and "Blockchain-Backed Provenance" [1,3].

The overarching aspiration for Sci-LLMs extends to fostering **Interdisciplinary Opportunities and Towards General Scientific Intelligence** (GSI) [29]. This involves leveraging their versatility across diverse scientific domains, from drug discovery to agriculture, and developing agentic AI systems capable of autonomous discovery [11,26]. The vision of GSI entails AI that can integrate multimodal knowledge, form internal "world models," and collaboratively build and query a 'Universal Scientific Knowledge Graph' to plan, execute, and iterate on scientific research [5,24]. This also extends to democratizing scientific education and research through LLM-assisted platforms [5].

Finally, the realization of these ambitions requires **Novel Benchmarks for Scientific Process Intelligence**. Current evaluation paradigms, often relying on static outputs and human-written ground truth, are insufficient for assessing the complexity of scientific inquiry [16]. The future necessitates a shift towards evaluating the *efficiency, validity, and novelty* of the entire scientific process, employing interactive virtual lab environments for process evaluation and "blind peer review" benchmarks that leverage human expertise for assessing plausibility and scientific rigor [18,29]. These dynamic, process-oriented evaluations are critical for truly measuring and advancing scientific intelligence in LLMs [18].

Collectively, these grand challenges and future research frontiers outline an ambitious roadmap for developing Sci-LLMs that are not only powerful and efficient but also deeply integrated into scientific practice, reliable, ethical, and capable of accelerating the pace of discovery across all domains [20].
### 13.1 Advancements in Efficient and Generalizable Architectures
Future research in scientific large language models (LLMs) is critically projected towards developing architectures that are not only more efficient but also universally applicable, capable of seamlessly integrating diverse scientific data types and performing sophisticated reasoning across domains [12]. This ongoing pursuit is driven by the imperative for architectures that can offer linear scalability without compromising performance, a crucial prerequisite for processing the ever-growing and increasingly complex datasets characteristic of scientific research [12].



**Future Architectural Advancements for Efficient Sci-LLMs**

| Advancement Category          | Description                                                 | Key Strategies / Examples                                   | Goals & Scientific Impact                                     |
| :---------------------------- | :---------------------------------------------------------- | :---------------------------------------------------------- | :------------------------------------------------------------ |
| **Post-Transformer Era & Hybrid Designs** | Moving beyond monolithic Transformers to modular, diverse, and hybrid architectures. | Modular designs, new computing primitives, algorithm-system-hardware co-design. | Linear scalability, maximum performance within compute budgets, efficient processing of diverse scientific data. |
| **Efficiency & Scalability**  | Achieve greater performance with reduced computational resources. | Compute-optimal scaling laws, streamlined/hybrid LLMs (e.g., Mamba-Transformer), predictable scaling mechanisms. | Lower barriers to research, faster inference, reduced energy consumption. |
| **Alternative Architectures** | Explore fundamentally different designs beyond Transformer's self-attention. | Parameterized State Space Models (SSMs) like Mamba, RWKV, RetNet, Hyena; Byte-level U-Net models; Assembly-of-Experts (AoE). | Overcome quadratic complexity for long sequences, improve efficiency, enhance generalizability. |
| **Parameter-Efficient Models** | Develop powerful models with fewer parameters for wider deployment. | Mixture-of-Experts (MoE) models (Kimi K2, ERNIE), compact models (MobileLLM-R1, Phi-4-mini-Flash-Reasoning), Multimodal-CoT. | Accessible powerful models, perform well on complex tasks with less compute. |
| **Domain-Specific Integration** | Architectures capable of natively handling diverse scientific data types. | Architectural modifications for GNNs, visual encoders, multi-modal integration, specialized designs for agriculture/materials. | Process long protein/gene sequences, handle complex 3D structures, unify diverse data modalities for deeper understanding. |

The field is actively transitioning beyond the confines of the singular, homogenous Transformer architecture into what has been termed a "post-Transformer era," envisioning a more diverse, modular, and hybrid ecosystem [12]. This paradigm shift necessitates the emergence of novel computing primitives and architectural designs. A core future direction involves the deep integration of "algorithm-system-hardware co-design," where algorithmic efficiency is developed in synergy with system software and hardware capabilities, such as leveraging GPU Tensor Cores, exemplified by innovations like FlashAttention [12]. This co-design approach is considered crucial for realizing genuinely efficient models, aiming to re-balance efficiency and capability to achieve a new Pareto optimal frontier where maximum model power is attained within given computational budgets [12].

Efforts to enhance efficiency are multifaceted. The discussion around compute-optimal scaling laws, such as KM and Chinchilla, highlights a continuous drive for more efficient resource allocation, implying a need for architectures that maximize performance under budget constraints [30]. Furthermore, techniques like predictable scaling mechanisms used by models such as GPT-4 to forecast performance with smaller compute during training also point towards future architectural advancements focused on efficiency [30]. To address the challenges of slow processing speeds inherent in autoregressive LLMs, future research calls for developing faster, streamlined LLM versions or hybrid systems that combine LLMs with smaller, task-specific models, thereby balancing speed and accuracy [19].

Alternative architectural paradigms are also gaining traction. Emerging architectures such as Mamba, RWKV, RetNet, and Hyena, which are variants based on State Space Models (SSM), are being explored to improve the efficiency and performance of LLMs beyond traditional Transformer limitations [28]. These developments aim to overcome the constraints of existing architectures and contribute to more efficient and potentially more generalizable AI systems [28]. Concurrently, the continuous development of optimized architectures like Mixture-of-Experts (MoE) models (e.g., Kimi K2, ERNIE-4.5-21B-A3B-Thinking, Hunyuan-A13B), hybrid Mamba-Transformers (e.g., Nemotron Nano 2), and byte-level U-Nets (from Meta AI Researchers) underscores a significant challenge in creating universally applicable and efficient AI systems [29]. The push for smaller, yet powerful models, such as MobileLLM-R1, Gemma 3 270M, and Phi-4-mini-Flash-Reasoning, further exemplifies this goal of designing efficient and generalizable architectures capable of effective performance across various tasks and deployment environments [29]. The development of Multimodal-CoT, a model with "under 1 billion parameters" that has demonstrated superior performance over larger models like GPT-3.5 in complex scientific reasoning tasks, highlights a promising direction towards parameter-efficient architectures through innovations in reasoning frameworks [32].

The high computational cost associated with LLM pre-training remains a significant challenge, limiting replication and ablation studies, thus emphasizing the critical need for more resource-efficient training and deployment mechanisms [17]. This necessitates "more resource-efficient methodologies concerning the training and inference of LLMs," with "sparse scaling" identified as a promising direction to increase parameters while maintaining constant training and inference costs (in FLOPs) [20]. Furthermore, lightweight rule compression algorithms are proposed to address the high computational resource dependency of LLM inference and feature generation, implying a demand for more efficient architectures or processing methods [22].

Beyond efficiency, the development of generalizable architectures capable of integrating diverse scientific data types is paramount. Current limitations in handling long protein/gene sequences due to context length constraints, for instance, underscore the need for architectural advancements that can process longer scientific data more effectively [22]. The survey implicitly calls for such advancements by showcasing a variety of architectural modifications, including adapters, GNN-nested Transformers, and hybrid experts, alongside multimodal integration techniques, all aimed at adapting LLMs to diverse scientific data and domains [31]. This points towards a need for more unified and capable architectures [31]. Specialized architectures are being explored for specific domains, such as agriculture [11] and materials science, where models must better handle complex domain-specific knowledge and challenges [7]. The complexity of scientific data, which is often multimodal, cross-scale, and domain-specific, demands architectures and representations that preserve domain invariance and enable cross-modal reasoning [18,23]. There is a particular emphasis on enhancing quantitative analysis and multimodal processing capabilities, crucial for tasks such as drug discovery and development, which require integrating diverse scientific data types and performing complex quantitative tasks, moving beyond current context window limitations [26].

To facilitate sophisticated reasoning, future research calls for modular frameworks that can simulate specialized scientific reasoning more accurately [19]. Addressing issues like hallucinations may require robust verification mechanisms or real-time feedback loops [19]. The quest for "more principled investigations" into the "secrets" of emergent abilities [30] implies a need for architectures that are better understood and inherently more generalizable across tasks. Moreover, new model architectures and learning objectives that are "more optimal for scientific language" are being investigated, focusing on methods to "more effectively capture semantic information from entire sequences, rather than relying solely on autoregressive generation" [1]. The identification of "Tasks Not Solvable By Scale" within the current autoregressive Transformer-based LLM scaling paradigm further implies a critical need for advancements beyond existing architectures to unlock novel scientific discoveries [16]. Overall, the continuous evolution of LLM technology, as it is "ever-evolving" and "constantly being refined to increase their accuracy," consistently underpins the need for more efficient and generalizable architectures that can adapt to new applications and improve performance [4].
### 13.2 Enhancing Domain Specificity and Knowledge Grounding
The future trajectory for Scientific Large Language Models (SciLLMs) necessitates a profound shift towards deep grounding in domain-specific knowledge, transcending generic capabilities to cultivate expert-level comprehension and reasoning across specialized scientific disciplines [7]. This imperative is driven by the recognized limitations of general LLMs in specialized fields, where their lack of specificity and precision poses critical risks, particularly in areas like medical education [20]. Specialized models, such as Biomni-R0 for biomedical research, AlphaGenome for genomics, and BloombergGPT for finance, exemplify this emerging trend, demonstrating superior performance when tailored to specific domains [4,29].

Future endeavors must concentrate on sophisticated techniques for knowledge representation, semantic integration of diverse scientific ontologies, and advanced information extraction from both unstructured and semi-structured scientific texts, as well as multimodal data [7]. The continuous need for "better multimodal dataset construction and information extraction techniques from scientific literature" is highlighted, particularly for capturing valuable domain-specific principles and knowledge [7,18]. This also includes the development of methodologies for recursively extracting structured information, such as the SPIRES method for medical texts [5]. The integration of knowledge graphs emerges as a pivotal strategy, directly facilitating knowledge grounding by unifying various knowledge sources, enhancing understanding, and mitigating hallucinations and factual inaccuracies [9,11,28,31]. Approaches like "Context Engineering" further assist in effectively grounding general models within specific domain contexts [29].

Further research is crucial for establishing robust frameworks for hypothesis generation and automated scientific discovery [7]. The concept of "materials science LLMs (MatSci-LLMs) that are grounded in domain knowledge" is proposed to enable hypothesis generation and testing within materials science [7]. Similarly, the LLM4SD framework integrates knowledge synthesis from literature and knowledge inference from data to predict molecular properties, with aspirations to extend its "domain generalization capability" to fields like materials science through multimodal data integration (e.g., crystal structures, spectral data) and cross-domain pre-training [22]. This framework also advocates for a "continuous learning mechanism" to ensure the rule base remains current and well-grounded [22]. Efforts to enhance LLM capabilities for hypothesis generation also involve improving training data collection and strategies [19]. For interdisciplinary research, there is a call to expand frameworks like HILMA to diverse disciplinary fields, acknowledging that a single framework may not suit all domains, and to build expert agents for multi-domain discussions [23].

The establishment of a 'Scientific Data Commons' is vital for cross-modal training. This platform should be an international, collaborative initiative with standardized data formats and APIs, accommodating diverse scientific data types including text, graphs, sequences, and experimental results [1]. Such a commons would address several critical challenges. Federated learning can effectively mitigate privacy concerns associated with sensitive data, aligning with principles of `12.3 Data Privacy, Security, and Governance` [1]. Concurrently, AI-assisted curation, validated by domain experts, can generate high-quality, fine-tuning datasets, thereby overcoming the pervasive issue of data scarcity, as discussed in `11.2 Data Scarcity and Quality` [1]. The vision of "AI Virtual Cells" in biology directly exemplifies this, aiming to simulate complex biological dynamics by leveraging vast, domain-specific datasets from initiatives like the Human Cell Atlas and the Human Genome Project [8]. The emphasis on "professional data" (e.g., scientific text, high-quality medical data) during pre-training is consistently highlighted as crucial for enhancing model capabilities in specific domains [21,28,30].

'Knowledge-Grounded Generation' through Retrieval Augmented Generation (RAG) models, specifically designed for scientific literature and experimental data, presents a potent solution to combat hallucinations, factual inaccuracy, and contextual coherence issues (`11.4 Performance and Reliability Issues (Hallucinations)`) [13]. By grounding outputs in empirical evidence and providing source traceability, RAG models enhance reliability. This approach is further bolstered by methods that allow LLMs to interact with external tools and domain-specific databases, such as GeneGPT utilizing NCBI Web APIs, chatIPCC grounding in authoritative documents like IPCC AR6, and ChemCrow augmenting LLMs with expert-designed chemistry tools [5]. The necessity for LLMs to overcome limitations in handling non-textual information, such as numerical calculations, often points to the need for external tools and better knowledge grounding [17].

Several open research questions and future trends emerge from this focus. A significant challenge lies in improving LLMs' "scientific understanding capabilities" and integrating biological insights for fields like drug discovery [26]. SciLLMs must overcome "insufficient adaptability to specific scenarios" in tasks like experiment planning and address "difficulty in understanding specialized terminology" in processes like peer review [13]. This requires fine-tuning LLMs with high-quality, domain-specific datasets and developing modular frameworks for specialized roles, alongside enhancing citation analysis for verifying reference relevance [19]. The integration of external knowledge sources like knowledge graphs (UMLS, GAKG) and symbolic reasoning engines (Alpha-Geometry) is crucial for grounding LLMs in domain knowledge for problem-solving [31]. The call for "deeper exploration, acceleration of R&D, leveraging underutilized experimental data, integrating computational and empirical approaches, and overcoming specific bottlenecks" underscores the comprehensive effort required to advance scientific discovery with LLMs [3,9]. Furthermore, the intriguing question of "reverse-engineering" the world representation and underlying "game engine" from an LLM, rather than assuming prior knowledge, highlights a frontier in knowledge grounding for more complex and less structured domains [24]. The continuous evolution of specialized models, such as BioGPT pre-trained on biomedical literature, further reinforces the efficacy of domain-specific training for enhancing performance in specialized NLP tasks [32]. Ultimately, achieving true expert-level understanding in science requires an ongoing commitment to building diverse, high-quality, multimodal datasets and tailoring training strategies for specific scientific languages and knowledge systems [18,31,35].
### 13.3 Novel Architectures and Learning Paradigms
The unique characteristics of scientific data and problems necessitate the exploration of novel architectural designs and learning paradigms that extend beyond standard Transformer models [1]. This imperative stems from challenges such as handling long-range dependencies, explicitly encoding scientific knowledge [1], and addressing fundamental limitations in current architectural paradigms, often highlighted by "Tasks Not Solvable By Scale" [16]. A deeper theoretical understanding of Large Language Models (LLMs) and the mechanisms behind their emergent abilities is also crucial [17,30].

A significant area of focus is the development of **Hybrid Geometric-Linguistic Architectures**. These designs aim to integrate geometric deep learning, such as equivariant graph neural networks (GNNs), with Transformer-based models to simultaneously process sequences and 3D coordinates [1]. This approach is critical for reasoning about spatial relationships in complex structures like molecules and proteins, directly addressing the limitations of current architectures in incorporating 3D structural information (Section 11.3, Model Architecture Limitations for Scientific Data) [1]. For instance, various approaches have been explored, including using GNNs as graph encoders alongside LLMs as text encoders, connected via contrastive learning; linearizing complex data like molecular graphs (SMILES) or particle coordinates for LLM input; employing visual encoders (e.g., ViT, Swin Transformer) to project images into multiple visual tokens for LLM processing; and developing CLIP-like architectures for joint training of text and image encoders using contrastive learning [31]. Such multimodal LLMs are envisioned to systematically retrieve and enhance information from diverse data types, including images, signals, and audio/video, prevalent in many scientific domains [23].

Beyond hybrid models, researchers are actively pursuing **alternative architectural designs** to overcome the limitations of the Transformer. Emerging architectures based on parameterized state space models (SSMs), such as Mamba, RWKV, RetNet, and Hyena, are being investigated as efficient alternatives to Transformer-based models [28]. These aim to improve model efficiency and performance, particularly in handling long sequences, a critical bottleneck in processing biological data like proteins and genes [22]. Further innovations include Meta AI's scalable byte-level autoregressive U-Net model as an alternative to token-based Transformers, and hybrid architectures like Mamba-Transformers (e.g., Nemotron Nano 2) and Assembly-of-Experts (e.g., DeepSeek-TNG R1T2 Chimera) [29]. The adoption of Mixture-of-Experts (MoE) architectures by models such as Ernie 4.5, Llama 4, Mistral, and Granite represents a novel paradigm for scaling capacity [15]. Future directions also advocate for dynamic adaptive architectures that can adjust their internal structure based on task complexity or input characteristics, revolutionary designs in memory mechanisms for more efficient long-context processing, and architectures capable of directly handling multi-dimensional, graph-like, or continuous data structures beyond one-dimensional sequences [12].

The development of **Specialized Multi-modal Pre-training** is crucial for addressing difficulties with specialized terminology and insufficient adaptability (Section 11.1, Technical and Resource Constraints) [19]. This involves large-scale, domain-specific multi-modal scientific datasets (text, chemical structures, protein sequences, images) with novel pre-training objectives that capture scientific relationships beyond linguistic patterns, potentially incorporating knowledge graphs or ontologies directly into the model's architecture [19]. Examples include the emergence of "foundation models" in life sciences, such as Evo, AlphaFold 3, RhoFold, and GET, which are trained on massive biological datasets and optimized for biological languages and their multi-scale complexities [8].

Novel **learning paradigms** are also gaining prominence. The Multimodal Chain-of-Thought (CoT) framework, for instance, offers a "decoupled training framework" that separates rationale generation from answer inference, integrating multimodal features to improve reasoning and mitigate hallucinated reasoning chains in complex scientific contexts [32]. Incorporating knowledge-enhanced training methods is suggested to achieve better performance and generate more consistent and trustworthy responses in specialized domains like medical examinations, moving beyond simple zero-shot application [21]. Reinforcement Learning (RL) has emerged as a major learning paradigm shift, with applications in tool-use (e.g., Grok-4-Fast), multi-turn interactions (e.g., Biomni-R0), long-context processing (e.g., MemAgent), and verifiable rewards (e.g., RLVR), optimizing LLMs for complex behaviors, robustness, and reasoning beyond simple next-token prediction [29]. Unsupervised training frameworks, such as Internal Coherence Maximization (ICM), further represent new learning objectives that do not rely on human supervision [29]. The LLM4SD framework, which combines LLMs for knowledge synthesis and pattern inference with traditional machine learning for interpretable prediction, exemplifies a novel learning paradigm for scientific discovery [3]. Furthermore, the fundamental inquiry into whether LLMs learn world models or merely surface statistics [24] points to future research directions focused on designing architectures and training objectives that explicitly aim to learn causal models and better capture semantic information from entire sequences, rather than solely relying on autoregressive generation [1,30]. This shift is also implicit in the push towards "Agentic Science," which requires architectures capable of autonomous planning, execution, reflection, and iteration, moving beyond existing Transformer-based models for complex scientific reasoning [18].

While some papers survey existing architectural components and broadly suggest "new methodologies and potential directions" [20], they often do not explicitly identify novel model architectures or learning objectives specifically optimized for scientific language beyond current Transformer-based models or methods to effectively capture semantic information from entire sequences versus autoregressive generation [20]. The continuous evolution of LLM technology, however, suggests an ongoing imperative for novel architectures and learning paradigms to overcome current limitations and expand capabilities, particularly for scientific applications [4].
### 13.4 Responsible AI Development and Policy Frameworks
The integration of Large Language Models (LLMs) into scientific research necessitates the concurrent development of robust ethical guidelines and comprehensive policy frameworks to ensure responsible innovation and deployment [2]. This is crucial for establishing trustworthy AI systems that can genuinely accelerate scientific discovery while effectively mitigating potential harms [18]. The imperative to develop LLMs ethically is a recurrent theme across the literature, emphasizing the critical importance of ethical considerations throughout the entire development and application lifecycle of scientific LLMs [1,3,34]. The overarching ambition is to achieve "Sci-LLMs Super-alignment with human values" [1], ensuring that these powerful tools are beneficial, honest, and harmless in their scientific applications [17].

The deployment of LLMs in scientific contexts introduces several critical risks that demand proactive mitigation strategies. These include algorithmic bias, which can lead to biased, inaccurate, or unfair responses across various applications [4,20,26]. The dissemination of misinformation or inaccurate content is another significant concern, exemplified by efforts like chatIPCC, designed to prevent such risks in climate science and ensure informed decision-making [5]. Privacy violations pose substantial challenges, particularly regarding user data privacy in LLMs [14] and the "Privacy Risks in LLM Reasoning Traces" [29], alongside potential "insider threat-like behaviors from large language model (LLM) agents" [29]. Within academic settings, LLM usage raises academic integrity concerns in scientific writing and peer review, necessitating robust governance mechanisms for detecting LLM-generated content, transparently tracking AI contributions, and maintaining reviewer authenticity [13,19]. Furthermore, the potential for misuse of scientific AI tools, as observed with platforms like ChemCrow, underscores the urgent need for responsible usage guidelines [5]. Broader societal and ethical considerations also encompass the environmental impact of large AI infrastructure, such as the emissions from facilities supporting models like Grok [15]. Legal challenges, including copyright infringement lawsuits stemming from training data usage, further highlight the necessity for clear policy frameworks [4].

To address these multifaceted risks, various guiding principles and frameworks have been proposed and implemented to ensure the responsible and transparent development and application of scientific LLMs [3]. A foundational principle involves aligning LLMs with human values and preferences, a task recognized as profoundly challenging [17,20,30]. Different methodologies have emerged to achieve this alignment. OpenAI has pioneered approaches such as Reinforcement Learning from Human Feedback (RLHF) for InstructGPT and ChatGPT, aiming to produce models that are "helpful, honest, and harmless" [30]. GPT-4 further refines this through "six months of iterative alignment" that incorporates additional safety reward signals and multiple intervention strategies to mitigate issues like hallucinations, privacy breaches, and overreliance. OpenAI's five-stage iterative deployment strategy represents a practical framework for effectively reducing potential risks during model usage [30,33]. In contrast, other models adopt explicit, rule-based systems: Claude's "constitutional AI" ensures outputs are "helpful, harmless and accurate" by adhering to a predefined set of principles [15]. Similarly, the o4-mini model employs "deliberative alignment" to proactively detect and prevent the generation of unsafe content [15]. Beyond general alignment, specific technical solutions address particular risks; for instance, VaultGemma integrates "differential privacy" directly into its training process to embed robust privacy guarantees [29]. Furthermore, researchers highlight that a deeper understanding of LLM internal mechanisms, specifically whether they learn world models or merely surface statistics, is a crucial prerequisite for effective ethical development and successful alignment with human values [24]. The design of human-AI interaction mechanisms also requires careful consideration to foster robust and balanced collaboration, preventing disproportionate influence from minor human opinions in multi-agent systems [23].

The complex and multidisciplinary nature of these challenges necessitates robust interdisciplinary collaboration [2]. This involves actively engaging AI researchers, domain experts, ethicists, and policymakers to collectively mitigate risks such as bias, misinformation, privacy violations, and socio-economic displacement [2]. For example, within the educational sector, calls for collaboration among educators, policymakers, and legal experts are vital for addressing regulatory challenges and ensuring ethical AI deployment [25]. The academic community bears a responsibility to establish clear guidelines and ethical standards for LLM usage in scientific writing and peer review to uphold academic integrity [19]. Crucially, policymakers are advised to proactively monitor the impact of LLMs across various industries, including agriculture, to anticipate consequences and establish robust frameworks and guidelines for their responsible use [2]. This proactive stance is essential for maximizing benefits while effectively mitigating negative outcomes in sensitive domains such as drug discovery, where addressing ethical and privacy issues and reducing bias are paramount [26]. In medical applications, including the early diagnosis of Alzheimer's Disease or performance in national medical examinations, considerations of data privacy, cognitive bias, and regulatory compliance are critical, demanding careful ethical guidelines for the deployment of such diagnostic tools [21,32]. While much of the discourse on ethical AI development remains general, there is an increasingly distinct and urgent need for specific policy frameworks tailored to industrial and scientific applications, transitioning from abstract ethical considerations to concrete regulatory action [20,28].
### 13.5 Ensuring Trustworthiness and Reproducibility
The widespread adoption and acceptance of scientific large language models (Sci-LLMs) in scientific practice critically hinge on their trustworthiness, interpretability, and reproducibility [16,18,34]. Preserving "good scientific practices and trust in science" is paramount as LLMs become more integrated into discovery processes [34]. This necessitates developing robust mechanisms, methodologies, and technical solutions to enhance confidence in AI-driven scientific outputs.

A primary challenge to trustworthiness stems from inherent limitations of LLMs, such as the generation of "hallucinations" or "plausible false assertions" [20,29]. These can manifest as "inconsistent reasoning" or "factual inaccuracy" in scientific writing and hypothesis generation, directly undermining confidence [5,13,28]. Furthermore, the "lack of interpretability" in many LLM architectures obscures the rationale behind their predictions, hindering verification and acceptance by scientists [20,26]. Reproducibility is severely hampered by the "huge cost of model pre-training" and the common practice of industry models not revealing "many important training details (e.g., data collection and cleaning)" [17,30]. This opacity makes it "very costly to carry out repetitive, ablating studies" by the broader research community, creating a "Lack of Reproducibility" [16,17,30]. Challenges also extend to evaluation, with "unreliable generation evaluation" and biases from "LLM-as-a-judge" mechanisms posing significant hurdles [28,29].

To address these critical issues, a multi-faceted approach is required. Strategies to improve transparency in model decision-making focus on enhancing interpretability. For instance, LLM4SD provides interpretability by using "simple rules to explain its analysis process, predictions, and results," enabling scientists to "trust and act on its insights" [3]. Similarly, experiments with Othello-GPT demonstrate how identifying and manipulating the internal "world model" can provide insights into decision-making, thereby moving beyond black-box behavior and enhancing the reproducibility of internal logic [24]. The Multimodal-CoT framework, by generating "effective rationales" and mitigating "hallucinated reasoning chains," directly tackles the problem of unreliable intermediate steps in reasoning [32]. This emphasis on explicit, understandable reasoning pathways contrasts with approaches that merely provide output without justification.

Enhancing the fidelity of experimental results generated or predicted by LLMs requires a strong emphasis on robust validation. "Real-world validation and application" are critical for confirming reliability beyond mere computational benchmarks, as highlighted in drug discovery [26]. Several studies underscore the "critical need for scientific rigor and reproducibility" through experimental verification, citing examples such as AlphaFold 3's predictions benchmarked against experimental accuracy, and the laboratory validation of SyntheMol's designed antibiotics and nanobodies from the Stanford Virtual Lab [8]. This treats experimental verification as the gold standard for AI-generated scientific outputs.

A key strategy is the development of an 'AI-Powered "Virtual Wet-Lab" for Robust Evaluation' [1]. In this concept, virtual lab environments or physics-based simulators allow Sci-LLMs to 'run' virtual experiments and predict outcomes. These virtual experiments are integrated into iterative design-test cycles, providing feedback to the LLMs. This approach aims to develop more robust computational benchmarks that demonstrably correlate with real-world experimental success, thereby bridging the gap between AI research and wet-lab validation [1]. Furthermore, domain expert verification is crucial to address potential "noise or redundancy" in LLM-generated rules and for "thorough information veracity review" of LLM-generated scientific content [5,22]. Automated experimental execution, varying by discipline (e.g., coding in computer science, robotics in chemistry/biology), is proposed as the "most reliable method for verifying hypothesis validity" [19].

To ensure the replicability of research findings, several approaches are gaining traction. Increased transparency in reporting, including "more complete details and descriptions," is essential for enabling independent verification [16]. For scientific writing, better citation verification mechanisms and improved multi-document synthesis are needed to maintain up-to-date and reliable content [19]. Moreover, "Blockchain-Backed Provenance" solutions are being investigated to immutably track the provenance and modifications of LLM-generated scientific content and data [13]. This system would record each LLM-assisted step, from hypothesis generation and experiment design to analysis and writing, along with the models and data utilized. Such a distributed ledger approach would provide unprecedented transparency and auditability, directly enhancing trust and reproducibility while addressing concerns about academic integrity and experimental verification [13]. In specific domains, frameworks like VERINA, which enable "end-to-end verifiable code generation with formal proofs," demonstrate how trustworthiness can be formally assured [29]. Addressing the high cost barrier to reproducibility, mechanisms like GPT-4's "predictable scaling," which allow performance prediction with smaller compute, could potentially reduce the expense of large-scale experimentation and foster more widespread independent analysis [30].

In summary, a roadmap for building trustworthy AI systems that can function as reliable partners in accelerating scientific discovery involves several key pillars: (1) **Enhanced Interpretability** through transparent reasoning pathways and understandable explanations; (2) **Robust Validation** via rigorous experimental verification, human expertise, and virtual wet-labs; (3) **Reproducibility Mechanisms** including comprehensive reporting, "Blockchain-Backed Provenance," and accessible, verifiable model details; and (4) **Ethical Alignment** by continually addressing biases and ensuring outputs adhere to scientific and human values [4,20,25]. By proactively developing and integrating these strategies, the scientific community can foster greater confidence in Sci-LLMs, accelerating their responsible and impactful deployment.
### 13.6 Interdisciplinary Opportunities and Towards General Scientific Intelligence
The intrinsic versatility of Scientific Large Language Models (Sci-LLMs) has opened expansive interdisciplinary research opportunities, enabling their application across a diverse array of scientific domains and fostering novel collaborative paradigms. Sci-LLMs are characterized by their broad applicability, extending from finance to healthcare and marketing, demonstrating a significant potential to serve varied scientific fields [4]. This adaptability is evident in drug discovery, where LLMs are employed across all stages, from genomics to clinical trials [26], and in materials science, where they show exciting possibilities for accelerating discovery and knowledge dissemination [7]. Surveys demonstrate LLM applications across numerous fields, including general science, mathematics, physics, chemistry, materials science, biology, medicine, geography, geology, and environmental science [18,31]. For instance, frameworks like LLM4SD have demonstrated applicability across physiology, biophysics, physical chemistry, and quantum mechanics for molecular property prediction, with ambitions to extend generalization capabilities to other domains like materials science [3,22]. Furthermore, LLMs are revolutionizing sectors such as agriculture by integrating AI with domain-specific knowledge for enhanced intelligence in production and decision-making [11]. This wide spectrum of applications underscores their capacity to accelerate research, enhance discovery, and foster interdisciplinary collaboration across both scientific and social sciences [1,5,20,25].

To address complex tasks that extend beyond the capabilities of pure large-scale statistical models, novel architectures and specialized model designs are crucial. The recognition that some "Tasks Not Solvable By Scale" [16] necessitates more sophisticated approaches, moving beyond merely increasing model parameters. For instance, the ability of LLMs to utilize "tool manipulation" expands their problem-solving capacity to domains not solely reliant on textual information, such as numerical calculations [17]. Future research emphasizes exploring internal reasoning structures for scientific discovery, potentially involving interdisciplinary efforts with philosophy of science [19]. Integrating domain-specific knowledge graphs or symbolic reasoning within LLM architectures, alongside the development of efficient and adaptive models like MemOS for evolving LLMs [29], is key to unlocking broader applicability and deeper scientific understanding [12,26]. This evolution from purely statistical models towards incorporating structured knowledge and advanced reasoning is vital for tackling intrinsically complex scientific problems.

The ultimate vision for Sci-LLMs is to achieve "general scientific intelligence" (GSI), an AI capable of operating across diverse scientific disciplines with comprehensive understanding and autonomous discovery capabilities [5,18,29]. This ambition resonates with the broader "rethinking of the possibility of Artificial General Intelligence (AGI)" [28,30], with some speculative assertions positioning models like GPT-4 as early versions of AGI [15]. GSI implies a shift from task-specific performance to more comprehensive and intelligent scientific reasoning. Agentic AI systems, such as AutoDS for autonomous scientific discovery and Universal Deep Research, exemplify this trajectory by unifying various domain-specific applications into more general intelligent systems [5,29]. The development of multimodal reasoning capabilities, as demonstrated by the Multimodal-CoT model's human-surpassing performance on the ScienceQA benchmark, signifies a critical step towards integrating knowledge across disparate scientific fields [32]. Concepts like "Audio General Intelligence" (e.g., Audio Flamingo 3) and "AI Virtual Cells" for life sciences further extend this vision to multimodal and multi-scale scientific understanding, aiming to "deconstruct the extreme complexity of life's language" [8,29]. A foundational aspect of GSI is the ability of LLMs to form internal world models from observational data, a prerequisite for sophisticated scientific reasoning and discovery [24].

A significant pathway towards GSI involves the development of a 'Universal Scientific Knowledge Graph (USKG) Construction and Querying with LLM Agents' [5]. In this paradigm, LLM-powered agents would autonomously construct, maintain, and query a dynamic, multimodal scientific knowledge graph. These agents would not only integrate information but also actively identify inconsistencies or gaps, leading to the generation of novel hypotheses [5,23]. Such closed-loop systems would empower LLMs to move beyond passive knowledge processing, enabling them to plan, execute, reflect, and iterate on scientific research, becoming true partners in accelerating discovery [18].

Beyond research, Sci-LLMs hold profound potential for 'LLM-Assisted Real-Time Scientific Education and Democratization' [5]. By leveraging LLMs to create interactive, personalized learning platforms and AI research assistants, knowledge and resource gaps can be bridged, particularly for underserved communities [5]. This aligns with the transformative impact LLMs are having in education, healthcare, and other sectors, as exemplified by their potential to enhance medical education and reduce medical burdens [21,25].

As Sci-LLMs advance towards GSI, the paramount importance of their super-alignment with human values becomes critical [1,30]. Ensuring that advanced AI in science serves humanity's best interests necessitates the establishment of robust ethical frameworks and dedicated research into responsible AI development [1,30]. This involves proactive consideration of potential societal impacts, bias mitigation, and human-AI collaboration frameworks that integrate seamlessly with human workflows while highlighting potential issues [19]. The ongoing discussion about AGI highlights the need to embed human values from the outset to guide the development of increasingly capable AI systems.
### 13.7 Novel Benchmarks for Scientific Process Intelligence
The evolving landscape of Large Language Models (LLMs) in scientific research necessitates a fundamental shift in evaluation paradigms, moving from static, output-centric assessments to more dynamic, process- and discovery-oriented evaluations. This transition, aligned with the emphasis on `10.1 Evaluation Frameworks and Evolving Paradigms`, underscores the crucial need for novel benchmarks specifically designed to measure 'Scientific Process Intelligence' [5,18]. Current evaluation methodologies frequently "fall short by relying on simplified, static scenarios" for complex scientific tasks [29], exhibiting "Brittle Evaluations" and an over-reliance on "Static, Human-Written Ground Truth" [16]. Furthermore, existing benchmarks like MMLU and BIG-bench, along with model-as-judge evaluations, suffer from issues such as unreliable generation assessment and inherent biases, often failing to capture the complexity of scientific inquiry [28]. The absence of comprehensive standardization and the challenge of interpretability further exacerbate these limitations [20].

To address these deficiencies and foster robust and reliable evaluation (`11.5 Robust and Reliable Evaluation`), two primary categories of novel benchmarks are proposed: interactive virtual lab environments and 'blind peer review' benchmarks.

**Interactive Virtual Lab Environments for Process Evaluation:**
Interactive virtual lab environments represent a significant advancement, where LLM agents' performance is evaluated on the *efficiency*, *validity*, and *novelty* of their entire scientific process. This approach moves beyond merely assessing final outputs to scrutinizing the intermediate steps, reasoning chains, and experimental designs. Examples of existing efforts that foreshadow this direction include SDBench and MAI-DxO, which evaluate "realistic, cost-aware clinical reasoning" by assessing the problem-solving process rather than just static answers [29]. Similarly, CRMarena-Pro advances evaluation by focusing on complex, multi-turn, sequential, and interactive tasks that mirror real-world scientific processes [29]. The OMEGA math benchmark also contributes by probing "reasoning limits" and generalization in mathematical reasoning, a core component of scientific intelligence [29].

Further supporting this shift, frameworks like MatSci-LLMs outline an evaluation process that includes "hypothesis generation followed by hypothesis testing" [7], indicating a focus on the entire discovery pipeline. The success of systems like Alpha-Geometry in solving complex math Olympiad geometry problems and FunSearch in combinatorial optimization demonstrates the potential for LLMs to engage in sophisticated scientific reasoning and problem-solving, requiring benchmarks that can assess the novelty and validity of their contributions [31]. The Multimodal-CoT framework, by generating "intermediate reasoning chains as the rationale to infer the answer," highlights the importance of evaluating the validity and effectiveness of an LLM's generated rationales, thereby assessing its "scientific process intelligence" [32]. Techniques like probing and intervention further aim to evaluate an LLM's internal representations and causal understanding, moving towards assessing an "interpretable world model" rather than just surface-level performance [24]. The LLM4SD framework, which "mimic key scientific discovery skills" and provides interpretable rules, underscores the need for metrics that evaluate the clarity, novelty, and scientific validity of AI-generated explanations and discovered patterns [3,22]. Ultimately, these interactive environments would require "real-world validation and application," particularly in domains like chemical experiments and clinical trials, moving beyond purely computational metrics to assess practical impact and verifiability [23,26].

**Blind Peer Review Benchmarks for Scientific Validation:**
Complementing virtual lab environments, 'blind peer review' benchmarks leverage human expertise to evaluate LLM-generated hypotheses or experimental designs for plausibility and novelty, mimicking real-world scientific validation [5,18]. This approach directly addresses critical limitations, such as the finding from the ChemCrow study where GPT-4 as an evaluator "failed to distinguish between correct and incorrect answers generated by other LLM outputs," highlighting the inadequacy of LLMs for self-assessment in complex scientific contexts [5]. The necessity for human expert review and rewriting of AI-generated scientific articles further emphasizes the current gap in automated evaluation of scientific rigor, factual accuracy, and novelty [5].

These benchmarks would explicitly incorporate "domain expert verification" to assess the reliability and scientific validity of LLM-generated rules, especially to identify and mitigate "noisy or redundant" outputs [22]. Such human involvement is essential for evaluating the scientific plausibility and utility of generated hypotheses, moving beyond computational metrics to include expert judgment [22]. The development of "public benchmarking datasets and fair evaluation metrics" [21], coupled with "continuous feedback and interaction with human experts from multiple dimensions" [21], is vital for creating dynamic, process-oriented benchmarks. Furthermore, comprehensive evaluation frameworks are needed for peer review, encompassing technical capabilities (e.g., language understanding, citation analysis), human-AI collaboration metrics, governance aspects, and bias assessment across disciplines and languages [19]. The challenges encountered with existing "specialized benchmarks" for hypothesis generation and experiment planning, particularly experimental verification difficulty and issues with accuracy and coherence, underscore the need for these more advanced, human-augmented evaluation methodologies [13].

In summary, the transition towards evaluating scientific process intelligence necessitates a multifaceted approach. While existing benchmarks offer foundational elements [13], the proposed interactive virtual lab environments and blind peer review benchmarks provide a robust framework for assessing the efficiency, validity, and novelty of LLM contributions to scientific discovery. This shift from static output evaluation to dynamic, process-oriented assessment, critically incorporating human expert judgment, is paramount for truly measuring and fostering general scientific intelligence in LLMs [18]. The future direction involves developing novel evaluation tools and benchmarks for Sci-LLMs [1] that can automatically collect accurate and well-structured data for scientific discovery, addressing the current reliance on small-scale, expert-dependent benchmarks [19].

## References

[1] 浙大团队发布75页科学语言大模型综述 [https://hub.baai.ac.cn/view/34868](https://hub.baai.ac.cn/view/34868) 

[2] LLMs赋能粮食生产：机遇与风险并存 [https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1326153/abstract](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1326153/abstract) 

[3] AI模拟科学家：LLM4SD赋能分子特性预测，登Nature子刊 [https://baijiahao.baidu.com/s?id=1825110412402370158&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1825110412402370158&wfr=spider&for=pc) 

[4] 大型语言模型（LLM）详解：AI理解和生成人类语言的深度学习模型 [https://www.investopedia.com/large-language-model-7563532](https://www.investopedia.com/large-language-model-7563532) 

[5] 大语言模型在科学研究中的N种可能性 [https://www.thepaper.cn/newsDetail_forward_22981693](https://www.thepaper.cn/newsDetail_forward_22981693) 

[6] LLMs在科学发现中的应用：从工具到自主科学家 [https://github.com/WEITU-AI/Awesome-LLM-Scientific-Discovery](https://github.com/WEITU-AI/Awesome-LLM-Scientific-Discovery) 

[7] 材料科学大模型：现状、挑战与未来 [https://www.nature.com/articles/s42256-025-01058-y](https://www.nature.com/articles/s42256-025-01058-y) 

[8] AI驱动生命语言：从分子到细胞的科学革命 [https://c.m.163.com/news/a/JSIN1MTV0511D05M.html](https://c.m.163.com/news/a/JSIN1MTV0511D05M.html) 

[9] 大模型综述V13更新：内容大幅扩充，涵盖最新研究进展 [https://ai.ruc.edu.cn/research/science/20231131002.html](https://ai.ruc.edu.cn/research/science/20231131002.html) 

[10] 大型语言模型：技术、历史与挑战 [https://www.britannica.com/topic/large-language-model](https://www.britannica.com/topic/large-language-model) 

[11] 大型语言模型赋能农业智能化：技术、应用与展望 [https://www.sciencedirect.com/science/article/pii/S2589721725000613](https://www.sciencedirect.com/science/article/pii/S2589721725000613) 

[12] 大模型架构演进：从Transformer到效率新范式 [https://baijiahao.baidu.com/s?id=1843051548750552139&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1843051548750552139&wfr=spider&for=pc) 

[13] LLM4SR: 大型语言模型在科学研究中的应用综述 [https://download.csdn.net/blog/column/12000999/146477503](https://download.csdn.net/blog/column/12000999/146477503) 

[14] LLMs深度解析：《A Survey of Large Language Models》看挑战与演进 [https://developer.baidu.com/article/details/3263938](https://developer.baidu.com/article/details/3263938) 

[15] 2025年27款顶尖大语言模型：回顾与展望 [http://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models](http://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models) 

[16] 大语言模型的挑战与应用概览 [https://baijiahao.baidu.com/s?id=1772382828882635403&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1772382828882635403&wfr=spider&for=pc) 

[17] 大型语言模型 (LLM) 调研：进展、技术与挑战 [http://kexue.love/index.php/archives/284/](http://kexue.love/index.php/archives/284/) 

[18] 科学大语言模型综述：数据为基石，代理为前沿 [https://blog.csdn.net/qq_27881833/article/details/151334010](https://blog.csdn.net/qq_27881833/article/details/151334010) 

[19] LLM4SR：大语言模型在科学研究中的应用综述 [https://www.i-newcar.com/index.php?m=home&c=View&a=index&aid=3668](https://www.i-newcar.com/index.php?m=home&c=View&a=index&aid=3668) 

[20] 大型语言模型（LLMs）：全面综述、技术框架与未来挑战 [https://link.springer.com/article/10.1007/s10462-024-10888-y](https://link.springer.com/article/10.1007/s10462-024-10888-y) 

[21] ChatGPT在中国国家医学考试中的表现：2017-2021年五年评估 [https://link.springer.com/10.1186/s12909-024-05125-7](https://link.springer.com/10.1186/s12909-024-05125-7) 

[22] LLM4SD：利用大语言模型驱动分子属性预测与科学发现 [https://blog.csdn.net/WRyaoyao/article/details/147168408](https://blog.csdn.net/WRyaoyao/article/details/147168408) 

[23] 基于人机协作的多智能体科学假设生成 [https://crad.ict.ac.cn/article/doi/10.7544/issn1000-1239.202440552?viewType=HTML](https://crad.ict.ac.cn/article/doi/10.7544/issn1000-1239.202440552?viewType=HTML) 

[24] LLM 学习世界模型还是表面统计？Othello-GPT 的探究 [https://thegradient.pub/othello/](https://thegradient.pub/othello/) 

[25] LLMs in Education: Challenges and Solutions [https://link.springer.com/article/10.1007/s43621-025-00815-8](https://link.springer.com/article/10.1007/s43621-025-00815-8) 

[26] 大模型在药物发现与研发中的应用综述 [https://blog.csdn.net/bagell/article/details/145704377](https://blog.csdn.net/bagell/article/details/145704377) 

[27] LLMs的工业应用：现状、挑战与未来 [https://link.springer.com/article/10.1038/s41598-025-98483-1](https://link.springer.com/article/10.1038/s41598-025-98483-1) 

[28] 大型语言模型（LLMs）综述 [https://blog.csdn.net/2401_85343303/article/details/144141800](https://blog.csdn.net/2401_85343303/article/details/144141800) 

[29] LLM 领域最新进展：模型、工具与研究动态 [https://www.marktechpost.com/category/technology/artificial-intelligence/large-language-model/?amp](https://www.marktechpost.com/category/technology/artificial-intelligence/large-language-model/?amp) 

[30] LLMs：《A Survey of Large Language Models》综述解读(一) [https://blog.csdn.net/qq_40647372/article/details/134869583](https://blog.csdn.net/qq_40647372/article/details/134869583) 

[31] 科学大语言模型及其在科学发现中的应用：全面综述 [https://blog.csdn.net/weixin_44362044/article/details/141753994](https://blog.csdn.net/weixin_44362044/article/details/141753994) 

[32] 每日学术速递：BioGPT、多模态思维链及阿尔茨海默病语义连贯性 [https://blog.csdn.net/muye_IT/article/details/128888401](https://blog.csdn.net/muye_IT/article/details/128888401) 

[33] LLM大型语言模型综述论文章节总结 [https://blog.csdn.net/weixin_45320238/article/details/143023925](https://blog.csdn.net/weixin_45320238/article/details/143023925) 

[34] LLMs in Science: Risks and Responsible Use [https://www.nature.com/articles/s42254-023-00581-4](https://www.nature.com/articles/s42254-023-00581-4) 

[35] 科学大语言模型：生物和化学领域综述 [https://download.csdn.net/blog/column/12000999/136233546](https://download.csdn.net/blog/column/12000999/136233546) 

