# A Survey on Multimodal Large Language Models

# 0. A Survey on Multimodal Large Language Models

## 1. Introduction
Multimodal Large Language Models (MLLMs) represent a significant evolution in artificial intelligence, extending the capabilities of traditional Large Language Models (LLMs) and Vision-Language Models (VLMs) by enabling comprehensive processing and understanding across diverse data modalities such as text, images, audio, video, and 3D point clouds [12,24,35]. While unimodal LLMs have demonstrated remarkable reasoning and generation capabilities within textual domains [13], they inherently lack the ability to comprehend and interact with non-textual information [2,19]. Similarly, VLMs, though capable of handling text and image inputs, often exhibit weaker reasoning capabilities compared to full-fledged MLLMs [17]. MLLMs overcome these limitations by integrating the robust reasoning prowess of LLMs with multimodal information processing, thereby fostering a more holistic AI system that mirrors human perception and interaction with the world [21,36].

The development of MLLMs is driven by a profound rationale: to advance artificial intelligence towards Artificial General Intelligence (AGI) [25,37]. 

**Core Advantages of Multimodal Large Language Models (MLLMs)**

| Advantage                     | Description                                                                                                                                                                                                                            |
| :----------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Improved Cross-Modal Understanding | Enables richer and more comprehensive intelligent interactions by integrating diverse information types (e.g., text, image, audio, video, 3D).                                                                                      |
| Enhanced Contextual Awareness  | Allows understanding of nuanced meanings in images and complex tasks like OCR-free mathematical reasoning.                                                                                                                               |
| Broad Application Scenarios    | Spans general multimodal tasks to specialized domains such as autonomous driving (perception, motion planning, human-vehicle interaction).                                                                                             |
| Enhanced Human-Computer Interaction | Provides more intuitive and user-friendly interfaces, mirroring human perception and interaction with the world.                                                                                                                    |
| Expanded Task-Solving Capabilities | Unlocks emergent abilities not seen in traditional multimodal approaches, like generating stories from images or writing website code from visual inputs.                                                                         |

This pursuit is underpinned by several core advantages inherent to MLLMs. Firstly, they offer improved cross-modal understanding, allowing for richer and more comprehensive intelligent interactions by integrating diverse information types [35]. Secondly, MLLMs possess enhanced contextual awareness, enabling them to understand nuanced meanings in images and perform complex tasks like OCR-free mathematical reasoning [10,31]. Thirdly, their broad application scenarios span from general multimodal tasks to specialized domains such as autonomous driving, revolutionizing areas like perception, motion planning, and human-vehicle interaction [36]. Fourthly, MLLMs facilitate enhanced human-computer interaction by providing more intuitive and user-friendly interfaces [19,36]. Finally, they offer expanded task-solving capabilities, including emergent abilities previously unseen in traditional multimodal approaches, such as generating stories from images or writing website code from visual inputs [23,31].

The research landscape of MLLMs has witnessed an accelerated evolution, particularly marked by a significant surge in interest and development following the advent of powerful models like GPT-4V [8,10,31]. MLLMs are characterized by their typically large model sizes and novel training paradigms, which include multimodal pre-training and multimodal instruction fine-tuning [8]. Influential models such as GPT-4V, GPT-4o, and Gemini showcase state-of-the-art performance across multiple domains, integrating the reasoning capabilities of LLMs with multimodal information processing [11,17]. This rapid progress is a testament to concerted efforts from both academic institutions and industrial giants aiming to push the boundaries of AI capabilities [31].

This survey aims to provide a comprehensive overview of Multimodal Large Language Models, synthesizing the latest advancements, architectural components, training strategies, and evaluation methodologies [3,12,31]. By systematically organizing the current knowledge, we intend to identify key trends, highlight challenges such as multimodal hallucination and object orientation understanding [10,22], and suggest promising future research directions in this dynamic field. The subsequent sections of this survey will delve into specific topics, beginning with a detailed discussion of MLLM architectures, followed by training strategies, evaluation benchmarks, and current applications. We will also explore emerging areas such as multimodal in-context learning and chain-of-thought reasoning, providing focused insights into the current status and future trajectory of MLLMs [10,25].
## 2. Background and Foundational Concepts

**Historical Stages of Multimodal AI Research**

| Stage Name                | Period        | Characteristics & Focus                                                                                                                                         |
| :------------------------ | :------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Single-modal Stage        | 1980-2000     | Research predominantly focused on processing and analyzing single modalities independently (e.g., computer vision, natural language processing).                      |
| Modality Translation Stage | 2000-2010     | Initial attempts at translating information between modalities (e.g., generating text descriptions from images, speech recognition to text).                          |
| Modality Fusion Stage     | 2010-2020     | Emphasized integration and fusion of information from multiple modalities to achieve comprehensive understanding (e.g., CLIP, sequence-to-sequence models unifying multimodal tasks). |
| Large-scale Model Stage   | 2020-Present | Integration of Large Language Models with multimodal capabilities (MLLMs); ability to receive, reason about, and output multimodal information (e.g., GPT-4V).      |


![Core Concepts and Interaction in Multimodal Large Language Models](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/-I_ayot-PTzG6K0bquFFY_/home/surveygo/data/requests/13515/survey/imgs/Core%20Concepts%20and%20Interaction%20in%20Multimodal%20Large%20Language%20Models.png)

The emergence of Multimodal Large Language Models (MLLMs) represents a significant advancement in artificial intelligence, building upon the foundations of Large Language Models (LLMs) by integrating diverse data modalities. To comprehend MLLMs, it is crucial to establish foundational knowledge concerning LLMs, visual encoders, modality adapters, and the historical trajectory of multimodal research.

Large Language Models (LLMs) serve as the backbone of MLLMs, renowned for their robust capabilities in language understanding and generation [11]. These models, often based on the Transformer architecture, have demonstrated emergent abilities through extensive scaling of data and model sizes, including instruction following, In-Context Learning (ICL), and Chain of Thought (CoT) reasoning [31]. While LLMs excel in processing discrete text and exhibiting impressive zero-shot or few-shot reasoning across various Natural Language Processing (NLP) tasks, they are inherently "blind" to visual information [31]. This limitation underscores the need for integrating non-textual modalities.

To overcome the textual limitation of LLMs, MLLMs incorporate visual encoders, which are specialized components designed to process image and video data. These encoders compress raw visual data into precise, informative representations [17]. Common architectures for visual encoding include Vision Transformers (ViT) and pre-trained models like CLIP, which effectively align visual and textual information in a shared representation space [11,17]. These multimodal encoders handle non-text signals, such as visual and auditory data, converting them into a format suitable for integration with LLMs [9,35].

A critical component in MLLMs is the modality adapter, also known as the modality interface or connector. Its primary function is to bridge the representational gap between different modalities, ensuring that features from non-textual inputs (e.g., vision) are effectively aligned and translated into a format understandable by the LLM backbone [9,11,17]. This "alignment" ensures compatibility between modality-specific encodings (from encoders like CLIP) and the token-based encoding used by LLMs [12]. Modality adapters can manifest in various architectural forms, including simple linear layers, Multi-Layer Perceptrons (MLPs), or more complex Transformer-based architectures like Q-Former, which facilitate the connection between the visual encoder and the LLM, enabling the LLM to process and reason over multimodal information [11,17].

Multimodality, at its essence, refers to the expression or perception of complex information through multiple channels, leveraging richer perceptual inputs beyond just language, such as visual, auditory, tactile, and gustatory senses, to simulate human-like understanding and expression [29,37]. It encompasses diverse data types, including images, numerical data, text, audio, and complex structures, providing a more holistic representation of information [29]. Multimodal data can be differentiated into two categories: homogeneous modalities, which involve multiple instances of the same data type (e.g., images from two distinct cameras), and heterogeneous modalities, which combine different data types (e.g., an image paired with text) [29].

The development of multimodal research has progressed through distinct historical stages, marked by evolving focuses and technological advancements [2,29]. These stages include:
1.  Single-modal Stage (1980-2000): This period was characterized by research predominantly focused on processing and analyzing single modalities independently, such as computer vision or natural language processing.
2.  Modality Translation Stage (2000-2010): This phase saw initial attempts at translating information between modalities. For example, generating text descriptions from images or converting speech recognition to text. This laid the groundwork for cross-modal understanding.
3.  Modality Fusion Stage (2010-2020): With the rise of deep learning, this stage emphasized the integration and fusion of information from multiple modalities to achieve a more comprehensive understanding. Representative efforts include unified representation learning (e.g., CLIP, which projects visual and textual information into a unified representation space [31]) and sequence-to-sequence models unifying multimodal tasks (e.g., OFA [31]).
4.  Large-scale Model Stage (2020-Present): The current era is defined by the integration of large language models with multimodal capabilities, leading to the emergence of MLLMs. These models, built on LLMs with billions of parameters, can receive, reason about, and output multimodal information, demonstrating novel capabilities such as generating website code from images or understanding memes with mathematical reasoning without relying on Optical Character Recognition (OCR) [31]. This stage leverages the complementarity between LLMs' reasoning prowess and Large Vision Models (LVMs)' visual understanding capabilities, synthesizing them into powerful, general-purpose multimodal agents [31].
## 3. Architectures of Multimodal Large Language Models

![Core Architectural Components of Multimodal Large Language Models](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/1Uo71uXZ1CzNlG00g896N_/home/surveygo/data/requests/13515/survey/imgs/Core%20Architectural%20Components%20of%20Multimodal%20Large%20Language%20Models.png)

Multimodal Large Language Models (MLLMs) are fundamentally designed to integrate and process information from diverse modalities, primarily relying on a core architectural framework comprising three essential components: modality encoders, a Large Language Model (LLM) backbone, and modality interfaces (or connectors) [6,8,11,12,17,25,27,28,31]. For MLLMs that support generation beyond text, an additional generator module is typically included [8,10,31].

The **modality encoders** serve as the initial processing layer, transforming raw data from modalities such as images, audio, or 3D point clouds into compact, semantically meaningful feature representations that are aligned for subsequent integration with the language model [12,28,31]. The choice and pre-training objectives of these encoders significantly influence the MLLM's ability to process and understand specific modal information, with pre-trained visual encoders (e.g., various ViT-based architectures like CLIP, EVA-CLIP, DINOv2, and InternViT-6B) being common for visual data [1,4,11,28,30,31,37]. Key optimization trends for encoders include robust high-resolution support, which is empirically shown to substantially improve performance, achieved either through direct scaling or block segmentation techniques [8,10,29,31]. The type of pre-training objective, such as contrastive versus reconstructive losses, directly impacts whether the MLLM excels at semantic understanding or dense prediction tasks [27].

The **Large Language Model (LLM) backbone** functions as the central "brain" of the MLLM, responsible for integrating diverse information, performing complex reasoning, and generating coherent text outputs [28]. Leveraging pre-trained LLMs, often causal decoder models like those in the GPT series or LLaMA family, endows MLLMs with extensive world knowledge and robust generalization abilities [4,11,31]. The parameter size of the LLM backbone significantly correlates with performance, with larger models generally yielding superior language understanding and reasoning capabilities, including enhanced multilingual support [8,10,28,31]. However, there exists a critical trade-off between model size, performance, and efficiency, leading to ongoing research into smaller, more efficient LLMs for resource-constrained environments, and the adoption of sparse models like Mixture of Experts (MoE) architectures to increase effective parameter counts without proportional computational cost [1,8,10,31].

**Modality interfaces**, also known as connectors or projectors, are critical intermediaries that translate encoded multimodal information into a format compatible with the LLM's input space, effectively bridging the inherent modality gap [6,12,17,31]. Common connector types include simple projection-based methods (e.g., linear layers or MLPs used in LLaVA and its derivatives) and more sophisticated query-based mechanisms (e.g., Q-Former in BLIP-2 and Flamingo, or C-Abstractor in MM1) [1,4,6,11,19,25,27,31]. These interfaces largely fall under token-level fusion, where features are converted into tokens and concatenated with text tokens for the LLM. While feature-level fusion, integrating modules directly within the LLM (e.g., cross-attention layers in Flamingo), conceptually allows for deeper interactions, empirical evidence suggests that token-level fusion can often achieve superior performance on benchmarks [28,31]. Notably, despite their pivotal role, connectors typically constitute a small fraction of the total MLLM parameters, and their specific design has been found to have comparatively negligible importance on overall performance compared to factors like the number of visual tokens and input image resolution [8,10,12,27,31].

The overall performance of an MLLM is a complex interplay of these architectural choices and their interactions. For instance, the quality of visual embeddings from the encoder has a greater performance impact than optimizing the text side alone [29]. The ongoing evolution of MLLM architectures focuses on optimizing these components, balancing computational efficiency with enhanced multimodal understanding and generation capabilities, moving towards more general-purpose AI assistants [34]. This systematic overview sets the stage for a detailed examination of each component in subsequent sections.
### 3.1 Modality Encoders
Modality encoders serve as the initial processing layer in Multimodal Large Language Models (MLLMs), transforming raw data from various modalities, such as images, audio, and depth, into compact feature representations that are semantically aligned with the textual modality [12,28,31]. This alignment is crucial for the subsequent integration with large language models.

For visual information, a predominant strategy involves leveraging pre-trained visual encoders, often building upon architectures like the Vision Transformer (ViT) [11,37]. Common visual encoders include various iterations of CLIP's visual branch, such as CLIP ViT-L/14 (e.g., in LLaVA-v1.5 and MobileVLM) [1,4], and its derivatives like EVA-CLIP (ViT-G/14), which supports higher resolutions and is employed by models like MiniGPT-4 and TinyGPT-V [1,11,28,31]. Other notable visual encoders include SigLIP (used in Imp-v1 and MiniCPM-V 2.0), DINOv2 (seen in Cobra and LLaVA-Gemma), and InternViT-6B (InternVL-1.5) [1,30]. While ViT-based models are prevalent, some architectures, like Osprey, utilize convolutional encoders such as ConvNext-L for higher resolution processing and multi-level feature extraction [28,31]. Furthermore, approaches like Fuyu-8b adopt an encoder-free architecture where image patches are directly projected to the LLM, enabling inherent flexibility in handling diverse image resolutions [31].

The performance characteristics of MLLMs are heavily influenced by the pre-training objectives and datasets used for these encoders. Pre-trained encoders often rely on methods that learn vision backbones, including supervised learning, image-text contrastive learning (e.g., CLIP), and image-only self-supervised representation learning, or combinations thereof [34]. Contrastive losses, when trained on large-scale image-text datasets such as DFN-5B, are highly effective in achieving a strong semantic understanding of image data [27]. In contrast, reconstructive losses are considered for tasks requiring dense prediction, as they explicitly capture all parts of an image, addressing limitations of CLIP-style models in such scenarios [27]. For instance, MM1's final model leverages a ViT-H with a 378×378 resolution, pre-trained with a CLIP objective on DFN-5B [27].

For modalities beyond vision, encoders are similarly designed to convert raw signals into structured representations. For audio, the CLAP model is commonly utilized, as seen in models like Pengi [28,31]. More ambitiously, models like ImageBind-LLM use the ImageBind encoder, which is capable of unifying encodings across a broad spectrum of modalities including images, text, audio, depth, thermal imaging, and IMU data, demonstrating a unified encoding approach [28,31].



![Strategies for Handling High-Resolution Images in MLLMs](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/6FjuN1Qa-nsJuNMesJMHJ_/home/surveygo/data/requests/13515/survey/imgs/Strategies%20for%20Handling%20High-Resolution%20Images%20in%20MLLMs.png)

A significant factor impacting MLLM performance is the handling of high-resolution images. Empirical evidence suggests that higher input resolution can substantially improve performance, with 448×448 resolutions outperforming 224×224, and some models supporting up to 1152 resolution [1,28,31]. Two primary approaches address high-resolution inputs:
1.  Direct Scaling: This method involves directly inputting higher resolution images into the encoder. This often necessitates either training the visual encoder to adapt to the higher resolution, as demonstrated by Qwen-VL, or replacing it with a higher-resolution pre-trained encoder [8,10,31]. The advantage lies in maintaining global context, but it demands more computational resources and specialized encoders.
2.  Block Segmentation: In this approach, high-resolution images are divided into multiple smaller blocks or sub-images. These blocks can then be fed individually into lower-resolution encoders, thereby indirectly increasing the effective input resolution [8,10,31]. For example, models like Monkey and SPHINX split large images into smaller blocks, often sending these sub-images along with a down-sampled high-resolution image to the encoder to capture both local details and global features simultaneously [31]. This method is more memory-efficient and allows reuse of existing lower-resolution encoders, but it might introduce challenges in maintaining coherence across blocks or integrating global context. Image tokenization, through methods like region-based, grid-based, and block-based approaches, further contributes to efficient image processing and representation [29].

The choice of encoder profoundly impacts an MLLM's ability to process and understand specific modal information. More complex visual embedding layers resulting from optimizing the visual feature side tend to yield greater performance gains than optimizing the text side alone [29]. The type of pre-training objective (e.g., contrastive vs. reconstructive) directly influences whether the MLLM excels at semantic understanding or dense prediction tasks [27]. Encoder-free architectures, such as Fuyu-8b, offer inherent flexibility in handling varied image resolutions, which can be critical for real-world applications [31].

In selecting an appropriate encoder for a given multimodal task, key considerations include the input resolution requirements of the task, the architecture and parameter size of the encoder, and the composition of its pre-training corpus [31]. While higher resolution consistently proves to be a significant performance driver, the parameter size and the specific training data composition are generally considered less critical than effective resolution handling [31]. The overall goal is to select an encoder that can efficiently and effectively transform raw multimodal data into rich, semantically meaningful representations suitable for subsequent processing by the language model.
### 3.2 LLM Backbones
The choice of the Large Language Model (LLM) backbone is foundational to the capabilities of Multimodal Large Language Models (MLLMs), serving as the "brain" that integrates diverse information, facilitates complex reasoning, and generates coherent text outputs [28]. Pre-trained LLMs, by virtue of their training on extensive network corpora, endow MLLMs with rich world knowledge, robust generalization abilities, and sophisticated reasoning capacities, obviating the need for training from scratch [31]. Most commonly utilized LLMs in MLLMs typically fall into the causal decoder category, akin to GPT-3 [31].

A diverse array of LLM backbones has been adopted in MLLM architectures. The LLaMA family, including its derivatives like Alpaca and Vicuna, is widely popular due to the availability of its weights, training on public datasets, and offering various sizes suited for different applications [11]. Specifically, Vicuna v1.5 and LLaMA-2 have been foundational models for projects like LLaVA [4]. Early MLLMs, such as BLIP-2 and InstructBLIP, leveraged the FlanT5 series, an encoder-decoder model pre-trained for multiple tasks [11,31]. Other notable LLM backbones include OPT, Magneto, MPT, GPT-3, ChatGPT, GPT-4, PaLM, BLOOM, Chinchilla, and GLM [11,25]. More recent and commercially prominent examples used in MLLMs encompass GPT-4V/4o, Claude3.5 Sonnet, Gemini 1.5Pro, Qwen2.5-VL-72B, and 腾讯混元Vision [35]. While LLaMA and LLaMA-2 are primarily decoder-only models, the Qwen series and InternLM series are also frequently chosen as backbones, with Qwen-VL notably using Qwen-7B [6,8,10].



![LLM Backbone Considerations: Performance vs. Efficiency Trade-offs](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/DLy-AFyud1fuH3jb9N718_/home/surveygo/data/requests/13515/survey/imgs/LLM%20Backbone%20Considerations%3A%20Performance%20vs.%20Efficiency%20Trade-offs.png)

The parameter size of the LLM backbone significantly impacts the MLLM's performance, influencing its language understanding, text generation quality, and reasoning capabilities. Scaling up LLM parameters generally leads to substantial performance gains, analogous to increasing input resolution [31]. For instance, experimental results with LLaVA-NeXT demonstrated that enlarging the LLM from 7 billion to 13 billion parameters, and further to 34 billion parameters, yielded significant improvements across various benchmarks [8,10,31]. A particularly compelling benefit of larger LLMs is their enhanced multilingual support; a 34-billion-parameter model, even when trained exclusively on English multimodal data, exhibited zero-shot Chinese language capabilities [8,10,28,31].

However, the choice between larger, more powerful LLMs and smaller, more efficient LLMs involves a critical trade-off between performance and efficiency. While larger models offer superior capabilities, smaller LLMs are preferred for deployment in resource-constrained environments, such as mobile devices [31]. Recent research has explored the integration of smaller LLMs into MLLMs to enhance efficiency. Examples include MobileLLaMA (2.7B parameters in MobileVLM and MobileVLM-v2-3B), Phi-2 (2.7B parameters in LLaVA-Phi, Imp-v1, TinyLLaVA, Bunny, MoE-LLaVA-3.6B, TinyGPT-V, and ALLaVA-Longer), Mamba-2.8b-Zephyr (2.8B parameters in Cobra), Gemma (2B parameters in Mini-Gemini and LLaVA-Gemma), Qwen (1.8B in Vary-toy and 0.5B in KarmaVLM), TinyLlama (1.1B in SPHINX-Tiny), DeepSeek-LLM (1.3B in DeepSeek-VL), Phi-1.5 (1.3B in moondream2), and Phi-3-Mini-4K (3.8B in Bunny-v1.1-4B) [1]. To bridge this gap, the Mixture of Experts (MoE) architecture has garnered significant attention. MoE models selectively activate parameters, enabling a substantial increase in the total model parameter count without a proportional rise in computational cost, making them more efficient than dense models. MM1 and MoE-LLaVA have provided empirical validation of MoE's superior performance in various benchmarks [8,10,31].

The impact of pre-training data on an LLM's language and domain capabilities is profound. Many popular LLM series, such as the LLaMA family, are predominantly pre-trained on English corpora, which inherently limits their native multilingual support, particularly for languages like Chinese [8,10,31]. In contrast, models like Qwen and InternLM series are designed to be bilingual, offering stronger support for both Chinese and English [8,10,28,31]. For instance, InternVL-Chat-V1.5, which incorporates the "书生·浦语" 20B model, exhibits strong Chinese recognition capabilities [30]. Beyond language, domain adaptation can be achieved through knowledge injection via data fine-tuning or by integrating tool use through API calls, enabling LLMs to process specialized information, such as adapting GPT-3 for document information extraction or fine-tuning GPT-2 for single-cell gene expression data analysis [28,34]. This highlights how the choice of LLM and its underlying pre-training data fundamentally dictates an MLLM's ability to generate coherent and relevant text across different languages and specialized domains.
### 3.3 Modality Interfaces (Connectors)
Modality interfaces, often referred to as connectors or projectors, serve as crucial intermediaries within Multimodal Large Language Models (MLLMs), translating encoded multimodal information into a format compatible with the Language Model (LLM) [12,17]. Their primary function is to effectively align visual and textual features, bridging the inherent modality gap to enable the LLM to process and understand diverse data types [6,31].



**Modality Interface Types and Fusion Mechanisms in MLLMs**

| Aspect              | Projection-Based Connectors                     | Query-Based Connectors                          | Token-Level Fusion (Mechanism)                   | Feature-Level Fusion (Mechanism)                 |
| :------------------ | :---------------------------------------------- | :---------------------------------------------- | :----------------------------------------------- | :----------------------------------------------- |
| Mechanism           | Simple linear layers or MLPs map features directly. | Learnable query tokens extract and compress features. | Converts encoder output features directly into tokens, then concatenates with text tokens. | Integrates additional modules directly within the LLM's architecture. |
| Examples            | LLaVA (MLPs), MobileVLM (LDP).                  | Q-Former (BLIP-2, Flamingo), C-Abstractor (MM1). | Q-Former, MLP-based projections (e.g., LLaVA).   | Flamingo (cross-attention layers), CogVLM (visual expert module within layers). |
| Information Flow    | Directly transforms visual embeddings for LLM consumption. | Provides concise, salient visual summary to the LLM. | Visual information tokenized before LLM processing. | Deeper, finer-grained interactions throughout the LLM's processing pipeline. |
| Typical Performance | Straightforward, can achieve good performance.   | Optimizes information flow, manageable representation. | Empirically superior on some benchmarks (e.g., VQA). | Conceptually richer, but not always empirically better than token-level fusion. |
| Parameter Footprint | Small fraction (~2% of total MLLM parameters). | Small fraction (~2% of total MLLM parameters). | Small compared to encoder or LLM components.       | Can increase LLM complexity and parameter count. |

Several distinct types of learnable interfaces have been developed to achieve this alignment, each with varying mechanisms for integrating modalities. A common approach involves **projection-based connectors**, which typically employ simple linear layers or Multilayer Perceptrons (MLPs) to map visual features directly into the LLM's linguistic or embedding space [6,11,19,31]. For instance, the LLaVA series utilizes one or two linear MLPs to project visual tokens and align feature dimensions with word embeddings [4,31], a strategy also observed in LLaVA-Phi, TinyLLaVA, Bunny, MoE-LLaVA-3.6B, Cobra, Mini-Gemini, DeepSeek-VL, and MiniGPT4-v2 [1,25]. Similarly, MedVInTTE adopts a two-layer MLP as a bridge [19], while MobileVLM employs Linear Decoder Prediction (LDP) and its updated version LDPv2 in MobileVLM-v2-3B [1]. These projection methods are straightforward, aiming to directly transform visual embeddings for LLM consumption.

In contrast, **query-based connectors** like the Q-Former, first introduced in BLIP-2 and subsequently adopted by various works including Flamingo, TinyGPT-V, and QWen-VL, represent another prominent category [11,19,25,31]. These interfaces utilize a set of learnable query tokens to extract and compress relevant visual information from a frozen image encoder, reducing a large number of visual tokens into a smaller, more manageable set of representation vectors [11,31]. This mechanism allows the LLM to receive a concise yet informative visual summary, optimizing the information flow by focusing on the most salient visual features [11]. Other specialized connectors include C-Abstractor used in MM1 and MM1-3B-MoE-Chat, and Perceiver Resampler in MiniCPM-V 2.0 [1,27].

The choice of connector significantly impacts the flow of information between the modality encoder and the LLM. Learnable connectors are designed to process multimodal data end-to-end, effectively translating visual content into a language-compatible format [19,31]. Alternatively, some MLLMs utilize **expert models**, such as image captioning tools or OCR systems, to convert visual inputs into text before feeding them to the LLM [28,31]. While this approach offers flexibility by leveraging pre-existing models without extensive training, it often suffers from poor flexibility and potential information loss, particularly concerning detailed spatio-temporal relationships in video data [19,31].

In terms of fusion mechanisms, connectors can be broadly categorized into token-level and feature-level fusion. **Token-level fusion** involves converting encoder output features directly into tokens that are then concatenated with text tokens and fed into the LLM [28,31]. This includes methods like Q-Former and MLP-based projections, where visual information is tokenized before being processed by the LLM. **Feature-level fusion**, conversely, integrates additional modules directly within the LLM's architecture, enabling deeper, finer-grained interactions between text and visual features [28,31]. For example, Flamingo inserts cross-attention layers between the frozen Transformer layers of the LLM to enhance language features with external visual cues [31]. Similarly, CogVLM integrates a visual expert module within each Transformer layer, facilitating dual interaction and fusion between visual and language features [31]. While feature-level fusion conceptually allows for richer, more interactive integration of modalities throughout the LLM's processing pipeline, empirical studies, such as that by Zeng et al., have shown that token-level fusion variants can sometimes achieve superior performance on tasks like VQA benchmarks [31].

From a computational perspective, modality interfaces typically represent a small fraction of the overall MLLM architecture, often accounting for only about 2% of the total parameters, significantly less than the encoder or LLM components [12,31]. This minimal parameter footprint contributes to their scalability and efficient integration. Models like LLaMA-Adapter and LaVIN further explore adaptation methods that minimize parameter requirements [23]. Despite the diversity in connector architectures, research indicates that the specific design of the visual-language connector often has a comparatively minor effect on overall MLLM performance, particularly in token-level fusion contexts. For instance, ablations in MM1 revealed that factors such as the number of visual tokens and the input image resolution are more critical determinants of performance than the specific connector architecture itself [8,10,27,31].

Overall, the design and implementation of modality interfaces play a pivotal role in ensuring that visual information is seamlessly and effectively incorporated into the language modeling process, with each connector type offering its unique advantages and limitations.
## 4. Training Strategies and Data

![Multi-stage MLLM Training Paradigm](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/Zz6obSm32US-mwMlRVUr5_/home/surveygo/data/requests/13515/survey/imgs/Multi-stage%20MLLM%20Training%20Paradigm.png)

The development and performance of Multimodal Large Language Models (MLLMs) are intrinsically linked to the strategic choices made regarding their training data and methodologies [11,13,19,27,28,31]. This section systematically analyzes the spectrum of training strategies, comparing alignment pre-training, multimodal instruction tuning (M-IT), and alignment tuning, while also discussing the critical role of data collection and optimization techniques.

The foundation of MLLM training lies in the **Data Sources and Preprocessing**. Datasets for MLLMs vary significantly in modality, scale, and quality, serving distinct purposes from broad pre-training to task-specific fine-tuning [31]. Pre-training typically leverages vast quantities of text-paired data, such as web-scale image-text datasets like LAION-5B and COYO-700M, alongside interleaved formats and extensive text-only corpora, aiming to imbue models with extensive world knowledge and establish foundational cross-modal understanding [11,27,31]. Beyond common modalities, specialized data representations like LMDX for document layout and Cell2Sentence for gene expression profiles demonstrate the breadth of data integration in MLLMs [34]. Data collection methodologies for MLLMs include **benchmark adaptation**, which converts existing datasets into instruction-following formats (e.g., adapting VQA for M-IT); **self-instruction**, where LLMs generate new instruction data from limited seed samples (e.g., LLaVA using GPT-4 to create multimodal instruction-following data, or generating egocentric instruction data for orientation understanding); and **hybrid composition**, which combines single-modal and multimodal data to enhance dialogue fluency [4,19,22,23,28]. A critical trade-off exists between using large, noisy datasets for broad coverage and smaller, high-quality, often LLM-generated datasets (e.g., ShareGPT4V via GPT-4V) for finer-grained alignment and improved performance [6,8,10,31]. While high-quality data generated by advanced MLLMs like GPT-4V can significantly improve alignment, its generation cost and limited scalability pose practical challenges [8,10]. Effective data preprocessing, including cleaning, normalization, and augmentation, is paramount for model robustness and generalization [6].

The **Training Methodologies** for MLLMs typically follow a multi-stage paradigm, most notably a three-stage process: pre-training, instruction tuning, and alignment tuning [10,28,31].
1.  **Pre-training**: This initial stage aims to establish a foundational understanding by mapping features from different modalities into a unified semantic space, enabling the model to absorb general knowledge from large-scale data [28]. Common techniques include freezing pre-trained visual encoders and LLM components while training new interface layers to prevent catastrophic forgetting and enhance efficiency [6]. Some models, like MM1, opt for unfrozen parameter training to maximize learning capacity [27]. Self-Supervised Learning (SSL) is fundamental, enabling models to learn from unlabeled data by predicting masked information [21]. Pre-training objectives include Image-Text Contrast (ITC), Masked Language Modeling (MLM), Masked Visual Modeling (MVM), and Text-Image Matching (TM) [29].
2.  **Multimodal Instruction Tuning (M-IT)**: This stage is crucial for transforming MLLMs into general-purpose multimodal chatbots capable of understanding and executing diverse user instructions and generalizing to unseen tasks [19,28]. M-IT enhances the In-Context Learning (ICL) capabilities of LLMs, improving analogical reasoning from few-shot examples [25]. Training typically involves optimizing an autoregressive objective on instruction-input-response triples, such as:
    $$ P(A|I,M;\theta) = \prod_{i=1}^N P(A_i|A_{<i}, I, M;\theta) $$
    where \(N\) is the length of the ground truth answer, and the loss function \(L_{it}\) supervises tuning on response tokens:
    $$ L_{it} = - \mathbb{E}_{(x,y) \sim D_{IT}} \sum_{t=1}^{|y|} \log p_{\theta}(y_t|y_{<t}, x) $$
    Here, \(D_{IT}\) is the instruction tuning dataset, \(x\) is the input, and \(y\) is the response [19,25]. The quality and diversity of instructions are often more critical than sheer quantity, with mixed text dialogue data acting as regularization to preserve core LLM capabilities [10,28]. Continual instruction tuning is also an active area of research [38].
3.  **Alignment Tuning**: The final stage aims to reduce undesirable model behaviors, such as hallucinations, and align generated content with human preferences and values [10,28]. This often relies on human preference data, traditionally collected through costly manual annotation, although automated methods like Silkie (using GPT-4V) are emerging to mitigate this expense [10]. Key techniques include Reinforcement Learning from Human Feedback (RLHF) with reward models and Proximal Policy Optimization (PPO), or more direct methods like Direct Preference Optimization (DPO), which can directly optimize policy based on preferences without an explicit reward model, as seen in D3PO [4,20,28].

Beyond the prevalent three-stage approach, variations in training methodologies exist, including **two-stage** training (e.g., LLaVA, MM1), typically involving pre-training followed by instruction tuning, and even **single-stage** or **end-to-end** training [4,6,11,27,34]. These variations present trade-offs in terms of efficiency, computational resources, and performance, often balancing the benefits of modularity (preventing catastrophic forgetting, leveraging pre-trained components) with the potential for holistic optimization.

**Loss Functions and Optimization** are integral to guiding MLLM training. Loss functions quantify the disparity between model predictions and target outputs, steering the learning process for objectives ranging from next-token prediction to image-text alignment and human preference integration. The integration of human preference data, crucial for alignment tuning, has driven innovations in loss function design and data collection, pushing towards automated and more economical alternatives to manual annotation [8,10,20]. Efficient optimization algorithms are essential for training MLLMs with their massive parameter counts. Precise tuning of hyperparameters like the learning rate (\(\eta\)) and weight decay (\(\lambda\)) is critical. Empirical studies suggest optimal peak learning rates can scale with the number of non-embedding parameters (\(N\)) as:
$$ \eta = 10^{-3} \sqrt{N} $$
with weight decay typically proportional to the learning rate (\(\lambda = 0.1\eta\)) [27]. Such meticulously calibrated optimization schedules are vital for achieving stable convergence and high performance in these complex models [13]. These sophisticated training and data strategies collectively aim to produce MLLMs that are not only capable of processing and generating multimodal content but also proficient in understanding and adhering to diverse instructions and human preferences.
### 4.1 Data Sources and Preprocessing

**Key Data Collection Methods for MLLMs**

| Method              | Description                                                                                                                                      | Advantages                                                                      | Disadvantages                                                                                             |
| :------------------ | :----------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------ | :-------------------------------------------------------------------------------------------------------- |
| Benchmark Adaptation | Converts existing benchmark datasets (e.g., VQA, OCR-VQA) into instruction-following formats.                                                  | Leverages existing, structured, and often curated data; straightforward to implement. | Answers in traditional datasets may be too concise; might not reflect real-world multi-turn dialogues.      |
| Self-Instruction    | Guides LLMs to generate new instruction-following data from small sets of manually annotated "seed" samples (e.g., LLaVA using GPT-4).            | Generates diverse, complex, multi-turn dialogue data; higher quality and finer-grained detail. | Can be costly and time-consuming (e.g., reliance on paid APIs like GPT-4V); smaller scale compared to web-scraped data. |
| Hybrid Composition  | Combines single-modal (text-only) and multimodal data to enhance dialogue fluency and instruction-following capabilities.                        | Enhances dialogue fluency and instruction adherence; regularizes LLM core capabilities. | Requires careful balancing and integration of data types to prevent negative interference.                  |

The development of Multimodal Large Language Models (MLLMs) critically depends on the quantity, diversity, and quality of their training data. Datasets utilized for MLLM training can be broadly categorized based on their contained modalities and intended applications, spanning from pre-training to fine-tuning for specific tasks.

For pre-training, the primary objective is to integrate different modalities and imbue the model with extensive world knowledge [31]. This stage typically relies on large volumes of text-paired data, such as caption datasets that describe visual, audio, or video content in natural language [31]. Common image-text datasets include LAION-5B, a vast web-scale dataset with over 5.85 billion image-text pairs (including a 2 billion English subset), and COYO-700M, which contains 747 million image-text pairs extracted from CommonCrawl [11,31]. Other frequently used pre-training datasets for captioned images are CC3M, CC12M, HQIPT-204M, Web Image-Text-1B, and VeCap, alongside interleaved image-text datasets like OBELICS and Web Interleaved, as well as extensive text-only corpora from webpages, code, social media, books, encyclopedias, and mathematical data [27]. For audio modalities, instruction-following datasets such as Audio-FLAN, containing 100 million samples, have been introduced for audio understanding and generation [39]. Specialized data formats—like the LMDX method that uses a coordinate-as-token scheme for layout information or Cell2Sentence which represents gene expression profiles as sequences—also contribute to diverse multimodal training [34].

Data collection methods for MLLMs primarily involve benchmark adaptation, self-instruction, and hybrid composition [23].

•  Benchmark Adaptation: This method leverages existing benchmark datasets, converting their input-output pairs into instruction-following formats [19]. For instance, Visual Question Answering (VQA) datasets like COCO, GQA, OCR-VQA, and VisualGenome, which inherently consist of image-question inputs and text answers, are directly adapted for multimodal instruction tuning (M-IT) [4,19]. Instructions can be manually designed or semi-automatically generated with assistance from large language models (LLMs) such as GPT [19]. A challenge with direct adaptation is that answers in traditional VQA or captioning datasets are often concise, potentially limiting the output length of MLLMs [19]. To mitigate this, strategies include modifying instructions to explicitly request short answers (e.g., ChatBridge, InstructBLIP) or extending answer lengths by prompting LLMs like ChatGPT with the original question, answer, and context (e.g., M3IT) [19].

•  Self-Instruction: This approach addresses the limitation of benchmark datasets in catering to real-world, multi-turn dialogue scenarios. It involves guiding LLMs to generate new instruction-following data from a small set of manually annotated "seed" samples [19]. LLaVA extended this method to the multimodal domain by translating images into textual descriptions—including captions and bounding boxes—and using these as context for GPT-4 to generate novel instruction data [19]. Similarly, MLLMs' ability to recognize object details and apply prior knowledge facilitates the generation of egocentric instruction data for orientation understanding [22].

•  Hybrid Composition: This method combines single-modal and multimodal data to enhance dialogue fluency and instruction-following capabilities [19]. Various training strategies, such as mixed instruction tuning (randomly shuffling both data types), sequential instruction tuning (training on text data before multimodal data), and adapter-based sequential instruction tuning, have been explored. Experimental results indicate that mixed instruction tuning performs comparably to solely training on multimodal data [19].

A significant trade-off exists between using large, noisy datasets and smaller, cleaner ones. Early MLLM training primarily relied on coarse-grained image-text pair data like LAION-5B, characterized by its immense scale (billions of images) but also high noise levels and short text descriptions, which can hinder proper alignment between modalities [8,10,31]. These large, noisy datasets are crucial for imparting broad world knowledge to models [10,31]. Conversely, recent work has explored using cleaner data with richer text content for finer-grained alignment. For instance, ShareGPT4V utilizes detailed descriptions generated by GPT-4V, which alleviates alignment issues and improves performance [8,10,31]. LLaVA, for example, used a 558K subset of LAION-CC-SBU with BLIP captions for feature alignment and 150K GPT-generated multimodal instruction-following data for visual instruction tuning [4]. Similarly, MiniGPT-4 refines Conceptual Caption, SBU, and LAION datasets with ChatGPT to generate high-quality image-text pairs [6]. While these high-quality, GPT-generated datasets offer improved alignment and performance, they typically are smaller in scale (millions of images) and consequently may contain more limited world knowledge [8,10].

The choice of data collection method profoundly impacts the diversity and quality of training data. Large-scale web-scraped datasets provide breadth but often lack depth and exhibit high noise. Conversely, data generated via self-instruction or MLLM-assisted methods can offer higher quality and finer-grained detail relevant to specific tasks, albeit at a smaller scale [10,31]. The importance of high-quality data for training visual foundation models and MLLMs is widely recognized [30].

Data preprocessing steps, including cleaning, normalization, and augmentation, are crucial for improving model robustness and generalization. Techniques such as filtering specific subsets (e.g., LLaVA using filtered CC3M) and refining existing datasets (e.g., MiniGPT-4 with ChatGPT) are employed [6]. The explicit removal of low-quality samples, particularly from large, noisy sources like LAION-5B, is implied to be beneficial, as high noise can adversely affect alignment [10].

The use of advanced MLLMs like GPT-4V for data generation has become a notable trend. GPT-4V excels at generating high-quality, fine-grained textual descriptions for images, facilitating improved multimodal alignment and performance [10,31]. This approach has been instrumental in creating datasets such as ShareGPT4V. However, a significant drawback is that GPT-4V is a paid service, which inherently limits the scalability of such data generation, resulting in smaller dataset sizes compared to web-scraped alternatives [8,10]. This cost-benefit analysis influences strategic decision-making in dataset creation for MLLMs.
### 4.2 Training Methodologies
The training of Multimodal Large Language Models (MLLMs) commonly adheres to a multi-stage paradigm, often comprising a three-stage process: pre-training, instruction tuning, and alignment tuning [28]. This structured approach is designed to progressively imbue MLLMs with general knowledge, instruction‐following capabilities, and human alignment, thereby enhancing overall model performance.

The initial stage, **pre-training**, focuses on establishing a foundational understanding of various modalities and their interconnections. Its objective is to map features from different modalities, such as images and audio, into a unified semantic space, enabling the model to absorb general knowledge from large-scale data [28]. Data utilized in this stage includes extensive coarse-grained image–text pairs like LAION-5B, as well as high-quality fine-grained datasets such as GPT-4V-generated ShareGPT4V [28]. During pre-training, techniques like calculating similarity between images and text and removing low-similarity samples are employed to refine data quality [28]. A key technique in this phase is freezing the pre-trained visual encoder and the Large Language Model (LLM) components while only training the newly introduced interface or projection layers, such as in LLaVA's feature alignment stage or MiniGPT-4's linear layer training [6]. This strategy prevents catastrophic forgetting of the LLM's learned knowledge and significantly improves training efficiency [28]. Conversely, some advanced models like MM1 are pre-trained with all parameters unfrozen to maximize learning capacity, using a diverse mix of interleaved image–text, image–text pair, and text-only documents [27]. Self-Supervised Learning (SSL) is also a fundamental technique in this stage, allowing models to learn from vast amounts of unlabeled data by predicting masked information, thus building a foundational understanding of language, patterns, and world knowledge [21].

The second stage, **instruction tuning** (also referred to as Multimodal Instruction Tuning or M-IT), is crucial for enabling MLLMs to understand and execute diverse user instructions and generalize to unseen tasks [19,28]. M-IT transforms MLLMs into multimodal chatbots and general-purpose task solvers by injecting external modal information into LLMs and leveraging their powerful reasoning capabilities [19,23]. Instruction tuning enhances the In-Context Learning (ICL) capabilities of LLMs, improving their analogical reasoning by allowing them to adapt to new tasks from a few examples without weight updates [25]. A typical multimodal instruction sample consists of an instruction (I), a multimodal input (M), and a true response (R) [31]. The training objective often follows the autoregressive objective: 

$$
P(A|I,M;\theta) = \prod_{i=1}^N P(A_i|A_{<i}, I, M; \theta)
$$

where \(N\) is the length of the ground truth answer [19]. Data for M-IT can be obtained by adapting existing benchmark datasets (e.g., VQA, OCR) or through self-instruction methods using LLMs to generate multi-turn dialogue data from limited manual annotations, which can be more complex and diverse but costlier [10,28,31]. Techniques like mixing pure text dialogue data during instruction tuning act as a regularization method to retain the LLM's original capabilities and embedded knowledge [8,10]. Moreover, instruction diversity (different sentence structures and task types) and data quality are often more critical than sheer data quantity, with instructions that include reasoning steps significantly improving model performance [28]. Various strategies exist for data fusion, such as mixed instruction fine-tuning (randomly shuffling language and multimodal data) and sequential instruction fine-tuning (text data followed by multimodal data) [31]. Continual instruction tuning is also an active area of research [38].

The final stage, **alignment tuning**, aims to reduce hallucinations and ensure that the model's generated content aligns with the input modality and human preferences, making outputs more human-friendly [28]. This stage primarily uses preference data, which is often collected through costly manual annotation [10]. Key techniques include Reinforcement Learning from Human Feedback (RLHF), where a reward model is trained using human preference data, and then optimization strategies like Proximal Policy Optimization (PPO) are applied [4,28]. An alternative, more direct approach is Direct Preference Optimization (DPO), which directly optimizes preference pairs without requiring an explicit reward model, as seen in D3PO for fine-tuning diffusion models [20,28]. Recent work, such as Silkie, has explored automated methods for collecting preference data, like calling GPT-4V, to reduce the reliance on expensive manual annotation [10].

While the three-stage process is a prominent paradigm, two-stage and even end-to-end training methodologies also exist. Many MLLMs, like LLaVA and MM1, adopt a two-stage approach, typically involving a pre-training (or feature alignment) stage followed by an instruction tuning stage [4,27]. This modular design allows for leveraging powerful pre-trained encoders and LLMs, minimizing catastrophic forgetting, and focusing subsequent stages on instruction adherence and alignment. Conversely, end-to-end training of multimodal LLMs is also explored [34].
### 4.3 Loss Functions and Optimization
The training of Multimodal Large Language Models (MLLMs) heavily relies on well-defined loss functions and efficient optimization algorithms to guide the model towards optimal performance. Loss functions quantify the discrepancy between model predictions and target outputs, thereby steering the learning process. In the context of MLLMs, these functions are critical not only for foundational pre-training objectives (e.g., next-token prediction, image-text alignment) but also for fine-tuning stages, particularly those incorporating human feedback.

A significant aspect of MLLM training involves the integration of human preference data, often in the third stage of training, to refine model behavior and alignment. This preference data, which typically reflects the desired quality or helpfulness of model responses, has traditionally been collected through costly manual annotation processes [8]. However, the high expense associated with manual labeling has prompted the exploration of automated alternatives, such as Silkie, which leverages powerful models like GPT-4V to generate preference rankings [8]. These data collection methodologies directly impact the nature and availability of supervision signals that feed into preference-based loss functions.

Furthermore, recent advancements have explored novel paradigms for incorporating human feedback. For instance, the D3PO framework demonstrates the capability to effectively train an optimal reward model using human feedback data without the need for explicitly training a separate reward model [20]. This approach highlights a potential trade-off, where implicit methods of integrating human preferences might offer efficiencies or stability advantages over explicit reward model training, by directly optimizing the policy based on preferences, thereby contributing to the overall training objective by enhancing alignment and quality without additional model complexity. These strategic choices in loss function design and data sourcing present crucial trade-offs in terms of training stability, performance, and resource expenditure [13].

Beyond the formulation of loss functions, efficient optimization algorithms are indispensable for training MLLMs, given their typically large number of parameters. These algorithms determine how the model's parameters are updated based on the gradients computed from the loss function. Key to this efficiency are parameters such as the learning rate and weight decay. Research indicates that the optimal peak learning rate (\(\eta\)) for MLLMs can be predicted based on the number of non-embedding parameters (\(N\)) using the empirical formula:
\
Additionally, weight decay (\(\lambda\)), a regularization technique that prevents overfitting, is often scaled proportionally to the peak learning rate, specifically as \(\lambda=0.1\eta\) [27]. Such meticulously tuned optimization schedules are critical for navigating the complex loss landscapes of MLLMs, ensuring stable convergence, and achieving high performance despite the immense scale of these models.
## 5. Evaluation and Benchmarking

**Comparison of MLLM Evaluation Paradigms**

| Aspect                | Closed-Set Evaluations                                    | Open-Set Evaluations                                                                 |
| :-------------------- | :-------------------------------------------------------- | :----------------------------------------------------------------------------------- |
| Nature                | Predefined tasks with objective, often single-correct-answer, metrics. | Conversational, creative, and flexible responses with arbitrary outputs.                                    |
| Metrics               | Accuracy, CIDEr, BLEU-4, F1 score.                        | Subjective qualities: coherence, creativity, relevance, safety, factuality.          |
| Evaluation Methods    | Automated, quantitative, using known, structured datasets.            | Manual scoring (human evaluators), GPT scoring (AI-assisted), qualitative case studies. |
| Use Case              | Quantifying performance on structured tasks (e.g., VQA, OCR).  | Assessing nuanced, open-ended generative capabilities (e.g., chatbots, content creation).              |
| Examples              | VQA v2, MathVista, TextVQA.                               | Chatbot interactions, story generation from images, complex multi-turn dialogues.                 |
| Limitations           | Limited for assessing nuanced, creative, or arbitrary outputs; may not reflect real-world usage. | Labor-intensive and costly (manual); challenges in standardizing subjective assessment; potential AI bias in GPT scoring. |

The robust evaluation and benchmarking of Multimodal Large Language Models (MLLMs) are paramount for driving their advancement and enabling meaningful comparisons across diverse models and capabilities [7,31]. This critical process provides essential feedback for model optimization and highlights areas requiring further research and development.

Current evaluation methodologies for MLLMs primarily distinguish between closed-set and open-set paradigms [19,25,31]. Closed-set evaluations typically involve predefined tasks with objective metrics such as accuracy, CIDEr, and BLEU-4, often utilizing known datasets and testing scenarios that include zero-shot and few-shot settings [19,27]. This approach is effective for quantifying performance on structured tasks, but it presents limitations when assessing the more nuanced and open-ended generative capabilities of MLLMs.

Conversely, open-set evaluations address the challenge of assessing MLLMs in more conversational and creative contexts, where responses are flexible and arbitrary [19,31]. Evaluating subjective qualities like coherence, creativity, and relevance in open-ended tasks remains a significant hurdle. Historically, manual scoring by human evaluators served as a primary method, despite its labor-intensive nature [31]. The advent of advanced language models, such as GPT-4V, has introduced more scalable alternatives like GPT scoring, alongside qualitative case studies, to assess the quality of MLLM-generated content [6,31]. The emphasis on human evaluation or human-aligned AI scoring underscores its importance for capturing the subtleties of MLLM performance in complex, open-ended scenarios [19,31]. Furthermore, specific evaluation aspects such as robustness to different instructions, hallucination detection using methods like POPE and CHAIR, and safety against adversarial attacks are gaining prominence, alongside metrics like FaithScore for factual verification [19,28,31].

The landscape of MLLM benchmarks is diverse and continuously evolving, classified broadly into three primary categories: foundational capabilities, model self-analysis, and extended applications [7].

Within **foundational capabilities**, benchmarks assess core MLLM abilities in perception, reasoning, and multimodal interaction. This includes comprehensive evaluation benchmarks like MME, MMBench, SEED-Bench, and MM-Vet, which gauge overall perception and reasoning often through chatbot interactions [7,15,18,31,32,33]. Other specific areas under foundational capabilities include Optical Character Recognition (OCR) (e.g., TextVQA, OCR-VQA) [7], chart and documentation understanding (e.g., ChartQA, DocVQA) [7], mathematical reasoning (e.g., MathVista, MathVerse) [7,18], and multidisciplinary knowledge (e.g., MMMU, ScienceQA) [7,18]. Multilingual capabilities are also tested using benchmarks like CMMMU and MTVQA [7,18], while instruction following is assessed via benchmarks such as MIA-Bench and OmniGenBench [7,16]. Furthermore, multi-round question answering (e.g., ConvBench), multi-image understanding (e.g., NLVR2), interleaved image and text understanding (e.g., SparklesEval), high-resolution image processing (e.g., MME-RealWorld), and visual grounding (e.g., RefCOCO series) constitute crucial areas of foundational assessment [7,14,18].

**Model self-analysis** focuses on the MLLM's ability to evaluate its own outputs, particularly for hallucination detection and safety checks, with benchmarks like BenchLLaVA [7].

**Extended applications** evaluate MLLMs in real-world contexts, including video understanding (e.g., Video-MME) [7,33], medical applications (e.g., MultiMedEval) [7], and autonomous driving (e.g., Talk2Car, DRAMA, EgoOrientBench), which assesses scene understanding and risk perception [7,17,22]. Surveys like [18,26] systematically categorize these benchmarks to provide a holistic view of MLLM evaluation.

Despite significant advancements, the current state-of-the-art MLLMs, while showing improved performance with scaling, still face substantial limitations on existing benchmarks [7]. A notable challenge is the absence of a unified evaluation standard, leading to disparate assessment methodologies and difficulties in cross-model comparison [18]. Many current benchmarks, particularly those for foundational capabilities, often prioritize perceptual tasks over complex reasoning abilities, resulting in a gap in rigorously evaluating true multimodal inference [25]. Specific weaknesses include struggles with fine-grained perception, visual mathematics (often relying on textual cues rather than genuine visual understanding), interpretation of long contexts in documents, processing of handwriting, and robust handling of non-Latin multilingual text [7,18]. The observed decline in model performance on enhanced benchmarks, such as MMMU-Pro, further suggests an over-reliance on textual components rather than comprehensive visual comprehension [18].

To address these limitations and propel MLLM research forward, future evaluation efforts must focus on developing more comprehensive and unbiased methods [7]. This includes:
1.  **Reasoning-centric Benchmarks**: Prioritizing the creation of benchmarks that demand deeper multimodal integration and complex inference, rather than merely pattern matching [25].
2.  **Robust Open-ended Evaluation**: Developing advanced, scalable methodologies to robustly assess open-ended, creative, and context-dependent MLLM outputs, going beyond conventional VQA or domain-specific tasks [31].
3.  **Standardized Protocols**: Establishing a unified evaluation standard and potentially a universal benchmark suite through collaborative efforts across the research community. This would facilitate more meaningful progress tracking and model comparisons [7].

By focusing on these directions, the field can develop more robust and representative evaluation frameworks that truly reflect the evolving capabilities and real-world applicability of MLLMs.
### 5.1 Benchmark Categories

![Classification of MLLM Benchmark Categories](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/l2pRaOzoakRCe6Lrgb28p_/home/surveygo/data/requests/13515/survey/imgs/Classification%20of%20MLLM%20Benchmark%20Categories.png)

The evaluation of Multimodal Large Language Models (MLLMs) is systematically categorized into three primary areas: foundational capabilities, model self-analysis, and extended applications [7]. This structured approach enables a comprehensive assessment of MLLMs across diverse tasks and real-world scenarios.

This category scrutinizes the core abilities of MLLMs in perception, reasoning, and multimodal interaction [7]. Evaluation typically involves either closed-set questions, where responses are measured against predefined metrics such as accuracy, CIDEr, and BLEU-4, or open-set evaluations utilizing human or GPT-4 ratings for aspects like relevance and factuality [28,31].

Comprehensive Evaluation benchmarks assess overall perception and reasoning, often simulating chatbot interactions [7]. Key benchmarks include VQA v2, VizWiz, MME, MMBench, SEED-Bench, SEED-Bench2, MMT-Bench, RealWorldQA, BLINK, MMVet, and TouchStone [7]. MME, notably considered the "first comprehensive evaluation benchmark for MLLMs," encompasses 14 perception and cognition tasks, with its leaderboards featuring over 50 advanced models like GPT-4V [31,32,33]. MMBench further evaluates multiple dimensions of model capabilities, employing a bilingual (English and Chinese) approach and utilizing large language models to convert free-form predictions into predefined options [18,31]. SEED-Bench evaluates 12 dimensions, including spatial and temporal understanding [15]. RealWorldQA focuses on understanding basic spaces in the real world [18], while MME-RealWorld extends this by focusing on high-resolution images and 43 subtasks across 5 real-world scenarios, posing challenges even for human performance [14]. MM-Vet focuses on complex multimodal tasks, defining 6 core visual-language abilities and 16 integrations [18]. Overall, while performance generally improves with LLM scale, challenges persist in fine-grained perception and visual mathematics [7].

Optical Character Recognition (OCR) capabilities are evaluated for text recognition in documents and real-world scenarios through benchmarks such as TextVQA, OCR-VQA, InfoVQA, WebSRC, SEED-Bench-2-Plus, OCRBench, and VCR [7]. MLLMs generally exhibit proficiency with regular text but encounter difficulties with handwriting and multilingual text, indicating a need for more robust OCR mechanisms in diverse textual environments [7].

Chart and Documentation Understanding assesses an MLLM's ability to interpret structured charts and documents. Benchmarks like ChartQA, DocVQA, InfoVQA, DocGenome, CharXiv, and MMLongBench-Doc are employed for this purpose [7]. A significant performance gap is observed in complex reasoning and the processing of long contexts within these documents, suggesting limitations in deeper semantic comprehension and contextual integration [7].

Mathematical Reasoning benchmarks, including MathVista, MathVerse, and We-Math, evaluate an MLLM's capacity to solve visual math problems [7]. MathVista, for instance, integrates 28 existing and 3 newly constructed mathematical multimodal datasets, totaling 6,141 samples [18]. Despite advancements, models like GPT-4V achieving 49.9% accuracy on MathVista, models still struggle with complex visual charts, often relying on textual cues or rote memorization rather than true visual understanding [7,18]. MathVerse further elaborates on this, with its 15,000 test samples designed to assess genuine visual understanding in mathematical problem-solving [18].

Multidisciplinary Knowledge is tested across various academic disciplines using benchmarks such as ScienceQA, MMMU, CMMU, and CMMMU [7]. MMMU, a notable benchmark, comprises 11,500 college-level multimodal questions from exams and textbooks, covering 6 core disciplines and 30 subject areas, and featuring 30 types of heterogeneous images [18]. Even highly advanced models like GPT-4V and Gemini Ultra achieve only 56% and 59% accuracy, respectively, on MMMU, highlighting substantial room for improvement in integrating diverse knowledge domains and reasoning over complex multimodal inputs [7,18]. MMMU-Pro, an enhanced version, rigorously evaluates models by filtering questions answerable by pure text and embedding question text into images, resulting in significantly lower model performance (16.8%-26.9%), which points to current MLLMs' potential over-reliance on textual components [18].

Multilingual benchmarks evaluate MLLM performance in non-English languages, including CMMMU, ViOCRVQA, Urdu-VQA, Swahili-STR, MTVQA, and M3Exam [7]. MTVQA, for example, includes 6,778 question-answer pairs in 9 different languages [18]. A critical observation is that performance tends to be superior in languages utilizing Latin alphabets, indicating a bias or underdevelopment in processing non-Latin script languages [7].

Instruction Following capabilities, which test the ability to execute complex user instructions, are assessed by benchmarks like MIA-Bench [7]. OmniGenBench extends this evaluation across 57 diverse sub-tasks, encompassing both perception-centric and cognition-centric dimensions, leveraging visual parsing tools and LLM-based judgers to align generated images with user instructions [16].

Multi-Round Question Answering simulates real-world dialogues, testing long-context processing through benchmarks such as ConvBench and MMDU [7]. The complexity of maintaining context and coherence over extended conversations remains a significant challenge, requiring further advancements in memory and reasoning capabilities for MLLMs.

Other foundational areas include multi-image understanding (e.g., NLVR2, MuirBench, Mementos), interleaved images and text understanding (e.g., MMMU, SparklesEval), high-resolution image processing (e.g., V*Bench, MME-RealWorld), and visual grounding (e.g., RefCOCO series) [7,18]. While specific benchmarks exist, a general limitation across many of these foundational capabilities is that a majority of existing multimodal benchmarks are not inherently reasoning-focused, often prioritizing perceptual tasks over complex inferential abilities [25].

This category focuses on the MLLM's ability to evaluate its own outputs, specifically in hallucination detection and safety checks [7]. Benchmarks like BenchLLaVA [7] contribute to this evaluation. Hallucination assessment employs methods such as POPE (Polling-based Object Probing Evaluation), which generates multiple-choice questions to detect inconsistencies, and CHAIR (Caption Hallucination Assessment with Image Relevance), which verifies the presence of extracted nouns in images [28,31]. FaithScore further refines this by splitting generated text into atomic facts and verifying them with visual models [28].

This category evaluates MLLMs in various real-world application contexts [7].

Video Understanding is assessed using benchmarks like Video-MME and ActivityNet-QA [7]. Video-MME is recognized as the first comprehensive evaluation benchmark specifically for MLLMs in video analysis, complemented by efforts like Video-ChatGPT and Video-Bench which focus on the broader video domain [31,33].

Medical Applications are evaluated through specialized benchmarks like MultiMedEval [7].

Autonomous Driving capabilities are tested using benchmarks such as Talk2Car and DRAMA [7]. Talk2Car, built upon the nuScenes dataset, evaluates MLLMs in scenarios requiring object detection, counting under occlusion, and comprehensive risk assessment for both vehicles and pedestrians, thereby assessing scene understanding capabilities [7,17]. Tasks also extend to sports analysis, involving scene description, player counting, and outcome prediction [17]. Furthermore, EgoOrientBench evaluates MLLMs' orientation understanding across diverse image domains [22].

Despite the proliferation of diverse benchmarks, a significant limitation is the absence of a unified evaluation standard for MLLMs, leading to disparate assessment methodologies and difficulties in cross-model comparison [18]. Many existing benchmarks, while comprehensive in covering various tasks, often fall short in rigorously evaluating complex reasoning abilities, tending to focus more on perceptual aspects [25]. Performance gaps in areas such as understanding complex visual charts, long contexts in documents, handwriting recognition, and non-Latin multilingual text underscore the need for more specialized and challenging datasets that explicitly target these weaknesses [7]. The observation that advanced models still struggle significantly on enhanced benchmarks like MMMU-Pro suggests that current models might rely more on textual cues than true visual understanding, necessitating benchmarks that disentangle these modalities more effectively [18].

Future improvements should prioritize the development of more reasoning-centric benchmarks that demand deeper multimodal integration and inference, rather than superficial pattern matching. Frameworks should incorporate more rigorous testing for visual grounding, counterfactual reasoning, and real-world nuanced scenarios that genuinely push model capabilities beyond current limitations. Establishing a standardized evaluation protocol, possibly through collaborative efforts on a universal benchmark suite, would facilitate more meaningful progress tracking and comparison within the MLLM research community.
### 5.2 Comprehensive Benchmark Surveys
Survey papers on Multimodal Large Language Model (MLLM) benchmarks play a crucial role in systematizing the understanding and evaluation of this rapidly evolving field, emphasizing the importance of comprehensive assessment [26]. Different survey approaches offer varied perspectives on organizing and analyzing the landscape of MLLM evaluation.

One prominent approach, as seen in [26], organizes benchmarks along multifaceted dimensions including perception and understanding, cognition and reasoning, specific domains, key capabilities, and other modalities. This multi-dimensional categorization provides a holistic view, highlighting the diverse abilities MLLMs are expected to possess. In contrast, another survey, [18], categorizes benchmarks by task types, such as college-level problems, mathematics-related tasks, and general visual question answering (VQA). While this approach offers a clear, task-oriented classification, it may be less granular in distinguishing between underlying cognitive or perceptual capabilities compared to the multi-dimensional framework. The collaborative initiative reflected by [33], which introduces MME-Survey alongside teams from MME, MMBench, and LLaVA, underscores a collective effort towards providing a comprehensive overview of MLLM evaluation, suggesting a community-driven approach to synthesizing benchmark information.

A significant challenge and an area for continuous evolution in MLLM benchmarks revolve around the evaluation of open-ended questions, particularly as MLLMs increasingly function as chatbots with flexible and arbitrary outputs [31]. Unlike closed-set questions with defined answers, the assessment of open-set responses presents considerable complexity [31]. The evolution of evaluation standards reflects this challenge. Historically, manual scoring, which involves human evaluation of generated responses, was a primary method, but its labor-intensive nature proved unsustainable for large-scale assessment [31]. This led to the exploration of more scalable alternatives, such as GPT scoring, where advanced language models like GPT-4V are leveraged to evaluate MLLM performance, particularly with the advent of visual interfaces for such models [31]. Complementary to these automated and semi-automated methods, case studies provide a qualitative approach to compare different MLLM capabilities [31].

The ongoing development of MLLM capabilities, especially in generating creative or conversational outputs, continually pushes the boundaries of existing benchmarks and their evaluation methodologies. A notable gap in the existing survey literature, implicitly highlighted by the challenges in open-ended evaluation, lies in the dynamic nature of defining and assessing success for increasingly complex, flexible, and context-dependent MLLM outputs. Future surveys must continually adapt to incorporate new evaluation paradigms that can robustly assess open-ended interactions, complex reasoning chains, and cross-modal understanding beyond conventional VQA or domain-specific tasks. The common themes emerging from these surveys underscore the critical need for a structured and continuously updated framework for MLLM evaluation, which accurately reflects their evolving capabilities and addresses the intricacies of human-like intelligence.
## 6. Advanced Techniques and Capabilities in MLLMs

**Comparison of Advanced MLLM Reasoning Techniques**

| Technique                    | Description                                                                                                                            | Primary Focus/Strength                                                                 | Implementation                                                                 | Advantages                                                                  |
| :--------------------------- | :------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------- | :-------------------------------------------------------------------------- |
| Multimodal In-Context Learning (M-ICL) | Enables MLLMs to learn by analogy from a few examples and optional instructions; facilitates few-shot learning for complex/unseen tasks. | Rapid adaptation; few-shot learning; "learning by analogy" for visual reasoning tasks. | Training-free implementation; uses demonstration sets of contextual samples.   | Quick adaptability; efficient for new tasks with minimal examples; flexible integration. |
| Multimodal Chain of Thought (M-CoT) | Extends Chain of Thought (CoT) reasoning to multimodal contexts; guides MLLMs to generate intermediate reasoning steps.            | Enhanced reasoning transparency; robust problem decomposition; multi-step, complex reasoning. | Explicit generation of intermediate steps; Visually Cued Chain-of-Thought (VC-CoT). | Improves accuracy and transparency in complex problem-solving; enables logical progression. |
| LLM-Aided Visual Reasoning (LAVR) | Leverages LLM reasoning & knowledge as auxiliary tools for visual reasoning systems; combines LLMs with external visual models or tools. | Broader application; generalization; emergent abilities; enhanced interactivity and controllability. | LLM acts as "Controller," "Decision Maker," or "Semantics Refiner"; tool-augmented LLMs. | Strong generalization to unseen concepts; emergent capabilities beyond traditional models; intuitive user control via natural language. |

The advancement of Multimodal Large Language Models (MLLMs) extends beyond basic performance metrics to sophisticated techniques that enhance their reasoning and interaction capabilities [8,10]. Key among these are Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR), each contributing distinctly to improved MLLM performance and versatility [19,23,33,36,37].

Multimodal In-Context Learning (M-ICL) builds upon the foundational In-Context Learning (ICL) paradigm, enabling MLLMs to learn by analogy from a few examples and optional instructions [19]. Unlike traditional supervised learning that relies on extensive data for implicit pattern recognition, M-ICL facilitates few-shot learning, allowing models to solve complex and unseen tasks with minimal examples [19,23]. A significant strength of M-ICL is its training-free implementation, offering flexibility in integration into various reasoning frameworks [19]. This approach is primarily applied in two scenarios: solving diverse visual reasoning tasks and enabling MLLMs to utilize external tools [19]. Models like MM1 have demonstrated strong few-shot learning capabilities through in-context predictions and instruction tuning, further enhancing M-ICL's effectiveness [27]. The extension of M-ICL with a demonstration set of contextual samples during the reasoning phase can further boost its performance [23].

Multimodal Chain of Thought (M-CoT) extends the Chain of Thought (CoT) reasoning paradigm from the natural language processing (NLP) domain to multimodal contexts [19,21,23]. M-CoT enhances MLLMs' ability to perform complex reasoning tasks by guiding them to generate intermediate reasoning steps, thereby promoting more accurate and transparent problem decomposition and processing [21]. A critical challenge in M-CoT is bridging the modality gap, ensuring seamless integration and interaction between different data types [19,23]. Various paradigms exist for acquiring M-CoT capabilities, including specific configurations and chain formations for generating thought processes. For instance, Visually Cued Chain-of-Thought (VC-CoT) explicitly references visual information to improve multi-step reasoning in MLLMs [39]. MM1 also exhibits strong Chain-of-Thought reasoning, enabled by large-scale multimodal pre-training [27]. Within M-CoT, Large Language Models (LLMs) can function as a "Controller" for single-round tasks or a "Decision Maker" for multi-round reasoning, facilitating the logical progression of thought [19].

LLM-Aided Visual Reasoning (LAVR) leverages the powerful reasoning capabilities and extensive knowledge of LLMs as auxiliary tools to construct task-specific or general-purpose visual reasoning systems [19,23]. Inspired by tool-augmented LLMs, LAVR combines LLMs with external tools or large visual models to achieve superior performance compared to traditional visual reasoning models [19]. LAVR systems demonstrate several notable advantages: (1) Stronger Generalization Ability: With rich open-world knowledge from large-scale pre-training, these systems can easily generalize to unseen objects or concepts, achieving excellent few-shot and zero-shot performance [19]. (2) Emergent Capabilities: The integration of LLMs enables these systems to perform complex tasks that might be beyond the scope of traditional visual models [19]. (3) Enhanced Interactivity and Controllability: Unlike traditional models that often require expensive and curated datasets for limited control mechanisms, LLM-based LAVR systems allow for fine-grained control through user-friendly interfaces, such as clicks and natural language queries [19]. In LAVR, LLMs can act as a "Controller" for overall task orchestration, a "Decision Maker" for complex choices, or a "Semantics Refiner" to enhance the understanding and generation of explanations [19].

Comparison and Synthesis: While M-ICL, M-CoT, and LAVR all aim to improve MLLM performance, they do so through distinct mechanisms. M-ICL excels in rapid adaptation and few-shot learning by demonstrating examples, offering a flexible and training-free approach to new tasks. Its strength lies in "learning by analogy" [19]. M-CoT, on the other hand, focuses on enhancing reasoning transparency and robustness for complex problems by explicitly generating intermediate steps, emphasizing the "how" of problem-solving. Its main contribution is enabling multi-step, complex reasoning through structured thought processes [21]. LAVR primarily harnesses LLMs as intelligent assistants or control agents for visual tasks, leveraging their knowledge and reasoning to achieve better generalization, emergent abilities, and user control. It is more about leveraging LLMs' intrinsic intelligence for external task execution and explanation generation [19]. Their strengths are complementary: M-ICL provides quick adaptability, M-CoT offers structured reasoning for complexity, and LAVR integrates LLM intelligence for broader application and control in visual domains. For instance, an MLLM might use M-ICL for initial few-shot understanding, then employ M-CoT to break down a complex multimodal query into sequential steps, and finally utilize LAVR's mechanisms for interacting with and refining visual outputs.

Beyond these specific techniques, MLLMs have undergone significant extensions to handle diverse input and output granularities, alongside progress in supporting various languages and scenarios [3]. Researchers have developed MLLMs with finer-grained support for user prompts, evolving from image-level control to region-level and even pixel-level manipulation [31]. The trend is towards increasing modality support, encompassing not only more multimodal content input such as 3D point clouds but also expanding the generation capabilities to include diverse modal responses like images, audio, and video [31]. Furthermore, techniques like egocentric instruction tuning aim to align MLLMs' understanding of object orientation with the user's perspective, improving contextual relevance and user interaction [22]. This continuous development in advanced techniques and expanded capabilities underscores the rapid evolution of MLLMs towards more versatile and intelligent multimodal AI systems.
## 7. Applications of Multimodal Large Language Models

Multimodal Large Language Models (MLLMs) are revolutionizing diverse sectors by significantly impacting various application areas through their ability to comprehensively understand and generate content across multiple modalities [1,2,9,11,37]. These models offer a paradigm shift by enabling more intelligent, personalized, and efficient interactions and operations, leading to their widespread adoption in both general-purpose assistants and specialized, real-world scenarios [9,31].

A foundational impact of MLLMs lies in **multimodal understanding**, where they process and synthesize information from diverse inputs to interpret complex visual content alongside other modalities [9,10]. Key tasks include image captioning, which involves generating descriptive textual narratives from visual inputs by capturing semantic information, detecting objects, actions, and inferring relationships [24,29]. Furthermore, MLLMs excel in Visual Question Answering (VQA), requiring them to answer complex queries based on visual and multimodal data, thereby demonstrating sophisticated natural language processing and visual perception capabilities [11]. Beyond these core tasks, MLLMs extend to specialized applications like object counting, risk assessment, and in-depth analysis of sports events by interpreting visual content to generate meaningful descriptions or predictions [17]. They also enhance environmental comprehension in dynamic contexts, such as traffic scene understanding in autonomous driving, by synergizing visual, textual, and other modal data [36].

Complementing understanding, MLLMs have significantly advanced **multimodal generation** capabilities, enabling content creation across diverse modalities [10,35]. This includes text-to-image synthesis, where natural language descriptions are translated into high-quality visual content, exemplified by models like DALL-E 2 and Imagen [9,24,29,35]. MLLMs can also perform intricate image editing based on textual commands, demonstrating a sophisticated understanding of both visual elements and linguistic instructions [11]. While generating full-fledged video from text remains a complex task, MLLMs are widely used for video captioning and summarization, analyzing video streams to produce concise summaries or detailed captions that enhance accessibility and content management [9,35]. Broader generative outputs include text-to-text, text-to-music, text-to-video, text-to-human-motion, and text-to-3D-objects, providing technical foundations for digital humans, gaming, and virtual reality [21,37]. A significant challenge in multimodal generation is producing content that is not only visually and audibly realistic but also semantically coherent and contextually accurate across modalities, addressing issues of fidelity, consistency, and avoiding hallucination [10].

MLLMs are profoundly transforming various **industry verticals and emerging fields**:
*   In **healthcare**, MLLMs offer substantial promise for medical image analysis, assisting in diagnostics, and contributing to medical visual learning [9,11,17]. Applications extend to biomedical analysis, such as training small models for radiology imaging [1]. However, their deployment necessitates careful consideration of ethical implications like data privacy, bias in diagnostic outcomes, and accountability for AI-assisted decisions.
*   For the **education** sector, MLLMs facilitate personalized learning experiences by analyzing student work, understanding complex mathematical formulas, and interpreting charts and experimental results, thereby supporting tailored educational approaches [9,35].
*   In **entertainment**, MLLMs are instrumental in content generation and interactive experiences, particularly through AI-Generated Content (AIGC) technologies that simplify the development of digital humans from photos, videos, or audio [29,37]. They also contribute to image content moderation, ensuring responsible content management [38].
*   **Robotics and autonomous systems** represent a critical frontier for MLLM integration. In robotics, MLLMs contribute to tasks such as object recognition, navigation, human-robot interaction, and robotic manipulation, aligning with the principles of embodied AI [25]. For autonomous driving, MLLMs offer solutions for environmental understanding, planning, and control by converting intricate driving scenarios into language model problems, enhancing safety, interpretability, and allowing fine-tuning of controller parameters for optimized driving experiences [30,36]. Generative AI components of MLLMs are also leveraged for creating simulation platforms for autonomous driving development [30]. A significant challenge in deploying MLLMs in real-world robotic systems and autonomous driving scenarios includes managing noisy data and ensuring robust safety protocols given the dynamic and unpredictable nature of these environments [11,36].
*   Other emerging applications span various fields, including **finance** for security applications like fraud detection [35], **intelligent monitoring and security** by analyzing video images and audio for abnormal behavior and real-time warnings [9,35], **urban governance** and **enterprise digitalization** through visual large models creating digital twins for efficient management [37], and **legal industry** for case analysis [37]. Furthermore, MLLMs are being adapted for specific downstream tasks with specialized knowledge in areas like **document understanding** (e.g., information extraction, chart analysis) and **remote sensing agriculture identification** [1,11,31,37]. The ability of MLLMs to process diverse inputs simultaneously makes them valuable in intelligent customer service and virtual assistants, offering more intelligent and personalized interaction experiences [9,35].

Overall, MLLMs are demonstrating transformative potential across a wide spectrum of applications, from enhancing fundamental understanding and generation tasks to revolutionizing specialized industry verticals. Despite their benefits, challenges related to data quality, semantic coherence, safety, and ethical implications remain critical considerations for their successful real-world deployment.
### 7.1 Multimodal Understanding
Multimodal Large Language Models (MLLMs) are fundamentally designed to address applications requiring the comprehensive understanding and interpretation of visual content alongside other modalities [9,10]. This capability enables MLLMs to process and synthesize information from diverse inputs, leading to more robust and humanized interactions [9]. Key applications in this domain include image captioning and visual question answering (VQA), which necessitate sophisticated visual perception integrated with natural language processing capabilities [11,35].

Image captioning stands as a prominent task for MLLMs, where their utility lies in generating descriptive textual narratives directly from visual input [24]. These models process an image and subsequently produce a detailed, contextually relevant description, effectively translating visual information into linguistic form [9]. This involves the MLLM interpreting various elements within the visual content, such as objects, actions, and their interrelationships, to construct coherent and accurate captions.

Beyond descriptive generation, MLLMs demonstrate significant prowess in Visual Question Answering (VQA) tasks. In VQA, models are required to answer complex questions predicated on the content of an image or other multimodal data [11]. This demands not only the ability to visually perceive and identify elements within an image but also to perform sophisticated natural language processing to comprehend the query and formulate an appropriate response based on the visual evidence [11]. The challenges inherent in VQA often revolve around accurately grounding linguistic concepts in visual information, ensuring that the model's answers are directly supported by the observed visual context. This intricate process of correlating textual queries with visual details underscores the advanced reasoning capabilities MLLMs bring to multimodal understanding.

The scope of multimodal understanding facilitated by MLLMs extends to a wide array of specialized tasks. These include analyzing complex visual scenes for applications such as object counting, assessing risks in dynamic environments, and performing in-depth analysis of sports events, all of which require interpreting visual content to generate meaningful textual descriptions or predictions [17]. Furthermore, in domains like autonomous driving, MLLMs enhance environmental comprehension by synergizing visual, textual, and other modal data, thereby improving traffic scene understanding and adaptability to evolving situations [36]. Such diverse applications highlight the critical role of MLLMs in interpreting and acting upon integrated multimodal inputs.
### 7.2 Multimodal Generation
Multimodal Large Language Models (MLLMs) have significantly advanced content creation by enabling generation across diverse modalities [10,35]. This capability encompasses a wide spectrum of applications, primarily focusing on synthesizing images from textual descriptions and producing descriptive summaries or captions for video content.

One prominent application is text-to-image synthesis, where MLLMs translate natural language descriptions into visual content. These models are adept at generating high-quality images and illustrations directly from text inputs [9,24,35]. Beyond static image generation, MLLMs can also perform intricate image editing, allowing users to modify existing visuals—such as altering object colors or shapes—through textual commands [11]. This demonstrates their sophisticated understanding of both visual elements and linguistic instructions.

In the domain of video content, MLLMs are employed to create descriptive captions and summaries. While generating full-fledged video from text remains a complex task, these models can analyze video streams and extract key information to produce concise summaries or detailed captions [9,35]. This functionality is crucial for enhancing content accessibility, improving searchability, and increasing the efficiency of video content management.

Despite these advancements, significant challenges persist in multimodal generation. A primary hurdle is producing content that is not only visually and audibly realistic but also semantically coherent and contextually accurate across modalities. Ensuring that generated images or video captions accurately reflect the nuances of the input—whether subtle emotional cues, complex actions, or intricate details—remains a complex task. Capturing the full spectrum of multimodal information, such as the temporal dynamics in videos or the interplay between different elements in an image, requires sophisticated models capable of deeply understanding and synthesizing information from heterogeneous sources. This involves addressing issues of fidelity, consistency, and the potential for hallucination, where models generate plausible but incorrect details.
### 7.3 Industry Verticals and Emerging Fields
Multimodal Large Language Models (MLLMs) are revolutionizing various industries by enhancing existing tasks and enabling novel applications across diverse sectors, showcasing their broad utility and transformative potential [35]. Major technology companies are actively developing a range of multimodal products, underscoring this trend [2]. Beyond general visual understanding tasks, MLLMs are increasingly customized for specific domains, often fine-tuned on existing MLLMs using domain-specific data to address complex tasks inherent to these fields [11].

In **healthcare**, MLLMs hold significant promise, particularly for medical image analysis and diagnosis. They are capable of analyzing complex medical images and assisting in diagnostics [9,17], and contribute to medical visual learning [11]. While these advancements offer substantial benefits, their implementation also necessitates careful consideration of ethical implications, such as data privacy, bias in diagnostic outcomes, and accountability for AI-assisted decisions.

For the **education** sector, MLLMs facilitate personalized learning experiences. They can analyze student work and explain complex concepts, thereby supporting tailored educational approaches [9]. In **finance**, MLLMs are poised to enhance security applications, including fraud detection, by processing and understanding diverse data types relevant to financial transactions [35].

**Robotics** and **autonomous systems** represent a critical frontier for MLLM integration. In robotics, MLLMs contribute to tasks like object recognition, navigation, human-robot interaction, and robotic manipulation. The architectural "brain" of embodied AI robots shares commonalities with autonomous driving systems, particularly in areas like path planning and open-scenario transferability [36]. This synergy allows for the adaptation of algorithms and methodologies across these domains. For autonomous driving, MLLMs offer solutions for planning and control by converting intricate driving scenarios into language model problems, leading to simplified processing and improved accuracy [36]. They enhance safety and interpretability by providing explanations for planned actions, fostering greater user trust [36]. Furthermore, MLLMs can fine-tune controller parameters to align with driver preferences, optimizing the driving experience and system responsiveness [36]. Beyond real-world applications, generative AI, a component of MLLMs, is leveraged for creating simulation platforms for autonomous driving development [30]. MLLMs are also instrumental in understanding and executing complex tasks related to traffic situation comprehension [11]. Challenges in this domain include managing noisy data and ensuring robust safety protocols.

In the **entertainment** industry, MLLMs are instrumental in content generation and interactive experiences. A prime example is the significant role of AI-Generated Content (AIGC) technologies, which simplify and enhance the development of digital humans [29]. Companies such as Meta and NVIDIA leverage MLLMs to enable users to create 3D digital humans from uploaded photos, videos, or audio [29]. Here, natural language generation influences the quality of human-computer interaction content, while computer vision technologies dictate the facial expressions and body movements of these digital entities [29]. Additionally, MLLMs are employed for image content moderation, ensuring responsible content management [38]. Other emerging applications include sports analysis [17] and document analysis, encompassing information extraction and chart analysis [11].
## 8. Challenges and Future Directions

![Major Challenges Facing Multimodal Large Language Models (MLLMs)](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/2qRL1Ar73LWVvQtNkb5-H_/home/surveygo/data/requests/13515/survey/imgs/Major%20Challenges%20Facing%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.png)

The rapid evolution of Multimodal Large Language Models (MLLMs) has unveiled significant capabilities, yet their widespread adoption and continued advancement are hindered by several critical challenges. Addressing these limitations is paramount for realizing the full potential of MLLMs in diverse real-world applications. This section summarizes the key challenges and identifies promising directions for future research [1,2,5,11,12,23,31,33].

A primary challenge confronting MLLMs is **hallucination**, characterized by the generation of content that is inconsistent with or inaccurately reflects the provided multimodal input, particularly visual information [8,10,11,31]. This manifests as existence hallucination (asserting the presence of non-existent objects) or attribute hallucination (misdescribing object characteristics) [31]. Root causes include modal bias, where models over-rely on one modality, insufficient multimodal understanding, and the inherent heterogeneity of vision and text that complicates complete alignment [5,8,25]. Hallucination also stems from incomplete visual perception or biases within the language model's learned representations [25]. Current visual information tokenization, multimodal alignment paradigms, and conflicts between multimodal data and the LLM's stored knowledge warrant deeper investigation as sources of hallucination [10]. While methods like robust training data, increased image resolution, and techniques such as aspect ratio padding, adversarial training, RLHF, and specific strategies like OPERA and Visual Contrastive Decoding show promise, object hallucination remains a prevalent issue affecting reliability [4,5,8,11,23,32,33].

Another significant concern is **bias and fairness**, as MLLMs trained on vast, often uncurated internet datasets can generate inappropriate or biased content, raising ethical implications [11]. Mitigation strategies involve incorporating explicit safety and fairness constraints during training, meticulous data cleaning and filtering, and adversarial training to enhance robustness against biased inputs [11]. Proactive curation of diverse and representative datasets is crucial to minimize inherent bias.

**Computational cost and efficiency** pose substantial barriers to the accessibility and deployment of MLLMs due to their immense model sizes and extensive data requirements [1,11,23]. Optimizing efficiency without compromising performance is a key research direction [35]. Approaches include parameter-efficient fine-tuning (PEFT) like LoRA, model compression, and quantization (e.g., 4-bit and 8-bit inference) [4,11]. Sparse activation architectures such as Mixture of Experts (MoE) models offer scalability by activating only a subset of "expert" sub-networks for a given input, reducing active computational load [1,27]. Further strategies include distributed training, hardware acceleration, parameter sharing, and distillation [1,11,23].

The ability to handle **long context processing and complex instruction following** remains a challenge for current MLLMs. They often struggle with extensive multimodal inputs, such as long videos or documents with interleaved images and text [3,8,25,28,31]. While models like GPT-4V show promise in interpreting complex commands, many MLLMs lack sufficient instruction following capabilities, often failing to produce expected outcomes even with explicit instructions, indicating a need for more effective instruction tuning [10,23]. Research focuses on robust multi-modal interleaved document understanding, improved context learning, and advanced chain-of-thought reasoning to overcome these limitations [8,10,28].

**Interpretability and explainability** are crucial for building trustworthy and debuggable MLLMs, especially in sensitive domains. However, the complex interactions within multimodal data fusion and deep neural architectures make their internal decision-making opaque [2]. Challenges lie in attributing contributions across heterogeneous data types and understanding the causal links between multimodal inputs and outputs. Future work requires novel model-agnostic techniques, inherently interpretable architectures, causal inference methods, human-in-the-loop frameworks, and concept-based explanations to provide transparency.

**Data requirements and quality** pose a significant hurdle, as collecting large-scale, high-quality multimodal data for extensive training is challenging [10,30]. Furthermore, existing **evaluation metrics and benchmarks** are often insufficient, necessitating more comprehensive measures to accurately assess MLLM performance, particularly in complex real-world scenarios and for high-resolution images [2,14,15,26]. Current MLLMs also exhibit limitations in their **perception capabilities**, often capturing incomplete or incorrect visual information, partly due to the trade-off between information capacity and computational burden, highlighting the need for methods like large visual foundation models (e.g., SAM) to compress visual data more effectively [19,23]. The **reasoning chain of MLLMs** can be fragile, leading to incorrect answers even when intermediate calculations are correct, underscoring the need to improve multimodal reasoning [19,23]. Additionally, MLLMs struggle with **robustness**, particularly in complex computer vision tasks such as object detection and counting in dynamic scenes, and accurately interpreting object orientation due to inconsistent training annotations [17,22]. There are also concerns regarding **safety risks**, as MLLMs can be misled by adversarial inputs to generate biased or undesirable responses [8,31]. Finally, the interplay between large-scale multimodal and single-modal data during joint training requires systematic investigation to determine whether co-training is mutually beneficial or detrimental [8,10]. MLLMs have also been noted to be "shape-blind," indicating limitations in reasoning with diagrams [39].

Based on these challenges, several promising **future directions** for MLLM research emerge:
* **Developing More Efficient Architectures and Training Methods**: Continued exploration of PEFT, model compression, quantization, and sparse architectures like MoE is essential to reduce computational overhead and enable broader deployment [1,11,23,35].
* **Improving Robustness and Addressing Safety Concerns**: This includes developing models resilient to adversarial attacks, mitigating biases, and ensuring ethical deployment [8,11,31].
* **Exploring New Modalities and Advanced Reasoning Paradigms**: Future work should extend self-supervised learning (SSL) to new modalities such as video, motion, and 3D data, focusing on modeling underlying dynamics rather than superficial predictions [21]. This also involves developing unified spatio-temporal representations and deep perceptual transfer. Enhancing chain-of-thought reasoning for non-textual modalities, improving context learning, and leveraging LLM-aided reasoning with external tools are crucial for more sophisticated multimodal understanding [8,10,23,25,31].
* **Enhancing Perception and Multimodal Alignment**: Integrating large visual foundation models (e.g., SAM) to improve perception and address the information capacity–computational burden trade-off is vital [19,23]. Finer-grained visual and textual modality alignment is also crucial for mitigating object hallucination [23].
* **Advancing Instruction Following and Egocentric Understanding**: Research should focus on instruction tuning to cover a wider range of tasks and improve generalization, along with enhancing object orientation understanding through egocentric instruction tuning [22,23].
* **Developing MLLM-based Agents**: A significant hotspot is the development of embodied agents capable of interacting with the real world, requiring comprehensive improvements in perception, reasoning, and planning capabilities, and the ability to learn and utilize tools [4,10,31,34,37].
* **Addressing Continual Learning and Catastrophic Forgetting**: Enabling MLLMs to continuously learn from new data and tasks without losing previously acquired knowledge is essential for achieving Artificial General Intelligence (AGI) [8,29].
* **Specialized and Vertical Applications**: Further development of MLLMs tailored for specific vertical fields like healthcare and finance, alongside unified visual models, is expected [9,35,37].
* **Systematic Evaluation and Benchmarking**: There is a need for more robust and comprehensive evaluation methodologies to truly assess MLLM capabilities, especially for complex real-world scenarios and high-resolution inputs [2].
* **Modular Expert Mechanisms**: Developing modular expert mechanisms can enhance system flexibility and performance [21,30,35].

Overall, the future of MLLMs lies in tackling fundamental issues related to data quality, model efficiency, reasoning depth, and human-like interaction, moving towards more robust, interpretable, and ethically aligned systems.
### 8.1 Hallucination
Hallucination in Multimodal Large Language Models (MLLMs) is characterized by the model generating responses that are inconsistent with or do not accurately reflect the visual content provided [8,11,31]. This phenomenon manifests in various forms, including existence hallucination, where the model incorrectly asserts the presence of objects in an image, and attribute hallucination, involving the misdescription of an object's characteristics, such as its color [31]. A notable example of such inaccuracy is GPT-4o's performance, which, despite providing detailed scene descriptions, exhibited errors in bounding box localization, with some coordinates exceeding image dimensions [17].



![Hallucination in MLLMs: Causes and Mitigation Strategies](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/_kPgPPw6dxOtv55rVVl1J_/home/surveygo/data/requests/13515/survey/imgs/Hallucination%20in%20MLLMs%3A%20Causes%20and%20Mitigation%20Strategies.png)

The primary causes of hallucination in MLLMs stem from several fundamental limitations. One significant factor is **modal bias**, where the model disproportionately relies on one modality (e.g., text) while neglecting crucial information from others [5]. Another root cause is **insufficient multimodal understanding**, which occurs when the model fails to fully comprehend the content of a given modality, leading to flawed reasoning and generation [5]. This challenge is compounded by the inherent heterogeneity of vision and text, making their complete alignment a formidable task [8]. Furthermore, hallucination may arise from an incomplete perception of visual signals or from biases and false correlations embedded within the language model's learned representations [25]. Existing MLLMs also exhibit a limited capacity for processing long multimodal contexts, posing substantial challenges for tasks like extended video understanding and interleaved image-text comprehension in long documents, an area that remains relatively unexplored despite advancements like Gemini 1.5 Pro in long video understanding [10]. Object hallucination is a prevalent issue affecting the reliability of MLLMs [23].

Addressing hallucination requires a multi-faceted approach, encompassing improvements in data, architecture, and training methodologies. Initial efforts include using more robust training data and increasing image resolution, which are considered direct methods for reduction [8]. Woodpecker is identified as a pioneering work in explicitly addressing and correcting hallucination issues in MLLMs [32,33].

Strategies for mitigation can be broadly categorized into pre-correction, process correction, and post-correction methods [31]. Specific techniques employed include:
*   **Data and Input Processing**: Using image aspect ratio padding, which minimally reduces hallucination by preventing image cropping and preserving context [4]. Training with adversarial samples can enhance the model's ability to distinguish relevant from irrelevant content, thereby mitigating inconsistencies [11].
*   **Architectural and Training Enhancements**: Enhancing visual understanding through more powerful visual encoders or two-stage training methods can improve MLLMs' fidelity [11]. Finer-grained visual and textual modality alignment, focusing on local image features and corresponding text descriptions, is crucial for mitigating object hallucination [23]. Reinforcement Learning from Human Feedback (RLHF) has been adopted to improve fact grounding and reduce hallucination [4]. The LMDX paper highlights the necessity of a grounding mechanism to ensure generated answers are not hallucinated [34].
*   **Addressing Unimodal Biases**: A significant strategy for mitigating unimodal biases involves a causal perspective. Researchers quantify and reduce these biases by constructing causal graphs to model the influence of various factors on model predictions in Visual Question Answering (VQA) tasks. This approach utilizes Total Causal Effect (TCE) and Direct Causal Effect (DCE) to measure individual factor impacts [5]. To facilitate research in this area, the MORE dataset was created to test model susceptibility to unimodal biases and multi-hop reasoning capabilities. Furthermore, the Causality-enhanced Agent Framework (CAVE) is proposed, which decomposes complex multi-hop problems into sub-problems and leverages a causality-enhanced reasoner and a validator to ensure answer accuracy [5].
*   **Mitigating Specific Hallucination Mechanisms**:
    *   For the "knowledge aggregation pattern," often occurring after tokens with columnar attention features, the OPERA (Over-trust Penalty and Retrospection-Allocation strategy) has been developed. OPERA applies an over-trust logit penalty during decoding and a retrospection-allocation strategy that re-decodes from a summary token if strong knowledge aggregation is detected [5].
    *   To counter object hallucinations related to visual noise sensitivity, where blurred images increase reliance on language priors, Visual Contrastive Decoding (VCD) is proposed. VCD contrasts output distributions from original and distorted images, penalizing candidate words over-estimated only in distorted conditions [5].
    *   Recognizing the visual encoder as a primary source of MLLM shortcomings, research utilizes "CLIP-blind pairs" (images with high CLIP similarity but low DINOv2 similarity) and the MMVP benchmark to expose these visual limitations. To address this, Mixture-of-Features (MoF) methods, including Additive-MoF and Interleaved-MoF, combine features from CLIP and DINOv2 to enhance visual capabilities [5].

Despite these advancements, challenges remain. Future research needs to delve deeper into the root causes and solutions for multimodal hallucination, specifically examining the impact of current visual information tokenization methods, multimodal alignment paradigms, and potential conflicts between multimodal data and the knowledge stored in Large Language Models (LLMs) [8].
### 8.2 Bias and Fairness
Multimodal Large Language Models (MLLMs) are susceptible to biases primarily due to their training on extensive, often uncurated, datasets sourced from the internet. This training methodology can lead to the generation of inappropriate content, thereby impacting model performance and raising significant ethical implications [11].

To address these inherent biases, several mitigation strategies have been proposed. One fundamental approach involves incorporating explicit safety and fairness constraints directly into the training process of MLLMs [11]. These constraints guide the model during learning, steering it away from generating undesirable or biased outputs. Concurrently, utilizing finer data cleaning and filtering techniques is crucial [11]. This meticulous data preparation is foundational for creating debiased training datasets by removing or mitigating problematic samples, thereby reducing the propagation of harmful patterns found in raw internet data. Additionally, adversarial training techniques represent another promising avenue for bias mitigation, aiming to improve model robustness against biased inputs and outputs [11].

The emphasis on refined data cleaning and filtering [11] also underscores a broader strategy for developing MLLMs: the proactive creation of more diverse and representative datasets. By curating data that spans a wider range of demographics, cultures, and perspectives, and by systematically addressing underrepresentation or overrepresentation, researchers can inherently reduce the sources of bias. Such proactive data collection and curation, complemented by debiasing techniques, are essential steps towards building MLLMs that exhibit enhanced fairness and ethical behavior, ultimately leading to more robust and trustworthy applications.
### 8.3 Computational Cost and Efficiency
Multimodal Large Language Models (MLLMs) inherently demand substantial computational resources for both their training and subsequent deployment, primarily due to their formidable model sizes and the extensive datasets required for effective learning [1,11]. This significant computational overhead critically impedes their broader accessibility and application, particularly in environments with constrained resources [11,23]. Consequently, optimizing model efficiency has emerged as a paramount research direction, aiming to strike a balance between maintaining high performance and reducing the associated computational burden [1,35].

To mitigate these challenges and enable more lightweight deployment, various strategies are being explored. Key approaches include parameter-efficient fine-tuning (PEFT) techniques, model compression, and quantization methods [11]. For instance, LLaVA supports Low-Rank Adaptation (LoRA) training, a PEFT technique that significantly reduces GPU memory requirements during fine-tuning [4]. Furthermore, LLaVA also leverages 4-bit and 8-bit inference, which are forms of quantization that drastically cut down the memory footprint and computational load during inference [4].

Another prominent strategy for enhancing efficiency and scalability is the adoption of sparse activation architectures, such as Mixture of Experts (MoE) models [1]. MoE designs allow for the scaling of dense models by incorporating multiple "expert" sub-networks within the language model's feed-forward network layers. In this paradigm, only a select subset of these experts is activated for any given input, thereby reducing the active computational graph. An illustrative example is the MM1 model family, which explored scaling through MoE, designing both a 3B-MoE model with 64 experts and a 7B-MoE model with 32 experts, building upon prior work like GShard and ST-MoE [27]. This approach contributes to improved efficiency by selectively engaging parameters rather than activating the entire model. Beyond these, other crucial strategies for cost reduction and performance maintenance include distributed training, hardware acceleration, parameter sharing, and distillation, all contributing to the overarching goal of achieving parameter-efficient training and inference [1,11,23]. The continued development and refinement of these techniques are essential to unlock the full potential of MLLMs across diverse and resource-limited applications.
### 8.4 Long Context Processing and Complex Instruction Following
The ability of Multimodal Large Language Models (MLLMs) to process extensive contexts and execute intricate instructions remains a significant challenge, crucial for their effective application in complex real-world scenarios [3,8,28].

A primary limitation in current MLLMs is their propensity to operate predominantly in short-context environments, lacking the necessary capabilities to robustly handle the diverse and often extended contexts encountered in real-world applications [25]. This constraint restricts their utility in tasks requiring comprehensive understanding across vast multimodal data streams.

Concurrently, MLLMs frequently exhibit insufficient capabilities in adhering to and executing complex instructions. While models like GPT-4V demonstrate a notable capacity to interpret sophisticated commands—such as generating elaborate question-answer pairs or integrating intricate reasoning into outputs—most other MLLMs exhibit a considerable deficiency in this area, indicating substantial room for improvement [10]. Furthermore, some MLLMs struggle to produce expected outcomes even when provided with explicit instructions, underscoring a critical need for more effective instruction tuning strategies to enhance task coverage and improve generalization across diverse tasks [23]. This highlights a general challenge where current MLLMs may falter in translating nuanced human intent into precise machine actions, impeding their reliability and practical deployment.

To address these limitations, several promising research directions are being explored [8,28]. One key area is the development of robust mechanisms for multi-modal interleaved document understanding. This involves enabling MLLMs to process and comprehend information presented across various modalities (e.g., text, images, diagrams) that are interspersed within long documents, mirroring how humans naturally read and integrate information. Additionally, improving context learning is pivotal, focusing on methods that allow MLLMs to effectively encode, retain, and recall relevant information from long sequences, thereby overcoming the "short-context" barrier. Lastly, advancing chain-of-thought reasoning capabilities is crucial for enabling MLLMs to break down complex instructions into sequential, manageable steps, providing transparency in their decision-making process and improving the accuracy of their responses to intricate prompts. These research trajectories aim to bolster MLLMs' proficiency in understanding and acting upon rich, extended multimodal inputs.
### 8.5 Interpretability and Explainability
Interpretability and explainability are paramount in Multimodal Large Language Models (MLLMs) to foster trustworthiness and facilitate effective debugging. As MLLMs integrate and process information from diverse modalities, their internal decision-making processes become increasingly opaque, necessitating methods to understand their reasoning. For critical applications, such as medical diagnosis or autonomous systems, understanding *why* an MLLM produces a particular output is crucial for user confidence, regulatory compliance, and ensuring ethical deployment. Furthermore, interpretability aids in debugging, allowing developers to identify and rectify biases, errors, or undesirable behaviors that may arise from complex inter-modal interactions, thereby enhancing model robustness and reliability.

However, developing truly interpretable MLLMs presents significant challenges, primarily stemming from the intricate nature of multimodal data fusion and processing. Understanding the complex interactions between modalities—for instance, how visual cues influence text generation or how audio features impact image understanding—is inherently difficult. MLLMs often employ deep neural architectures, such as cross-attention mechanisms or sophisticated fusion layers, which combine information from different modalities in non-linear and high-dimensional spaces. This complexity makes it challenging to pinpoint which specific input features from which modality contribute most significantly to a given output or decision. Unlike unimodal models where attribution methods might focus on pixel-level or token-level importance, MLLMs require methods that can attribute contributions across heterogeneous data types and their combined representations. This multi-layered abstraction obscures the direct causal links between specific multimodal inputs and final predictions, complicating efforts to trace information flow and decision pathways.

To address these challenges and cultivate more transparent MLLMs, several strategies can be pursued. One approach involves developing novel **model-agnostic interpretation techniques** that can be applied universally, regardless of the underlying MLLM architecture. These methods often perturb inputs or analyze feature importance post-hoc to infer contributions. Another strategy focuses on **inherently interpretable model architectures**, designing MLLMs with built-in transparency mechanisms, such as attention visualization tools that explicitly highlight the regions of images or segments of text that the model attends to during processing. While traditional attention maps can offer some insights, more advanced techniques are needed to capture the nuances of cross-modal interactions. Further research could explore **causal inference methods** to establish direct causal relationships between specific multimodal inputs and outputs, moving beyond mere correlations. Developing **human-in-the-loop interpretability frameworks** where experts can interact with the model to test hypotheses about its behavior and gain insights is also a promising direction. Additionally, **concept-based explanations** can provide higher-level insights by identifying which human-understandable concepts (e.g., "object presence," "sentiment") the model has learned and utilizes in its decision-making, offering a more intuitive understanding than low-level feature attributions. The ultimate goal is to move beyond simply seeing *what* the model attends to, towards understanding *how* it integrates and reasons across modalities to arrive at its conclusions.
### 8.6 Emerging Directions and Paradigms

The evolution of Multimodal Large Language Models (MLLMs) is marked by a significant expansion of their generation capabilities across diverse modalities, moving beyond traditional text-to-text (T2T) interactions. This includes advancements in generating images (T2I), music (T2M), video (T2V), 3D content (T2-3D), and even integrating human-machine interfaces (T2HM) [21]. Core technologies enabling this cross‐modal generation often involve converting various input modalities, such as vision, speech, and 3D data, into discrete representations rather than high‐dimensional vectors. This approach facilitates decoupling perception‐side models from the large language models (LLMs), allowing for more flexible and modular architectures [30]. Models like NExT-GPT and Unified-IO 2 exemplify this trend by connecting LLMs with multimodal adapters and diffusion decoders to generate diverse outputs including video, audio, and 3D data [11]. This indicates a clear shift towards unified models capable of handling and generating multiple data types, evolving from specialized models to more general-purpose multimodal assistants [34].

Future research directions are poised to significantly impact the MLLM landscape. One crucial area is the expansion of self-supervised learning (SSL) to new modalities, although specific mechanisms for this are still under exploration. Another critical aspect involves modularizing expert mechanisms, which aligns with the trend of decoupling perception models from LLMs and developing more specialized models for specific domains [30,35]. This modularity can lead to unified visual models and LLM-supported multimodal large models, enhancing overall system flexibility and performance [37].

Developing chain-of-thought reasoning for non-textual modalities represents another significant challenge and opportunity. While multimodal in-context learning and chain-of-thought research are still in preliminary stages, it is recognized that multimodal reasoning is inherently more complex than pure text reasoning due to the increased information sources and intricate logical relationships involved [8,10]. Improving the model's ability to focus on context, as demonstrated by models trained on interleaved image-text data, is crucial for advancing few-shot performance [8]. Furthermore, leveraging LLM's embedded knowledge and capabilities, often with external tools, can lead to robust visual reasoning systems without necessarily relying on end-to-end training [8].

Enhancing object orientation understanding through egocentric instruction tuning is also identified as a promising direction, particularly in frameworks such as EgoOrientBench [22]. This approach aims to address critical challenges like object hallucination and improve instruction following in MLLMs [23].

A major research hotspot is the development of MLLM-based agents, which necessitates comprehensive improvements in the model's perception, reasoning, and planning capabilities to achieve real-world applications [10,37]. Projects like LLaVA-Plus exemplify this by enabling MLLMs to learn and utilize tools for creating multimodal agents [4]. The progression from specialized models to general-purpose visual assistants further underscores the importance of agent development [34].

The field also faces the challenge of enabling MLLMs to continuously learn (continual learning) without experiencing catastrophic forgetting. This is crucial for models to adapt to new data and tasks over time while retaining previously acquired knowledge.

Finally, a fundamental question within MLLM development revolves around the interplay between large-scale multimodal and single-modal data during joint training. Despite the common practice of unfreezing LLMs during training and adding single-modal text data, systematic and in-depth research is still lacking to definitively determine whether this co-training mutually benefits or harms each other [10]. Addressing this will be vital for optimizing MLLM training strategies and improving overall model performance. Future research must also focus on balancing and correcting biases between language and visual modalities, and incorporating updating mechanisms or domain databases to mitigate hallucination risks and address the current lack of deep "language understanding" in MLLMs [5].
## 9. Conclusion
This survey has provided a comprehensive overview of Multimodal Large Language Models (MLLMs), dissecting their foundational architectures, intricate training strategies, robust evaluation methodologies, and diverse real-world applications. Across the architectural landscape, MLLMs are characterized by their ability to integrate various modalities, driven by advancements in visual encoders and the strategic use of diverse pre-training data. For instance, the MM1 family of models exemplifies the importance of factors such as image resolution, visual encoder loss and capacity, and pre-training data, alongside the critical role of interleaved and text-only training for robust performance across different tasks [27]. Furthermore, the field is witnessing efforts to categorize alignment strategies, including parameter-tuning and parameter-frozen approaches, to bridge the gap between different modalities [13].

Evaluation, a cornerstone for advancing the field, has seen the proliferation of over 200 benchmarks categorized by perception, cognition, and domain, providing essential resources for researchers [26]. Benchmarks like SEED-Bench have been introduced to specifically assess generative comprehension capabilities in MLLMs [15]. Despite these advancements, limitations persist, particularly in accurately interpreting object orientation, a challenge being addressed through specialized approaches like egocentric instruction tuning and evaluation benchmarks such as EgoOrientBench [22]. MLLMs have demonstrated their potential across numerous applications, from enhancing autonomous driving systems through improved user-vehicle interaction and motion planning [36] to enabling more comprehensive AI systems that process diverse data types [24,29]. However, challenges remain, such as limitations in complex computer vision tasks like accurate object detection and counting [17].

The significance of MLLMs in advancing AI research cannot be overstated. By enabling machines to understand and process diverse data types, MLLMs push the boundaries towards more comprehensive and intelligent systems, ultimately driving AI closer to Artificial General Intelligence (AGI) [24,29]. They represent a paradigm shift, moving beyond text-centric intelligence to simulate human-like perception and understanding across modalities [37]. Current trends indicate an evolution towards greater intelligence and efficiency, characterized by larger model scales, support for more modalities, enhanced real-time interaction, specialization in vertical fields, and optimized model efficiency [9,35].

Despite the rapid progress, several challenges and promising research directions have been identified. The technology stack for multimodal large models has not yet fully converged, with multimodal learning and cross-modal alignment remaining significant technical hurdles [37]. Key challenges include reducing hallucination, preventing harmful content, and lowering computational costs, especially given the push towards wider adoption in resource-constrained environments [1,11]. Fragile reasoning chains, limited perception, and object hallucination are also critical issues that demand further attention [23]. Future research should prioritize improving reasoning capabilities, integrating large visual foundation models, and achieving finer-grained modality alignment [23,25]. The next wave of generative AI is anticipated to be driven by fundamentally multi-modal models capable of reasoning, perceiving, and creating across domains in a unified and adaptive manner [21]. Addressing challenges in modal expansion, training time, lifelong learning, and catastrophic forgetting will be crucial for achieving AGI [29].

In conclusion, the field of MLLMs is vibrant and dynamic, presenting immense potential to address complex real-world problems. We encourage researchers to tackle the identified challenges—ranging from enhancing model efficiency and robustness to improving multimodal reasoning and mitigating limitations like hallucination and domain-specific inaccuracies. Continuous innovation in architectures, training strategies, and comprehensive evaluation methodologies is paramount. By fostering collaborative efforts and focusing on these critical areas, the research community can collectively contribute to the further advancement of MLLMs, unlocking their full potential and ushering in an era of more intelligent, versatile, and human-centric AI systems.

## References

[1] Efficient Multimodal LLMs: A Survey [https://github.com/swordlidev/Efficient-Multimodal-LLMs-Survey](https://github.com/swordlidev/Efficient-Multimodal-LLMs-Survey) 

[2] Multimodal Large Language Models: A Survey [http://www.paperreading.club/page?id=195408](http://www.paperreading.club/page?id=195408) 

[3] MM-LLMs: A Survey of Recent Advances in MultiModal Large Language Models [https://readpaper.com/paper/2153289143097548288](https://readpaper.com/paper/2153289143097548288) 

[4] LLaVA: Large Language and Vision Assistant [https://github.com/HaoRenkk123/LLaVA](https://github.com/HaoRenkk123/LLaVA) 

[5] 多模态大语言模型幻觉成因与缓解方法研究 [https://www.modb.pro/db/1898924166858944512](https://www.modb.pro/db/1898924166858944512) 

[6] 近期多模态大模型 (MLLM) 总结：结构、训练与表现 [https://zhuanlan.zhihu.com/p/655770704](https://zhuanlan.zhihu.com/p/655770704) 

[7] 多模态大语言模型评估基准全面综述 [https://zhuanlan.zhihu.com/p/1890407658248464000](https://zhuanlan.zhihu.com/p/1890407658248464000) 

[8] 多模态大语言模型综述 [https://aidc.shisu.edu.cn/c2/8f/c13626a180879/page.htm](https://aidc.shisu.edu.cn/c2/8f/c13626a180879/page.htm) 

[9] 2024主流AI多模态大模型盘点与应用指南 [https://baijiahao.baidu.com/s?id=1836701297484734844&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1836701297484734844&wfr=spider&for=pc) 

[10] 多模态大语言模型综述重大升级，GitHub 8.3K Stars！ [https://juejin.cn/post/7356175306938286116](https://juejin.cn/post/7356175306938286116) 

[11] 多模态大模型进化之路：arXiv最热NLP论文解读 [https://baijiahao.baidu.com/s?id=1793091313508699038&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1793091313508699038&wfr=spider&for=pc) 

[12] 多模态大语言模型综述论文解读：MM-LLMs [https://zhuanlan.zhihu.com/p/689385182](https://zhuanlan.zhihu.com/p/689385182) 

[13] Multilingual Large Language Models: A Survey of Resources, Taxonomy, and Frontiers [https://multilingual-llm.net/](https://multilingual-llm.net/) 

[14] MME-RealWorld: A High-Resolution, Real-World MLLM Benchmark Challenging Human-Level Performance [http://www.paperreading.club/page?id=247963](http://www.paperreading.club/page?id=247963) 

[15] SEED-Bench: 评估具有生成理解能力的多模态LLM [http://www.paperreading.club/page?id=176503](http://www.paperreading.club/page?id=176503) 

[16] OmniGenBench: A Comprehensive Benchmark for Omnipotent Multimodal Generation [https://ui.adsabs.harvard.edu/abs/arXiv:2505.18775](https://ui.adsabs.harvard.edu/abs/arXiv:2505.18775) 

[17] MLLM如何重塑计算机视觉：深层机制揭秘与模型测评 [https://mp.weixin.qq.com/s?__biz=MzU2NjU3OTc5NA==&mid=2247587721&idx=1&sn=2b115ec0e5298310958894b9f6f3f2d2&chksm=fdbfc91ddf5508bbfcd440a9a1858740383070303540d5f3d26d05ef401026fc63a2c3f87490&scene=27](https://mp.weixin.qq.com/s?__biz=MzU2NjU3OTc5NA==&mid=2247587721&idx=1&sn=2b115ec0e5298310958894b9f6f3f2d2&chksm=fdbfc91ddf5508bbfcd440a9a1858740383070303540d5f3d26d05ef401026fc63a2c3f87490&scene=27) 

[18] MLLM/vLLM 常用 Benchmark 总结 [https://zhuanlan.zhihu.com/p/31375584664](https://zhuanlan.zhihu.com/p/31375584664) 

[19] 多模态大语言模型综述：技术、应用与未来方向 [https://zhuanlan.zhihu.com/p/641866192](https://zhuanlan.zhihu.com/p/641866192) 

[20] AI日报：清华研究无需奖励模型微调扩散模型 [https://hub.baai.ac.cn/view/32929](https://hub.baai.ac.cn/view/32929) 

[21] 多模态大型语言模型最新综述：突破文本边界，迈向通用AI [https://baijiahao.baidu.com/s?id=1835117405409053646&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1835117405409053646&wfr=spider&for=pc) 

[22] Egocentric Instruction Tuning Enhances Object Orientation Understanding in MLLMs [http://www.paperreading.club/page?id=268402](http://www.paperreading.club/page?id=268402) 

[23] 多模态大语言模型综述：关键技术、挑战与未来方向 [https://baijiahao.baidu.com/s?id=1770171893845054228&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1770171893845054228&wfr=spider&for=pc) 

[24] 多模态大模型盘点：国内外AI新势力角逐 [https://zhuanlan.zhihu.com/p/1908195617349559886](https://zhuanlan.zhihu.com/p/1908195617349559886) 

[25] 多模态推理综述：探索大模型的推理能力 [https://zhuanlan.zhihu.com/p/714058080](https://zhuanlan.zhihu.com/p/714058080) 

[26] MLLM Benchmarks: A Comprehensive Survey [https://github.com/Timothyxxx/Evaluation-Multimodal-LLMs-Survey](https://github.com/Timothyxxx/Evaluation-Multimodal-LLMs-Survey) 

[27] MM1: Building High-Performance Multimodal LLMs Through Architecture and Data Ablations [https://ar5iv.labs.arxiv.org/html/2403.09611](https://ar5iv.labs.arxiv.org/html/2403.09611) 

[28] 多模态大语言模型（MLLMs）综述：架构、训练与扩展 [https://blog.csdn.net/weixin_42475060/article/details/147985504](https://blog.csdn.net/weixin_42475060/article/details/147985504) 

[29] 多模态大模型论文分享(1)：综述与技术指南 [https://blog.csdn.net/weixin_41964296/article/details/140544471](https://blog.csdn.net/weixin_41964296/article/details/140544471) 

[30] CVPR 2024：视觉AI Foundation Model 发展现状与突破 [https://cloud.tencent.com/developer/article/2434192](https://cloud.tencent.com/developer/article/2434192) 

[31] 多模态大型语言模型（MLLM）最新进展综述 [https://baijiahao.baidu.com/s?id=1821393355881312370&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1821393355881312370&wfr=spider&for=pc) 

[32] Awesome Multimodal Large Language Models [https://github.com/mbrukman/Awesome-Multimodal-Large-Language-Models](https://github.com/mbrukman/Awesome-Multimodal-Large-Language-Models) 

[33] Awesome Multimodal Large Language Models: Survey, VITA, and Benchmarks [https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) 

[34] 爱可可 AI 前沿推介(9.25)：扩散模型、强化学习、多模态、文档信息提取与生物语言 [https://zhuanlan.zhihu.com/p/658179933](https://zhuanlan.zhihu.com/p/658179933) 

[35] 2024主流AI多模态大模型超全指南 [https://baijiahao.baidu.com/s?id=1836704419860221929&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1836704419860221929&wfr=spider&for=pc) 

[36] MLLM：自动驾驶的新解法 [https://www.thepaper.cn/newsDetail_forward_25773965](https://www.thepaper.cn/newsDetail_forward_25773965) 

[37] 多模态大模型：探索、研究方向与应用领域 [https://xueqiu.com/8558902897/284551232](https://xueqiu.com/8558902897/284551232) 

[38] Guang-Neng Hu's Publications [https://dblp.uni-trier.de/pid/165/3128.html](https://dblp.uni-trier.de/pid/165/3128.html) 

[39] 2025年2月24日多模态大模型论文速递 [https://zhuanlan.zhihu.com/p/26322903740](https://zhuanlan.zhihu.com/p/26322903740) 

[40] 一译文档翻译平台 [https://www.yiyibooks.cn/__src__/arxiv/2401.13601v4/index.html](https://www.yiyibooks.cn/__src__/arxiv/2401.13601v4/index.html) 

