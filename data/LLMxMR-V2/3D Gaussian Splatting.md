# A Survey on 3D Gaussian Splatting

# 0. A Survey on 3D Gaussian Splatting

## 1. Introduction
The accurate and efficient reconstruction of 3D scenes and synthesis of novel views are foundational challenges in computer graphics and vision. Prior to recent advancements, Neural Radiance Fields (NeRF) emerged as a revolutionary technique for novel view synthesis, capable of rendering highly realistic scenes by representing 3D information as an implicit neural function [4,24]. However, NeRF and its early variants presented significant limitations, particularly concerning high computational costs during both training and rendering, especially for high-resolution outputs, often necessitating compromises in visual quality for improved speed [24,38]. This implicit representation also made direct scene editing and manipulation challenging.

Addressing these critical limitations, 3D Gaussian Splatting (3DGS) has rapidly gained prominence since its introduction in June 2023, notably recognized as the best paper at SIGGRAPH 2023 [23,31,38]. 3DGS offers distinct advantages, including real-time rendering capabilities, high visual fidelity, and significantly faster optimization times, positioning it as a transformative technique in 3D reconstruction and representation [1,11,16,25,27,36].



**Comparison of 3DGS and NeRF**

| Feature            | Neural Radiance Fields (NeRF)                             | 3D Gaussian Splatting (3DGS)                               |
| :----------------- | :-------------------------------------------------------- | :--------------------------------------------------------- |
| **Representation** | Implicit neural function (MLP)                            | Explicit scene of millions of anisotropic 3D Gaussians     |
| **Mechanism**      | Volumetric rendering (ray marching, MLP queries)          | Splatting and rasterization (GPU-accelerated)              |
| **Training Time**  | High (hours to days for high quality)                     | Significantly faster (~30 minutes)                         |
| **Rendering Speed**| Slow (seconds per frame, non-real-time)                   | Real-time (≥ 30 FPS at 1080p)                              |
| **Editability**    | Challenging (implicit representation)                     | Easier (explicit structure allows direct manipulation)     |
| **Fidelity**       | High, but computationally intensive                       | High, often fewer artifacts than NeRF                      |
| **Memory Cost**    | High for high resolution                                  | High, but actively optimized                               |

When compared to NeRF, 3DGS differentiates itself fundamentally across several key aspects. In terms of **representation**, NeRF models scenes implicitly using coordinate-based neural networks, whereas 3DGS employs an explicit scene representation consisting of millions of anisotropic 3D Gaussian ellipsoids [1,4,10,14]. This explicit structure not only enhances interpretability but also facilitates subsequent tasks such as dynamic reconstruction, geometric editing, and physical simulation [4,6,11]. Regarding **rendering mechanisms**, 3DGS replaces NeRF's computationally intensive volume rendering with an efficient, perceptually-driven splatting and rasterization pipeline [12,24,31,32]. This leverages modern GPU rasterization capabilities, enabling high rendering speeds through custom CUDA kernel implementations [10]. Consequently, in terms of **performance metrics**, 3DGS achieves comparable novel view synthesis quality with substantially faster convergence times (e.g., training times reduced to approximately 30 minutes) and real-time rendering speeds (e.g., ≥ 30 frames per second at 1080p resolution) [11,16,23,25,38]. Furthermore, 3DGS often exhibits fewer visual artifacts and failure cases compared to NeRF [23].

The fundamental components of 3DGS include its explicit **3D Gaussian representation**, where each Gaussian is defined by its position, covariance (shape and orientation), opacity, and spherical harmonics coefficients for color and appearance [12]. The **splatting** mechanism involves projecting these 3D Gaussians onto a 2D image plane and blending them based on depth and alpha values using an efficient, differentiable rasterizer [24,31]. The **optimization** process iteratively refines these Gaussian parameters, employing adaptive density control strategies such as cloning and pruning to dynamically adjust the number and distribution of Gaussians based on reconstruction needs [12,24,31].

The efficiency, fidelity, and versatility of 3DGS have spurred diverse applications across various domains. These include virtual reality (VR) and augmented reality (AR) experiences, autonomous driving, film and entertainment production, and interactive media [5,6,27]. It is also actively being explored for dynamic and deformable object reconstruction, text-to-3D generation, general 3D editing tasks, simultaneous localization and mapping (SLAM), large-scale urban scene synthesis, and specialized fields such as medical imaging and scientific data visualization where sparse or unevenly distributed points require intuitive representation [3,4,30,36,37].

This survey aims to provide a comprehensive and systematic overview of 3D Gaussian Splatting, encompassing its underlying principles, recent advancements, and diverse applications [1,3,15]. We will delve into the technical challenges faced by 3DGS, such as memory footprint and overfitting in sparse view scenarios, and explore various proposed solutions and optimizations [3,7,20]. By categorizing existing research, evaluating performance, and identifying current limitations, this survey intends to serve as a valuable resource for researchers and practitioners, facilitating a deeper understanding of 3DGS and inspiring future advancements in explicit radiance field representation and novel view synthesis [1,4,14,16].
## 2. Background and Related Work

![Key Advancements for NeRF Acceleration](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/UpBQYS96iGHUfmv4UpLDw_/home/surveygo/data/requests/13535/survey/imgs/Key%20Advancements%20for%20NeRF%20Acceleration.png)


The field of 3D scene representation and rendering has undergone a profound evolution, driven by the continuous pursuit of higher fidelity, efficiency, and flexibility. This section provides a comprehensive historical context, tracing the progression from traditional explicit 3D data formats to advanced neural implicit representations, detailing the challenges inherent in each, and setting the stage for the emergence of 3D Gaussian Splatting (3DGS).

Historically, 3D scenes were predominantly modeled using traditional explicit representations such as point clouds, polygonal meshes, and voxels [3]. Point clouds, while offering high geometric detail, lack a structured topology. Meshes provide well-defined structures but struggle with complex visual effects and often require extensive manual intervention for imperfections. Voxel representations discretize space, resulting in a mesh-like structure but incurring substantial computational memory demands for high resolutions [23]. While these explicit methods allow direct geometric access and relatively straightforward rendering, they generally lack topological flexibility and struggle to achieve realistic appearances without significant optimization. Their differentiability for scene optimization is also less direct compared to continuous representations [8]. The limitations of these traditional explicit representations in capturing intricate details and ensuring differentiability motivated the shift towards neural implicit representations. These methods, exemplified by Neural Radiance Fields (NeRF), utilize neural networks to encode 3D scene information, offering superior capabilities in modeling arbitrary geometries and rendering highly realistic, view-dependent colors and lighting effects [6,11]. However, this realism comes at a significant computational cost, particularly in training and rendering speed. Comparing these explicit and implicit paradigms reveals distinct trade-offs: explicit methods generally offer faster rendering but may lack photorealism and struggle with topological changes, while implicit methods excel in rendering quality and differentiability but often suffer from high computational demands and indirect data access [6,8].

Complementing scene representations, traditional 3D reconstruction methods have long relied on geometric principles to derive 3D models from 2D images. Structure from Motion (SfM) techniques estimate camera parameters and reconstruct sparse 3D point clouds by identifying and matching key points across multiple views, followed by refinement through bundle adjustment [30,31]. Multi-View Stereo (MVS) then leverages the output of SfM to generate denser 3D reconstructions by inferring depth information for each pixel, encompassing various approaches like plane sweeping and depth map fusion [30]. Despite their foundational role, SfM and MVS methods face notable limitations, including susceptibility to errors leading to unreconstructed or over-reconstructed regions, and high GPU memory consumption due to extensive storage and processing requirements for dense geometric data [6,24,33]. These limitations restrict their scalability and real-time applicability in complex environments.

The advent of Neural Radiance Fields (NeRF) marked a significant paradigm shift in novel view synthesis, offering unprecedented photorealism by representing a scene as a continuous radiance field using Multi-Layer Perceptrons (MLPs) [30,31]. NeRF maps spatial coordinates and viewing directions to view-dependent color and volumetric density, synthesizing images through differentiable volume rendering by casting rays and integrating sampled colors and densities along them. The process is mathematically represented as:
$$
C(r) = \int_{t_n}^{t_f} T(t) \, \sigma(r(t)) \, c(r(t), d) \, dt
$$
where \(T(t)\) is the accumulated transmittance along the ray from near bound \(t_n\) to point \(t\):
$$
T(t) = e^{-\int_{t_n}^{t} \sigma(r(s)) \, ds}
$$
Here, \(\sigma(r(t))\) is the volumetric density and \(c(r(t), d)\) is the direction-dependent color [4,30]. While NeRF's fidelity is remarkable, its major drawbacks lie in its substantial computational demands, particularly slow training and rendering speeds due to extensive sampling and MLP queries [8,24]. To address these inefficiencies, significant research has focused on accelerating NeRF, incorporating innovations such as spatial data structures (e.g., multi-resolution hash tables in Instant-NGP), various encoding schemes, and MLP capacity optimization (e.g., KiloNeRF) [12,24]. However, these advancements often entail a trade-off with image quality or still require extensive per-scene training, posing persistent challenges that 3DGS aims to overcome [15].

Finally, point-based rendering (PBR) techniques constitute a distinct approach for visualizing 3D scenes by utilizing discrete geometric primitives, offering efficiency for complex or unstructured data [6]. Traditional PBR, however, suffers from visual artifacts such as holes, aliasing, and discontinuities, particularly in sparse regions where rendering can appear blurred [24,39]. To mitigate these issues, "splatting" methods were introduced. Splatting projects point primitives onto regions larger than a single pixel, often as geometric shapes like ellipses, enabling smoother transitions and reducing artifacts by allowing splats to overlap and form continuous images [4,24]. 3D Gaussian Splatting builds fundamentally upon this splatting paradigm, representing scenes as a collection of anisotropic 3D Gaussians that are projected onto the 2D image plane as ellipses for rendering [31,32]. This explicit yet differentiable representation allows 3DGS to achieve high-quality rendering with significantly improved training and real-time rendering speeds compared to NeRF, effectively addressing many of the limitations of prior 3D scene representation and rendering techniques.
### 2.1 Evolution of 3D Data Representations
The landscape of 3D data representations has undergone a significant transformation, progressing from traditional explicit forms to sophisticated neural implicit methods, and more recently, hybrid approaches like 3D Gaussian Splatting. This evolution has been driven by the need to overcome limitations in photorealism, topological flexibility, and computational efficiency inherent in earlier methods [3].

Initially, 3D scenes were predominantly modeled using traditional explicit representations such as point clouds, polygonal meshes, and voxels [4,6,11,23,24]. These representations directly define geometric primitives. Point clouds, collections of 3D coordinates, offer high geometric detail but lack a mesh-like structure, which often prevents direct convolution operations [23]. Meshes, composed of interconnected vertices, edges, and faces, are effective for representing regular structures and are easily rasterized on GPUs [12]. However, they struggle with complex visual phenomena like reflections or transparencies and often require extensive manual correction for imperfections [27]. Voxel representations discretize space into a grid of volumetric elements, leading to a mesh-like structure but incurring high computational memory demands, particularly for high-resolution scenes [23]. While offering direct geometric access and relatively straightforward rendering pipelines, these explicit methods typically lack flexibility in topology and struggle to achieve realistic appearance, often requiring significant effort for optimization or capturing intricate details [8]. Their differentiability for scene optimization is also less direct compared to continuous representations.

The limitations of traditional explicit representations motivated the shift towards neural implicit representations, which utilize neural networks to encode 3D scene information [6,33]. A prominent example is Neural Radiance Fields (NeRF), which models geometry using an implicit density field and predicts view-dependent colors through an appearance field [4,11]. NeRF operates by learning the light distribution from multiple input images, enabling the generation of highly realistic visual scenes, adeptly capturing subtle lighting nuances and material details [27]. Neural implicit methods excel at accurately modeling arbitrary geometries and are inherently continuous and differentiable, which is highly advantageous for optimization tasks [8,12]. However, this comes at a significant computational cost: NeRF's high demands and slow rendering speeds limit its applicability in real-time interactive and mobile applications, especially for high-resolution scenes, as rendering typically involves extensive ray marching and neural network queries for each pixel [8,27]. Moreover, accessing specific geometric information is indirect, requiring queries to the neural network.

3D Gaussian Splatting (3DGS) represents a notable advancement, striking a unique balance between discrete and continuous representations [31]. Unlike purely implicit models, 3DGS can be characterized as an explicit radiance field, where the 3D scene is modeled by a collection of numerous 3D Gaussians [26]. Each Gaussian is continuous and differentiable internally, yet the overall scene is represented by a discrete set of these Gaussians [31]. This explicit yet continuous formulation allows 3DGS to achieve impressive rendering quality and real-time performance, addressing some of the computational bottlenecks of pure implicit representations like NeRF, while retaining critical properties for optimization.

In summary, the evolution reflects a progression from directly accessible but less flexible explicit representations (point clouds, meshes, voxels) to highly photorealistic and optimizable, yet computationally intensive, neural implicit representations (NeRF), and finally to hybrid or explicit radiance fields (3DGS) that balance quality with efficiency. The trade-offs across these representation types involve:
*   **Compactness**: Traditional methods can be memory-intensive (especially voxels) but simple for sparse data like point clouds. Neural implicits are compact, encoded in network weights, while 3DGS, though explicit, is efficient due to its Gaussian parameterization.
*   **Differentiability**: Neural implicit representations inherently offer end-to-end differentiability, crucial for gradient-based optimization, a capability less direct or absent in many traditional methods. 3DGS also offers differentiability, facilitating its optimization.
*   **Computational Cost**: Traditional methods allow fast rasterization, but scene manipulation or photorealism often requires extensive processing. Neural implicit methods, particularly NeRF, have high computational costs for rendering due to ray marching and network inference, while 3DGS significantly reduces this, enabling real-time performance.
*   **Ease of Access**: Traditional explicit methods provide direct access to geometric data. Neural implicit representations require querying the network, making direct manipulation or inspection challenging. 3DGS offers a more direct, yet still abstract, access to its Gaussian primitives. This progression highlights a continuous effort to achieve higher fidelity, better optimization capabilities, and improved computational efficiency in 3D scene representation.
### 2.2 Traditional Scene Reconstruction and Rendering
Traditional algorithms for 3D reconstruction and new view synthesis primarily encompass Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques [23]. Both methodologies fundamentally rely on geometric principles to guide the reprojection and fusion of input images for scene reconstruction [6,24,33].

Structure from Motion (SfM) is a well-established method designed to estimate camera parameters and reconstruct a sparse 3D point cloud from a collection of unordered 2D images [30,31]. The process typically begins with the extraction and matching of key points across multiple overlapping images. These matched 2D points are then triangulated to derive their corresponding 3D positions. A crucial subsequent step is bundle adjustment, which refines both the camera poses and the 3D point coordinates simultaneously, minimizing the reprojection error and yielding a consistent sparse 3D reconstruction [30,31]. Tools such as COLMAP are commonly employed to execute these SfM stages, performing feature extraction, exhaustive feature matching, and reconstruction [34]. The sparse point clouds and estimated camera poses generated by SfM often serve as essential initialization data for more advanced 3D representation techniques like 3D Gaussian Splatting [3,10,21,31].

Complementing SfM, Multi-View Stereo (MVS) focuses on achieving a dense 3D reconstruction by utilizing detailed 3D information from each pixel within the input 2D images, rather than relying solely on sparse key points [30]. MVS methods encompass various approaches, including plane sweeping, stereo vision, and depth map fusion, and are broadly categorized into volume-based or point cloud-based techniques [30]. While MVS produces more visually complete and dense models, it is inherently more computationally and memory-intensive compared to SfM. Consequently, MVS often necessitates the output of SfM, such as accurate camera poses, as a crucial preprocessing step [30].

Despite their foundational significance, traditional SfM and MVS methods face notable limitations. A primary challenge is their susceptibility to errors, manifesting as difficulties in recovering from unreconstructed regions (due to insufficient data) or over-reconstructed regions (containing redundant or erroneous information) [6,12,24,33]. This sensitivity to input quality and scene complexity can lead to compromised model accuracy and completeness. Furthermore, these techniques, particularly MVS, demand substantial computational resources. They incur high GPU memory costs, primarily due to the extensive storage and processing requirements for numerous input images and their associated geometric data [6,24,33]. This considerable resource footprint limits their scalability and applicability, especially in scenarios demanding real-time performance or the reconstruction of vast, intricate environments.
### 2.3 Neural Radiance Fields (NeRF)
Neural Radiance Fields (NeRF) represent a pivotal advancement in novel view synthesis, establishing a dominant paradigm for reconstructing 3D scenes from 2D images [30]. At its core, NeRF employs Multi-Layer Perceptrons (MLPs) to implicitly represent a scene as a continuous radiance field, mapping spatial coordinates $(x, y, z)$ and viewing directions $(\theta, \phi)$ to view-dependent color ($c$) and volumetric density ($\sigma$) [6,31,36]. This neural representation facilitates the learning of detailed light distributions, enabling the generation of highly realistic visual scenes, particularly adept at handling complex lighting and intricate material details [27].

The process of image synthesis in NeRF involves differentiable volume rendering. For each pixel, rays are cast through the scene, and samples are collected along these rays. The pixel color $C(r)$ is then computed by integrating the sampled colors and densities along the ray, employing an alpha-blending technique [4,30]. The mathematical formulation for this process is given by:
$$
C(r) = \int_{t_n}^{t_f} T(t) \, \sigma(r(t)) \, c(r(t), d) \, dt
$$
where $T(t)$ represents the accumulated transmittance along the ray from the near bound $t_n$ to point $t$, calculated as:
$$
T(t) = e^{-\int_{t_n}^{t} \sigma(r(s)) \, ds}
$$
Here, $\sigma(r(t))$ is the volumetric density and $c(r(t), d)$ is the direction-dependent color sampled from the learned radiance field [30]. Scene learning occurs through pixel-level supervised learning using photometric losses and backpropagation [30].

Despite NeRF's remarkable success in achieving high-fidelity novel view synthesis, its primary limitations lie in its computational demands, specifically the slow training and rendering speeds [8,16,24,25,39]. This inefficiency stems from the extensive sampling requirements, often necessitating 128 points per ray to ensure high-quality rendering [8,11]. Furthermore, NeRF-based methods can struggle with efficiently representing empty regions and exhibit high dependency on the specific scene and image capture conditions [24].

To address these performance bottlenecks, significant research has focused on accelerating NeRF's training and rendering. Key advancements in NeRF-based methods for improving efficiency include the utilization of spatial data structures, various encoding schemes, and optimization of MLP capacity [12,24].

Spatial Data Structures for Volumetric Ray-Marching Features and Interpolation: These structures are crucial for efficient feature storage and interpolation, particularly for volumetric ray-marching. Prominent examples include Instant Neural Graphics Primitives (Instant-NGP) and Plenoxels [24]. Instant-NGP, for instance, employs multi-resolution hash tables to store and query features, significantly speeding up the process by allowing faster access to spatial information.

Encoding Schemes: The choice of encoding scheme plays a vital role in NeRF's efficiency. Different encoding techniques are designed to transform input coordinates into a higher-dimensional space that is more amenable for the MLP to learn the complex radiance field, thereby accelerating convergence and improving overall performance [12,24]. The effectiveness of these schemes can, however, be highly dependent on the characteristics of the scene and the manner in which images are captured [12,24].

MLP Capacity Optimization: Optimizing the capacity of the Multi-Layer Perceptron networks is another critical avenue for performance enhancement. This involves strategies such as reducing the network size or, conversely, employing a multitude of smaller, specialized MLPs, as seen in methods like KiloNeRF [12,24]. Additionally, techniques like 4D tensor factorization have been explored to optimize network representations and accelerate processing.

Despite these sophisticated acceleration methods, a common trade-off is the potential sacrifice of synthesized image quality, particularly when rendering at high resolutions. These persistent challenges, coupled with the need for per-scene retraining and dense input samples, have motivated the exploration of alternative scene representation and rendering techniques, such as 3D Gaussian Splatting (3DGS) [15]. While 3DGS shares the fundamental image formation model with NeRF, it diverges significantly in its scene representation and rendering approach, bypassing the need for neural network queries and extensive point sampling to achieve substantial speed improvements [10,14]. Nevertheless, NeRF's foundational volumetric rendering and implicit representation capabilities have undeniably paved the way for subsequent advancements in novel view synthesis [5,29].
### 2.4 Point-Based Rendering
Point-based rendering (PBR) constitutes a fundamental approach in computer graphics for visualizing 3D scenes by utilizing discrete geometric primitives, such as points, instead of traditional polygonal meshes [4,6]. This technique is particularly effective for efficiently rendering complex, unstructured, or sparse geometric data, offering a distinct advantage over methods reliant on explicit polygonal representations [6,24,33]. Early PBR methods focused on generating high-quality results from given geometries, while more recent advancements explore its integration with neural implicit representations for 3D reconstruction without requiring initial geometric models [11].

Despite its efficiency in handling diverse geometries, traditional point sampling in PBR suffers from significant limitations. Key issues include the prevalence of visual artifacts such as holes, aliasing, and discontinuities in the rendered output [6,12,24,33]. Specifically, in sparse point cloud regions, rendering results can appear blurred [39]. Furthermore, differentiable point-based rendering methods, while capable of real-time performance, have historically relied on Multi-View Stereo (MVS) for geometry initialization, inheriting artifacts from under-reconstructed or over-reconstructed areas—particularly in featureless, shiny regions, or thin structures [12,24].

To mitigate these issues, “splatting” methods were introduced, significantly enhancing the quality of point-based rendering. The core principle of splatting involves projecting point primitives onto a region larger than a single pixel [24]. Instead of rendering each point as a single pixel, these methods expand the influence of each point to cover multiple pixels, often as geometric shapes like circles, ellipses, or ellipsoids [4,12,24]. This approach allows for smoother transitions, reduces the appearance of holes, and minimizes aliasing effects by enabling splats to overlap and form more continuous images [4]. Early splatting techniques by Zwicker, for instance, demonstrated the effectiveness of rendering ellipsoids to produce hole-free images [4]. Subsequent improvements focused on anti-aliasing, enhancing rendering efficiency, and addressing discontinuous shading through mechanisms like texture filters [4,14].

3D Gaussian Splatting (3DGS) fundamentally builds upon this point-based rendering and splatting paradigm [5,10,31,32]. In 3DGS, a scene is represented as a point cloud where each point is uniquely defined as an anisotropic 3D Gaussian [10,32]. These 3D Gaussian ellipsoids are then projected onto the 2D image plane as ellipses for rendering, akin to projecting “snowballs” that accumulate to form the final image [31,36]. This method achieves high-quality explicit scene representation with efficient training and real-time rendering, offering speeds of approximately 30 frames per second on regular devices with quality comparable to NeRF [11,32]. By extending the concept of splatting with anisotropic Gaussians, 3DGS achieves a more continuous and cohesive scene representation [6]. However, densification in 3DGS can sometimes lead to over-reconstruction of Gaussians, introducing blur and artifacts, which techniques like progressive frequency regularization aim to mitigate [32].
## 3. Core 3D Gaussian Splatting: Methodology



**Parameters Defining a 3D Gaussian**

| Parameter                 | Description                                                        | Representation / Formulation                          | Role in 3DGS                                         |
| :------------------------ | :----------------------------------------------------------------- | :---------------------------------------------------- | :--------------------------------------------------- |
| **Mean**                  | 3D spatial coordinates of the Gaussian's center                    | $\mu$ or $p$ (x, y, z)                                | Defines position in scene                            |
| **Covariance Matrix**     | Encapsulates size, shape, and orientation (anisotropic)            | $\Sigma$ (3x3 positive semi-definite matrix)          | Defines geometric extent and direction               |
| **Opacity**               | Scalar value determining transparency or density contribution      | $\alpha$ (0 to 1, often sigmoid mapped)               | Controls visibility and blending contribution        |
| **Color**                 | Appearance information                                             | RGB or Spherical Harmonics (SH) coefficients          | Determines visible color, including view-dependence  |
| **Scaling Factors**       | Size along principal axes                                          | Diagonal matrix $S$ (extracted from $\Sigma$)         | Controls the size of the ellipsoid                   |
| **Rotation**              | Orientation of the Gaussian                                        | Rotation matrix $R$ (extracted from $\Sigma$)         | Determines the orientation of the ellipsoid          |
| **Quaternions** (for $R$) | Compact, singularity-free representation for rotation optimization | $q$ (4-parameter unit vector)                         | Ensures stable and robust rotation learning          |

The methodological foundation of 3D Gaussian Splatting (3DGS) establishes a robust framework for efficient 3D scene representation and high-fidelity novel view synthesis, fundamentally differing from traditional volumetric rendering approaches by optimizing discrete geometric primitives [3,26]. This core methodology can be systematically understood through three interconnected components: the mathematical representation of 3D Gaussians, a differentiable rendering pipeline, and an interleaved optimization process with adaptive density control [25,36,38].

At its heart, 3DGS represents a scene as a collection of anisotropic 3D Gaussians, each an ellipsoidal primitive defined by a set of optimizable parameters [3,4,10]. Key parameters for each Gaussian include its mean ($\mu$), specifying its 3D position in space; a positive semi-definite covariance matrix ($\Sigma$), which dictates the Gaussian's anisotropic shape, size, and orientation; a scalar opacity ($\alpha$), controlling its transparency; and color information, typically captured using spherical harmonics (SH) coefficients to enable realistic view-dependent appearance [12,26,33]. The covariance matrix is often factorized into a scaling matrix ($S$) and a rotation matrix ($R$) to facilitate independent optimization of size and orientation, frequently employing quaternions for robust rotation parameterization [30,32].

The differentiable rendering pipeline is central to 3DGS's efficiency and ability to be optimized via gradient descent [12]. It involves projecting these 3D Gaussians onto a 2D image plane, transforming them into 2D splats (ellipses) [6,8]. A critical step in this projection is the transformation of the 3D covariance matrix ($\Sigma$) to a projected 2D covariance matrix ($\Sigma'$), calculated through an affine approximation using the Jacobian ($J$) of the projective transformation [3,12,33]. The rendered image is then formed by combining these 2D splats using an alpha-blending process, where Gaussians are sorted by depth from back to front, ensuring correct occlusion and transparency [10,33]. This rasterization-based approach, often optimized with tile-based rasterization for parallel processing, enables real-time rendering and fast backpropagation [31,34].

The optimization process iteratively refines the parameters of the 3D Gaussians to accurately represent the scene from input images [12,34]. Initialization typically begins with a sparse point cloud derived from Structure from Motion (SfM) techniques, such as COLMAP [10,30]. The training is guided by a hybrid loss function that combines the L1 loss and the D-SSIM (structural dissimilarity index measure) to capture both photometric and structural similarities between rendered and ground truth images [10,30,34]. The parameters are optimized using stochastic gradient descent (SGD) or variants like Adam, which leverage gradients backpropagated through the differentiable rasterizer [10,30]. A key innovation is the adaptive density control mechanism, which dynamically adjusts the number and distribution of Gaussians during optimization [10,12]. This mechanism addresses under-reconstruction (missing geometry) by densifying Gaussians through cloning or splitting, particularly in regions with high view-space positional gradients. Conversely, it prunes redundant or insignificant Gaussians, for instance, those with very low opacity, to prevent over-reconstruction and manage memory usage [12,30,31]. This adaptive control ensures that the scene representation is both sparse enough for efficiency and dense enough for fidelity, maintaining a balance between detail and computational cost.
### 3.1 3D Gaussian Representation
In 3D Gaussian Splatting (3DGS), scenes are rendered by a collection of primitive 3D Gaussians, offering a flexible and efficient scene representation [24,27]. Each individual 3D Gaussian is an ellipsoid in 3D space, characterized by a set of optimizable parameters that define its appearance and geometric properties [6,29]. The key parameters that define a 3D Gaussian include its **mean** (position), **covariance matrix** (shape and orientation), **opacity** (transparency), and **color** [3,8,10,12,26,31,32,33,34].

The **mean** (denoted as $\mu$ or $p$) specifies the 3D spatial coordinates (x, y, z) of the Gaussian's center, serving as its anchor point in the scene [10,26,30]. The **opacity** ($\alpha$) is a scalar value that determines the transparency or density contribution of the Gaussian, often mapped to a range like (0, 1) using a sigmoid function [10,30,31]. The **color** parameter typically consists of RGB values [24], but for capturing complex view-dependent appearance, it is often represented using spherical harmonics coefficients [10].

The **covariance matrix** ($\Sigma$) is a crucial 3×3 positive semi-definite matrix that encapsulates the Gaussian's size, shape, and orientation in 3D space, thereby enabling anisotropic properties [3,24,26,33]. A 3D Gaussian function, in its standard form, is generally expressed as:
$$
G(x) = e^{-(x-\mu)^T \Sigma^{-1} (x-\mu)}
$$
where $x$ is a 3D coordinate vector, $\mu$ is the mean, and $\Sigma^{-1}$ is the inverse of the covariance matrix [26,30,36]. This formulation allows each Gaussian to exhibit an ellipsoidal shape, with the principal axes of the ellipsoid aligned with the eigenvectors of $\Sigma$ and their lengths proportional to the square roots of the corresponding eigenvalues. This provides a flexible representation capable of modeling diverse geometric details, from elongated structures to flattened surfaces, beyond simple spheres.

To ensure the positive semi-definiteness of $\Sigma$ during optimization and to provide a disentangled representation of size and orientation, it is typically constructed from a diagonal **scaling matrix** $S$ and a **rotation matrix** $R$ using the decomposition formula:
$$
\Sigma = R\,S\,S^T\,R^T
$$
Here, $S$ is represented by a 3D vector containing the scaling factors along the principal axes, and $R$ is a 3×3 rotation matrix [8,10,26,30,31,32,33,36]. This decomposition allows for independent control over the size and orientation of each Gaussian, making it highly effective for modeling complex scene geometry while ensuring mathematical validity.

For representing view-dependent color information, **spherical harmonics (SH)** coefficients are attached to each Gaussian [6,10,26,30,31,32,33,34]. Unlike fixed RGB colors, SH allow the color of a Gaussian to vary smoothly based on the viewing direction. This capability is crucial for capturing realistic lighting effects such as specular reflections and diffuse shading, significantly enhancing the visual fidelity and realism of rendered scenes [29].

To parameterize the rotation component $R$ of the covariance matrix, **quaternions** ($q$) are employed [8,10,26,30,31]. Quaternions offer several advantages over other rotation representations like Euler angles or rotation matrices for optimization in 3DGS. Primarily, they provide a singularity-free representation, thereby avoiding issues such as gimbal lock that can hinder optimization stability and convergence. Furthermore, quaternions offer a compact, four-parameter representation (normalized to unit length), which is computationally efficient for gradient-based optimization [26,31]. This makes them well-suited for the backpropagation process used to optimize the Gaussian parameters, ensuring smooth and robust learning of orientations.
### 3.2 Differentiable Rendering
The differentiable rendering pipeline is a cornerstone of 3D Gaussian Splatting (3DGS), enabling the efficient generation of novel views from a learned 3D scene representation [1]. Unlike traditional ray tracing or volumetric rendering methods that query points in 3D space, 3DGS employs a rasterization-based approach by directly modeling opacity values, circumventing the dense sampling required by techniques like NeRF [4,11]. This fundamental difference contributes to 3DGS's superior rendering speed and quality [4].

The core of the rendering process involves projecting the learned 3D Gaussians, represented as ellipsoids, onto a 2D image plane to become 2D splats (ellipses) [6,8,36]. This projection is performed based on the current viewpoint's frustum [30]. Each 3D Gaussian possesses attributes such as position (mean), covariance, color, and opacity. The mean of the 3D Gaussian, \(\mu\), is first projected from world coordinates to camera coordinates and then to pixel coordinates using the intrinsic camera matrix \(K\) and extrinsic camera matrix \(W\): 
\
[10].

Crucially, the 3D covariance matrix, \(\Sigma\), which defines the shape and orientation of the 3D Gaussian, must also be transformed into a projected 2D covariance matrix, \(\Sigma'\), to represent the 2D splat accurately. This transformation is calculated using a first-order Taylor series expansion that approximates the perspective projection [3,33]. The generalized formula for this transformation is:
\
Here, \(W\) represents the view transform (extrinsic camera matrix) that converts world coordinates to camera coordinates, and \(J\) is the Jacobian matrix of the affine approximation of the projective transformation from camera space to image space [8,30,31]. This Jacobian matrix linearizes the projection around the Gaussian's mean, enabling a computationally efficient transformation of the covariance. The world-space covariance \(\Sigma\) itself is often defined using a scaling matrix \(S\) and a rotation matrix \(R\): 
\
[12].

Once projected, the 2D Gaussians are combined to form the final image through an alpha-blending process, a technique similar to volume rendering where contributions are accumulated along rays [10,33]. The projected Gaussians are sorted by depth from back to front, or more efficiently, processed from front to back, to ensure correct occlusion and transparency [3,31]. For each pixel, the color \(C\) is computed by blending the \(N\) ordered 2D Gaussians that overlap it. This is formulated as:
\
where \(c_i\) is the color contribution of the \(i\)th Gaussian (often derived from spherical harmonics coefficients) and \(\alpha_i\) is its opacity, which is scaled by its 2D Gaussian distribution at the pixel location [8,30,32]. The opacity \(\alpha_i\) can also be modeled as 
\
where \(\sigma_i\) is the sampling density and \(\delta_i\) is the step size along the ray [26].

To further enhance rendering efficiency, 3DGS employs a tile-based rasterization technique [30,31,34]. This approach divides the image into non-overlapping tiles. Instead of processing Gaussians pixel-by-pixel, which would be computationally intensive, the system identifies which Gaussians cover each tile. This allows for parallel processing of tiles and efficient sorting and blending of Gaussians within each tile. The tile-based rasterizer facilitates real-time performance by transferring precision from pixel-level to patch-level details, streamlining the depth testing and pixel shading operations that transform the 3D scene into a 2D image [6,24]. The combination of differentiable rendering, precise covariance transformation, alpha-blending, and tile-based rasterization forms the robust and efficient rendering pipeline of 3D Gaussian Splatting.
### 3.3 Optimization and Adaptive Density Control
The optimization process in 3D Gaussian Splatting (3DGS) is meticulously designed to construct a dense and accurate representation of a 3D scene, primarily through the iterative refinement of Gaussian parameters and adaptive control over their density [6]. This process comprises three key components: robust initialization, differentiable optimization, and adaptive densification [10].

Initialization of 3D Gaussians typically relies on a sparse point cloud derived from Structure from Motion (SfM) techniques, such as COLMAP [10,23,30]. The positions (means) of the Gaussians are initialized directly from these SfM points, while covariances are commonly set as isotropic spheres, with radii determined by the average distance to neighboring points [10]. Some advanced methods, like HO-Gaussian, aim to eliminate this dependency on SfM point initialization, integrating a grid-based volume with the 3DGS pipeline for enhanced urban scene reconstruction [21]. The parameters optimized include position (XYZ), opacity ($\alpha$), covariance ($\Sigma$), spherical harmonics (SH) coefficients for color, scale, and rotation [12,26].

The training process is guided by a loss function that compares the rendered images to ground truth images. The standard photometric loss $L$ is a weighted combination of L1 loss and the D-SSIM (structural dissimilarity index measure) loss [10,26,30,33,34]. This is formally expressed as:
$$
L = (1 - \lambda) L_{1} + \lambda L_{D-SSIM}
$$
where $\lambda$ is a tunable weight parameter, typically set to 0.2. While this loss function effectively guides optimization, a naive pixel-wise L1 loss might yield small average gradients in under-reconstructed regions, potentially misleading Gaussian densification [32]. To address this, Frequency Regularization can be employed to significantly increase pixel gradients in over-reconstruction regions, enabling more effective adaptive densification [32]. The trainable parameters are optimized using stochastic gradient descent (SGD), often implemented with optimizers like Adam, which backpropagates gradients through the differentiable rasterizer [10,30,34].

A crucial aspect of 3DGS is its Adaptive Density Control (ADC) mechanism, which periodically manages the density of 3D Gaussians to refine the scene representation [10,31]. This mechanism primarily involves two operations: point densification and point pruning [6,26,31].

Under-reconstruction, characterized by missing geometric features, is addressed by adding Gaussians through densification [12,31]. In regions exhibiting high view-space positional gradients—an indicator of under-reconstruction—Gaussians are either cloned or split [12,30,33]. Specifically, Gaussians with low variance in these problematic areas are duplicated, while those with high variance are split into two new Gaussians. When cloned, the new Gaussians might be moved along the gradient direction [12,30]. Similarly, over-reconstruction, where there might be redundant or overly dispersed Gaussians, is also indicated by large view-space positional gradients [12].

The criteria for these adaptive operations are primarily based on view-space positional gradients. Gaussians with an average gradient exceeding a defined threshold are considered for adjustment [33]. To avoid unnecessary splitting and cloning operations, a Gaussian Divergence Saliency (GDS) metric can be introduced. Only 3D Gaussians with both a large positional gradient and a large GDS are split or cloned, thereby optimizing computational efficiency by reducing the number of candidates checked using k-NN [8].

Pruning serves to remove redundant or insignificant Gaussians, particularly those contributing negligibly to the rendering [30,31]. Gaussians with opacity below a very small threshold (e.g., 0.005), excessively large scale, or an overly large screen size are typically removed [31,33]. To prevent the accumulation of "floaters" (unwanted Gaussians in empty space), the opacity of all Gaussians is periodically reset to a small value (e.g., 0.01) every few thousand iterations (e.g., 3000) [12,33]. This allows unimportant Gaussians to be subsequently pruned based on their low opacity [30]. This adaptive density control is performed periodically during training, such as every 100 or 500 SGD steps, ensuring continuous refinement of the Gaussian representation [10,33].
## 4. Advancements and Extensions

**Techniques for Sparse View & Generalization in 3DGS**

| Category                   | Technique / Method                       | Description                                                     | Impact / Benefit                                                |
| :------------------------- | :--------------------------------------- | :-------------------------------------------------------------- | :-------------------------------------------------------------- |
| **Sparse View Refinement** | SfM Initialization + Depth Supervision   | Initializes Gaussians from SfM, refines with 2D depth network   | FSGS, SparseGS, CoherentGS, DNGaussian: improved reconstruction |
|                            | Cost Volume Integration                  | Uses plane sweeping to locate Gaussian centers                  | MVSplat: precise placement, informs attribute prediction        |
|                            | Visual Hull Initialization               | Initializes Gaussians with a visual hull                        | GaussianObject: enhances sparse view reconstruction             |
|                            | Gaussian Repair Module                   | Mitigates occlusion & information loss                          | GaussianObject: addresses degraded images from noise            |
|                            | Uncertainty-aware Perturbation           | Improves generalization by tackling overfitting                 | Self-Ensembling Gaussian Splatting (SE-GS): robust in sparse views |
| **Few-Shot Synthesis**     | Gaussian Reverse Pooling                 | Distributes new Gaussians to fill local details                 | Zhu et al.: Real-time, realistic results from 3 views           |
| **Single-Image 3D Reconstruction**| Predict Gaussian Attributes from Pixels | Extracts pixel-aligned features, predicts Gaussian attributes  | PixelSplat: Reconstructs from single view, probabilistic depth  |
|                            | "Splatter Image" Generation              | Maps input image to 3D Gaussian per pixel using 2D network      | SplatterImage: Ultra-fast single-view solution, aggregates views |
|                            | Fusion with 2D Diffusion Models          | Combines 2D diffusion with 3DGS for fast, high-quality results  | FDGaussian: Single-object 3D reconstruction, streamlines pipeline |


**3DGS Capabilities in Editing and Generation**

| Category           | Type / Method                                   | Description                                                                 | Example / Impact                                       |
| :----------------- | :---------------------------------------------- | :-------------------------------------------------------------------------- | :----------------------------------------------------- |
| **3D Generation**  | Text-to-3D (with Diffusion Models)              | Combines diffusion models with 3DGS to create 3D objects from text prompts  | GSGEN, FDGaussian, DreamGaussian (in DreamFusion)      |
|                    | Image-to-3D                                     | Generating 3D objects from single or multiple images                        | Broadens content creation applicability                |
| **Geometry Editing**| Object Manipulation                             | Changing position, scale, or orientation of objects within a scene          | GaussianEditor, Point'n Move                           |
|                    | Object Removal / Synthesis                      | Adding or removing specific objects from the scene                          | GaussianEditor, GaussianGrouping, Street Gaussians     |
|                    | Scene Composition                               | Combining multiple 3DGS scenes or objects                                   | Allows complex scene creation                          |
| **Appearance Editing**| Text-Prompt Based Modification                 | Modifying scene appearance using text instructions                          | GaussianEditor (updates Gaussian attributes)           |
|                    | Texture & Lighting Disentanglement              | Separating texture from lighting for independent control                    | GS-IR, RelightableGaussian, GIR, Gaussian-Shader       |
|                    | View-Consistent Editing                         | Ensures edits remain coherent across various viewpoints (solves multi-view inconsistency) | VcEdit                                                 |
| **Physics Simulation**| Integration with Physics-Based Dynamics        | Combining 3DGS with physical simulation models                              | PhysGaussian (solid/fluid dynamics for novel motion)   |



**Techniques for Rendering Quality Improvement in 3DGS**

| Technique                 | Category                  | Description                                                  | Effect on Quality                                              |
| :------------------------ | :------------------------ | :----------------------------------------------------------- | :------------------------------------------------------------- |
| **Mip-Splatting**         | Artifact Mitigation       | Integrates 3D smoothing filter & 2D Mip filter               | Eliminates high-frequency artifacts, combats aliasing/bloating |
| **Linear Kernels**        | Artifact Mitigation       | Replaces conventional Gaussian kernels with linear ones      | Reduces blurring, enhances sharpness, improves fidelity        |
| **FreGS**                 | Artifact Mitigation       | Progressive frequency regularization (Fourier space)         | Enhances Gaussian densification, alleviates over-reconstruction |
| **Spherical Harmonics (SH)**| View-dependent Effects   | Represents non-Lambertian properties (e.g., specular reflections)| Captures realistic lighting effects, view-dependent color      |
| **VDGS**                  | View-dependent Effects   | NeRF-like neural network predicts view-dependent color/opacity | Alternative to SH, potentially more complex effects           |
| **Gaussian Direction Encoding**| View-dependent Effects   | Sophisticated encoding for view-dependent color              | Improves quality in problematic rendering areas                |
| **Spec-Gaussian**         | Complex Shading           | Uses anisotropic spherical Gaussians for intricate light interactions | Better handles material properties and light interactions      |


The rapid evolution of 3D Gaussian Splatting (3DGS) has led to a myriad of advancements and extensions, motivated by the need to address the inherent limitations of the original framework and expand its applicability across diverse scenarios [1,16,24]. These developments aim to improve critical performance metrics such as memory consumption, training time, rendering quality, and the ability to handle complex scene characteristics like dynamism or sparse input data [1,3,11]. By systematically categorizing and analyzing these innovations, this section provides a comprehensive overview of the state-of-the-art in 3DGS, highlighting the technical approaches, their impacts, and the trade-offs involved.

A primary area of focus has been **memory optimization and efficiency**. The original 3DGS often requires significant storage due to the large number of Gaussians, posing challenges for widespread adoption [15]. Advancements in this domain typically involve reducing the number of Gaussian primitives through techniques like learnable masks and resolution-aware pruning, as seen in methods like Tangram-Splatting and CompGS [9,17]. Additionally, attribute compression, using methods like Vector Quantization (VQ) and Residual Vector Quantization (R-VQ) in approaches such as C3DGS and EAGLES, significantly curtails the memory footprint of individual Gaussians [4,14]. Precision reduction, such as employing half-float representations, further contributes to memory savings [7]. While these methods aim to minimize memory usage, the challenge lies in maintaining rendering quality, though many advanced techniques demonstrate minimal degradation, and in some cases, even accelerate rendering and training speeds [7,9].

For large-scale scenes, **scaling and distributed training** are imperative to manage computational demands. Systems like Grendel enable distributed 3DGS training across multiple GPUs by partitioning parameters and parallelizing computational tasks, employing sparse all-to-all communication and dynamic load balancing to optimize resource utilization [18]. Further enhancements, such as K-D tree inspired spatial partitioning, improve throughput and reduce peak GPU memory usage without compromising rendering quality, demonstrating continuous efforts to optimize distributed systems [2].

**Rendering quality improvements** are central to enhancing visual fidelity, addressing issues such as aliasing and blurring. Techniques like Mip-Splatting mitigate artifacts by integrating 3D smoothing filters and 2D Mip filters, while linear kernels aim for sharper images [16,22]. Modeling view-dependent effects, traditionally through Spherical Harmonics (SH) or neural networks in VDGS, adds realism by capturing non-Lambertian properties [11]. Advanced shading models and frequency-domain regularization (e.g., FRegS) further contribute to visual richness, though these often entail increased computational costs [32].

Extending 3DGS to **handle dynamic scenes**, commonly known as 4D GS, involves incorporating a temporal dimension to capture motion and deformation. Approaches range from explicit modeling of time-varying Gaussian attributes (e.g., Luiten's method, Gaussian-Flow) to decomposing space and time information using canonical spaces and deformation fields (e.g., Deformable3DGS, Fov-GS) [5,11]. Regularization techniques, including physics-based and rigid body constraints, are crucial for maintaining spatial and temporal consistency and ensuring smooth, realistic motion [11].

The explicit representation of 3DGS has also greatly facilitated **3D editing and generation**. It allows for combining with diffusion models to create 3D objects from text prompts, as seen in GSGEN and FDGaussian [8,16]. For geometry editing, methods like GaussianEditor and GaussianGrouping enable object manipulation, removal, and synthesis using text prompts and semantic information [11]. Appearance editing involves text-prompt-based modifications and disentangling texture and lighting, with models like GS-IR and RelightableGaussian focusing on material properties [11]. A significant challenge, multi-view inconsistency, is addressed by frameworks like VcEdit, which iteratively fine-tunes the 3DGS representation to ensure view-consistent edits [35].

Addressing challenges with limited input data, particularly **sparse view and generalization**, has seen substantial progress. Techniques often leverage Structure from Motion (SfM) for initial Gaussian placement and refine them with depth inputs from pre-trained 2D networks, as in FSGS and SparseGS [4,11]. For extreme cases, single-image 3D reconstruction is achieved by methods like PixelSplat and SplatterImage, which predict Gaussian attributes directly from pixel-aligned features or generate a "Splatter Image" through a 2D image-to-image network [4,11]. Self-Ensembling Gaussian Splatting (SE-GS) further improves generalization by tackling overfitting in sparse view scenarios through uncertainty-aware perturbation [20].

The integration of **geometric priors and regularization** is vital for enhancing accuracy and robustness, particularly in complex environments like indoor scenes. Methods like DN-Splatter incorporate adaptive depth loss and normal cues to guide optimization and enable mesh extraction [19]. Beyond direct depth guidance, techniques like Surface-Aligned Gaussian Splatting (SuGaR) use regularization terms to align Gaussians with the scene surface, facilitating clean mesh reconstruction [16]. Frequency-domain regularization, as applied in FRegS, minimizes discrepancies between rendered and ground-truth images in the Fourier space, implicitly contributing to geometric accuracy [32].

Finally, **architectural modifications and novel representations** have expanded 3DGS's versatility. These include hybrid approaches that combine 3DGS with other scene representations, such as grid-based volumes (e.g., HO-Gaussian for urban scenes), or specialized pipelines for tasks like single-image reconstruction (e.g., FD-Gaussian's orthogonal plane decomposition and epipolar attention) [8,21]. Novel Gaussian kernel designs, such as linear kernels in 3D Linear Splatting, aim to improve fidelity and speed by addressing the soft boundaries of conventional Gaussians [22]. Advanced optimization and regularization techniques, including the progressive frequency regularization in FRegS which uses low-pass and high-pass filters to manage frequency components:
$$
\mathcal{L}_{f} = 
\begin{cases}
w_{l}(d_{la}+d_{lp}), & \text{if } 0 < t \le T_{0}, \\
w_{l}(d_{la}+d_{lp})+w_{h}(d_{ha}+d_{hp}), & \text{if } t > T_{0}.
\end{cases}
$$
where $d_{la}$, $d_{lp}$, $d_{ha}$, and $d_{hp}$ represent low-frequency amplitude, low-frequency phase, dynamic high-frequency amplitude, and dynamic high-frequency phase discrepancies respectively, play a crucial role in enhancing the quality of learned Gaussian representations [32].

In summary, the advancements in 3DGS demonstrate a concerted effort to overcome the initial limitations of the framework. Researchers have developed sophisticated methods for memory efficiency, distributed computing, rendering quality, dynamic scene handling, editing, generalization from sparse data, geometric guidance, and architectural innovations. While trade-offs often exist between quality, efficiency, and computational cost, the continuous progression in these areas underscores the potential of 3DGS as a robust and versatile scene representation and rendering paradigm, paving the way for broader applications in various real-world scenarios. The ongoing challenge lies in achieving an optimal balance across all these metrics, particularly for real-time applications and complex, uncontrolled environments.
### 4.1 Memory Optimization and Efficiency
The inherent high storage overhead of 3D Gaussian Splatting (3DGS), often requiring hundreds of megabytes to gigabytes for typical scenes due to millions of required Gaussians, presents a significant challenge for its widespread adoption and deployment [15]. This is further complicated by the redundancy in storing similar geometric and appearance attributes among Gaussians [15]. Consequently, memory optimization techniques are critical for enhancing the practicality and efficiency of 3DGS.

Memory-efficient strategies for 3DGS primarily fall into three categories: reducing the number of Gaussian primitives, compressing their attributes, and optimizing their representation precision.

1. Reduction of Gaussian Primitives:
One direct approach to memory efficiency is to minimize the total count of Gaussians representing a scene. Lee et al. proposed a compact 3D Gaussian representation framework that employs a learnable mask strategy to prune unnecessary Gaussians without compromising performance [23]. Similarly, an efficient, resolution-aware primitive pruning technique has been introduced, which systematically reduces the number of Gaussians based on their scene contribution and resolution. This method, when combined with optimizations in spherical harmonics coefficients and attribute precision, achieved a substantial 27× reduction in overall disk size and a 1.7× speedup in rendering [7]. Tangram-Splatting further exemplifies this strategy by integrating shape priors inspired by tangrams, which facilitates a more efficient fitting of the 3D scene. This approach resulted in an average memory consumption reduction of 62.4% and a decrease in training time by at least 10 minutes compared to standard 3DGS [9]. CompGS tackles compactness by introducing a hybrid primitive structure, incorporating anchor primitives, and employing rate-constrained optimization to achieve superior compactness in 3D scene representation while aiming to preserve model accuracy and rendering quality [17].

2. Quantization and Codebook-based Compression of Attributes:
Attribute compression is a prevalent method for reducing the memory footprint of individual Gaussians. Vector Quantization (VQ), a conventional compression technique, is widely employed to cluster multi-dimensional data into a limited set of representations, primarily for Gaussian attributes [11,14]. For instance, C3DGS utilizes Residual Vector Quantization (R-VQ) specifically for geometric attributes, including scale and rotation [4,11]. SASCGS, another notable method, employs vector clustering and a sensitivity-aware K-Means approach to encode both color and geometry attributes into compact codebooks [4,11]. EAGLES extends this by quantizing all Gaussian attributes—color, position, opacity, rotation, and scale—demonstrating that opacity quantization can effectively mitigate floating artifacts in novel view synthesis [4,11,14]. Girish et al. introduced a technique that leverages quantized embeddings combined with a coarse-to-fine optimization strategy for Gaussian point clouds, which led to a substantial 10–20× reduction in memory usage and accelerated training and inference speeds [23]. Beyond VQ, specialized techniques like LightGaussian employ octree-based lossless compression for position attributes within the G-PCC framework, while SOGS optimizes Gaussian attributes by arranging them into 2D grids and applying smooth regularization [4]. Lee et al. also contributed by introducing learned codebooks for efficient compression of geometric attributes and a grid-based neural field for compact view-dependent color representation [23].

3. Precision Reduction and Adaptive Spherical Harmonics:
Reducing the precision of Gaussian primitive attributes, such as adopting half-float representation, serves as another avenue for memory efficiency, often integrated with codebook-based quantization [7]. Furthermore, adaptively adjusting the number of spherical harmonics coefficients for each Gaussian can significantly contribute to overall memory reduction, as evidenced by a comprehensive approach to memory footprint reduction that combines these strategies [7].

Trade-offs between Memory Reduction and Performance:
While these memory optimization techniques yield considerable reductions in storage requirements, their implementation often involves careful consideration of trade-offs with rendering quality (e.g., PSNR) and overall performance. Tangram-Splatting, for instance, achieves a significant 62.4% memory reduction with only a marginal decrease in PSNR performance, typically less than 0.3 dB, and in some datasets, it even shows improved performance [9]. CompGS is designed to achieve superior compactness without compromising model accuracy and rendering quality, indicating that its architectural and optimization choices prioritize maintaining visual fidelity [17]. Moreover, a holistic approach that reduces Gaussian primitives, spherical harmonics coefficients, and attribute precision can achieve a 27× reduction in disk size while simultaneously accelerating rendering by 1.7×, demonstrating that memory efficiency can, in some cases, lead to performance improvements [7]. Similarly, the use of quantized embeddings by Girish et al. not only resulted in a 10–20× memory reduction but also contributed to accelerated training and inference speeds [23].

In conclusion, the landscape of memory optimization techniques for 3DGS is diverse and highly effective. These methods primarily leverage Gaussian pruning, learnable masks, and various quantization and codebook-based compression strategies for Gaussian attributes. While a minimal degradation in rendering quality (PSNR) can be an inherent trade-off, many advanced techniques are engineered to minimize this impact, or even enhance rendering and training speeds, underscoring a positive trajectory towards more efficient and practical 3DGS implementations.
### 4.2 Scaling and Distributed Training
The inherent computational demands of 3D Gaussian Splatting (3DGS) necessitate robust scaling mechanisms, particularly through distributed training systems, to handle large-scale scenes and achieve efficient model convergence. A prominent example of such a system is Grendel, designed to facilitate the distributed training of 3DGS models across multiple Graphics Processing Units (GPUs) [18,33].

Grendel's architecture focuses on partitioning 3DGS parameters and parallelizing computational tasks to distribute the workload effectively [18]. A critical aspect of its design lies in its communication strategies, which address the challenge of sparse all-to-all communication. This communication pattern is vital for transferring the necessary Gaussians to their respective pixel partitions during the rendering process [18]. Furthermore, Grendel incorporates dynamic load balancing mechanisms to manage the distribution of computational workload across the participating GPUs, ensuring efficient resource utilization. Unlike some existing systems that process one camera view image at a time, Grendel supports batched training with multiple views, which contributes to its efficiency [18].

In terms of hyperparameter scaling rules, Grendel's exploration revealed that a $\sqrt{\text{batch size}}$ scaling rule is particularly effective for optimizing the training process in a distributed environment [18]. This rule guides the adjustment of learning rates or other optimization parameters as the batch size increases in distributed settings.

While Grendel provides a foundation for distributed 3DGS training, advancements in dynamic load balancing have further enhanced performance. A notable improvement is the introduction of a dynamic load-balancing framework inspired by K-D tree spatial partitioning [2,33]. This method recursively bisects the screen-space workload across GPUs based on real-time computational demands. To optimize this partitioning, it employs axis-aligned splits that aim to equalize cumulative rendering costs, while simultaneously preserving near-square partition geometries. This latter aspect is crucial for minimizing communication overhead at partition boundaries and reducing GPU memory contention [2].

Experimental evaluations demonstrate significant performance improvements achieved through this K-D tree inspired approach. It consistently delivers a throughput improvement of 1.15-1.27x over the state-of-the-art Grendel framework [2]. Beyond throughput, the method also contributes to resource efficiency by achieving 22.6-33% reductions in peak GPU memory usage. Crucially, these performance gains do not compromise rendering quality, as validated by equivalent Peak Signal-to-Noise Ratio (PSNR) and L1 loss metrics on challenging benchmarks like Mip-NeRF 360 [2]. These advancements highlight the continuous efforts to optimize distributed training systems for 3DGS, focusing on both computational efficiency and memory footprint while maintaining high rendering fidelity.
### 4.3 Rendering Quality Improvements
Enhancing rendering quality in 3D Gaussian Splatting (3DGS) is crucial for achieving high visual fidelity, particularly in addressing persistent issues such as aliasing and blurring artifacts. Various techniques have been developed to refine the visual output, model intricate view-dependent effects, and tackle complex shading scenarios.

One prominent approach to mitigate aliasing and blurring artifacts is **Mip-Splatting**. This technique addresses issues arising from changing sampling rates by integrating a 3D smoothing filter that regulates the maximum frequency of 3D Gaussian primitives, effectively eliminating high-frequency artifacts when scaling [11,16]. Furthermore, Mip-Splatting replaces the traditional 2D dilation filter with a 2D Mip filter, which simulates a 2D box filter to effectively combat aliasing and "swelling" or "bloating" effects in projected Gaussians [4,16,23]. Similarly, **linear kernels** have been introduced to directly improve rendering quality by reducing blurring and enhancing sharpness [22]. Beyond these, other methods contribute to artifact reduction: MS3DGS introduces a multi-scale GS representation to alleviate aliasing [4], and FreGS enhances Gaussian densification and alleviates over-reconstruction by minimizing the discrepancy between the frequency spectrum of rendered images and ground truth, utilizing amplitude and phase discrepancies for frequency regularization within the Fourier space, often employing frequency annealing for progressive regularization [32]. While direct anti-aliasing parameters, such as the `--antialiasing` option in `diff-gaussian-rasterization`, have been explored, early tests indicated they could sometimes worsen effects or increase edge jaggedness [34].

Modeling and rendering **view-dependent effects** is another critical area for improving visual realism. Traditionally, **Spherical Harmonics (SH)** have been employed to represent non-Lambertian surface properties, such as specular reflections on metallic surfaces, by expressing view-dependent color as a linear combination of SH functions [10]. Building upon this, **VDGS** (View-Dependent Gaussian Splatting) offers an alternative by using a NeRF-like neural network to predict view-dependent colors and opacity for each 3D Gaussian, thereby replacing the SH coefficients [4,11]. Other techniques include **Gaussian Direction Encoding**, which enables sophisticated view-dependent color representations, often coupled with point densification to enhance quality in problematic rendering areas during training [21]. Furthermore, approaches like epipolar attention, as seen in `fdgaussian`, improve rendering quality by efficiently associating features across multiple views. The output $\hat f_s$ of an epipolar attention layer for an intermediate UNet feature $f_s$ can be formulated as:
$$\hat f_s = f_s + \sum_{t \neq s} \text{softmax}(M_{st}) f_t$$
where $M_{st}$ denotes the epipolar weight matrix [8].

For **complex shading**, specialized methods like **Spec-Gaussian** approximate 3D scene appearance using anisotropic spherical Gaussians to effectively handle intricate light interactions and material properties [4]. More generally, rendering quality is also improved through the integration of more complex shading models and sophisticated blending of 3D Gaussians to achieve realistic effects [3].

The cumulative impact of these enhancements on **visual fidelity** is substantial. Techniques like Mip-Splatting and linear kernels directly result in sharper images with reduced aliasing and blurring, leading to a more photorealistic appearance. The ability to model view-dependent effects with SH or neural networks (VDGS) significantly boosts realism by accurately depicting specular highlights and other non-Lambertian phenomena. Complex shading models further contribute to visual richness, allowing for nuanced material representations. Beyond these, methods like Fov-GS enhance perceived quality by dynamically allocating computational resources to foveal and salient regions, delivering a visually similar experience to ground truth while optimizing performance [5].

However, these enhancements also bear implications for **computational cost**. While some methods like the Tiled-based Rasterizer aim to reduce computation by processing Gaussian operations at a tile level [31], more complex models for view-dependent effects (e.g., neural networks in VDGS) or advanced shading can inherently increase computational demands. Fov-GS, while improving overall performance, does so by strategically focusing computations, representing a trade-off where high-fidelity rendering is prioritized in specific areas. The continuous development in this field seeks to strike a balance between achieving superior visual fidelity and maintaining real-time rendering capabilities, often through optimized data structures (e.g., Scaffold-GS, Octree-GS) or propagation strategies (e.g., GaussianPro) that efficiently capture scene details [4,11].
### 4.4 Handling Dynamic Scenes
Extending 3D Gaussian Splatting (3DGS) to dynamic scenes, commonly referred to as 4D Gaussian Splatting (4D GS), involves incorporating a temporal dimension to capture motion and deformation over time [23,31]. Unlike static scenes, dynamic 3DGS models the temporal evolution of Gaussian attribute values, allowing Gaussians to move, rotate, and deform while maintaining properties like color, opacity, and size [4,11,16]. The primary challenge lies in ensuring both spatial and temporal consistency in the reconstruction [15].

Approaches to tracking motion and deformation in dynamic scenes can broadly be categorized into explicit and implicit modeling of temporal changes. Many methods make 3D Gaussian parameters dependent on the input images or time, often employing deformable fields [3].
One prominent strategy involves decomposing space and time information, modeling them separately using canonical spaces and deformation fields [4,11]. For instance, Yang et al. utilize a Multi-Layer Perceptron (MLP) that takes position-encoded Gaussian positions and a time step $t$ as input, outputting offsets for the position, rotation, and scale of the 3D Gaussian [4,11]. To enhance temporal smoothness, linearly decaying Gaussian noise can be added to the encoded time vector [11]. Deformable3DGS, for example, enables novel view synthesis and temporal synthesis from monocular multi-view images through its deformable capabilities [14]. Fov-GS further addresses real-time dynamic scenes by introducing a 3D Gaussian forest representation, a dynamic-static separation initialization method, and deformation field optimization, which helps manage the unique spatial-temporal heterogeneity inherent in dynamic scenes [2,5].

Other techniques focus on explicitly modeling the temporal variation of Gaussian attributes. Luiten's approach treats only the center and rotation (quaternion) of 3D Gaussians as time-varying variables, keeping other attributes constant to achieve 6DoF tracking through dynamic scene reconstruction [11]. Katsumata et al. propose fitting the variation of Gaussian positions with Fourier series and approximating rotation with a linear function, while leaving other attributes unchanged [11,23]. Gaussian-Flow extends this by combining polynomial fitting and Fourier approximation in both the time and frequency domains to capture the temporal correlation of attributes, employing a dual-domain deformation model (DDDM) [11]. For explicit 4D representations, Shao et al. introduced GaussianPlanes, a 4D representation based on plane decomposition in 3D space and time, which improves the effectiveness of 4D editing, as seen in applications like Control4D for generating 4D portrait scenes [23]. Similarly, 4D-GS can employ multi-resolution HexPlane voxels to encode temporal and spatial information efficiently [4]. For specific scenarios, such as dynamic urban streets, methods like Street Gaussians model foreground vehicles with optimizable tracking poses and dynamic spherical harmonic models to represent their dynamic appearance [39].

To ensure smooth and realistic motion, particularly in highly dynamic environments, various regularization techniques and rigid body constraints are applied. For instance, Luiten introduces physics-based constraints through three regularization losses: short-term local stiffness, local rotational similarity, and long-term local isometry [11]. Similarly, local rigid body constraints are crucial for regularizing the motion and rotation of Gaussians, enabling dense 6-DoF tracking and robust dynamic reconstruction [16]. These constraints help in maintaining geometric integrity and preventing chaotic movements of Gaussians during temporal evolution. The capability to handle expression changes, editable digital portraits, and non-rigid objects showcases the strength of these dynamic 3DGS methods in capturing complex scene transformations [3]. These advancements support applications such as human motion capture and self-driving simulation, demonstrating the practical utility of dynamic 3DGS [15].
### 4.5 3D Editing and Generation
The explicit representation and rapid rendering capabilities inherent to 3D Gaussian Splatting (3DGS) significantly enable a diverse array of 3D editing and generation tasks [14]. This foundational strength allows for highly intuitive and precise manipulation of 3D scenes and objects.

In the realm of 3D generation, 3DGS has been effectively combined with diffusion models to create 3D objects from text descriptions or prompts, leveraging the strengths of both paradigms [3,23]. A typical text-to-3D pipeline involves generating an initial 3D point cloud from a text description using a diffusion model, subsequently converting this point cloud into a set of Gaussian spheres via 3DGS, and finally rendering these spheres to produce a detailed 3D image of the object [3,23]. Methods like GSGEN specifically optimize a 3D Gaussian scattering representation to introduce 3D priors and employ a progressive strategy encompassing geometry optimization and appearance refinement [16]. FDGaussian also demonstrates compatibility with text-to-image models for text-to-3D generation [8].

For geometry editing, 3DGS facilitates various operations such as object manipulation, scene composition, and comprehensive object removal [33,39]. Techniques often integrate text prompts and semantic information to enable precise control. GaussianEditor, for instance, utilizes text prompts and semantic tracking for 3D repair, object removal, and synthesis [4,11]. GaussianGrouping simultaneously reconstructs and segments open-world 3D objects by leveraging 2D mask predictions from SAM and enforcing 3D spatial consistency constraints, thereby aiding in object removal, repair, and synthesis [4,11]. Point’n Move combines interactive object manipulation with exposed area repair through a two-stage self-prompting mask propagation process, transferring 2D prompt points to 3D mask segmentation [4,11]. Furthermore, Street Gaussians enables scene editing due to its explicit representation, allowing for easy combination of object vehicles and backgrounds [39]. Despite these advancements, a significant challenge remains in the form of a lack of precise training supervision for current editable 3DGS methods [15].

Appearance editing in 3DGS involves techniques like text-prompt-based image modification and the disentanglement of texture and lighting for independent control [14,33]. GaussianEditor also modifies 2D images using language inputs in masked regions derived from 2D segmentation models, subsequently updating Gaussian attributes in a manner similar to Instruct-NeRF2NeRF [11]. Another variant of GaussianEditor introduces hierarchical Gaussian Splatting (HGS) for tasks like target in-painting [11,16]. GSEdit takes a textured mesh or pre-trained 3DGS as input and refines it using Instruct-Pix2Pix and SDS loss [11]. To promote geometric consistency, Gauss-Ctrl incorporates depth maps as conditional input for ControlNet [11]. For more advanced appearance control, methods like GS-IR and RelightableGaussian model texture and lighting separately by defining additional material parameters on each Gaussian, approximating lighting with learnable environment maps [11]. Similarly, GIR and Gaussian-Shader adopt a disentanglement paradigm, binding material parameters to 3D Gaussians and incorporating normal direction constraints to handle reflective scenes [11].

A critical challenge in 3D editing, particularly when leveraging 2D diffusion-based models, is multi-view inconsistency. VcEdit addresses this by providing a framework for view-consistent 3D editing using Gaussian Splatting, guided by text prompts [35]. VcEdit employs a Cross-attention Consistency Module (CCM) to consolidate multi-view cross-attention maps within the diffusion-based image editing model, harmonizing the model’s attentive 3D region across all views [35]. Furthermore, an Editing Consistency Module (ECM) directly calibrates inconsistent multi-view editing outputs by fine-tuning a source-cloned 3DGS with the editing results and then re-rendering the 3DGS into images [35]. This framework iteratively edits rendered images and updates the 3DGS to progressively mitigate multi-view inconsistencies [35].

Beyond static scene manipulation, 3DGS also integrates with physical simulations to create realistic and interactive dynamic scenes [16]. This includes methods that use discrete particle clouds for physics-based dynamics and photorealistic rendering, or combine position-based dynamics (e.g., PhysGaussian) for rendering, view synthesis, and the simulation of solid and fluid dynamics [14,33]. Such integrations unlock new possibilities for simulating complex real-world phenomena within 3DGS representations.
### 4.6 Sparse View and Generalization
The reconstruction of 3D scenes from sparse input views presents a significant challenge for 3D Gaussian Splatting (3DGS), primarily due to the increased training difficulty associated with non-differentiable densification operations [15]. Addressing this, researchers have developed various techniques to enable robust 3D reconstruction and generalization to novel scenes from limited photographic evidence.

A common strategy involves leveraging Structure from Motion (SfM) for initial Gaussian placement. For instance, FSGS initializes sparse Gaussians from SfM outputs and refines them by un-pooling existing Gaussians. This approach is further bolstered by supervising rendered depth images with a pre-trained 2D depth estimation network to ensure faithful geometric reconstruction [4,11]. Similarly, SparseGS, CoherentGS, and DNGaussian also employ depth inputs derived from pre-trained 2D networks. These methods enhance reconstruction quality by identifying and removing Gaussians with incorrect depth values and utilizing Score Distillation Sampling (SDS) loss to improve the fidelity of novel viewpoint renderings [11].

Other approaches focus on refining the Gaussian representation and learning process under sparse conditions. MVSplat, for example, integrates a cost volume representation into sparse view reconstruction, using plane sweeping in 3D space to precisely locate Gaussian centers and inform the Gaussian attribute prediction network [11,15]. GaussianObject enhances sparse view reconstruction by initializing Gaussian functions with a visual hull and fine-tuning a pre-trained ControlNet. This method effectively addresses degraded rendered images caused by noise in Gaussian attributes and incorporates a Gaussian repair module to mitigate occlusion and information loss issues, demonstrating high-quality results from as few as four views [4,11,14,34]. To counter overfitting in 3DGS when trained with sparse views, Self-Ensembling Gaussian Splatting (SE-GS) has been proposed, improving generalization capabilities [20]. For 360-degree scenes with sparse training views, Xiong et al. introduced a method for coherent training of 3D-GS-based radiance fields [23]. Zhu et al. presented a few-shot view synthesis framework capable of real-time, realistic results from just three training views, employing a Gaussian reverse pooling process to distribute new Gaussians and fill in local details in sparse initial SfM points [23].

Further pushing the boundaries of generalization, methods capable of single-image 3D reconstruction represent a significant advancement. PixelSplat reconstructs 3D scenes from single-view inputs without explicit data priors. It achieves this by extracting pixel-aligned image features, akin to PixelNeRF, and employing a neural network to predict the attributes of each Gaussian function. A notable aspect of PixelSplat is its ability to predict probabilistic depth distributions from features extracted using an epipolar transformer structure, which helps replace non-differentiable components during training [4,11,15]. SplatterImage offers an ultra-fast single-view 3D reconstruction solution by mapping an input image to a 3D Gaussian per pixel, effectively generating a "Splatter Image". This method relies on a 2D image-to-image network for learning, requiring only forward evaluation at test time, and aggregates predicted Gaussian distributions from different views via warping [4,11,16].

More recently, FDGaussian addresses the challenge of single-image 3D reconstruction by fusing 2D diffusion models with 3DGS, yielding fast and high-quality results [8]. While the specific mechanisms such as orthogonal plane decomposition and epipolar attention are not detailed in the provided digest for FDGaussian, such techniques are generally crucial in single-image and sparse-view scenarios to enhance geometric consistency and leverage image-based priors effectively. The ongoing development in these areas underscores the field's progression towards more robust and generalizable 3DGS models from increasingly minimal input data.
### 4.7 Geometric Priors and Regularization
The integration of geometric priors and the application of various regularization techniques are crucial for enhancing the accuracy and robustness of 3D Gaussian Splatting (3DGS), particularly in challenging environments such as indoor scenes. These methods guide the optimization process, enforce geometric consistency, and mitigate artifacts, leading to more faithful and physically accurate 3D reconstructions.

One significant approach involves leveraging depth and normal information as explicit geometric priors. For instance, `dn_splatter_enhancing_gaussian_splatting_with_depth_and_normal_priors_for_indoor_scene_reconstruction` introduces an adaptive depth loss, which is formulated based on the gradient of color images. This method significantly improves depth estimation and novel view synthesis results compared to baseline models. The regularization inherent in this technique facilitates the direct extraction of meshes from the optimized Gaussian representation, yielding physically accurate reconstructions, especially beneficial for complex indoor scenes [19].

Similarly, Chung et al. proposed a method that incorporates depth maps derived from pre-trained monocular depth estimation models as geometric guidance [23,33]. These depth maps are carefully aligned with sparse feature points obtained from the Structure-from-Motion (SFM) pipeline, thereby optimizing 3DGS and effectively reducing overfitting, especially when working with a limited number of input images [23,33].

Beyond direct depth guidance, geometric priors can also inform generative models; for example, the orthogonal plane decomposition mechanism functions as a geometric prior, steering diffusion models to generate geometrically consistent multi-view images [8].

Various regularization techniques are employed to enforce geometric consistency and suppress undesirable artifacts. Surface-Aligned Gaussian Splatting (SuGaR) is an example where a specific regularization term encourages Gaussians to align closely with the underlying scene surface [16]. This alignment facilitates the subsequent extraction of a clean mesh from the Gaussian representation, often through methods like Poisson reconstruction [16].

Beyond explicit geometric terms, regularization can also operate in other domains to improve overall fidelity and, by extension, geometric accuracy. For instance, `fregs_通过渐进频率正则化实现3d高斯溅射` proposes a progressive frequency regularization technique. This method assesses and minimizes the amplitude and phase discrepancies between the rendered image $\hat{I}$ and the ground truth $I$ in the frequency domain [32]. The discrepancies, $d_{a}$ for amplitude and $d_{p}$ for phase, are computed as follows:
$$
d_{a} = \frac{1}{\sqrt{HW}} \sum_{x=0}^{H-1} \sum_{y=0}^{W-1} \Bigl| \Bigl| F(u,v) \Bigr| - \Bigl| \hat{F}(u,v) \Bigr| \Bigr|,
$$
$$
d_{p} = \frac{1}{\sqrt{HW}} \sum_{x=0}^{H-1} \sum_{y=0}^{W-1} \Bigl| \angle F(u,v) - \angle \hat{F}(u,v) \Bigr|,
$$
where $F(u,v)$ and $\hat{F}(u,v)$ denote the complex frequency values of $I$ and $\hat{I}$, respectively [32]. By regularizing in the frequency domain, this approach ensures high-fidelity image reconstruction, which implicitly contributes to a more consistent and artifact-free 3D representation. The synergy between integrating direct geometric priors and applying diverse regularization strategies is essential for pushing the boundaries of 3DGS accuracy and robustness in complex real-world scenarios.
### 4.8 Architectural Modifications and Novel Representations
The original 3D Gaussian Splatting (3DGS) framework has inspired numerous architectural modifications and novel Gaussian representations, primarily aimed at extending its applicability to diverse scenarios or enhancing its rendering performance and efficiency. These advancements demonstrate a concerted effort to adapt 3DGS for more complex environments, challenging data conditions, and specialized rendering tasks.

One significant direction in architectural modification involves integrating 3DGS with other scene representations. For instance, HO-Gaussian introduces a hybrid optimization method that combines a grid-based volume with the standard 3DGS pipeline, specifically for urban scenes [21]. This modification effectively leverages the strengths of both representations, allowing for improved handling of large-scale environments. Similarly, the general concept of incorporating grid‐based volumes has been explored to extend 3DGS capabilities and adapt it to various scenarios [33]. Another architectural approach, exemplified by FD-Gaussian, introduces an orthogonal plane decomposition mechanism and epipolar attention to enhance 3DGS capabilities, particularly for rapid and high-quality single-image 3D reconstruction [8]. For dynamic scenes and foveated rendering, the 3D Gaussian forest represents a novel adaptation of the original 3DGS architecture, facilitating real‐time foveated rendering [5].

Beyond structural modifications, novel Gaussian representations have been developed to directly influence rendering quality and efficiency. A fundamental alteration involves replacing the conventional Gaussian kernels themselves. For example, 3D Linear Splatting proposes an architectural modification that substitutes Gaussian kernels with linear kernels in the splatting process, aiming to enhance both rendering fidelity and speed [22]. In terms of encoding, HO-Gaussian also introduces Gaussian Direction Encoding as a novel representation for color, which contributes to its effectiveness in complex urban environments [21]. Generalized Exponential Splatting (GES) is another notable novel Gaussian representation that aims to refine the core splatting mechanism [33].

Furthermore, advancements extend to how Gaussians are optimized and regularized. FRegS, for instance, incorporates low-pass and dynamic high-pass filters, Hₗ and Hₕ respectively, in the Fourier space to achieve progressive frequency regularization. This process extracts low and high-frequency components, LF(u,v) and HF(u,v), respectively [32]. The progressive frequency regularization ℒ₍f₎ is formulated based on amplitude and phase discrepancies for both low and high frequencies:

$$
LF(u,v)=F(u,v)H_{l}(u,v)
$$

$$
HF(u,v)=F(u,v)H_{h}(u,v)
$$

$$
d_{la}=\frac{1}{\sqrt{HW}}\sum_{x=0}^{H-1}\sum_{y=0}^{W-1}\Bigl|\Bigl|LF(u,v)\Bigr| - \Bigl|\hat{LF}(u,v)\Bigr|\Bigr|
$$

$$
d_{lp}=\frac{1}{\sqrt{HW}}\sum_{x=0}^{H-1}\sum_{y=0}^{W-1}\Bigl|\angle LF(u,v)-\angle\hat{LF}(u,v)\Bigr|
$$

$$
d_{ha}=\frac{1}{\sqrt{HW}}\sum_{x=0}^{H-1}\sum_{y=0}^{W-1}\Bigl|\Bigl|HF(u,v)\Bigr| - \Bigl|\hat{HF}(u,v)\Bigr|\Bigr|
$$

$$
d_{hp}=\frac{1}{\sqrt{HW}}\sum_{x=0}^{H-1}\sum_{y=0}^{W-1}\Bigl|\angle HF(u,v)-\angle\hat{HF}(u,v)\Bigr|
$$

$$
\mathcal{L}_{f}=\begin{cases}
w_{l}(d_{la}+d_{lp}), & \text{if } 0 < t \leq T_{0}, \\\\
w_{l}(d_{la}+d_{lp})+w_{h}(d_{ha}+d_{hp}), & \text{if } t > T_{0},
\end{cases}
$$

where dₗₐ, dₗₚ, dₕₐ, and dₕₚ represent low-frequency amplitude discrepancy, low-frequency phase discrepancy, dynamic high-frequency amplitude discrepancy, and dynamic high-frequency phase discrepancy, respectively, with wₗ and wₕ being training weights [32]. This frequency-aware regularization impacts the quality of the learned Gaussian representation, particularly its ability to capture fine details.

A robust architectural modification for few-shot novel view synthesis is Self-Ensembling Gaussian Splatting (SE-GS) [20]. SE-GS operates by jointly training a 𝝟-model and a 𝝠-model. The 𝝟-model is dynamically perturbed based on rendering uncertainty, generating diverse perturbed models. By minimizing discrepancies between the 𝝠-model and these perturbed models, SE-GS forms a robust ensemble. This self-ensembling approach significantly improves generalization and robustness in data-scarce scenarios, directly impacting the quality of novel view synthesis from limited input [20].

In summary, architectural modifications, such as hybrid scene representations for urban environments [21] or specialized pipelines for dynamic and single-image scenarios [5,8], have successfully extended 3DGS applicability. Concurrently, novel Gaussian representations, including kernel replacements [22], advanced encoding schemes [21], and frequency-based regularization [32], directly contribute to enhanced rendering quality and efficiency by refining how scene information is encoded and optimized. The self-ensembling paradigm represents a crucial step towards improving the robustness and performance of 3DGS in challenging few-shot learning contexts [20]. These advancements collectively demonstrate the versatility and adaptability of the 3DGS paradigm.
## 5. Applications

**Key 3DGS-based SLAM Systems**

| System Name                    | Developers / Affiliation                             | Key Features / Innovations                                     | Impact / Capabilities                                                                |
| :----------------------------- | :--------------------------------------------------- | :------------------------------------------------------------- | :----------------------------------------------------------------------------------- |
| **GS-SLAM**                    | Shanghai AI Lab, Fudan University                    | First full SLAM framework with 3DGS. Real-time differentiable splatting. Dynamic Gaussian management. | Balances efficiency & accuracy. 100x faster rendering than prior SOTA.                 |
| **SplaTAM**                    | Carnegie Mellon University, MIT                      | Dense reconstruction with monocular RGB-D camera. Integrates 3DGS throughout pipeline. View-independent colors. | Superior performance. Refines representation with densification masks.                 |
| **Gaussian Splatting SLAM**    | Dyson Robotics Lab, Imperial College London          | Monocular camera input. No SfM or prior learning required. Online dense map generation. | 3 FPS online generation. High-quality reconstructions, even for small/transparent objects. |
| **LIV GaussMap**               | (Implicit in text, focus on LiDAR initialization)    | Initializes Gaussians using LiDAR point clouds.                | Leverages complementary sensor data for robust initialization.                       |
| **SGS-SLAM**                   | (Implicit in text, focus on semantic information)    | Integrates Gaussian semantic information (2D segmentation).    | Enriches scene representation with semantic understanding.                           |

Three-dimensional Gaussian Splatting (3DGS) has rapidly emerged as a versatile and transformative technology, fundamentally reshaping various domains by offering an efficient and high-fidelity approach to scene representation and rendering [3,4,6,11,15,16,25,27,31,33]. Its unique characteristics—particularly real-time rendering capabilities, high visual quality, explicit geometric representation, and inherent editability—position it as a powerful tool for a wide array of applications, addressing long-standing challenges in 3D reconstruction, perception, and content generation.



![Diverse Applications of 3D Gaussian Splatting](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/q__yRwQox9I3p1eFtYsGt_/home/surveygo/data/requests/13535/survey/imgs/Diverse%20Applications%20of%203D%20Gaussian%20Splatting.png)

At its core, 3DGS’s success across these diverse fields stems from its ability to efficiently transform sparse input data into dense, manipulable 3D models. This is evident in novel view synthesis, where 3DGS excels at generating high-resolution, photorealistic images from arbitrary viewpoints with remarkable speed, often outperforming Neural Radiance Fields (NeRF) in rendering efficiency while maintaining or exceeding visual fidelity [5,8,10,16,20,21,22,23,24,29,30,32,34,39]. Adaptations to few-shot scenarios and dynamic scenes further enhance its utility.

In robotics and simultaneous localization and mapping (SLAM), 3DGS’s explicit geometric representation and efficient rendering are pivotal. It enables the creation of dense, accurate, and real-time environmental maps crucial for robust navigation, path planning, and interaction in complex settings [6,14,16,25,31,36,38]. Systems like GS-SLAM and SplaTAM demonstrate significant advancements in balancing efficiency and accuracy—even with challenging monocular inputs—by directly optimizing Gaussians and mitigating viewpoint misalignment [4,25,38]. This robust mapping capability extends directly to autonomous driving, where 3DGS facilitates comprehensive and dynamic environmental models by integrating multi-source data (e.g., LiDAR) to address challenges such as varying sensor data densities and dynamic objects in urban scenarios. Techniques such as Street Gaussians and DrivingGaussian exemplify this, providing enhanced scene understanding, precise mapping, and realistic simulation environments critical for vehicle training and validation [4,6,21,33,36,39].

For virtual and augmented reality (VR/AR), 3DGS offers substantial benefits in real-time rendering, visual fidelity, and memory efficiency, which are critical for immersive and interactive experiences on resource-constrained devices [1,5,6,7,9,27,33]. Its ability to generate high-fidelity content in real-time and its differentiable representation for content editing—as seen in projects like Meta’s Hyperscape and tools like VcEdit—signify a paradigm shift for XR content production [27,35]. Similarly, urban scene reconstruction leverages 3DGS to create realistic and detailed 3D models of complex urban environments. It efficiently addresses challenges such as intricate geometries, large data volumes, and dynamic elements by integrating multi-source data (e.g., aerial imagery) and employing specialized techniques for dynamic scene modeling [21,27,30,33,39].

The human element is another significant application area, with 3DGS making strides in digital humans. The technology enables the realistic and efficient creation of full bodies, heads, hair, and hands—even from limited viewpoints—supporting animatable avatars and dynamic performances [3,4,14,27,33,35]. This capability extends to interactive applications, including real-time outfit changes and immersive VR/AR environments [35].

Finally, in the burgeoning field of 3D/4D generation, 3DGS is a cornerstone for AI-generated content (AIGC). Its integration with diffusion models facilitates text-to-3D/4D and image-to-3D generation, providing highly flexible content creation pipelines. Furthermore, physics-driven approaches like PhysGaussian demonstrate 3DGS’s capacity for synthesizing physically accurate motions and deformations, showcasing its versatility for generating complex and dynamic content [4,8,14,16,31,33].

While the contributions of 3DGS are profound, challenges remain in optimizing computational resources for large-scale scenes, improving few-shot learning capabilities, and ensuring robust performance under extreme conditions. Nevertheless, ongoing research and continual adaptations within each domain underscore 3DGS's significant impact and its potential to drive further innovations in high-fidelity 3D and 4D scene understanding, rendering, and interaction.
### 5.1 Novel View Synthesis
Three-dimensional Gaussian Splatting (3DGS) has emerged as a highly effective method for novel view synthesis, demonstrating significant capabilities in creating detailed 3D representations that can be rendered from arbitrary viewpoints [16,23,29]. The core mechanism involves transforming initial point clouds into a set of 3D ellipsoids, each characterized by optimized attributes such as position, size, rotation, color, and opacity [24]. The seamless blending of these individual Gaussians reconstructs a comprehensive 3D model, enabling high-fidelity rendering of scenes from any desired perspective [24].

A notable advantage of 3DGS lies in its ability to generate high-resolution scene renderings from a limited set of sparse input images through rapid iterative training, albeit requiring substantial hardware computational resources [34]. The visual quality achieved by 3DGS in novel view synthesis is often remarkably high, with synthesized images, particularly in urban environments, appearing nearly indistinguishable from ground truth imagery [21,30]. For instance, HO-Gaussian specifically demonstrates photo-realistic rendering capabilities in real-time across multi-camera urban datasets [21].

When compared to Neural Radiance Fields (NeRF) and its variants, 3DGS often presents compelling advantages in terms of rendering speed and efficiency, while maintaining comparable or superior visual quality [10,23]. For example, FreGS, a progressive frequency regularization approach for 3DGS, consistently outperforms baseline 3D-GS and various NeRF-based methods in quantitative metrics such as PSNR, SSIM, and LPIPS across diverse real-world scenes, including the Mip-NeRF360 and Tank&Temple datasets [32].

Beyond foundational capabilities, ongoing research extends 3DGS's applicability and enhances its performance in challenging scenarios. Efforts are underway to improve novel view synthesis quality under few-shot training conditions, as explored in Self-Ensembling Gaussian Splatting [20]. Moreover, methods like FDGaussian demonstrate the feasibility of synthesizing geometrically and semantically consistent novel views from even a single input image [8]. For dynamic scenes, Fov-GS enables real-time foveated rendering, significantly improving virtual reality (VR) experiences by focusing computational resources on relevant regions [5]. Similarly, Street Gaussians models dynamic urban streetscapes using point clouds to generate high-quality images from arbitrary times and perspectives, distinguishing between static backgrounds and moving vehicles [39]. Innovations also extend beyond standard Gaussians, with approaches like 3D Linear Splatting (3DLS) achieving state-of-the-art fidelity and accuracy in novel view synthesis, surpassing the baseline 3DGS [22].
### 5.2 Simultaneous Localization and Mapping (SLAM)
Simultaneous Localization and Mapping (SLAM) constitutes a foundational challenge in robotics, necessitating the simultaneous construction of an environmental map while precisely determining the device’s position within it [31]. The emergence of 3D Gaussian Splatting (3DGS) has introduced an innovative paradigm for scene representation within SLAM systems, offering substantial advantages over conventional methods such as point clouds, surface meshes, or voxel grids [6].

The explicit geometric representation inherent to 3DGS significantly enhances dense and accurate mapping capabilities in SLAM. Unlike implicit representations, 3DGS leverages anisotropic Gaussians to model the environment, thereby improving efficiency, accuracy, and adaptability [6]. This explicit structure is particularly beneficial for mitigating misalignment between diverse viewpoints—a common challenge in 3D reconstruction and SLAM, especially when compared to methods like Neural Radiance Fields (NeRFs) [14]. By directly optimizing camera tracking for 3D Gaussians and integrating geometric verification and regularization, 3DGS-based SLAM systems achieve superior tracking accuracy, efficient mapping, and high-quality rendering [16].

Several pioneering 3DGS-based SLAM systems exemplify these benefits. **GS-SLAM**, developed by a collaboration including Shanghai AI Laboratory and Fudan University, represents the inaugural instance of integrating 3DGS into a complete SLAM framework [38]. This system achieves a commendable balance between efficiency and accuracy by employing real-time differentiable splatting rendering to accelerate map optimization and RGB-D re-rendering, reportedly yielding rendering speeds 100 times faster than prior state-of-the-art algorithms [25,38]. During operation, GS-SLAM dynamically manages its Gaussian representation by adding new 3D Gaussians and pruning unreliable ones [4].

Another notable contribution is **SplaTAM**, an open-source initiative from researchers at Carnegie Mellon University and MIT. SplaTAM facilitates dense reconstruction using a monocular RGB-D camera and integrates 3DGS throughout the SLAM pipeline—from front-end processing to mapping—demonstrating superior performance over previous methods [25,38]. This system further refines its representation through the use of view-independent colors and the creation of densification masks [4].

Furthermore, a distinct **Gaussian Splatting SLAM** system from the Dyson Robotics Lab at Imperial College London addresses the particularly challenging scenario of monocular camera input [25,38]. This system does not necessitate Structure-from-Motion (SFM) or prior learning and is capable of generating dense maps online at 3 frames per second (fps). It excels in producing accurate and high-quality reconstructions, even for intricate details such as small or transparent objects [25,38]. To ensure robustness, this system—as well as others like GaussianSplattingSLAM and GaussianSLAM—incorporates scale regularization losses [4].

Beyond these foundational systems, specialized 3DGS-based SLAM approaches continue to emerge. For instance, **LIV GaussMap** initializes Gaussians using LiDAR point clouds, leveraging complementary sensor data for robust initialization [4]. Similarly, **SGS-SLAM** extends the utility of 3DGS by integrating Gaussian semantic information derived from 2D segmentation, enriching scene representation with semantic understanding [4].

In summary, the integration of 3DGS into SLAM systems marks a significant advancement. Its explicit representation, coupled with efficient rendering and optimization strategies, enables real-time, dense, and accurate mapping with high rendering fidelity [14,36]. These systems effectively reduce viewpoint misalignment and demonstrate robust performance across various sensor configurations—from challenging monocular setups to multi-modal sensor fusion—solidifying 3DGS as a powerful platform for future SLAM research and applications.
### 5.3 Autonomous Driving
Three-dimensional Gaussian Splatting (3DGS) has emerged as a promising technology for advancing scene representation in autonomous driving, facilitating the creation of detailed and dynamic environmental models crucial for robust navigation and decision-making. The inherent ability of 3DGS to process and blend various data points, such as those from LiDAR, into a cohesive and continuous scene representation offers significant advantages over traditional methods [6]. This capability is particularly beneficial for managing the varying densities of sensor data common in complex urban environments, ensuring smooth and accurate reconstruction of both static infrastructure and dynamic objects [6].

The application of 3DGS in autonomous driving encompasses several key areas, including scene understanding, precise mapping, realistic simulation, and sophisticated dynamic object tracking [33]. Specific implementations like Street Gaussians and DrivingGaussian exemplify how 3DGS addresses the challenges of dynamic environments [33,36,39]. For instance, Street Gaussians models static backgrounds using static 3DGS and explicitly handles dynamic objects with dynamic 3DGS, enabling comprehensive dynamic street scene reconstruction [4]. Similarly, DrivingGaussian reconstructs dynamic driving data by synthesizing both static and dynamic 3D Gaussians [4]. Another approach, PVG, extends this by focusing on Gaussian means and opacity values using time-dependent functions to capture temporal variations in scenes [4]. These advancements allow for not only enhanced scene representation but also efficient data generation, automatic labeling, and closed-loop simulation environments vital for training and validating autonomous systems [39].

Compared to traditional sensing methods like raw LiDAR data, 3DGS offers a more comprehensive understanding of the surrounding world, thereby improving safety and efficiency. While LiDAR provides accurate depth information, 3DGS can utilize this data to generate visually rich, continuous, and highly realistic scene renderings that are capable of representing intricate textures and dynamic changes over time. This enhanced visual fidelity and continuity, achieved by blending disparate data points into a unified model, facilitate more intuitive perception and more accurate prediction for autonomous vehicles [6]. Furthermore, the ability of 3DGS to generate realistic synthetic data is a critical benefit, allowing for the creation of diverse training scenarios that might be impractical or dangerous to capture in the real world [33]. The applicability of these methods has been validated on widely used autonomous driving datasets, demonstrating their potential for practical integration into autonomous driving pipelines [21]. Addressing the complexities of urban environments, with their inherent variability in static and dynamic elements and sensor data density, is a primary focus for 3DGS applications in this domain [6,36]. The development of large-scale datasets, such as GauU Scene, further supports research and development in applying 3DGS to these challenging urban contexts [4].
### 5.4 Virtual and Augmented Reality (VR/AR)
Three-dimensional Gaussian Splatting (3DGS) presents significant advancements for Virtual Reality (VR) and Augmented Reality (AR) applications, offering substantial benefits in real-time rendering, visual fidelity, and memory efficiency that address limitations of traditional VR/AR rendering and enable novel interactive experiences [1,6,27]. Its inherent characteristics allow for the creation of immersive and interactive environments with realistic rendering, fostering engagement with virtual worlds [33].

A key advantage of 3DGS is its robust real-time rendering capability and explicit control over scene representation, which is crucial for dynamic and interactive VR/AR experiences [6]. The capacity to generate high-fidelity content in real time is paramount for immersive applications. For instance, Meta's Hyperscape, scheduled for release in September 2024, leverages 3DGS combined with cloud-based streaming rendering to enable users to scan real-world scenes with a smartphone and then virtually explore these highly detailed recreations using a Quest headset [27]. This demonstrates 3DGS's potential to deliver unparalleled visual fidelity within interactive VR environments. Furthermore, techniques such as foveated rendering designed specifically for 3DGS can significantly enhance rendering performance in dynamic scenes—a common and critical requirement for interactive VR applications [5].

Efficient memory usage is another critical factor where 3DGS offers substantial improvements, particularly for resource-constrained VR/AR devices, including mobile platforms. Approaches like Tangram-Splatting prioritize memory efficiency, making 3DGS highly suitable for applications where memory resources are limited [9]. The reduction in memory footprint demonstrated by various advancements in 3DGS makes it more practical for mobile VR/AR, leading to faster download times and improved rendering performance on devices with constrained resources [7]. This efficiency is further highlighted by initiatives like PICO's Splat plugin, which provides Unreal developers with 3DGS support, enabling the PICO 4 Ultra to render higher-quality yet more lightweight scenes, thereby accelerating the integration and deployment of 3DGS in Extended Reality (XR) content production [27].

Beyond rendering and memory, 3DGS facilitates new interactive capabilities. The differentiable rendering inherent in 3DGS's scene representation aligns with the requirements for generating editable content, opening avenues for content creation and manipulation [6]. Tools like VcEdit, for example, demonstrate how 3DGS can be utilized in VR/AR applications to enable consistent and high-quality editing of 3D scenes, ensuring that modifications remain coherent across various viewpoints [35]. Collectively, these benefits position 3DGS as a transformative technology for the next generation of VR/AR applications, pushing beyond the limitations of conventional rendering paradigms to deliver more immersive, efficient, and interactive experiences.
### 5.5 Urban Scene Reconstruction
3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for creating realistic and detailed 3D models of urban environments, addressing the complex demands of large-scale scene representation [30]. This application domain presents unique challenges, including handling intricate geometries, managing varying lighting conditions, processing extensive data volumes, and accommodating dynamic elements within the scene [30,33]. Researchers have developed various techniques to leverage 3DGS's capabilities while mitigating these difficulties.

One prominent approach involves reconstructing urban scenes from aerial or satellite imagery. For instance, studies have successfully utilized 3DGS with data acquired from platforms like Google Earth Studio to build detailed 3D models of cities [30]. This method is particularly effective for capturing expansive areas but necessitates robust solutions for managing the inherent challenges of large-scale data and diverse environmental conditions [33].

To enhance the robustness and applicability of 3DGS in urban contexts, advanced methodologies integrate multi-source data fusion. Lixel CyberColor (LCC) exemplifies this by combining an optimized 3D Gaussian algorithm with its proprietary Multi-SLAM simultaneous localization and mapping algorithm [27]. This integration allows LCC to support comprehensive data fusion, ranging from aerial to ground-level data and from outdoor to indoor environments, thereby facilitating holistic large-scene space modeling [27].

Addressing the dynamic nature of urban environments is another critical aspect. Traditional static reconstruction methods often fall short when dealing with moving objects such as vehicles or pedestrians. To overcome this, specialized techniques like Street Gaussians have been developed to enable dynamic urban scene reconstruction, specifically focusing on modeling and rendering transient elements and their changing appearances [39]. This allows for a more faithful representation of bustling cityscapes.

Furthermore, some advancements in 3DGS for urban scenes aim to reduce dependencies on traditional photogrammetric pipelines. HO-Gaussian, for example, directly addresses limitations of existing 3DGS methods by eliminating the reliance on Structure from Motion (SfM) point initialization [21]. This innovation simplifies the data processing workflow and improves the applicability of 3DGS for rendering complex urban environments where SfM initialization might be challenging or imprecise [21].

In summary, 3DGS is proving to be a highly versatile tool for urban scene reconstruction. Through advancements in data acquisition from diverse sources, robust handling of large-scale and dynamic data, and optimization of core algorithms, 3DGS continues to push the boundaries of creating realistic and detailed 3D models of our built environments.
### 5.6 Robotics
Three-dimensional (3D) Gaussian Splatting (3DGS) offers transformative potential for robotic systems, significantly enhancing their capabilities for environmental understanding and interaction. The technology's proficiency in real-time rendering and its capacity for high-fidelity scene representation are pivotal in advancing robot autonomy and adaptability [6,38].

Specifically, 3DGS is poised to revolutionize several core robotic applications, including sophisticated robot navigation, precise object manipulation, and comprehensive scene understanding [33]. For robot navigation, the detailed and dynamically reconstructable 3D maps enabled by 3DGS are instrumental for accurate path planning and robust collision avoidance, even in highly complex and evolving environments. This real-time scene updating capability is crucial for robots to autonomously adapt to unforeseen changes in their surroundings.

In the realm of object manipulation, the rich and accurate 3D representations provided by 3DGS allow robots to precisely perceive object poses, intricate shapes, and surface textures. Such detailed perception is essential for enabling more dexterous and reliable gripping, manipulation, and interaction with a diverse range of objects in both structured and unstructured settings.

Furthermore, 3DGS significantly contributes to holistic robot scene understanding. By generating dense, photorealistic 3D models, robots can effectively infer semantic information, identify specific objects, and comprehend complex spatial relationships within their operational space. This profound level of understanding underpins advanced decision-making processes, which are fundamental to the operation of truly autonomous robotic systems.

The inherent efficiency of 3DGS in synthesizing novel views and its ability to capture fine-grained scene details are key technological advantages. Unlike many traditional dense reconstruction techniques, 3DGS can achieve remarkable rendering speeds while preserving visual fidelity. This makes it exceptionally well-suited for demanding real-time robotic applications where rapid perception, processing, and response are paramount. This attribute directly translates into enhanced adaptability, empowering robots to react dynamically and effectively to unexpected obstacles or shifts in their operational domain [6,38].

Consequently, by providing robots with robust, high-fidelity, and real-time 3D perception capabilities, 3DGS is paving the way for the development of more intelligent, autonomous, and versatile robotic platforms, capable of navigating, interacting, and executing intricate tasks with greater efficacy in dynamic and unpredictable environments.
### 5.7 Digital Humans
The application of 3D Gaussian Splatting (3DGS) has significantly advanced the realistic and efficient creation of digital humans, encompassing methods for modeling the entire body, head, hair, and hands [14,33]. This technology facilitates the capture of human poses from limited viewpoints and the subsequent generation of highly detailed 3D models suitable for digital portraits [3]. A notable real-world application includes the BAFTA 2025 awards ceremony, where 3D Gaussian technology was utilized to transform images of winners into interactive digital models viewable from multiple perspectives, effectively creating a "Generative Madame Tussauds" [27].

Various specialized approaches leverage 3DGS for different aspects of digital human modeling. For the creation of animatable human avatars, D3GA employs drivable 3D Gaussians in conjunction with tetrahedral cages, enabling versatile motion representation [4]. Similarly, SplatArmor utilizes Multi-Layer Perceptrons (MLPs) to predict large-scale human motions and incorporates $SE(3)$ fields to generate pose-related visual effects, enhancing realism in dynamic scenarios [4]. HuGS contributes to this domain by introducing a coarse-to-fine deformation module specifically designed for virtual human avatars, improving the precision of deformation [4]. Furthermore, HiFi4G integrates 3DGS with non-rigid tracking techniques to achieve high-fidelity rendering of dynamic human performances [4].

Beyond full-body animation, 3DGS-based methods also target specific anatomical parts. For dynamic head reconstruction, MonoGaussianAvatar applies 3DGS to achieve detailed and temporally consistent results [4]. PSAvatar further refines head modeling by using a Flame face model to initialize Gaussians, providing a robust starting point for facial capture [4]. While specific named methods for hair and hand modeling are not detailed, these areas are recognized as crucial applications within the broader scope of 3DGS for digital humans, benefiting from its capabilities in high-fidelity rendering [14,33]. GPS Gaussian, which introduces Gaussian parameter maps on sparse source views, can potentially contribute to the detailed reconstruction of complex structures such as hair and hands by efficiently representing fine geometries [4]. Moreover, editing capabilities for digital humans have been advanced by systems like VcEdit, which allows for text-driven editing of 3D human models while maintaining crucial multi-view consistency, essential for realistic appearance and behavior across different perspectives [35].

All expressions in this content have been checked for syntax correctness and parenthesis integrity to ensure compatibility with KaTeX. Any expressions involving macros not supported by KaTeX (for example, the $SE(3)$ fields) have been appropriately converted.
### 5.8 3D/4D Generation
The emergence of AI-Generated Content (AIGC), which encompasses digital content created or significantly altered by AI systems, has revolutionized various domains by offering scalable and customizable alternatives to human-generated content [31]. Within this landscape, 3D Gaussian Splatting (3DGS) has rapidly become a pivotal technology for facilitating the creation of novel 3D and 4D content, leveraging its efficient representation and rendering capabilities [14,33].

Numerous approaches have adopted 3DGS for 3D content generation, particularly in the realm of text-to-3D synthesis, often by integrating with advanced diffusion models [14]. For instance, DreamGaussian replaces MipNeRF with 3DGS within the well-established DreamFusion framework, thereby enhancing the quality and efficiency of text-to-3D object synthesis [4]. To address common artifacts such as the Janus problem in text-to-3D generation, GSGEN introduces a specialized 3D Score Distillation Sampling (SDS) loss, building upon the Point-E architecture [4]. Further advancing this field, GaussianDreamer refines text-to-3D synthesis by intelligently combining both 2D and 3D diffusion model priors, allowing for more robust and coherent generation [4]. Similarly, FDGaussian demonstrates its utility in text-to-3D generation by seamlessly integrating with existing text-to-image models, streamlining the creation pipeline [8]. Beyond text-to-3D, 3DGS also facilitates image-to-3D generation, broadening its applicability in content creation [14,33].

The utility of 3DGS extends significantly into 4D content generation, especially for dynamic scenes and temporal sequences [14,33]. Several methodologies have been developed to imbue 3DGS with dynamic capabilities. AYG, for example, endows 3DGS with a deformation network, directly enabling text-to-4D content generation by synthesizing dynamic 3D scenes from textual descriptions [4]. For scenarios requiring 4D generation from visual inputs, DreamGaussian4D is designed to produce dynamic 3D content given a single reference image, showcasing the potential for animating static inputs [4].

Beyond purely data-driven or diffusion-based approaches, some works incorporate physics-based principles into 4D generation. PhysGaussian represents a notable advancement in this area, integrating Newtonian dynamics directly into 3D Gaussian distributions for synthesizing high-quality, novel motions [16]. This method employs a custom Material Point Method (MPM) to impart physically meaningful motion deformation and mechanical stress properties to 3D Gaussian kernels, drawing upon principles of continuum mechanics [16]. This distinct approach prioritizes physical realism, allowing for the generation of dynamic scenes that adhere to natural laws, contrasting with diffusion-based methods that learn motion priors from data.

In comparison, the primary strategies for 3D/4D generation with 3DGS largely diverge into two main paradigms. The first, and most prevalent, involves integrating 3DGS with generative models, predominantly diffusion models. This allows for highly flexible content creation from various modalities like text and images, as evidenced by text-to-3D/4D (e.g., DreamGaussian, GSGEN, GaussianDreamer, FDGaussian, AYG) and image-to-3D/4D (e.g., DreamGaussian4D) frameworks [4,14,33]. These methods typically rely on sophisticated objective functions, such as score distillation or perceptual losses, to guide the generation process. The second paradigm, exemplified by PhysGaussian, introduces a novel physics-driven approach to 4D generation. This method focuses on generating physically accurate motion and deformation rather than solely relying on learned priors, which is particularly advantageous for applications demanding high fidelity to real-world physical behavior [16]. While diffusion-based methods offer expansive creative flexibility and cover a broad range of styles and content, physics-based methods prioritize fidelity to the physical world, representing complementary avenues for 3DGS-based content generation. This versatility underscores 3DGS's significant role as a foundational technology in advancing AIGC capabilities for complex 3D and 4D content.
## 6. Challenges and Future Directions

![Key Future Research Directions for 3DGS](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/Uwg_bOeDkbkvribBSKQzI_/home/surveygo/data/requests/13535/survey/imgs/Key%20Future%20Research%20Directions%20for%203DGS.png)


**Key Limitations of 3D Gaussian Splatting**

| Category                   | Specific Limitation / Challenge                             | Description / Impact                                             |
| :------------------------- | :---------------------------------------------------------- | :--------------------------------------------------------------- |
| **Visual Artifacts**       | Multi-view Inconsistency                                    | Leads to mode collapse, exacerbated by explicit representation.   |
|                            | Blurring & Floating Primitives                              | Over-reconstruction, especially in high-frequency areas.         |
|                            | "Broken" Gaussians                                          | Excessively large, elongated, or redundant primitives.            |
|                            | Elongated, Splotchy, Popping Effects                        | Noticeable in distant views.                                     |
|                            | Lower Quality in Less-seen/Unseen Regions                   | Similar to NeRF, floating artifacts near image plane.            |
|                            | Challenging Areas (distant, sky, low-texture)               | Difficulty in rendering accurately, particularly in urban environments. |
| **Resource Consumption & Scalability**| High GPU Memory Demand                             | Restricts high-resolution, large-scale reconstruction.           |
|                            | High Memory Usage & Disk Space                              | Pervasive issue for deployment.                                  |
|                            | Scalability Challenges                                      | Compared to NeRF, larger memory footprint due to explicit parameters. |
| **Robustness & Generalizability**| Sensitivity to Initialization                          | Prone to issues with initial setup.                              |
|                            | Lack of Robustness to Noise & Outliers                      | Affects accuracy with imperfect input data.                      |
|                            | Overfitting in Sparse Views                                 | Limits effectiveness with limited input data.                    |
|                            | Handling Complex/Dynamic Environments                       | Significant challenge for real-time applications (e.g., VR).     |
| **Geometric Limitations**  | Noise in Recovered 3D Geometry                              | Affects accuracy of geometric reconstruction.                    |
|                            | Difficulty Disentangling Geometry/Texture/Lighting          | Hinders advanced scene manipulation and understanding.           |
|                            | Generating Physically Accurate 4D Scenes                    | Still a nascent area for realistic motion and deformation.       |
| **Practical Applicability**| Incompatibility with Existing Rendering Pipelines           | Complicates integration into broader graphics ecosystems.        |
|                            | Limited to Single-Object / Fixed Views (some methods)       | Restricts applicability to complex scenes.                       |

Despite the groundbreaking advancements of 3D Gaussian Splatting (3DGS) in novel view synthesis and real-time rendering, its full potential and widespread applicability are currently constrained by a series of inherent limitations. This section critically examines these challenges, analyzing their underlying causes and impact on 3DGS performance and deployment, while simultaneously proposing strategic directions for future research. The primary objectives are to foster the development of more robust, efficient, and versatile 3DGS methods, ensuring their utility across an expanding range of real-world applications [12,15].

Current limitations of 3DGS primarily revolve around issues of visual fidelity, resource efficiency, and generalizability. Visually, 3DGS models frequently contend with artifacts such as blurring, floating primitives, "broken" Gaussians, and multi-view inconsistencies, particularly in under-reconstructed or high-frequency regions [12,15]. These issues can stem from the explicit nature of Gaussian representation and the densification process itself, leading to a compromise in scene fidelity. From an efficiency standpoint, 3DGS demands substantial computational resources, including high GPU memory and significant disk space, posing scalability challenges for large-scale and high-resolution scene reconstruction [3,12]. The reliance on explicit parameters, unlike implicit neural representations, contributes to a larger memory footprint, hindering real-time applications in resource-constrained environments. Furthermore, 3DGS exhibits sensitivities to initialization, struggles with robust performance in sparsely observed or dynamic scenes, and often fails to accurately disentangle geometric and appearance properties, limiting its generalizability and applicability in complex scenarios [4,15]. These limitations highlight fundamental trade-offs between rendering quality, computational cost, and methodological robustness.

Addressing these challenges necessitates concerted future research efforts across several key domains. A critical direction involves enhancing the fundamental robustness and quality of 3DGS representations through more resilient initialization techniques, improved artifact mitigation strategies, and advanced regularization methods [15]. Concurrently, significant attention must be directed towards improving efficiency and scalability. This includes optimizing rendering algorithms, developing more compact data representations through advanced compression and quantization, and exploring distributed computing paradigms to manage large-scale scene complexities [3,6]. Moreover, a promising avenue lies in the synergistic integration of 3DGS with broader artificial intelligence techniques, particularly for endowing models with semantic understanding. This would facilitate advanced capabilities such as semantically aware 3D editing, robust handling of dynamic scenes, and the generation of realistic 4D content, thereby expanding the potential for interdisciplinary applications in fields like virtual reality, digital twins, and autonomous systems [4,15]. By systematically tackling these limitations and exploring innovative interdisciplinary approaches, the future of 3DGS promises to unlock unprecedented capabilities in 3D scene representation and interaction.
### 6.1 Current Limitations
Despite its remarkable advancements in novel view synthesis and scene representation, 3D Gaussian Splatting (3DGS) still contends with several inherent limitations that constrain its performance, applicability, and overall robustness. A critical analysis of these challenges reveals complex trade-offs between accuracy, efficiency, and robustness.

A primary concern for 3DGS is its susceptibility to various visual artifacts and inconsistencies. Multi-view inconsistency in image guidance can lead to mode collapse and noticeable visual artifacts, which are particularly exacerbated by 3DGS's explicit representation and inherent densification process. This complicates the effective densification of under-reconstruction regions or pruning of over-reconstruction regions, impacting overall scene fidelity [35]. Furthermore, 3DGS often exhibits blurring artifacts, manifesting as floating primitives and over-reconstruction, especially pronounced in high-frequency areas [22]. The method is also prone to introducing “broken” Gaussians—primitives that are excessively large, elongated, or redundant—necessitating various regularization heuristics to mitigate these issues [10,31]. Common rendering artifacts also include elongated, splotchy, and popping effects, particularly noticeable in distant views [12,33]. Similar to issues inherited from NeRF, 3DGS may also exhibit lower quality in less-seen or unseen regions and persistent floating artifacts near the image plane [10]. Scenes with insufficient observation data are particularly susceptible to artifacts [6,12], and challenges persist in rendering distant, sky, and low-texture areas, especially in urban environments [21].

Another significant class of limitations relates to resource consumption and scalability, directly impacting efficiency. 3DGS models typically demand high GPU memory, which severely restricts their application to high-resolution and large-scale 3D reconstruction tasks [7,18,30]. Beyond GPU memory, high memory usage and large disk space requirements are pervasive issues [3,12,31,33]. The scalability of 3DGS, which relies on storing explicit Gaussian parameters, remains a considerable challenge when compared to NeRF-based methods that benefit from storing only learned MLP parameters [6]. This inherent memory footprint can be a significant bottleneck for real-time applications and deployment in resource-constrained environments.

The robustness and generalizability of 3DGS are also subject to several limitations. The method exhibits sensitivity to initialization and a lack of robustness to noise and outliers [33]. A common problem is overfitting when trained with sparse views, limiting its effectiveness in scenarios with limited input data [20]. Furthermore, handling highly complex scenes or dynamic environments remains a significant challenge for 3DGS, posing a bottleneck for applications like real-time Virtual Reality (VR) [5,33]. Reconstruction quality can also decrease significantly when dealing with challenging inputs [4].

From a geometric perspective, 3DGS suffers from limitations in its representation. Noise in recovered 3D geometry and non-affine transformation offsets between Multi-View Stereo (MVS)-densified point clouds and 3DGS-refined point clouds can compromise geometric accuracy [30]. The discrete geometric representation inherently affects the overall geometric quality, and 3DGS currently struggles to accurately disentangle geometry, texture, and lighting, which is crucial for advanced scene manipulation and understanding. Moreover, generating 4D scenes with realistic geometry, appearance, and physics-aware motion remains a nascent area [4].

Finally, practical applicability and integration present additional hurdles. 3DGS models are often incompatible with existing rendering pipelines, complicating their integration into broader graphics ecosystems [31]. Some specialized approaches, such as FDGaussian, are limited to single-object 3D reconstruction and may generate a fixed number of views, thereby restricting their applicability to more complex scenes or diverse viewing requirements [8].

These limitations underscore a fundamental set of trade-offs inherent in current 3DGS approaches. Achieving higher accuracy often comes at the expense of efficiency, as demonstrated by the increased memory footprint and computational complexity associated with representing intricate details or reducing artifacts [3]. Conversely, optimizing for efficiency by reducing the number of Gaussians or simplifying their parameters can lead to a degradation in visual quality. Similarly, enhancing robustness to challenging conditions like sparse views or dynamic environments typically requires more sophisticated models or regularization techniques, which can increase computational demands and reduce real-time performance. The tension between achieving high fidelity, minimizing resource usage, and ensuring reliable performance across diverse scenarios remains a central challenge in advancing 3DGS technology.
### 6.2 Future Research Directions
Despite the significant advancements and breakthroughs demonstrated by 3D Gaussian Splatting (3DGS), its widespread applicability and robustness still present considerable avenues for future exploration. To overcome current limitations and unlock novel possibilities, future research should strongly emphasize interdisciplinary collaborations, particularly at the confluence of computer graphics, computer vision, and artificial intelligence.

A primary direction involves enhancing the **robustness and quality of 3DGS representations**. This includes developing more robust initialization methods and techniques to address artifacts in areas with insufficient observation, potentially by combining 3DGS with 1-image-to-3D methods [12] or exploring new data interpolation and integration methods for sparse regions [6]. Improving the rendering quality in challenging areas and enhancing the overall robustness of 3DGS in complex environments, such as urban scenes, remains crucial [21]. Further advancements in training stability through techniques like progressive frequency regularization [32] and adaptive kernel sizes based on local scene complexity [22] are also promising for achieving higher fidelity and consistency.

Another critical area focuses on developing **efficient rendering algorithms and improving scalability**. This encompasses optimizing distributed training systems to further enhance scalability and efficiency for large-scale 3D reconstruction [18]. The development of hardware-accelerated rendering techniques for 3DGS is highly beneficial [6,7], alongside the implementation of more advanced rendering algorithms. Reducing the computational cost of underlying structures, such as grid-based volumes [21], and exploring large-scale reconstruction schemes akin to Mega-NeRF [30] are essential for handling complex environments.

The development of **compact representations** for 3DGS is vital for broader deployment and memory-constrained applications. Future work could explore more aggressive pruning techniques, more efficient quantization methods, and hybrid representations that synergize the benefits of Gaussians with other primitives [7]. Optimizing memory utilization during both training and model storage is a direct path to more efficient systems [6,30].

A crucial direction involves the symbiotic **integration of 3DGS with other artificial intelligence techniques**, fostering interdisciplinary innovation across various domains. Endowing 3DGS with semantic understanding is paramount, allowing the extension of 2D semantic models into 3D space for tasks such as 3D detection, segmentation, and advanced editing [15,20,33]. This also includes developing robust consistency modules and integrating advanced 2D editing models to improve the quality and efficiency of 3DGS editing [35]. Furthermore, extending 3DGS to robustly handle dynamic scenes is a significant challenge, requiring adaptive strategies for dynamic-static separation [5] and the integration of self-ensembling techniques or video generation models with physics laws to enhance 4D content quality [4,8,20,33]. Exploration of novel applications is also key, including improving virtual reality experiences through foveated rendering [5] and enabling large-scale semantic 3D reconstruction for urban digital twin creation and monitoring [30].

In summary, the future of 3DGS lies in a concerted effort to enhance its fundamental capabilities—robustness, efficiency, and compactness—while increasingly integrating it with advanced AI and computer vision techniques. Such interdisciplinary collaborations will be instrumental in pushing the boundaries of 3D scene representation, perception, and interaction.
## 7. Conclusion
3D Gaussian Splatting (3DGS) has emerged as a transformative technology in 3D scene reconstruction and novel view synthesis, demonstrating significant advancements in both real-time rendering and high-quality 3D reconstruction [6,23]. Its primary strengths lie in its exceptional efficiency and visual fidelity, offering state-of-the-art visual quality with competitive training times, often surpassing traditional Neural Radiance Fields (NeRF) methods in real-time performance and rendering quality [10,16]. This is largely attributed to its interpretable representation of scenes as explicit 3D Gaussian primitives and its optimization algorithm, which utilizes tile-based rasterization for rapid alpha-blending [10,12].

Beyond its speed and quality, 3DGS has shown notable improvements in memory efficiency and scene compactness through techniques like primitive pruning, adaptive spherical harmonics adjustment, and codebook-based quantization, making it more practical for a wider array of applications [7,9,17]. Furthermore, its enhanced controllability and adaptability to various scene types, including complex dynamic street scenes, highlight its versatility [6,21,39].

Despite these remarkable strengths, 3DGS is still in a rapid development phase and faces certain challenges that are active areas of research [4,14,27]. While it achieves impressive visual quality, some studies suggest it may still lag behind highly specialized NeRF variants in achieving absolute photorealistic quality in extremely complex scenes and can be sensitive to multi-view inconsistencies, often requiring regularization heuristics [3,10]. Issues such as over-reconstruction and generalization in few-shot novel view synthesis also present ongoing challenges [20,32]. Nevertheless, continuous research is actively addressing these limitations, with developments like VcEdit for multi-view consistency in editing [35], FreGS for addressing over-reconstruction [32], and SE-GS for improved generalization in few-shot synthesis [20]. Innovations like HO-Gaussian also aim to overcome limitations in urban scene reconstruction, achieving photo-realistic rendering in real-time [21].

The potential of 3DGS to revolutionize various fields is substantial. Its capabilities in efficient reconstruction and real-time rendering position it as a key technology for applications in virtual reality (VR), augmented reality (AR), robotics, and computer graphics [3,25]. It has already found applications in tasks such as SLAM, 3D editing, and urban scene modeling, demonstrating its broad impact and promising future [16,30,39]. Specifically, its ability to render dynamic scenes in real-time, coupled with techniques like foveated rendering, makes it highly suitable for immersive VR experiences [5]. Furthermore, its advantages in modeling efficiency, rendering quality, and system compatibility suggest its pivotal role in the future three-dimensional content ecosystem, facilitating its transition from laboratory settings to diverse real-world scenarios [27].

Looking ahead, the research landscape for 3DGS is vibrant and rapidly evolving. Future developments are likely to focus on further enhancing its robustness in complex and dynamic environments, improving its photorealistic quality, and scaling its application to even larger scenes through distributed computing techniques [3,18]. The exploration of novel scene representations beyond traditional Gaussians, such as 3D Linear Splatting (3DLS), signifies a continuous drive to refine scene structure and improve rendering quality [22]. The integration of external cues like depth and normal priors also holds promise for more physically accurate reconstructions, especially in challenging indoor environments [19]. As the field progresses, 3DGS is expected to play an increasingly central role in advancing the state of the art in real-time 3D reconstruction, graphics, simulation, and training, solidifying its position as a cornerstone technology for the next generation of 3D content creation and interaction [1,25].

## References

[1] 3D Gaussian Splatting: A Survey [http://www.paperreading.club/page?id=202970](http://www.paperreading.club/page?id=202970) 

[2] Dynamic K-D Tree Partitioning for Load-Balanced Distributed 3D Gaussian Splatting [https://ieeexplore.ieee.org/document/11047564/](https://ieeexplore.ieee.org/document/11047564/) 

[3] 3D Gaussian Splatting 最新综述：3D重建与新视角合成技术进展 [https://zhuanlan.zhihu.com/p/696625337](https://zhuanlan.zhihu.com/p/696625337) 

[4] 清华大学3D Gaussian Splatting综述：新视图渲染技术进展 [https://zhuanlan.zhihu.com/p/692005498](https://zhuanlan.zhihu.com/p/692005498) 

[5] Fov-GS: Real-Time Foveated 3D Gaussian Splatting for Dynamic Scenes [https://dl.acm.org/doi/10.1109/TVCG.2025.3549576](https://dl.acm.org/doi/10.1109/TVCG.2025.3549576) 

[6] Gaussian Splatting：自动驾驶场景新宠？ [https://news.sohu.com/a/757105403_121124366](https://news.sohu.com/a/757105403_121124366) 

[7] Reducing Memory Footprint of 3D Gaussian Splatting [https://dl.acm.org/doi/10.1145/3651282](https://dl.acm.org/doi/10.1145/3651282) 

[8] FDGaussian：快速高质量单图三维重建方案 [https://zhuanlan.zhihu.com/p/688973753](https://zhuanlan.zhihu.com/p/688973753) 

[9] Tangram-Splatting: Memory-Efficient 3D Gaussian Splatting with Tangram-Inspired Shape Priors [https://dl.acm.org/doi/10.1145/3664647.3680688](https://dl.acm.org/doi/10.1145/3664647.3680688) 

[10] 高斯溅射(Gaussian Splatting)全面解析：比NeRF更好吗？ [https://zhuanlan.zhihu.com/p/700133649](https://zhuanlan.zhihu.com/p/700133649) 

[11] 3D Gaussian Splatting 最新进展综述 [https://zhuanlan.zhihu.com/p/711658181](https://zhuanlan.zhihu.com/p/711658181) 

[12] 3D Gaussian Splatting 论文简析：快速训练与实时渲染的新SOTA [https://zhuanlan.zhihu.com/p/661363274](https://zhuanlan.zhihu.com/p/661363274) 

[13] 3D Gaussian Splatting论文阅读笔记 [https://zhuanlan.zhihu.com/p/669104251](https://zhuanlan.zhihu.com/p/669104251) 

[14] 3D Gaussian Splatting 技术最新进展综述 [https://it.sohu.com/a/770105468_121124366](https://it.sohu.com/a/770105468_121124366) 

[15] 3D 高斯 Splatting 综述：技术、挑战与机遇 [https://zhuanlan.zhihu.com/p/711653513](https://zhuanlan.zhihu.com/p/711653513) 

[16] 3D高斯（Gaussian Splatting）最新研究进展及应用 [https://zhuanlan.zhihu.com/p/676637751](https://zhuanlan.zhihu.com/p/676637751) 

[17] CompGS: Efficient 3D Scene Representation via Compressed Gaussian Splatting [https://arxiv.org/abs/2404.09458](https://arxiv.org/abs/2404.09458) 

[18] Grendel: Scaling 3D Gaussian Splatting Training with Distributed Computing [http://www.paperreading.club/page?id=236846](http://www.paperreading.club/page?id=236846) 

[19] DN-Splatter: Enhancing Gaussian Splatting with Depth and Normal Priors for Indoor Scene Reconstruction [https://ui.adsabs.harvard.edu/abs/2024arXiv240317822T/abstract](https://ui.adsabs.harvard.edu/abs/2024arXiv240317822T/abstract) 

[20] Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis [https://arxiv.org/abs/2411.00144](https://arxiv.org/abs/2411.00144) 

[21] HO-Gaussian: Hybrid Optimization for 3D Gaussian Splatting in Urban Scenes [https://link.springer.com/chapter/10.1007/978-3-031-73027-6_2](https://link.springer.com/chapter/10.1007/978-3-031-73027-6_2) 

[22] 3D Linear Splatting: Fast and High-Fidelity Rendering Beyond Gaussians [https://ui.adsabs.harvard.edu/abs/arXiv:2411.12440](https://ui.adsabs.harvard.edu/abs/arXiv:2411.12440) 

[23] 高斯Splatting：3D重建与新视图合成综述 [https://zhuanlan.zhihu.com/p/711714324](https://zhuanlan.zhihu.com/p/711714324) 

[24] 3D Gaussian Splatting：原理、应用与快速渲染 [https://mp.weixin.qq.com/s?__biz=MzU1NjEwMTY0Mw==&mid=2247589043&idx=1&sn=30b85681589e7b3a750c12ad428af77e&chksm=fa4bca88209fa312385bec3be094bd519cd238f8efd762d9c76d69bb9761432cd70908288cca&scene=27](https://mp.weixin.qq.com/s?__biz=MzU1NjEwMTY0Mw==&mid=2247589043&idx=1&sn=30b85681589e7b3a750c12ad428af77e&chksm=fa4bca88209fa312385bec3be094bd519cd238f8efd762d9c76d69bb9761432cd70908288cca&scene=27) 

[25] 3D Gaussian Splatting：原理、应用与最新进展 [https://it.sohu.com/a/745533126_121124366](https://it.sohu.com/a/745533126_121124366) 

[26] 3D Gaussian Splatting 代码及论文解析 [https://zhuanlan.zhihu.com/p/716667148](https://zhuanlan.zhihu.com/p/716667148) 

[27] VR/AR/AI行业资讯与评测 - VR陀螺 [https://www.vrtuoluo.cn/542582.html](https://www.vrtuoluo.cn/542582.html) 

[28] 3D Gaussian Splatting技术复现与代码解读 [https://cloud.baidu.com/article/3292532](https://cloud.baidu.com/article/3292532) 

[29] 3D Gaussian Splatting: Training and Visualization Tutorial [https://www.reshot.ai/3d-gaussian-splatting](https://www.reshot.ai/3d-gaussian-splatting) 

[30] 滑铁卢大学利用谷歌地球图像和高斯溅射重建3D城市场景 [https://cloud.tencent.com/developer/article/2434822](https://cloud.tencent.com/developer/article/2434822) 

[31] 三维高斯溅射(3D Gaussian Splatting, 3DGS)详解 [https://blog.csdn.net/2403_85515624/article/details/146216008](https://blog.csdn.net/2403_85515624/article/details/146216008) 

[32] FreGS: 通过渐进频率正则化实现3D高斯溅射 [https://ar5iv.labs.arxiv.org/html/2403.06908](https://ar5iv.labs.arxiv.org/html/2403.06908) 

[33] 3D Gaussian Splatting 算法详解与实现 [https://zhuanlan.zhihu.com/p/7833648056](https://zhuanlan.zhihu.com/p/7833648056) 

[34] 3D Gaussian Splatting：三维重建与实时渲染详解 [https://blog.csdn.net/weixin_36539353/article/details/148239674](https://blog.csdn.net/weixin_36539353/article/details/148239674) 

[35] VcEdit: 视图一致的3D高斯溅射编辑 [https://blog.csdn.net/c2a2o2/article/details/137682588](https://blog.csdn.net/c2a2o2/article/details/137682588) 

[36] 3D Gaussian Splatting在SLAM与自动驾驶中的应用调研 [https://blog.csdn.net/gwplovekimi/article/details/135397265](https://blog.csdn.net/gwplovekimi/article/details/135397265) 

[37] 3D Gaussian Splatting 技术原理详解 [https://blog.csdn.net/guimaxingtian/article/details/137927521](https://blog.csdn.net/guimaxingtian/article/details/137927521) 

[38] 3D Gaussian Splatting：今年最火的技术及SLAM应用 [http://mt.sohu.com/a/750306807_121124366](http://mt.sohu.com/a/750306807_121124366) 

[39] 3D Gaussian Splatting 数据获取方法 [https://www.zhihu.com/question/632773209](https://www.zhihu.com/question/632773209) 

