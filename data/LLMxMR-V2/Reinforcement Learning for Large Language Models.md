# A Survey on Reinforcement Learning for Large Language Models

# 0. A Survey on Reinforcement Learning for Large Language Models

## 1. Introduction
The advent of Large Language Models (LLMs), exemplified by foundational models such as BERT and GPT-3 and further advanced by generative powerhouses like GPT-4, has profoundly reshaped the landscape of natural language processing [14,35]. These models demonstrate sophisticated capabilities in understanding, reasoning, and task planning. However, their utility and reliability are constrained by several critical challenges, including inconsistent adherence to complex instructions, the generation of harmful or biased content, difficulties in accurate source citation, and achieving precise human alignment [10,13,35]. Furthermore, Multimodal Large Language Models (MLLMs) struggle with fine-grained visual perception tasks, such as object localization and precise counting, indicative of an incomplete comprehension of the visual world [1,8,17]. LLMs also exhibit limitations in complex reasoning, intrinsic self-correction without external input, and dynamic information retrieval [6,7,18,22,26,30].

Reinforcement Learning (RL) has emerged as a transformative paradigm to address these multifaceted limitations, fundamentally enhancing LLM capabilities and aligning them with human expectations [10,35]. 

**LLM Challenges & RL Benefits**

| Category         | LLM Challenges                                                                                                   | RL's Transformative Role                                                                            |
| :--------------- | :--------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------- |
| **Alignment**    | Inconsistent adherence to complex instructions, harmful/biased content, difficulties in human alignment          | Aligning with human preferences (helpful, honest, harmless)                                         |
| **Reasoning**    | Limitations in complex reasoning, intrinsic self-correction, dynamic information retrieval                        | Improving complex reasoning, leading to Large Reasoning Models (LRMs)                               |
| **Perception**   | MLLMs struggle with fine-grained visual perception (object localization, counting)                               | Enhancing fine-grained visual perception in MLLMs (Perception-R1)                                   |
| **Efficiency**   | Supervised Fine-tuning (SFT) struggles with non-differentiable objectives, catastrophic forgetting               | Mitigating high-dimensional state spaces, complex environments, sample inefficiency in RL           |
| **Scalability**  | CoT incurs computational overhead, depends on costly human data; RM requires vast human preference data          | Addressing reward function design, generalization capabilities                                      |
| **Algorithmic**  | PPO sensitivity to hyperparameters, multiple models, scaling difficulties; lack robustness to OOD tasks          | Developing more robust and scalable RL techniques (DPO, RLAIF, GRPO)                              |

The motivation for a comprehensive survey on the integration of RL with LLMs stems from its proven capacity to significantly elevate performance beyond basic text generation, impacting areas such as dialogue systems, machine translation, complex reasoning, and ethical alignment [24]. Initially deployed for aligning LLMs with human preferences to ensure helpful, honest, and harmless responses, RL's application has expanded to directly foster and improve complex reasoning, leading to the development of Large Reasoning Models (LRMs) [23,30]. Concurrently, the integration of LLMs into RL, termed LLM-enhanced RL, presents a novel research avenue to mitigate intrinsic RL challenges, including high-dimensional state spaces, complex environments, sample efficiency, reward function design, and generalization capabilities [14,34].

Traditional methods have proven inadequate for many of these challenges. Supervised Fine-tuning (SFT), while effective for initial alignment, struggles to optimize for complex, non-differentiable sequence-level objectives and risks catastrophic forgetting of prior knowledge [3,27]. Methods relying on human-prompted intermediate reasoning steps, such as Chain-of-Thought (CoT), incur substantial computational overhead and are heavily dependent on costly, extensive, and potentially biased human-annotated data, thereby limiting the exploration of potentially superior, non-human-like reasoning paths [22]. The Reward Model (RM), a core component in many RL-based alignment methods, traditionally demands vast amounts of human-annotated preference data, making its training expensive, susceptible to biases, and increasingly inefficient as LLMs scale [20,31]. This bottleneck in acquiring high-quality human preference labels remains a critical impediment to scaling Reinforcement Learning from Human Feedback (RLHF) [11,20,25,27]. Furthermore, directly applying language-centric RL techniques to visual tasks in MLLMs has not consistently yielded expected benefits, highlighting the need for tailored enhancement strategies [17].

The limitations extend to existing RL algorithms themselves. For instance, Proximal Policy Optimization (PPO), a cornerstone algorithm in RLHF, is known for its sensitivity to hyperparameters, the necessity for multiple models, and inherent difficulties in scaling to larger parameter counts [4,24]. Both online and offline RLHF methods, including PPO and Direct Preference Optimization (DPO), often exhibit optimal solutions that are highly task-dependent and lack robustness to out-of-distribution (OOD) tasks or shifts in the behavior policy, potentially undermining prior Supervised Fine-Tuning or pre-training efforts [16]. Moreover, the inherent complexity of RL-driven paradigms, involving multi-module coordination and multi-stage processes, necessitates highly efficient, scalable, and user-friendly RL system frameworks for practical implementation, especially given the significant computational resource demands for large-scale RL in LRMs [2,15,23,26]. These collective limitations underscore the urgent demand for advanced RL techniques and a comprehensive understanding of their synergistic integration with LLMs.

This survey provides a comprehensive overview of the rapidly evolving field of Reinforcement Learning for Large Language Models. Its primary scope encompasses the application of RL to enhance LLM capabilities, particularly their reasoning abilities, transforming them into Large Reasoning Models (LRMs) [23,26,30]. Concurrently, it investigates the diverse roles LLMs assume within RL frameworks, such as Information Processors, Reward Designers, Decision Makers, and Generators, thereby exploring LLM-enhanced RL methodologies [14,34]. Synthesizing insights from extensive research, including works published since the emergence of models like DeepSeek-R1, this survey delineates foundational components, core problems, training resources, and downstream applications [23,26].



![Survey Structure and Roadmap](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/KF9bnLqPEkxxz-09QI1MW_Survey%20Structure%20and%20Roadmap.png)

To provide a clear roadmap for researchers [10], this survey is systematically organized to progressively build an understanding of the field. It first establishes the foundational concepts of RL and elucidates how the RL pipeline is adapted and integrated with LLMs [29]. Subsequently, it delves into specific algorithms and techniques that form the core of RL for LLMs, including a detailed analysis of reward design strategies, policy optimization algorithms, and sampling strategies crucial for enhancing LLM reasoning capabilities [30]. The survey reviews widely adopted methods such as Reinforcement Learning from Human Feedback (RLHF), Reinforcement Learning from AI Feedback (RLAIF), and Direct Preference Optimization (DPO), alongside advanced strategies like Group Relative Policy Optimization (GRPO) [10,29]. It further categorizes the specific functions and methodologies through which LLMs augment RL, offering a structured taxonomy of their roles [14,34]. Furthermore, the survey explores the diverse applications of RL in LLMs across various domains, from code generation to tool-augmented reasoning [10], and general downstream applications related to enhanced reasoning [23,26]. It critically examines the technical evolution pathways for improving LLM reasoning, including perspectives on data evolution, model evolution, and self-evolution, tracing the progression from tree-search based short Chain-of-Thought (CoT) to RL-based long CoT [6]. Following the exposition of techniques and applications, the survey addresses critical challenges and methodological shortcomings inherent in the current research landscape, such as reward hacking, computational costs, and the complexities of scalable feedback collection [10,29]. Finally, it identifies and discusses emerging directions and future research opportunities, including hybrid RL algorithms, verifier-guided training, and multi-objective alignment frameworks, providing forward-looking perspectives for advancing the field towards more general artificial intelligence [6,10,23,26,34]. This structured approach aims to furnish researchers with a comprehensive understanding of current advancements, prevailing open problems, and promising avenues for future exploration.
### 1.1 Background and Motivation
The rapid evolution of Large Language Models (LLMs) since foundational models like BERT and GPT-3 has fundamentally transformed natural language processing, enabling advanced generative capabilities exemplified by models such as GPT-4 [14,35]. Despite these remarkable advancements in understanding, reasoning, and task planning, LLMs face critical challenges that impede their broader utility and reliability [10,13,35]. These challenges include effectively following complex instructions, mitigating the generation of harmful or biased outputs, accurately citing sources, and achieving precise human alignment [10,24,35]. Furthermore, Multimodal Large Language Models (MLLMs), while integrating language understanding with image processing, often struggle with fine-grained visual perception tasks such as precise object localization, accurate counting, and optical character recognition, indicating a gap in their ability to deeply understand the visual world [1,8,17]. LLMs also demonstrate limitations in complex reasoning, intrinsic self-correction without external input, and dynamic information retrieval [6,7,18,22,26,30].

Reinforcement Learning (RL) has emerged as a transformative approach to address these multifaceted challenges, fundamentally enhancing LLM capabilities and aligning them with human user needs [10,35]. The motivation for surveying the integration of RL with LLMs stems from this capacity to move beyond basic text generation and significantly elevate performance in areas like dialogue systems, machine translation, complex reasoning, and ethical alignment [24]. Initially applied to align LLMs with human preferences to ensure responses are helpful, honest, and harmless, RL's role has expanded to directly motivate and improve complex reasoning capabilities, leading to the development of Large Reasoning Models (LRMs) [23,30]. Concurrently, the integration of LLMs into RL, termed LLM-enhanced RL, also offers a novel research avenue to address intrinsic RL limitations such as high-dimensional state spaces, complex environments, sample efficiency, reward function design, and generalization [14,34].

Traditional methods often fall short in addressing these challenges. Supervised Fine-tuning (SFT), while effective for initial alignment, struggles with optimizing for complex, sequence-level objectives that are not easily differentiable and can suffer from catastrophic forgetting, leading models to lose previously acquired broad capabilities [3,27]. Methods relying on human-prompted intermediate reasoning steps, such as Chain-of-Thought (CoT), incur high computational costs and depend heavily on extensive, costly, and potentially biased human-annotated examples, which limits the model's ability to explore non-human-like, potentially superior reasoning paths [22]. The core component of many RL-based alignment methods, the Reward Model (RM), traditionally requires extensive human-annotated data, making its training expensive, prone to biases, and increasingly ineffective as LLMs become more powerful [20,31]. This bottleneck of gathering high-quality human preference labels is a critical challenge for scaling Reinforcement Learning from Human Feedback (RLHF) [11,20,25,27]. Furthermore, the direct application of language-centric RL techniques to visual tasks in MLLMs has not consistently yielded expected benefits, underscoring the need for tailored enhancement techniques [17].

Key motivators for advanced RL techniques also arise from the limitations of existing RL algorithms themselves. For instance, Proximal Policy Optimization (PPO), a widely used algorithm in RLHF implementations, presents critical challenges due to its sensitivity to hyperparameters, the requirement for multiple models, and inherent difficulties in training and scaling to larger parameter counts [4,24]. Both online and offline RLHF methods, including PPO and Direct Preference Optimization (DPO), exhibit optimal solutions that are highly task-dependent and lack robustness to out-of-distribution (OOD) tasks or changes in the behavior policy, potentially undoing prior Supervised Fine-Tuning or pre-training [16]. Moreover, the complexity of RL-driven paradigms, involving multi-module coordination and multi-stage processes, necessitates highly efficient, scalable, and user-friendly RL system frameworks for practical implementation, especially given the significant computational resource demands for large-scale RL in LRMs [2,15,23,26]. These limitations collectively highlight the urgent need for advanced RL techniques and a comprehensive understanding of their integration with LLMs.

---
**Note on Task 1 (Reference Consolidation):** The provided content already adheres to the specified format `` for all reference groups. There were no instances of multiple consecutive single-item reference lists (e.g., ` `) that needed consolidation.

**Note on Task 2 (KaTeX Syntax Check and Conversion):** The provided content does not contain any mathematical formulas or expressions that would require KaTeX syntax checking or conversion. Therefore, this task was not applicable.
### 1.2 Scope and Organization of the Survey
This survey provides a comprehensive overview of the rapidly evolving landscape of Reinforcement Learning (RL) for Large Language Models (LLMs). The primary scope encompasses both the application of RL to enhance LLM capabilities and the utilization of LLMs to augment traditional RL paradigms. Specifically, it delves into how RL techniques contribute to improving LLM functionalities, particularly their reasoning abilities, transforming them into Large Reasoning Models (LRMs) [23,26,30]. Concurrently, it examines the diverse roles LLMs assume within RL frameworks, such as Information Processors, Reward Designers, Decision Makers, and Generators, thereby exploring LLM-enhanced RL methodologies [14,34]. The survey synthesizes insights from extensive research, including works published since the emergence of significant models like DeepSeek-R1, to delineate foundational components, core problems, training resources, and downstream applications [23,26].

To offer a clear roadmap for researchers [10], this survey is systematically organized into several sections, each designed to progressively build understanding of the field. Initially, it establishes the foundational concepts of RL and elucidates how the RL pipeline is adapted and integrated with LLMs [29]. Subsequently, the survey delves into specific algorithms and techniques that constitute the core of RL for LLMs. This includes a detailed analysis of reward design strategies, policy optimization algorithms, and sampling strategies, which are crucial for enhancing LLM reasoning capabilities [30]. It further reviews widely adopted methods such as Reinforcement Learning from Human Feedback (RLHF), Reinforcement Learning from AI Feedback (RLAIF), and Direct Preference Optimization (DPO), alongside advanced strategies like Group Relative Policy Optimization (GRPO) [10,29]. The systematic analysis extends to categorizing the specific functions and methodologies by which LLMs augment RL, offering a structured taxonomy of their roles [14,34].

Furthermore, the survey explores the diverse applications of RL in LLMs across various domains, ranging from code generation to tool-augmented reasoning [10], and general downstream applications related to enhanced reasoning [23,26]. It also critically examines the technical evolution pathways for improving LLM reasoning, including perspectives on data evolution, model evolution, and self-evolution, tracing the progression from tree-search based short Chain-of-Thought (CoT) to RL-based long CoT [6]. Following the exposition of techniques and applications, the survey addresses the critical challenges and methodological shortcomings inherent in the current research landscape, such as reward hacking, computational costs, and the complexities of scalable feedback collection [10,29]. Finally, it identifies and discusses emerging directions and future research opportunities, including hybrid RL algorithms, verifier-guided training, and multi-objective alignment frameworks, providing forward-looking perspectives for advancing the field towards more general artificial intelligence [6,10,23,26,34]. This structured approach aims to provide researchers with a comprehensive understanding of current advancements, open problems, and promising avenues for future exploration.
## 2. Background: Large Language Models and Reinforcement Learning
The current landscape of artificial intelligence is significantly shaped by the rapid evolution of Large Language Models (LLMs) and the burgeoning interest in Reinforcement Learning (RL). This survey provides a comprehensive overview of how these two powerful paradigms intersect, specifically focusing on the application of RL to enhance and align LLMs. This background section sets the stage by introducing the fundamental principles and recent advancements in both LLMs and RL, culminating in their foundational integration through Reinforcement Learning from Human Feedback (RLHF).

Large Language Models (LLMs) have emerged as highly capable systems, primarily leveraging the Transformer architecture and extensive pre-training on vast datasets [35]. These models, such as BERT and GPT-3, demonstrate remarkable proficiency in language understanding, reasoning, and even multimodal information processing, acquiring fundamental language structures and statistical regularities through objectives like next-token prediction [13,14,20,29,35]. Advanced architectures like Llama 2 and DeepSeek-V2 incorporate innovations such as RMSNorm, SwiGLU, and Mixture-of-Experts (MoE) for improved efficiency and enhanced capabilities [28,29]. Following the initial pre-training phase, Supervised Fine-tuning (SFT) plays a crucial role in aligning these foundational models with human expectations and specific tasks, transforming general-purpose base models into instruction-tuned variants capable of generating optimal responses [13]. The sheer scale of LLMs, involving trillions of tokens for pre-training and billions of parameters, presents substantial computational demands, necessitating the development of efficient fine-tuning techniques [2,29]. Furthermore, the advent of Multimodal Large Language Models (MLLMs), including models like GPT-4o and Gemini, integrates language understanding with other modalities such as images, showcasing a broader interpretive ability [1,8,17]. Despite their impressive capabilities across diverse tasks, LLMs still exhibit limitations, notably in areas requiring advanced self-correction, which continues to motivate research into their refinement [18].

Reinforcement Learning (RL) provides a powerful computational framework for how an autonomous agent learns to make optimal decisions by interacting with an environment to maximize a cumulative reward signal [14,24,29]. In contrast to supervised learning, RL agents acquire knowledge through trial-and-error, receiving feedback in the form of rewards based on their actions [29]. The standard theoretical foundation for RL is the Markov Decision Process (MDP), formally defined by a tuple $M=(S, A, P, R, \gamma)$ [5,27]. Key components of this framework include the **Agent** (the decision-making entity, often the LLM itself), the **Environment** (the external system for interaction, e.g., input context or external tools), the **State** ($s_t$) representing the agent's current observation, an **Action** ($a_t$) chosen from available options (e.g., generating the next token), a **Reward** ($r_t$) as a scalar feedback signal indicating action desirability, and a **Policy** ($\pi$) which guides the agent's behavior to maximize expected cumulative reward over time [5,7,13,27,29]. The iterative interaction between agent and environment aims to maximize the total accumulated reward [5,29]. When applied to LLMs, the model acts as the agent, its parameterized generation process defines the policy, and individual token predictions constitute actions. The reward structure for LLMs is often sparse, with feedback typically provided at the completion of an entire response, reflecting its overall quality [13,27]. Despite challenges posed by high-dimensional state spaces and the need for sample efficiency, RL techniques enable LLMs to explore and develop complex reasoning strategies, moving beyond the limitations of imitation learning by actively correcting cumulative errors [14,20,22].

The initial and most influential intersection of LLMs and RL is Reinforcement Learning from Human Feedback (RLHF), which has become a de facto standard for aligning LLM outputs with human preferences [16,23,35]. RLHF emerged from the difficulty of explicitly defining optimal LLM behavior for generating high-quality, helpful, honest, and harmless responses, where human judgment is more readily available than explicit rule sets [11,13,30,35]. The RLHF pipeline typically comprises three stages: First, an initial pre-trained LLM undergoes **Supervised Fine-Tuning (SFT)** on a high-quality dataset of instruction-following examples to yield an SFT model ($\pi^{\text{SFT}}$) [2,4,25,27,29]. Second, a **Reward Model (RM)** is trained by collecting human preference data, where annotators rank or select preferred LLM-generated responses. This RM learns to assign a scalar quality score to text sequences based on these human preferences, typically using a loss function such as:
$$
\mathcal{L}_r(\phi) = - \mathbb{E}_{(x,y_w,y_l) \sim D} \left
$$
where $\sigma$ is the sigmoid function, and $r_\phi(x,y)$ represents the reward assigned by the model to response $y$ for input $x$ [13,27,35]. Finally, the SFT model undergoes **Policy Optimization** using RL algorithms, most commonly Proximal Policy Optimization (PPO), with the trained RM providing the reward signal. To prevent policy drift and maintain natural language generation, a Kullback-Leibler (KL) divergence term is often incorporated into the objective function:
$$
J(\theta) = \mathbb{E}_{x \sim D, y \sim \pi_\theta(\cdot|x)} \left \right]
$$
where $\beta$ is a hyperparameter balancing reward maximization and policy regularization [2,12,13,19,27,29]. RLHF's effectiveness in optimizing complex, sequence-level objectives through human judgments has made it a foundational technology for leading AI systems such as ChatGPT and LLaMA 2 [2,9]. However, significant challenges persist, including the high cost and difficulty of acquiring large volumes of high-quality human preference data, as well as vulnerability to out-of-distribution tasks. These limitations motivate ongoing research into more scalable and robust alternatives for LLM alignment and enhancement [2,5,11,16,19,25,31].
### 2.1 Large Language Models: Architectures and Pre-training
The landscape of Large Language Models (LLMs) has undergone a profound evolution, primarily driven by advancements in neural network architectures and the availability of massive datasets. The modern era of LLMs commenced with foundational developments like BERT and its pioneering use of the Transformer architecture, coupled with self-supervised pre-training and supervised transfer learning, which laid the groundwork for sophisticated generative models such as GPT-4 [35]. Early models like BERT were primarily focused on understanding tasks, while models such as T5 demonstrated the effectiveness of supervised transfer learning for generative applications [35]. Today, LLMs, often exemplified by models like BERT and GPT-3, leverage the Transformer architecture to achieve remarkable capabilities in language understanding, reasoning, task planning, and even multimodal information processing [14].

The core of an LLM's development lies in its pre-training phase, where models are exposed to vast corpora of text data. This typically involves objectives such as cross-entropy loss or standard next-token prediction, which enables the model to acquire fundamental language structures, statistical regularities, and extensive factual knowledge, thereby forming a robust "base" model [13,20,29]. For instance, Llama 2 models are built upon a standard Transformer architecture, incorporating pre-normalization with RMSNorm, SwiGLU activation functions, and rotary positional embeddings (RoPE), alongside improvements like increased context length and grouped-query attention (GQA) for larger models (e.g., 34B and 70B parameters) [28]. Similarly, DeepSeek-V2 utilizes a Mixture-of-Experts (MoE) architecture to enhance training and inference efficiency [29]. This foundational knowledge and advanced general capabilities are prerequisites for understanding their utility in downstream tasks [34].

Following pre-training, a critical step in aligning LLMs with human expectations and specific tasks is Supervised Fine-tuning (SFT). During SFT, the base model is further trained on curated datasets, often consisting of question-answer pairs, to elicit specific optimal responses and enhance instruction-following capabilities [13]. This phase transforms general-purpose base models into instruction-tuned models, which are often more suitable for direct user interaction. Famous examples of instruction-tuned LLMs that serve as strong baselines for further refinement include BLOOMZ, Flan-T5, Flan-UL2, and OPT-IML [2]. Other notable instruction-tuned models mentioned in research include the PaLM 2 family (Large, Small, Extra-Small), Llama series, Mistral 7B, Qwen2.5-Math-7B-Instruct, and advanced models like GPT-4 and ChatGPT [9,27,31,32]. While instruction-tuned models often converge faster during subsequent fine-tuning, base models can ultimately achieve similar performance with sufficient training [7].

The sheer scale of these models is a defining characteristic. Pre-training datasets for LLMs are astronomically large; for example, Nemotron-4 340B was pre-trained on 9 trillion tokens, Llama 3 on 15 trillion multi-lingual tokens, and Qwen2 on over 7 trillion high-quality tokens [29]. Correspondingly, the models themselves comprise billions to hundreds of billions of parameters. Llama 3 models range from 8 billion to 405 billion parameters, while DeepSeek-V2 features 236 billion total parameters with 2.1 billion active parameters due to its MoE architecture [29]. Models like Qwen2.5-7B-base and Qwen3-30B-A3B-base further illustrate this scale [15].

This immense scale leads to significant computational demands. For instance, models with 10 billion parameters or more often require substantial GPU memory; a full-precision (float32) model might need up to 40GB of GPU memory just to be loaded, without even considering training. The memory footprint directly correlates with precision, where float32 requires 4GB per billion parameters, float16 needs 2GB, and int8 requires 1GB [2]. This computational burden naturally necessitates the development and application of efficient fine-tuning techniques to make these powerful models more accessible and manageable for specific tasks and research endeavors.

Beyond text, Multimodal Large Language Models (MLLMs) integrate language understanding with the ability to process other modalities, particularly images. Models like GPT-4o, Gemini, Qwen-VL, and LLaVA are prime examples, enabling interactions about image content by combining textual and visual inputs [1,8,17]. These MLLMs, often building upon existing capable LLMs like Qwen2-VL-2B-Instruct, represent a significant stride in AI's ability to interpret complex information in conjunction with natural language [17]. While LLMs demonstrate impressive capabilities in diverse tasks, including mathematical reasoning and programming, they still exhibit limitations in areas such as self-correction, highlighting ongoing challenges and opportunities for further refinement [18]. The continued evolution of LLM architectures and pre-training methodologies remains a cornerstone of advancing artificial intelligence.
### 2.2 Reinforcement Learning Fundamentals

![Markov Decision Process (MDP) for LLMs](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/ZTpy72KKlDe8FFHN0OISN_Markov%20Decision%20Process%20%28MDP%29%20for%20LLMs.png)

Reinforcement Learning (RL) is a machine learning paradigm concerned with how an autonomous agent learns to make optimal decisions by interacting with an environment to maximize a cumulative reward signal [14,24,29]. Unlike supervised learning, which relies on labeled data, RL agents learn through trial-and-error, receiving feedback in the form of rewards based on their actions [29]. This fundamental concept, rooted in early AI research, has enabled agents to surpass human performance in complex environments, such as game playing, solely by maximizing clear reward signals [23].

The standard RL framework is formally described as a Markov Decision Process (MDP), characterized by a tuple $M=(S, A, P, R, \gamma)$ [5,27]. Within this framework, several core components define the interaction:
*   **Agent**: The decision-making entity being trained, which in the context of Large Language Models (LLMs) is typically the model itself [1,29].
*   **Environment**: The external system with which the agent interacts. This can include the input context, external tools like search engines, or the task definition itself [7,29].
*   **State ($s_t$ or $X_t$)**: Represents the current situation or observation of the agent at timestep $t$. In LLMs, this often corresponds to the concatenation of the input prompt and the text generated thus far [27,29].
*   **Action ($a_t$ or $A_t$)**: The choice made by the agent from a set of available options at timestep $t$. For LLMs, actions typically involve generating the next token in a sequence [13,27,29].
*   **Reward ($r_t$ or $R_t$)**: A scalar feedback signal received from the environment after taking an action, indicating the desirability of that action. The reward function is critical as it encodes indicators of task performance [5,29].
*   **Policy ($\pi$)**: A strategy that maps states to probabilities of selecting each possible action, guiding the agent's behavior. The objective of RL is to learn an optimal policy that maximizes the expected cumulative reward over time [5,29].

The interaction proceeds iteratively: at each timestep $t$, the agent observes a state $s_t$, selects an action $a_t$ according to its policy $\pi$, receives a reward $r_t$, and transitions to a new state $s_{t+1}$. This cycle continues until a terminal state is reached, with the overarching goal of maximizing the total accumulated reward, known as the return [5,29]. This principle of maximizing rewards to optimize responses is central to various RL applications [18].

When conceptually adapted to the domain of Large Language Models, the general RL framework provides a powerful mechanism for enhancing model capabilities [29]. In this context, the LLM acts as the agent, and its parameterized generation process defines the policy, which determines the probability of generating subsequent tokens given a current input context [13]. The "environment" for an LLM can be multifaceted, encompassing the input prompt, user interactions, or external tools it can leverage, such as search engines [7]. Actions are the individual token predictions made by the LLM. The state at any given step is the partial sequence of tokens generated so far, concatenated with the original input [27].

A key distinction in applying RL to LLMs is often found in the reward structure. Rewards ($R_t$) are frequently sparse, meaning feedback is typically provided only at the completion of a full response ($R_T$), with intermediate rewards often set to zero [27]. The cumulative reward, therefore, often corresponds to a "quality" rating of the entire generated trajectory or sequence [13]. This quality can be evaluated based on the correctness of a final prediction, as seen in systems optimizing for visual perception or reasoning tasks [1,22,32]. While manual reward engineering can be labor-intensive and prone to sub-optimal outcomes, binary signals (e.g., correct/incorrect) derived from rule-based verification functions offer a direct and effective mechanism for reward design [5,32].

The integration of RL techniques, such as Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO), allows LLMs to optimize their performance across diverse tasks [2,10,15,32]. This enables LLMs to explore and autonomously develop complex reasoning strategies, moving beyond the limitations of standard imitation learning by effectively correcting cumulative errors inherent in next-token prediction [20,22]. Despite the benefits, traditional RL methods face challenges when dealing with the high-dimensional state spaces, complex environments, and sample efficiency demands characteristic of LLMs [14]. Nevertheless, the core RL paradigm remains fundamental to optimizing LLMs for self-correction, emergent reasoning capabilities, and enhanced multi-modal understanding [17,18].
### 2.3 Early Intersections: Reinforcement Learning from Human Feedback (RLHF)
Reinforcement Learning from Human Feedback (RLHF) represents a pivotal initial application of Reinforcement Learning (RL) in the era of large language models (LLMs), emerging as a standard method for aligning LLM outputs with human preferences [16,23,35]. Its genesis stemmed from the inadequacy of previous supervised learning techniques to optimally enhance LLM performance, particularly in achieving nuanced alignment with complex human instructions and societal values [13,35]. The core motivation behind RLHF is to enable LLMs to generate high-quality, helpful, honest, and harmless responses, aligning them with human intentions in scenarios where explicitly defining optimal behavior is challenging, but judging existing responses is relatively straightforward [11,13,30,35]. This approach has been instrumental in significantly improving LLM alignment and expanding their applicability to complex tasks, including reasoning and agent interactions [15].



The typical RLHF pipeline is a multi-stage process, leveraging human feedback to refine model behavior. This workflow often involves three interconnected steps, which can be executed iteratively or concurrently [9,13]:

1.  **Supervised Fine-Tuning (SFT)**: An initial pre-trained LLM is fine-tuned on a high-quality dataset, often consisting of instruction-following examples. This results in an SFT model ($\pi^{\text{SFT}}$) capable of generating structured responses to human prompts [2,4,27,29,33].

2.  **Reward Model (RM) Training**: A critical component of RLHF, this stage involves collecting human preference data. Human annotators are presented with multiple LLM-generated responses for a given input and are asked to rank or select their preferred output. This simplifies the annotation process, as humans find it easier to judge than to produce optimal responses [13,35]. This preference dataset, denoted as $D=\{(x, y_w, y_l)\}$ (where $x$ is the prompt, $y_w$ is the preferred response, and $y_l$ is the dispreferred response), is then used to train a separate reward model ($r_\phi$) [27]. The reward model is typically another LLM that outputs a scalar value indicating the quality or desirability of a given text sequence. It is trained to predict human preferences by minimizing a loss function such as:
    $$
    \mathcal{L}_r(\phi) = - \mathbb{E}_{(x,y_w,y_l) \sim D} 
    $$
    where $\sigma$ is the sigmoid function, and $r_\phi(x,y)$ represents the reward assigned by the model to response $y$ for input $x$ [13,27]. This creates a contrastive mechanism, teaching the RM to assign higher rewards to preferred responses and lower rewards to others [13].

3.  **Policy Optimization**: In this final stage, the SFT model is further fine-tuned using reinforcement learning algorithms, most commonly Proximal Policy Optimization (PPO), with the trained reward model providing the scalar reward signal [2,12,13,19,29]. The LLM acts as the policy, the current text sequence as the state, and the prediction of the next token as the action [29]. The policy, denoted as $\pi^{\text{RL}}_\theta$, is optimized to maximize the cumulative reward from the RM. To prevent the model from drifting too far from the initial SFT model and generating unnatural language (a phenomenon known as "reward hacking"), a Kullback-Leibler (KL) divergence term is often incorporated into the objective function:
    $$
    J(\theta) = \mathbb{E}_{x \sim D, y \sim \pi_\theta(\cdot|x)} \left
    $$
    where $\beta$ is a hyperparameter balancing reward maximization and policy regularization [27]. This online training process requires continuous generation and reward prediction, often necessitating periodic retraining of the reward model to mitigate distribution shifts [13].

Compared to earlier alignment methods, RLHF offers significant advantages. It allows for the optimization of complex, sequence-level objectives that are difficult to specify through explicit rules or supervised examples [27]. RLHF inherently mitigates reward hacking by allowing humans to communicate goals implicitly through preferences rather than requiring hand-specified reward functions [9]. This capability to leverage human judgments, which are often easier to provide than detailed demonstrations, makes RLHF particularly effective for learning intricate solutions and fine-tuning LLMs to exhibit emergent reasoning capabilities and stronger performance in various complex tasks [1,8,9,17]. These benefits have made RLHF a foundational technology for powerful AI systems such as ChatGPT and LLaMA 2 [2,9].

Despite its effectiveness, RLHF faces challenges. A primary limitation is the high cost and difficulty associated with collecting large volumes of high-quality human preference labels, posing a significant bottleneck for scalability [5,11,25,31]. Furthermore, current RLHF methods exhibit a strong dependency on the distribution of preference data, making them vulnerable to "out-of-distribution (OOD) tasks" [16]. Architectural demands, such as the need for multiple model copies during PPO training, also impose substantial GPU memory requirements [2,19]. These limitations have spurred research into alternative approaches, such as DeepSeek-R1's deviation from human-annotated reasoning patterns to explore optimal non-human reasoning paths, and SCoRe's utilization of self-generated data to move beyond traditional RLHF data acquisition for self-correction tasks [18,22].
## 3. A Taxonomy of LLM Roles in Reinforcement Learning
The integration of Large Language Models (LLMs) into Reinforcement Learning (RL) frameworks has introduced a spectrum of novel capabilities, leading to a systematic classification of their diverse contributions. 

**Taxonomy of LLM Roles in Reinforcement Learning**

| LLM Role            | Primary Function                                     | Key Methodologies / Examples                                           | Benefits                                                                                | Challenges                                                                                  |
| :------------------ | :--------------------------------------------------- | :--------------------------------------------------------------------- | :-------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------ |
| **Information Processor** | Processes raw data into actionable insights for RL agents | State representation (ELLM Rewards), reward interpretation (Generative Judge), action space understanding (SEARCH-R1), MLLMs for visual details (Perception-R1) | Bridges semantic gap, simplifies learning, improves alignment, structured representation  | Computational overhead, potential for hallucination, increased processing for natural language |
| **Reward Designer** | Designs or shapes reward functions                    | Preference inference (RLAIF, Direct-RLAIF), explicit code generation (Eureka, Text2Reward, CARD), dynamic feedback (Self-Evolved Reward Learning, Endogenous Rewards) | Scalable reward engineering, addresses sparsity/misalignment, nuanced human-aligned rewards | Potential biases, reward hacking risk, computational cost of refinement, "RM staleness"      |
| **Decision Maker**  | Directly influences agent behavior; policy learning, planning | Action generation (Llama 2-Chat), strategic guidance (Eureka), self-correction (SCoRe, DeepSeek-R1), tool interaction (Agentic Tasks, SEARCH-R1) | Advanced reasoning, common sense, symbolic manipulation, effective tool utilization       | Reasoning depth limitations, consistency issues, alignment with desired outcomes            |
| **Generator**       | Creates synthetic data, diversifies samples, builds env models | Synthetic data (Magpie, SRLM), reward code (CARD), multimodal outputs (Perception-R1), reasoning chains (Data Evolution, DeepSeek-R1), environment models | Addresses data scarcity, reduces human costs, improves exploration/generalization         | Data quality/diversity/bias management, potential for unhelpful/biased outputs              |

This section establishes a taxonomy of LLM roles in RL, categorizing them primarily as Information Processors, Reward Designers, Decision Makers, and Generators [14,34]. This classification serves as an analytical lens, providing a comprehensive understanding of how LLMs augment RL capabilities, the specific methodologies they employ, and the unique challenges they address or introduce across various RL problem settings. By delineating these distinct functions, we lay the theoretical and practical groundwork for a detailed exploration of the various RL techniques for LLMs, which will be further elaborated in the subsequent sections, demonstrating how different approaches manifest these identified roles.

As **Information Processors**, LLMs leverage their inherent capacity to process and interpret vast amounts of textual and often multimodal data, transforming raw observations into structured, actionable insights that are critical for RL agents [14,34]. This role significantly enhances state representation by converting complex environmental states into natural language descriptions or by extracting salient features, thereby simplifying the learning process for underlying neural networks. Methodologies in this domain include techniques like ELLM Rewards, which translate current states into natural language for goal generation, or Multimodal Large Language Models (MLLMs) in frameworks such as Perception-R1, which process visual input to distill relevant details for tasks like object localization [17,29]. Furthermore, LLMs excel at interpreting and generating reward signals (e.g., RLAIF, Generative Judge) and understanding action spaces by translating high-level natural language instructions into formalized actions or generating search queries for external knowledge (e.g., SEARCH-R1) [7,27]. While addressing the semantic gap between human intent and machine execution, this role introduces challenges related to computational overhead and potential hallucination [34].

In their capacity as **Reward Designers**, LLMs tackle the intricate problem of manual reward engineering, offering scalable solutions by designing or shaping reward functions based on their extensive pre-trained knowledge, common sense reasoning, and code generation abilities [5,34]. This role is crucial for overcoming issues of reward sparsity, misalignment, and high human annotation costs. Methodologies include inferring human preferences to train reward models (e.g., RLAIF, Direct-RLAIF) [11,33], explicitly generating executable reward function code from natural language descriptions (e.g., Eureka, Text2Reward, CARD) [5,29], and providing dynamic, direct feedback as reward signals during the RL process (e.g., Self-Evolved Reward Learning, Endogenous Rewards) [20,31]. Although offering significant advancements in generating nuanced, human-aligned rewards, this role necessitates careful consideration of potential biases in inferred preferences, the risk of reward hacking, and the computational implications of iterative refinement [5].

When acting as **Decision Makers**, LLMs directly influence agent behavior, serving as policy learners, generating action sequences, and contributing to high-level planning and strategy. They leverage their advanced reasoning and generative capabilities to explore and execute intricate strategies in complex and long-horizon tasks [34]. This encompasses direct action generation, such as the Llama 2-Chat model functioning as a policy to generate human-aligned responses [28], or MLLMs generating specific outputs in multimodal perception tasks within frameworks like Perception-R1 [17]. Beyond direct actions, LLMs provide strategic guidance, for instance, by optimizing reward *code* (e.g., Eureka) [29]. A significant strength lies in their capacity for self-correction and internal reasoning, enabling them to refine outputs and explore diverse reasoning processes autonomously through techniques like SCoRe, DeepSeek-R1, and various Long CoT methods [6,18]. This role is particularly advantageous for tasks demanding common sense reasoning, symbolic manipulation, and effective tool utilization, yet it requires continuous efforts to ensure consistent reasoning depth and alignment with desired outcomes, especially human preferences [34].

Finally, as powerful **Generators**, LLMs significantly contribute by creating synthetic data, diversifying training samples, and constructing environment models to enhance RL efficiency and accelerate learning in data-scarce or complex scenarios [23,34]. They generate a wide array of synthetic data, including instruction-response pairs and prompts for fine-tuning (e.g., Magpie, SRLM) [29], executable reward *code* (e.g., CARD) [5], multimodal outputs for visual tasks (e.g., Perception-R1) [17], and complex reasoning chains or challenging tasks for data evolution [6]. Their generative nature is instrumental in diversifying training samples and improving exploration, as demonstrated in RLHF/RLAIF where LLMs generate diverse responses for subsequent evaluation [33]. Furthermore, LLMs can simulate complex environments, effectively acting as "world models" for model-based RL [14]. While this role profoundly addresses data scarcity and reduces human annotation costs, it mandates diligent management of data quality, diversity, and potential biases through mechanisms such as format rewards, self-correction, and alignment techniques like RLHF [18,35].
### 3.1 LLM as Information Processor
Large Language Models (LLMs), functioning as sophisticated information processors, significantly enhance the understanding and contextualization within Reinforcement Learning (RL) environments by distilling vast amounts of data into actionable insights [14,34]. This role leverages the LLM's inherent ability to process and interpret information, providing a crucial bridge between raw environmental observations and the structured representations required for effective decision-making in RL tasks [34]. The foundational knowledge embedded within pre-trained LLMs, acquired from extensive training corpora, establishes a robust basis for this information processing capability, which is then further refined in RL-augmented contexts [28].

A primary mechanism through which LLMs operate as information processors is by improving **state representation**. They achieve this by transforming complex or high-dimensional environmental states into more comprehensible and actionable forms. For instance, in methods like ELLM Rewards, the current state of an RL agent is converted into a natural language description, which the LLM then processes to generate exploration goals, such as specific actions or target locations [29]. This involves distilling environmental information into concise, semantic objectives. Similarly, LLMs can extract salient environmental representation features or task instruction representations, thereby accelerating the learning process for the underlying neural networks in RL agents [14]. In the context of multimodal RL, such as with Multimodal Large Language Models (MLLMs) in the Perception-R1 framework, the LLM processes visual input by extracting and understanding relevant visual details from images. This "perception policy" involves distilling specific visual cues for tasks like object localization, instance counting, or text recognition, which are critical for enhancing fine-grained visual perception and guiding task execution [1,8,17]. This demonstrates the LLM's capacity to convert sensory data into structured, meaningful representations for decision-making.

Another critical contribution lies in **reward signal interpretation and generation**. LLMs can interpret candidate outputs and provide detailed natural language explanations for their evaluations, as seen in methods like Generative Judge, thereby aligning RL agents with human preferences through detailed reasoning [29]. More profoundly, LLMs can implicitly embed information about desired outcomes during their standard training, which can be extracted as an "endogenous reward" for self-evaluation by an RL agent, negating the need for explicitly constructed reward models [20]. In frameworks like Reinforcement Learning from AI Feedback (RLAIF), LLMs act as information processors by interpreting input contexts and candidate responses to generate preference labels or direct scores [27]. This process involves crafting specific prompts, utilizing few-shot exemplars, and extracting log-probabilities to compute preference distributions. The integration of Chain-of-Thought (CoT) reasoning further enhances alignment by requiring explanations and rationales, showcasing the LLM's capacity for explicit evaluative processing [27]. Furthermore, in the CARD framework, an LLM-based Coder processes environment descriptions and task goals to synthesize initial reward function code, directly distilling textual information into executable reward logic [5]. The SRPO framework extends this by enabling an "in-context self-improving model" where the LLM recursively analyzes and refines its own outputs based on learned alignment rules from preference data, effectively acting as an internal critic and reward re-evaluator [16].

Regarding **action space understanding**, LLMs facilitate the translation of complex natural language task instructions into formalized, task-specific languages, thereby reducing the complexity of the learning process for RL agents [14]. This includes guiding the generation of actionable objectives from high-level natural language commands [29]. For tasks requiring external knowledge, frameworks like SEARCH-R1 enable LLMs to autonomously generate search queries, interact with real-time retrieval mechanisms, and process the retrieved content to inform subsequent reasoning and response generation, effectively expanding and contextualizing the agent's understanding of its action space and potential outcomes [7].

The strengths of employing LLMs as information processors in RL are evident in their ability to bridge the semantic gap between human instructions or complex observations and the numerical inputs typically required by RL algorithms [34]. This leads to simplified learning processes, accelerated network learning, and improved alignment with human preferences in complex, information-rich RL tasks [14,27]. However, this approach also introduces potential limitations, such as significant computational overhead due to the inherent scale of LLMs and the increased processing required for natural language or multimodal inputs. Furthermore, the risk of hallucination, where LLMs generate factually incorrect or nonsensical information, remains a concern, potentially leading to suboptimal or harmful decisions within the RL environment [34]. Addressing these limitations is crucial for the robust and reliable integration of LLMs as information processors in advanced RL systems.
### 3.2 LLM as Reward Designer
The efficacy of Reinforcement Learning (RL) agents is profoundly influenced by the quality of their reward functions. Manually engineering these reward functions is a notoriously challenging task, often characterized by high human cost, potential for sub-optimality, and unexpected outcomes due to reward sparsity or misaligned objectives [5,34]. Large Language Models (LLMs) are emerging as powerful "Reward Designers" to mitigate these challenges, leveraging their extensive pre-trained common sense knowledge, code generation capabilities, and in-context learning abilities to design or shape reward functions for RL tasks [14,34]. This paradigm aims to harness LLMs' understanding of human preferences and task objectives to create more effective and aligned reward signals, reducing reliance on laborious manual engineering [34].

Different strategies for LLM-driven reward generation have been explored, each offering distinct mechanisms to capture nuanced human preferences or objective task criteria. These strategies can be broadly categorized into preference inference, explicit reward rule or code generation, and direct dynamic feedback.

**Preference Inference**
One prominent approach involves LLMs acting as AI preference labelers to infer and generate feedback signals. Reinforcement Learning from AI Feedback (RLAIF) exemplifies this, where an off-the-shelf LLM generates preference labels for model responses, effectively replacing human annotators in training a Reward Model (RM) [25,33]. In Canonical RLAIF, these LLM-generated preferences are used to train a separate RM, which then assigns rewards to policy responses during RL fine-tuning [27]. The LLM is often prompted with task descriptions and potential responses, employing techniques like Chain-of-Thought (CoT) to refine its feedback and consider factors such as position bias [33]. This method is distinct from approaches like Llama 2, where reward models are trained *based on* human preferences rather than LLMs designing the preference criteria themselves [28].

**Explicit Reward Rule or Code Generation**
Another strategy involves LLMs in the direct generation or modification of reward function code. Methods like Reward Design with Language Models (RDLM) allow users to define desired behaviors through natural language, which the LLM then uses to generate reward signals by evaluating agent actions against these standards [29]. Advanced systems such as Eureka leverage LLMs (e.g., GPT-4) to automatically generate and iteratively optimize reward function code. Starting from an initial code generated from a task description, Eureka refines it using evolutionary strategies based on the RL agent's performance, demonstrating superiority over human-designed rewards in complex robotics tasks [29]. Similarly, Text2Reward provides a framework where LLMs generate dense and interpretable reward function code from natural language, enabling efficient reward shaping and iterative refinement through human feedback [29]. The Self-Refined LM approach further pushes this boundary by using LLMs to generate and iteratively refine reward functions for Deep Reinforcement Learning (DRL) tasks through self-optimization, reducing external intervention [29]. The CARD framework represents a significant advancement in this area, utilizing an LLM-based Coder to generate and verify reward code and an Evaluator to provide dynamic feedback. It addresses the high human and computational costs of traditional reward engineering by iteratively refining reward function code without human intervention or repeated full RL training at each step, instead employing Trajectory Preference Evaluation (TPE) for dynamic feedback. This has shown to match or even outperform human-designed rewards on complex robotic manipulation tasks [5].

**Direct/Dynamic Feedback Generation**
Beyond training a separate reward model or generating code, LLMs can directly provide dynamic reward signals during the RL process. Direct-RLAIF (d-RLAIF) is an example where an off-the-shelf LLM is prompted directly to rate the quality of a generated response on a scale, with this direct score normalized and used as the reward signal for the RL agent [11,27]. This method effectively circumvents the separate training of a reward model and addresses the "RM staleness" issue, where an RM trained on initial policy generations may become out-of-distribution as the policy evolves [27]. Another innovative approach is Self-Evolved Reward Learning (SER), where an LLM (acting as a reward model) progressively designs and refines its own reward function and feedback signals through a self-evolution process. This involves self-annotating data and iteratively improving its judgment capabilities, thereby reducing dependence on human preferences [31]. Furthermore, research suggests the existence of an "endogenous reward" (EndoRM) within LLMs, which can function as a powerful generalist reward model. This EndoRM allows the LLM to design or infer its own reward functions, guided by prompts, acting as a dynamic, promptable judge without relying on expensive human annotation [20]. "Generative Rewards" also fall into this category, where an LLM is trained to generate textual evaluations or scores as reward signals for subjective tasks that lack objective answers, such as assessing article creativity [30].

**Benefits and Challenges**
The primary benefits of utilizing LLMs as reward designers include significantly enhanced scalability and adaptability. These methods offer a cost-effective alternative to human preference labeling, often achieving comparable or even superior alignment performance [27]. They can infer implicit objectives from human language or demonstrations, translating them into actionable rewards that are more nuanced and human-aligned, addressing issues like reward sparsity and misaligned objectives [34].

However, the integration of LLMs as reward designers also introduces specific challenges. While not explicitly detailed in every digest regarding "bias propagation" or "reward hacking" in the context of LLM-designed rewards, these are inherent risks when automating reward generation. Misalignment of the LLM's inferred preferences with true human intent could lead to biased reward signals. Additionally, sophisticated LLMs might generate rewards that, while optimizing a proxy, fail to achieve the true underlying objective or inadvertently encourage undesirable behaviors (reward hacking). The "RM staleness" problem in RLAIF highlights a challenge related to the dynamic nature of learning policies, where a static reward model can become ineffective [27]. Furthermore, practical limitations exist; for instance, the CARD framework, despite its advantages, notes that token consumption increases linearly with each iteration, which could become substantial for complex tasks requiring extensive refinement [5]. It is also crucial to distinguish these methods from approaches that do not rely on LLMs for reward design, such as DeepSeek-R1, which utilizes verifiable external feedback based on correctness against standard answers [22]. Similarly, the SCoRe method employs reward shaping to encourage self-correction, but the LLM itself does not design the rewards [18].

In summary, LLMs offer a promising avenue for automating and enhancing reward engineering in RL, moving beyond manual efforts towards scalable and adaptive solutions. The diverse strategies—ranging from preference inference to direct code generation and dynamic feedback—underscore the versatility of LLMs in crafting more effective and aligned reward signals, despite the need to carefully address potential issues like bias and computational costs.
### 3.3 LLM as Decision Maker
Large Language Models (LLMs) are increasingly being leveraged as sophisticated decision-makers within reinforcement learning (RL) frameworks, particularly in complex and long-horizon tasks. In this capacity, LLMs can function as policy learners, generate action sequences, or contribute to high-level decision-making processes such as planning and strategy generation [34]. This paradigm shift enables LLMs to directly influence and determine agent behavior, exploiting their advanced reasoning and generative capabilities to explore and execute intricate strategies [34].

The role of LLMs as decision-makers manifests in several forms. One primary function involves the direct generation of actions or policies. For instance, in the Llama 2 framework, the Llama 2-Chat model acts as the policy itself, trained via Reinforcement Learning from Human Feedback (RLHF) to generate token sequences that optimize for human-aligned reward signals, thereby making decisions conducive to helpfulness and safety in conversational contexts [28]. Similarly, in multimodal settings, the MLLM in the Perception-R1 framework operates as a decision-maker by generating specific outputs, such as boundary box coordinates for object localization or transcribed text for OCR tasks, after processing visual information [17]. This involves optimizing a "perception policy" that guides the MLLM's internal strategy for handling visual tasks, learning to interpret visual information and execute logical operations like comparing positions or identifying instances [1,8]. The ROLL framework further exemplifies this by supporting Agentic RL, allowing LLMs to learn and execute actions and strategies in diverse environments like games and multi-round dialogues [15]. Furthermore, LLMs can generate actions or guide the action selection for RL agents, utilizing their extensive knowledge bases to improve sample and exploration efficiency in RL processes [14]. Fine-tuning with endogenous rewards also enables LLMs to act as policy learners, making token generation decisions that maximize internal reward signals, leading to self-improvement effects [20].

Beyond direct action generation, LLMs provide high-level guidance and contribute to strategic planning. This distinction from traditional RL agents lies in the ability of LLMs to perform more abstract and language-based planning, offering advantages in tasks requiring common sense reasoning or symbolic manipulation [34]. For example, the Eureka system empowers LLMs to dictate complex strategies for robotic skills by generating and optimizing reward *code*, thereby implicitly guiding the RL agent's decision-making framework [29].

A critical aspect of LLMs as decision-makers is their capacity for self-correction and internal reasoning. Methods like SCoRe train LLMs to perform intrinsic self-correction by making sequential decisions within their internal thought processes to refine outputs based on prior attempts and learned rewards, particularly in scientific domains [18]. DeepSeek-R1 further demonstrates this by autonomously exploring and generating diverse reasoning processes, including verification, reflection, and exploring alternative methods, without explicit human instruction [22]. The concept of "Implicit Trial-and-Error Search" or "Long CoT" (Chain-of-Thought) highlights LLMs' ability to linearize entire search processes, enabling self-evaluation and self-correction during reasoning. Models such as O1 Journey, DeepSeek-R1, Kimi-k1.5, and T1, trained with RL, generate long reasoning chains, incorporating error detection, backtracking, and correction to make informed decisions on their thought processes [6]. O1, specifically, engages in extensive internal thought processes to refine strategies, correct errors, and decompose complex problems, effectively acting as a decision-maker within its reasoning trajectory [29].

LLMs also function as decision-makers when interacting with external environments and tools. In "Agentic Tasks," LLMs learn to make strategic decisions on when and how to utilize external tools (e.g., calculators), invoke APIs, browse the internet, or interact with graphical user interfaces (GUIs) to achieve complex objectives [30]. The SEARCH-R1 framework illustrates this by training an LLM to autonomously generate search queries, deciding the initiation and termination of searches to integrate retrieved information for complex task solving [7].

The integration of LLMs with traditional RL algorithms presents distinct advantages, particularly in handling open-ended problem spaces and facilitating natural language interaction. By leveraging their powerful reasoning and generative capabilities, LLMs can contribute to improving the sample and exploration efficiency of RL processes [14]. This role offers conceptual advantages in handling tasks that demand common sense reasoning or symbolic manipulation, which are often challenging for traditional, purely numeric RL agents [34].

However, several challenges persist, including limitations in reasoning depth, ensuring consistency across complex decision sequences, and maintaining alignment with desired outcomes, especially human preferences [34]. Efforts like SRPO address the alignment challenge by enabling LLMs to make decisions on modifying and enhancing generated completions based on established human preferences, facilitating iterative refinement [16]. The robust reasoning decisions targeted in models like Kimi-k1.5 also underscore the ongoing work to deepen reasoning capabilities and consistency [29].
### 3.4 LLM as Generator
Large Language Models (LLMs) serve as powerful generators within Reinforcement Learning (RL) frameworks, primarily leveraging their capacity to create synthetic data, diversify training samples, and construct environment models [23,34]. This generative capability is crucial for enhancing RL efficiency, accelerating learning, and enabling exploration in complex or data-scarce environments [34].

One of the most significant utilities of LLMs as generators is in **synthetic data generation**. This addresses the challenge of data scarcity and aims to improve the generalization and robustness of RL agents [34]. LLMs can produce a wide array of synthetic data:
*   **Instruction-response pairs and prompts**: Methods like Magpie utilize an aligned LLM's autoregressive nature to self-synthesize user queries and corresponding responses, generating large datasets for fine-tuning [29]. Similarly, Self-Rewarding Language Models (SRLM) empower the LLM to generate new prompts and multiple candidate responses, effectively creating its own training data [29]. The SCoRe method also relies on "completely self-generated data" to train an LLM's self-correction capabilities [18].
*   **Reward signals and evaluations**: LLMs can generate explicit evaluations and detailed natural language reasoning for preferences, as seen in Generative Judge via Self-generated Contrastive Judgments (Con-J) [29]. More profoundly, frameworks such as Eureka, Text2Reward, and CARD demonstrate LLMs' ability to generate executable reward *code* from natural language descriptions of tasks and environments [5,29]. This automates a traditionally manual and labor-intensive process, significantly reducing engineering overhead and training costs [5].
*   **Multimodal outputs**: In multi-modal contexts, such as the Perception-R1 framework, MLLMs generate specific formatted outputs for visual perception tasks. This includes producing boundary box coordinates for object localization, numerical counts for object counting, and transcribed text for Optical Character Recognition (OCR) [8,17]. These generations are guided by optimized "perception policies" and reinforced through rule-based rewards to ensure accuracy and correct formatting [17].
*   **Reasoning chains and tasks**: Under the "Data Evolution" paradigm, LLMs are instrumental in generating diverse, challenging, and reliable tasks, alongside high-quality Chain-of-Thought (CoT) reasoning data [6]. They can modify data types, rephrase questions, or increase task complexity by adding constraints and deepening queries. For CoT generation, LLMs employ "meta-operations" and search algorithms like BFS/DFS, Beam Search, A*, and MCTS, sometimes with explicit tree search or implicit trial-and-error search involving self-evaluation and self-correction [6]. DeepSeek-R1 further exemplifies this by generating diverse and complex reasoning behaviors through pure RL, allowing it to "self-evolve" its reasoning patterns [22].
*   **Search queries and partial solutions**: LLMs in frameworks like SEARCH-R1 generate search queries and final answers interleaved with search actions [7]. Similarly, Prefix-Guided Sampling in LPPO leverages the LLM's generative capacity to complete solutions given expert prefixes, balancing exploration and learning from examples [32].

Beyond synthetic data, LLMs significantly contribute to **diversifying training samples and enhancing exploration**. In methods like RLHF and RLAIF, the LLM itself acts as the policy model, generating responses (actions) that are subsequently evaluated by a reward model [27,33]. This generative process, often involving techniques like rejection sampling, where the model generates multiple (K) outputs for a given prompt and selects the best, is critical for exploring the action space and identifying higher-quality trajectories [28]. The ROLL framework incorporates a "Rollout Scheduler" that manages prompt samples during generation and employs dynamic sampling to improve sampling efficiency [15]. DeepSeek-R1's training process enables it to generate diverse and complex reasoning behaviors, demonstrating how RL can foster emergent generative capabilities [22]. Furthermore, Ghost Attention (GAtt) in Llama 2 uses the LLM's generative capacity to synthesize data for multi-turn consistency training [28].

LLMs also find utility in **constructing environment models**. They can simulate complex environments, effectively acting as "world models" to produce accurate trajectories for model-based reinforcement learning [14,34]. This ability to simulate complex interactions and environments without extensive real-world data collection directly accelerates the learning process and facilitates exploration in novel or dangerous situations [34].

The impact of LLM-generated data on model generalization and training costs is substantial. By providing a scalable source of feedback through self-annotation, LLMs as reward models can significantly enhance RL training efficiency [31]. The automatic generation of tasks, reward functions, and diverse training samples reduces the need for costly human annotation and manual feature engineering, leading to improved generalization and robustness of RL agents [34].

However, the use of LLMs as generators introduces crucial **trade-offs between data quality, diversity, and the potential for introducing new biases**.
*   **Data Quality**: While LLMs can generate diverse data, ensuring its quality and factual accuracy is paramount. Techniques like "format rewards" in Perception-R1 explicitly check if generated outputs conform to expected structures, enforcing quality [8]. In task generation, fine-tuned LLMs score tasks, inconsistencies are filtered, and programming tasks are verified with interpreters or predefined rules to ensure reliability [6]. Generative Adversarial Networks (GANs) are also employed, with critics improving the reliability of synthesized tasks [6]. Self-correction mechanisms, such as those in "Implicit Trial-and-Error Search" for CoT generation, allow LLMs to detect and rectify errors internally [6].
*   **Diversity**: LLMs naturally offer high diversity through their generative nature and sampling strategies. Methods like dynamic sampling and rejection sampling [15]. Such diversity allows for more comprehensive exploration of the state-action space, which is critical for robust policy learning.
*   **Bias**: A primary concern is the potential for LLMs to generate biased or unhelpful outputs, stemming from their training data or inherent limitations. Reinforcement Learning from Human Feedback (RLHF) directly addresses this by aligning the generative capabilities of LLMs with human user needs, guiding them to produce responses that are more helpful, honest, and safe [23,35]. This process effectively mitigates the introduction of undesirable biases by actively refining the generative process based on human preferences. In RLAIF and RRHF, LLMs generate responses that are then evaluated and ranked, often by other LLMs or human experts, for alignment with preferences, implicitly addressing potential biases in raw generation [4,33].

In summary, LLMs serve as versatile generators across various facets of RL, from creating diverse synthetic data for training and exploration to simulating complex environments and automating reward function design. While these capabilities offer substantial benefits in efficiency and generalization, careful design, incorporating mechanisms for quality control and alignment, is essential to manage the inherent trade-offs related to data quality, diversity, and potential biases.
## 4. Core Reinforcement Learning Techniques for LLM Alignment
This chapter delves into the fundamental Reinforcement Learning (RL) techniques that have been specifically adapted and developed for aligning Large Language Models (LLMs) with human preferences and desired behaviors. The evolution of these methods traces a clear progression, beginning with the foundational Reinforcement Learning from Human Feedback (RLHF) paradigm and extending to more advanced preference-based optimization strategies, innovative AI-driven feedback loops, and a diverse array of simplified policy optimization techniques. Collectively, these approaches aim to imbue LLMs with properties such as helpfulness, harmlessness, and honesty, making them safer, more controllable, and ultimately more capable for a wide range of applications [10,23,24,28,29,35]. This overview evaluates their respective mechanisms, highlights their advantages in achieving robust and efficient alignment, and acknowledges their inherent limitations, setting the stage for understanding the current research landscape and future directions.

The journey towards LLM alignment is largely anchored by the **Reinforcement Learning from Human Feedback (RLHF) paradigm**, which has revolutionized how language models are fine-tuned to reflect human intentions and values [8,17]. RLHF typically involves a multi-stage process: an initial Supervised Fine-tuning (SFT) of a pre-trained LLM, followed by the training of a Reward Model (RM) using human preference data, and culminating in Reinforcement Learning fine-tuning, most commonly employing Proximal Policy Optimization (PPO) [13,28,29,33]. While PPO-based RLHF has demonstrated significant success, enabling models like ChatGPT, it is plagued by substantial challenges. 

**Challenges of PPO-based RLHF**

| Challenge Category       | Specific Issue                                                                           | Description & Impact                                                                                                                                                                                                                                                                                          |
| :----------------------- | :--------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Algorithmic Complexity** | Hyperparameter Sensitivity                                                               | PPO notoriously sensitive to tuning, requiring meticulous configuration for stable and optimal performance [4].                                                                                                                                                                                               |
|                          | Multiple Model Requirement                                                               | Necessitates active policy, reference policy, reward model, (sometimes critic) models. Leads to high memory pressure & GPU demands, especially for large LLMs. Complicates scaling [4, 15, 19].                                                                                                                 |
|                          | Training Complexity & Resource Consumption                                               | Multi-model architecture & iterative online nature result in high complexity, substantial computational resources. Time-consuming due to many interactions with environment/RM [6, 24].                                                                                                                       |
| **Performance Issues**   | Mode Collapse                                                                            | Policy generates limited diversity of responses, collapsing to deterministic outputs rather than exploring full spectrum of desired behaviors [12].                                                                                                                                                           |
|                          | Poor Sample Efficiency                                                                   | Requires many interactions (samples) to learn effectively. Expensive & time-consuming given LLM inference costs [12].                                                                                                                                                                                        |
|                          | Instability & Robustness                                                                 | PPO can be unstable during training. Optimal solutions are often task-dependent and lack robustness to Out-of-Distribution (OOD) tasks or shifts in behavior policy, potentially undoing prior SFT/pre-training [12, 16].                                                                                   |
| **Data & Feedback**      | High Cost & Bias of Human Feedback                                                       | Primary bottleneck for scalability. Collecting high-quality human preference data is costly, time-consuming, and prone to biases. Can suffer from data conflicts & strong subjectivity [5, 11, 25, 31].                                                                                                     |
|                          | Reward Hacking                                                                           | Model exploits shortcuts to maximize reward without genuinely improving, leading to unintended behaviors. KL divergence penalty attempts to mitigate but doesn't eliminate [27, 28].                                                                                                                         |

These include PPO's notorious sensitivity to hyperparameters, the need for multiple models (active policy, reference policy, reward model) leading to high memory pressure, and considerable training complexity and computational resource consumption [4,9,15,19,24]. Furthermore, PPO can suffer from mode collapse, poor sample efficiency, and robustness issues to out-of-distribution (OOD) tasks, alongside the inherent cost and potential bias of collecting high-quality human feedback [12,16,31]. These limitations have spurred extensive research into more efficient and robust alternatives.

Responding to the complexities of PPO and the resource demands of reward modeling, **Advancements in Preference-Based Optimization** have emerged as a critical area of development. Direct Preference Optimization (DPO) stands as a seminal example, simplifying RLHF by directly optimizing the language model's policy using an offline preference dataset, thus bypassing the explicit training of a separate reward model [19,29]. While offering enhanced stability and efficiency, DPO still faces limitations concerning exploration, fine-grained optimization, and robustness to OOD scenarios [13,16]. Kahneman-Tversky Optimization (KTO) further simplifies feedback collection, operating on single-instance desirable/undesirable feedback rather than costly pairwise comparisons, thereby reducing annotation costs and implementation complexity [24]. For algorithmic improvements, Advantage-Induced Policy Alignment (APA) addresses PPO's instability and sample inefficiency through a novel squared error loss function incorporating estimated advantages, preventing mode collapse and enhancing response diversity [12]. Focusing on generalization, Self-Improving Robust Preference Optimization (SRPO) tackles the robustness challenges of conventional methods in OOD tasks and policy shifts, conceptualizing preference learning as a continuous self-improvement mechanism with mathematical independence from the behavior policy [16].

To address the scalability bottlenecks associated with human feedback, **Reinforcement Learning from AI Feedback (RLAIF)** has gained prominence. This paradigm replaces human preference labeling with synthetic feedback generated by powerful, off-the-shelf LLMs, significantly reducing reliance on human annotators and enhancing efficiency, consistency, and cost-effectiveness [11,33]. RLAIF has demonstrated performance comparable to, and in some cases even surpassing, RLHF, particularly in domains like harmless dialogue generation [25,27]. Key innovations within RLAIF include advanced prompt construction techniques (e.g., Chain-of-Thought prompting) for AI labelers, the concept of LLM self-improvement where models refine themselves using self-generated feedback, and Direct-RLAIF (d-RLAIF). The latter circumvents explicit reward model training by directly prompting an LLM to rate responses during the RL phase, thus resolving issues like "RM staleness" and further streamlining the alignment process [11,27].

The broader trend of **Simplifying Policy Optimization** encompasses a variety of techniques aimed at making RL-based fine-tuning more accessible and efficient by reducing model complexity, hyperparameter sensitivity, and resource intensiveness. Beyond DPO and KTO, which offer direct simplifications, other notable methods include Rank Responses to Align Language Models with Human Feedback (RRHF), which uses a ranking loss function to align probabilities with human preferences, requiring fewer models and less hyperparameter tuning than PPO [4]. Critic-free algorithms like Group Relative Policy Optimization (GRPO) avoid the overhead of auxiliary critic models by relying on sequence-level rewards, improving scalability for large reasoning models [6,30]. REINFORCE variants and Rejection Sampling Fine-tuning (e.g., in Llama 2) also contribute to simplification by reducing dependence on value models or transforming RL into selection and supervised fine-tuning tasks [28]. Moreover, continuous efforts are being made to simplify reward acquisition through frameworks like CARD for automated reward design and the exploration of "endogenous rewards," further contributing to the overall efficiency of policy optimization [5,20].

In essence, the landscape of RL techniques for LLM alignment reflects a dynamic interplay between foundational principles and continuous innovation. While RLHF laid the groundwork, its challenges have propelled the development of sophisticated preference-based methods like DPO, KTO, APA, and SRPO to enhance efficiency and robustness. Concurrently, RLAIF offers a scalable solution for feedback acquisition by leveraging AI's evaluative capabilities. The collective drive across these advancements is to overcome the inherent complexities of traditional RL, minimize model dependencies, simplify feedback mechanisms, and achieve more stable, efficient, and generalizable alignment of LLMs with human preferences. Despite significant progress, common challenges persist, including the consistent quality and cost of preference data, achieving truly fine-grained control, and ensuring robustness across highly diverse and out-of-distribution tasks. Future research will likely focus on hybrid approaches that integrate the strengths of these diverse techniques, aiming for even more data-efficient, inherently robust, and ethically aligned LLMs.
### 4.1 The RLHF Paradigm: Reward Modeling and Policy Optimization
Reinforcement Learning from Human Feedback (RLHF) stands as a foundational paradigm for aligning Large Language Models (LLMs) with human preferences, intentions, values, and ethical standards [4,6,9,10,12,23,24,29,35]. This approach aims to imbue LLMs with properties such as helpfulness, harmlessness, and honesty, making them safer and more capable for diverse applications, as exemplified by models like ChatGPT and LLaMA 2 [9,28,29]. RLHF has been credited with driving a paradigm shift in language models, unlocking emergent reasoning capabilities and setting a successful precedent for applying reinforcement learning to enhance LLM functionality [1,8,17].

The typical RLHF process involves a multi-stage pipeline designed to progressively refine an LLM's behavior based on human input. While iterative or concurrent, these stages generally include Supervised Fine-tuning (SFT), Reward Model (RM) training, and Reinforcement Learning fine-tuning [4,13,15,28,29,33].

1.  **Supervised Fine-tuning (SFT)**: This initial stage prepares a pre-trained LLM by fine-tuning it on a high-quality dataset of human-annotated instruction-following examples [27,28,29,33]. The objective is to improve the model's ability to follow instructions and generate a baseline of desirable responses, focusing on quality over quantity [28].
2.  **Reward Model (RM) Training**: The core of RLHF relies on learning a proxy for human judgment. This involves gathering human preference data, where annotators compare and rank multiple responses generated by the LLM for a given prompt [13,27,28,29,35]. This method significantly reduces the cognitive burden on annotators compared to requiring them to write entire responses [35]. A separate reward model, often another LLM, is then trained on this comparative data to predict a scalar reward indicating the quality of a response [13,28]. The reward model's loss function typically uses a contrastive mechanism, such as binary cross-entropy or a preference loss, to differentiate between preferred ($y_w$) and rejected ($y_l$) responses for a given input $x$:
    $$L_{RM} = -\log\sigma(r(x, y_w) - r(x, y_l))$$
    or more generally
    $$ \mathcal{L}_r(\phi) = - E_{(x,y_w,y_l) \sim D}  $$
    where $r$ or $r_\phi$ is the reward model, $\sigma$ is the sigmoid function, and $D$ is the dataset of human preferences [13,27,33]. This training enables the RM to assign scalar scores that act as a proxy for human feedback, guiding the LLM to generate high-quality responses while avoiding low-quality ones [13]. It has been noted that reward models can be more consistent and accurate than automatic evaluation metrics like ROUGE [35]. For instance, Llama 2-Chat trains distinct reward models for helpfulness and safety using a modified binary ranking loss with a margin [28].
3.  **Reinforcement Learning Fine-tuning**: In the final stage, the SFT-tuned LLM (referred to as the policy model or actor LLM) is further fine-tuned using the trained reward model as a feedback mechanism [19,33]. Proximal Policy Optimization (PPO) is the most commonly employed Reinforcement Learning algorithm for this purpose [6,13,24,28,29,33]. PPO iteratively updates the LLM's generation policy to maximize the expected cumulative reward predicted by the RM [33]. The process involves generating responses from the actor LLM, evaluating them with the RM to obtain rewards, and then applying PPO updates [13]. To prevent the policy from deviating excessively from the original SFT model and maintain response quality, a Kullback-Leibler (KL) divergence penalty is often incorporated into the RL objective function [27,28,33]. This penalty helps stabilize training and mitigate "reward hacking" [27,28]. The general objective function for PPO can be expressed as:
    $$ \mathop{max}\limits{\theta} E_{(x,y) \sim \pi_\theta}  $$
    where $\pi_\theta$ is the current policy, $\pi_{SFT}$ is the SFT model, and $\beta$ is a hyperparameter controlling the KL divergence penalty [33]. The training is online, necessitating periodic retraining of the reward model to prevent distribution shifts [13]. Llama 2-Chat additionally utilizes Rejection Sampling Fine-tuning alongside PPO as an alternative RL method for refining its model [28].

While the general RLHF pipeline, comprising SFT, RM training, and PPO, is widely adopted across various studies [4,13,33], some approaches diverge from this classical setup. For instance, the SCoRe method fundamentally differs by using "completely self-generated data" instead of human feedback for training, bypassing explicit reward model training based on human preferences [18]. Similarly, DeepSeek-R1's methodology for reasoning enhancement bypasses the SFT stage entirely and employs a "pure reinforcement learning" strategy focused solely on the correctness of the final output, without explicit preference modeling for intermediate steps [22]. The Reinforcement Learning with Verifiable Reward (RLVR) paradigm, as seen in LPPO, simplifies reward modeling to a rule-based metric (e.g., mathematical correctness) determined by an automatic verifier, rather than a human-preference-trained RM [32].

Despite PPO's effectiveness and widespread use in RLHF, its implementation introduces several significant challenges that motivate ongoing research and advancements in the field [4,9,12]. These limitations include:

*   **Sensitivity to Hyperparameters**: PPO is notoriously sensitive to the choice of hyperparameters, requiring meticulous tuning to achieve stable and optimal performance [4].
*   **Requirement for Multiple Models**: The standard PPO implementation in RLHF necessitates the deployment and coordination of multiple models—typically an active policy model, a reference policy model (often a frozen SFT model), a reward model, and sometimes a critic model [4,15,19]. This setup imposes significant memory pressure, especially for larger LLMs, as two copies of the model (active and reference) are often required to calculate logits, complicating scaling on single GPU devices [2].
*   **Training Complexity and Resource Consumption**: The multi-model architecture, coupled with the iterative online nature of PPO, results in high training complexity and substantial computational resource consumption [6,24]. PPO training often requires a large amount of interaction data, leading to challenges in efficiency [24].
*   **Mode Collapse and Instability**: PPO can suffer from issues like mode collapse, where the policy generates a limited diversity of responses, and general instability during training [12].
*   **Poor Sample Efficiency**: The algorithm often exhibits poor sample efficiency, meaning it requires many interactions with the environment (or the reward model) to learn effectively, which can be time-consuming and expensive given the inference costs of LLMs [12].
*   **Cost and Bias of Human Data**: The reliance on high-quality human-annotated data for reward model training presents a significant bottleneck due to its high cost, time consumption, and potential for bias, limiting RLHF's scalability and efficacy [5,25,31].
*   **Robustness Issues**: Existing RLHF methods, including PPO, may yield optimal solutions that are highly task-dependent and lack robustness to out-of-distribution (OOD) tasks or changes in the behavior policy. This can render preference datasets and reward models less effective and potentially degrade prior SFT or pre-training efforts [16].

These inherent limitations of PPO within the RLHF paradigm have served as key drivers for the development of alternative alignment techniques and efficiency improvements in the field.
### 4.2 Advancements in Preference-Based Optimization

**Comparison of Advanced Preference-Based Optimization Methods**

| Method                                    | Core Idea                                                                                               | Key Advantages                                                                                                       | Limitations / Challenges                                                                                                        |
| :---------------------------------------- | :------------------------------------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------- |
| **Direct Preference Optimization (DPO)**  | Directly optimizes policy using offline preference data, converting RL to SFT task. Avoids explicit RM. | Simpler, more stable, computationally efficient than PPO. Requires fewer models (2 vs 4). Widely adopted.         | Exploration limitations (OOD prompts), insufficient fine-grained optimization, potential for overfitting/performance collapse, task-dependency [13, 16] |
| **Kahneman-Tversky Optimization (KTO)**   | Uses single-instance desirable/undesirable feedback, inspired by human decision-making theory.        | Significantly reduces data annotation costs (no pairwise comparisons), simplified implementation, high efficacy.     | Focus on binary feedback might miss nuance of preferences.                                                              |
| **Advantage-Induced Policy Alignment (APA)** | Replaces PPO's clipped objective with squared error loss using estimated advantages.                     | Enhanced stability, improved sample efficiency, prevents mode collapse, augments response diversity.                  | Still requires an independent reward model for evaluation.                                                               |
| **Self-Improving Robust Preference Optimization (SRPO)** | Conceptualizes preference learning as continuous self-improvement, uses min-max objective.          | Mathematically independent of behavior policy, robust to OOD tasks and policy shifts, superior generalization (e.g., +15% WR vs DPO on XSum OOD). | More complex optimization (min-max) than DPO.                                                                           |

The landscape of Large Language Model (LLM) development has seen significant strides in aligning model outputs with human preferences, moving beyond traditional Reinforcement Learning from Human Feedback (RLHF) toward more efficient, robust, and nuanced optimization strategies. These advancements primarily aim to simplify the training process, reduce computational and data annotation costs, and enhance the stability and generalization capabilities of aligned LLMs. This section provides an integrated overview of prominent methodologies that have emerged to address the complexities inherent in preference-based fine-tuning.

Direct Preference Optimization (DPO) [19] stands out as a foundational advancement, offering a streamlined and computationally efficient alternative to Proximal Policy Optimization (PPO)-based RLHF methods [24,29]. Introduced in 2023, DPO achieves the goals of RLHF without necessitating the explicit training of a separate reward model, transforming the reinforcement learning problem into a more manageable supervised fine-tuning (SFT) task [10,19]. Its core principle relies on directly optimizing the language model's policy using an offline preference dataset, where each entry consists of a prompt, a "chosen" response, and a "rejected" response [13,19]. The DPO objective function is designed to maximize the probability ratio of chosen over rejected responses between the actor and a fixed reference model, implicitly learning human preferences. A commonly used formulation for the DPO loss function is given by:
$$
L_{DPO}(\pi_{\theta}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}
$$
Here, $\pi$ is the actor model, $\pi_{\text{ref}}$ is the reference model, $\beta$ is a temperature hyperparameter, and $\sigma$ is the sigmoid function [13,19]. Despite its widespread adoption in models like Llama 3 and Qwen 2 [29], DPO exhibits limitations such as exploration constraints, insufficient fine-grained optimization for complex tasks, potential instability leading to overfitting, and a lack of robustness to out-of-distribution (OOD) tasks or changes in behavior policy [13,16].

Building upon the preference-based paradigm, Kahneman-Tversky Optimization (KTO) [24] introduces a novel, cost-effective alignment method inspired by human decision-making theory. KTO distinguishes itself by operating on single-instance feedback, where responses are classified simply as desirable or undesirable, in contrast to DPO's reliance on pairwise comparisons [24]. This simplified feedback collection significantly reduces data annotation costs and offers substantial practical advantages, especially in resource-constrained environments, while demonstrating efficacy across various model sizes and datasets [24].

Addressing the limitations of PPO, which often include mode collapse, policy instability, and suboptimal sample efficiency, Advantage-Induced Policy Alignment (APA) [12] emerges as an algorithmic advancement. APA fundamentally departs from PPO's clipped surrogate objective by leveraging a squared error loss function that incorporates estimated advantages [12]. This design fosters enhanced stability and improved sample efficiency, preventing models from collapsing to deterministic outputs and augmenting the diversity of LLM responses. Empirical evaluations affirm APA's superior performance across various language tasks, particularly when an independent reward model is used for evaluation [12].

Furthermore, Self-Improving Robust Preference Optimization (SRPO) [16] directly tackles the robustness challenges faced by conventional online and offline RLHF methods, including DPO, when confronted with OOD tasks or shifts in the behavior policy used for data generation. SRPO conceptualizes preference learning as a continuous self-improvement mechanism, formalized through a min-max objective [16]. A key theoretical advantage of SRPO is its mathematical independence from the behavior policy, rendering it robust to variations in the data-generating process, unlike DPO and Implicit Preference Optimization (IPO) [16]. SRPO involves learning an in-context self-improving model and then training the generative LLM such that its outputs intrinsically require minimal improvement by this learned model. Experimental validation has demonstrated SRPO's superior robustness and generalization capacity, particularly on OOD tasks, achieving significant performance gains over DPO [16].

In summary, the advancements in preference-based optimization reflect a concerted effort to refine LLM alignment strategies. DPO provided a crucial simplification to RLHF, inspiring methods like KTO to further reduce data requirements. Meanwhile, APA and SRPO address critical issues of stability, efficiency, and robustness that plague earlier or related methods. Common challenges across these approaches include the dependence on high-quality and consistent human annotation data, limitations in fine-grained optimization, and robustness to out-of-distribution scenarios [13,16,24]. Future research directions will likely focus on developing methods that are even more data-efficient, inherently robust to diverse and shifting environments, and capable of nuanced, fine-grained control over LLM behavior, potentially through hybrid approaches that combine the strengths of these disparate techniques [29].
#### 4.2.1 Direct Preference Optimization (DPO)
Direct Preference Optimization (DPO) has emerged as a streamlined and computationally efficient alternative to traditional Reinforcement Learning from Human Feedback (RLHF) methods, such as Proximal Policy Optimization (PPO) [19,24,29]. Introduced by Rafailov et al. in 2023, DPO achieves the goals of RLHF without requiring an explicit reward model or engaging in complex reinforcement learning training processes, positioning it as an advanced strategy within RL techniques tailored for Large Language Models (LLMs) [10,13,19,29].

A core tenet of DPO is its mathematical equivalence to RLHF, but with significantly simplified implementation [13,24]. Unlike traditional RLHF, which often necessitates training a separate reward model to quantify human preferences, DPO directly optimizes the language model's policy based on preferences, thereby bypassing this computationally intensive step [6,19,24,29]. This approach effectively transforms the reinforcement learning problem into a supervised fine-tuning (SFT) task, rendering the training process more straightforward and efficient [19].

DPO offers notable advantages, particularly for open-source initiatives, due to its inherent simplicity, stability, and efficiency, circumventing the complex multi-model setups and sampling issues prevalent in PPO-based RLHF [19,24,29]. It directly leverages feedback from human annotations to align LLMs with desired outputs, addressing issues of data conflict and strong subjectivity often encountered in traditional RLHF labeling [24,29]. However, DPO's efficacy is critically dependent on the availability and quality of human annotation data. This reliance introduces challenges related to annotation consistency, the substantial volume of data required, and potential issues of scalability [13,24].

DPO operates as an offline method, implicitly defining rewards through preference data and thereby positioning itself between online RLHF and pure behavioral cloning [13]. The process involves optimizing an "actor model" (the LLM being fine-tuned) against a "fixed, untrainable reference model" [13,19,29]. Both models are typically initialized from a prior supervised fine-tuning phase [19]. The foundation of DPO is a fixed offline preference dataset [13,29]. Each entry in this dataset typically consists of a prompt ($x$), a "chosen" response ($y_w$), and a "rejected" response ($y_l$) for that prompt [13,19,29]. The primary objective of DPO is to maximize the implicit "reward" for chosen responses relative to rejected ones, thereby effectively learning human preferences without an explicit reward function [19].

The DPO objective function is meticulously designed to increase the probability ratio of the chosen response to the rejected response between the actor ($\pi$) and reference ($\pi_{\text{ref}}$) models [13,19]. The loss function implicitly captures the reward signal by comparing the probabilities assigned by the policy to chosen versus rejected responses. A commonly used formulation for the DPO loss function, when `label_smoothing` is set to 0, is given by:
$$
L_{DPO}(\pi_{\theta}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left
$$
In this equation, $\pi$ represents the actor model, $\pi_{\text{ref}}$ denotes the fixed reference model, $\beta$ is a temperature hyperparameter that modulates the magnitude of divergence between the policy and reference models (analogous to a KL-divergence penalty), and $\sigma$ signifies the sigmoid function [13,19]. This loss function actively promotes the growth of the actor and reference models' chosen-to-rejected response probability ratio, guiding the model to robustly prefer chosen responses and effectively disfavor rejected ones [13,19].

Despite its conceptual advantages and widespread adoption, DPO is not without limitations. It may exhibit "exploration limitations" when encountering prompts outside the training dataset, and its "fine-grained optimization is insufficient" for certain complex tasks [6,13]. Critical analysis has also revealed potential instability issues, where minimizing the DPO loss might paradoxically lead to a decrease in the actor model's log-probability for the chosen response relative to the reference model. This can result in "model overfitting and performance collapse," particularly when alignment goals are highly specific or when rejected responses are otherwise coherent [13]. Furthermore, DPO's optimal solution is noted to be "highly dependent on the task" and may lack robustness to out-of-distribution (OOD) tasks or changes in the behavior policy used for data collection, thereby limiting its generalization capabilities when evaluation distributions significantly diverge from training data [16].

Nonetheless, DPO has been widely adopted in prominent LLMs such as Llama 3, Qwen 2, Phi-3, and Hermes 3 for preference alignment [29]. For instance, Llama 3 integrates DPO within an iterative optimization framework, incorporating adjustments like masking formatted tokens and Negative Log-Likelihood (NLL) regularization for enhanced training stability [29]. Qwen2 employs DPO in an offline learning phase to maximize the likelihood difference between preferred and dispreferred responses from a pre-compiled dataset [29]. Some advanced approaches, such as Nemotron-4 340B, combine DPO with Reward-aware Preference Optimization (RPO) to mitigate DPO's limitation of not fully considering the quality differences between chosen and rejected responses, which can otherwise lead to overfitting and forgetting valuable information [29]. Additionally, Kimi-k1.5 utilizes DPO to optimize models for conciseness by constructing paired preference data where shorter correct solutions are treated as positive samples [29]. The flexibility of DPO also enables its integration with other techniques, such as its use in O1 Journey in conjunction with long reasoning chain generation to enhance reasoning capabilities [6].
#### 4.2.2 Kahneman-Tversky Optimization (KTO)
Kahneman-Tversky Optimization (KTO) emerges as a novel and cost-effective alignment method for Large Language Models (LLMs), distinguishing itself from traditional preference-based feedback mechanisms [24]. This innovative approach is fundamentally rooted in Kahneman and Tversky's seminal research on human decision-making, thereby drawing parallels between LLM optimization and cognitive processes [24]. Unlike standard Reinforcement Learning from Human Feedback (RLHF) methods, which typically necessitate comparative preferences between multiple responses, KTO operates on single-instance feedback, classifying responses simply as either desirable or undesirable [24].

This unique feedback paradigm offers substantial practical advantages, particularly in resource-constrained environments. The simplification of feedback collection significantly lowers data annotation costs, as annotators only need to evaluate individual outputs rather than performing pairwise comparisons [24]. Furthermore, KTO boasts simplified implementation procedures, contributing to its ease of adoption and deployment [24]. Despite its streamlined data requirements and operational simplicity, KTO has demonstrated considerable efficacy, achieving significant performance improvements compared to existing alignment methods across models of various sizes and on public datasets, all while effectively maintaining overall model performance [24]. These attributes position KTO as an attractive alternative for efficiently aligning LLMs.
#### 4.2.3 Advantage-Induced Policy Alignment (APA)
Advantage-Induced Policy Alignment (APA) emerges as a novel algorithmic advancement, specifically engineered to mitigate critical weaknesses identified in Proximal Policy Optimization (PPO) when applied to the alignment of Large Language Models (LLMs) with human preferences [12]. PPO, while widely adopted, often encounters challenges such as mode collapse, policy instability, and suboptimal sample efficiency in this context [12]. APA directly addresses these issues, positioning itself as a direct improvement for more robust and effective fine-tuning through Reinforcement Learning from Human Feedback (RLHF) [12].

The core mechanism of APA fundamentally departs from PPO's clipped surrogate objective by leveraging a squared error loss function, which is meticulously crafted to incorporate estimated advantages [12]. This algorithmic novelty is central to APA's design, aiming to provide a more stable and efficient means of controlling policy updates [12]. By utilizing this specific loss function, APA fosters enhanced stability and improved sample efficiency during the training process, thereby augmenting the diversity of LLM outputs and overcoming the inherent limitations of PPO [12].

A significant advantage of APA lies in its ability to maintain stable control over the deviation from the model's initial policy [12]. This precise control is instrumental in preventing the model from collapsing to deterministic outputs, a common manifestation of mode collapse frequently observed with PPO [12]. The theoretical underpinnings of the APA loss function further provide a robust justification for its empirical successes [12].

Empirical evaluations consistently demonstrate APA's superior performance relative to PPO across a variety of language tasks, particularly when an independent reward model is utilized as an evaluator [12]. This enhanced efficacy is directly attributable to APA's superior stability and its refined control over policy updates, which translates into more reliable and high-quality LLM alignment [12]. Therefore, APA represents a substantial step forward in addressing the complexities of aligning LLMs with human preferences, offering a more stable, efficient, and controllable alternative to existing methods.
#### 4.2.4 Self-Improving Robust Preference Optimization (SRPO)
Self-Improving Robust Preference Optimization (SRPO) is presented as an innovative framework designed to enhance the robustness of preference learning in large language models, particularly in the context of Reinforcement Learning from Human Feedback (RLHF) [16]. SRPO addresses a fundamental challenge observed in conventional online and offline RLHF methods, such as Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO), which often exhibit significant dependence on the specific task and lack robustness when confronted with out-of-distribution (OOD) tasks or alterations in the behavior policy [16]. This dependency can lead to a degradation in the utility of preference data and reward models, potentially undermining the benefits of supervised fine-tuning or pre-training if the underlying model undergoes substantial changes [16]. SRPO conceptualizes the process of learning human preferences as a continuous self-improvement mechanism, formalized through a min-max objective [16]. It has also been recognized as an algorithmic advancement contributing to robustness in reinforcement learning [32].

A core tenet of SRPO's design is its mathematical robustness. Unlike methods such as DPO and Implicit Preference Optimization (IPO), SRPO's derived solution is mathematically proven to be entirely independent of the behavior policy ($\mu$) used to generate the preference data [16]. This independence confers a significant theoretical advantage, rendering SRPO robust to variations or shifts in the data-generating policy, a crucial factor for deploying LLMs in dynamic environments. In contrast, the performance and stability of DPO and IPO are known to be intricately linked to the characteristics of the behavior policy, making them susceptible to shifts in data distribution or policy evolution [16].

The operational mechanism of SRPO involves two interconnected continuous optimization processes. Initially, an in-context self-improving model is learned by leveraging paired preference data. This model essentially learns how to transform less-preferred completions into more-preferred ones, typically estimated through supervised direct preference optimization schemes similar to existing methodologies [16]. Subsequently, the generative LLM ($\pi$) is trained using this acquired self-improvement strategy. The objective here is to optimize $\pi$ such that its generated completions intrinsically require minimal improvement by the learned optimal self-improvement model, signifying a high degree of alignment with human preferences from the outset [16].

The central mathematical formulation underpinning SRPO is a min-max objective. This objective is ingeniously designed to concurrently optimize both the self-improvement strategies and the generative strategies in an adversarial manner [16]. For a given context $z$, the SRPO objective typically includes Kullback-Leibler (KL) regularization terms and aims to achieve two goals: first, identify the most effective self-improvement strategy that is optimal relative to the preference distribution $p$ while maintaining proximity to a reference policy; and second, minimize this objective to derive a robust policy whose generated outputs inherently demand the least amount of improvement [16]. For practical implementation, this complex min-max optimization problem is transformed into a standard supervised learning problem that can be effectively solved using offline paired preference datasets [16].

The experimental validation of SRPO underscored its robustness, particularly concerning OOD generalization. The framework was evaluated on the Reddit TL;DR Summarization dataset, representing an in-distribution task, and critically, on the XSum dataset, serving as an out-of-distribution task [16]. Evaluation was conducted by calculating win rates (WR) against human-written gold standards using GPT-4-0613. The results demonstrated that SRPO's self-improvement process, iterative over up to five revisions, substantially enhanced performance. Notably, on the OOD XSum dataset, SRPO with five self-revisions (SRPO 5-rev) achieved a 90% WR, marking a significant 15 percentage point improvement over DPO. This empirical evidence convincingly supports SRPO's superior robustness and its capacity for generalization to OOD tasks [16].
### 4.3 Reinforcement Learning from AI Feedback (RLAIF)
Reinforcement Learning from AI Feedback (RLAIF) emerges as a highly promising solution designed to mitigate the inherent scalability challenges of Reinforcement Learning from Human Feedback (RLHF) [25,33,35]. Its core mechanism involves replacing human preference labeling with synthetic feedback generated by a powerful, off-the-shelf Large Language Model (LLM) [11,33]. This approach leverages the advanced understanding and evaluation capabilities of existing LLMs to automate the typically costly and time-consuming process of human annotation, thereby reducing reliance on human evaluators and enhancing efficiency, consistency, and cost-effectiveness [29,33].



**RLAIF vs. RLHF Comparison**

| Feature / Aspect          | Reinforcement Learning from Human Feedback (RLHF)                               | Reinforcement Learning from AI Feedback (RLAIF)                                                                |
| :------------------------ | :------------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------- |
| **Feedback Source**       | Human annotators (ranking/selecting responses)                                  | Powerful, off-the-shelf LLMs (generating preferences/scores)                                                   |
| **Scalability**           | Limited due to high cost & time of human annotation                             | Highly scalable, AI feedback is ~10x cheaper & faster; automates annotation                                  |
| **Cost**                  | High (human labor, expert annotation)                                           | Lower (computational for LLM inference)                                                                        |
| **Consistency**           | Can be inconsistent due to human subjectivity, fatigue, bias                    | More consistent (AI can follow rules rigorously), but can propagate AI biases                                  |
| **Performance**           | High performance, often gold standard; can show sustained gains with more data  | Comparable to/exceeds RLHF; faster to reach good performance, but may plateau without novel input features     |
| **RM Training**           | Separate Reward Model (RM) trained on human preference data                     | Typically trains RM on AI-generated preferences (Canonical RLAIF); can bypass RM with Direct-RLAIF (d-RLAIF)   |
| **Key Advantage**         | Captures nuanced human values, ethics, and subjective preferences directly      | Efficient, automated, reduces human bottleneck; enables LLM self-improvement                                  |
| **Key Limitation**        | Costly data collection, subjectivity/bias, "reward hacking," RM staleness       | Potential for AI bias transfer, "RM staleness" (in canonical), performance plateau, prompt sensitivity, lack of novel input |
| **Example Models**        | ChatGPT, Llama 2 (primary alignment)                                            | Starling-7B, models using UltraFeedback, SCoRe (for self-correction)                                           |

Systematic comparisons between RLAIF and RLHF consistently demonstrate that RLAIF can achieve performance levels comparable to, or even exceeding, RLHF, thus presenting a scalable alternative to human feedback. For instance, in summarization and helpful dialogue generation tasks, experimental results indicate that models trained with RLAIF perform statistically similar to RLHF models in terms of human preference, with human evaluators preferring outputs from both RLAIF and RLHF over Supervised Fine-tuned (SFT) baselines in approximately 70% of cases [25,27,33]. Notably, RLAIF has been observed to outperform RLHF in specific domains, such as harmless dialogue generation, achieving an 88% harmless rate compared to RLHF's 76% [27]. While RLAIF can rapidly achieve a good performance level with less data, its improvement trajectory may eventually plateau without introducing new input features. In contrast, RLHF often exhibits slower but more sustained performance gains with increasing data [33].

The process of generating AI feedback in RLAIF typically involves several meticulously designed steps. These include careful **Prompt Construction**, where the LLM labeler receives a task description, optional few-shot examples, a specific query, and an ending prompt. To mitigate **Positional Bias**, where an LLM's evaluation can be sensitive to the order of responses, pairs of summaries are often queried twice with swapped positions, and results are averaged. Furthermore, **Chain-of-Thought (CoT)** prompting is frequently employed to optimize the LLM's evaluation, guiding it to explain its reasoning before providing a final preference, thereby enhancing feedback quality [29,33]. The final output consists of preference scores or probabilities for candidate responses [33]. The quality of the AI labeler is strongly correlated with its size, with larger models generally yielding superior annotations. Tuning prompts, particularly the preamble, significantly impacts the alignment between AI and human labels, while CoT typically provides beneficial gains [33].

RLAIF introduces unique aspects, particularly concerning "self-improvement" and the innovation of "direct-RLAIF (d-RLAIF)." The concept of **self-improvement** signifies an LLM's capacity to refine itself through self-generated feedback. RLAIF can significantly outperform an SFT baseline even when the AI labeler (the LLM generating preferences) is of the same size or even the exact same checkpoint as the policy LLM being fine-tuned [11,27]. For instance, a "same-size RLAIF" model for summarization was preferred 68% of the time over SFT, demonstrating substantial gains [27]. Advanced self-referential RLAIF methods like SCoRe leverage "completely self-generated data," where the LLM itself acts as the feedback source for self-correction, eliminating the need for external human or pre-trained AI labelers [18]. Other self-rewarding techniques include Self-Refined LM, which iteratively refines reward functions for DRL tasks in a closed loop, and Self-Rewarding Language Models (SRLM), where the LLM serves as both generator and evaluator, scoring responses for DPO training and outperforming state-of-the-art models like GPT-4 and Claude 2 on benchmarks [29].

A significant innovation within RLAIF is **direct-RLAIF (d-RLAIF)**, which circumvents the explicit Reward Model (RM) training step entirely [11,27]. Instead of training a separate RM, an off-the-shelf LLM is directly prompted during the reinforcement learning phase to rate the quality of a generated response, with this direct score being normalized and used as the reward signal [27]. This direct approach offers superior performance compared to canonical RLAIF and addresses the "RM staleness" issue, where an RM's performance degrades as the policy generates out-of-distribution responses from the RM's training data. Furthermore, d-RLAIF eliminates the time-consuming stages of AI preference labeling and RM training, leading to greater efficiency [27]. A strict instance of LLM self-improvement has been demonstrated with d-RLAIF where the LLM providing rewards and the initial policy are identical model checkpoints, achieving significant win rates over SFT baselines [27].

Various datasets and methodologies underpin RLAIF research. Examples include **UltraFeedback**, a large-scale dataset featuring over 1 million GPT-4 feedback annotations for training reward models, often utilizing CoT reasoning [29]. **Magpie** employs a self-synthesis method for generating instruction-response pairs, while **HelpSteer2** and **OffsetBias** offer open-source preference datasets with multi-dimensional annotations for nuanced alignment and bias mitigation, respectively [29]. Beyond training explicit RMs, LLMs can also be prompted directly as reward functions, as seen in **Exploring with LLMs (ELLM)** for goal generation, **Reward Design with Language Models (RDLM)** for direct reward value output, **Eureka** for automated reward function code generation, and **Text2Reward** for generating dense, interpretable reward code [29]. Starling-7B, for instance, specifically uses RLAIF with the Nectar dataset, which comprises 3.8 million GPT-4 generated paired comparisons, alongside PPO improvements for length control and critic pre-training [29]. While DeepSeek-R1's reward signal comes from an objective verifier, not explicit LLM preferences, its minimization of human involvement aligns with RLAIF's core objective [22]. Researchers are also exploring methods like SER to enable reward models to self-generate and refine their training data, advancing beyond static feedback [31]. Overall, RLAIF represents a dynamic and evolving field aiming to revolutionize LLM alignment by harnessing AI's capabilities to streamline and scale the feedback process.
### 4.4 Simplifying Policy Optimization

**Simplified Policy Optimization Methods for LLMs**

| Method                                        | Simplification Approach                                                                | Key Characteristics & Benefits                                                                                                                                                                                                                                                                                                                                                                          |
| :-------------------------------------------- | :------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Direct Preference Optimization (DPO)**      | Eliminates separate Reward Model (RM) training                                         | Transforms RL to supervised fine-tuning; requires fewer models (2 vs PPO's 4); more stable, efficient; avoids online sampling overhead [19].                                                                                                                                                                                                                                                    |
| **Kahneman-Tversky Optimization (KTO)**       | Reduces feedback complexity                                                            | Uses single desirable/undesirable feedback (vs. pairwise comparisons); lowers data annotation cost & implementation complexity; effective across model sizes [24].                                                                                                                                                                                                                               |
| **Rank Responses to Align Language Models with Human Feedback (RRHF)** | Reduces model count and hyperparameter tuning                                      | Scores sampled responses using log-conditional probabilities, aligns with ranking loss; requires 1-2 models (fewer than PPO); robust alignment without complex tuning; acts as "best-of-n learner" [4].                                                                                                                                                                                      |
| **Group Relative Policy Optimization (GRPO)** | Critic-free policy optimization                                                        | Avoids overhead of auxiliary critic models; relies on final, sequence-level rewards; uses Monte Carlo group-wise normalization for baseline; scalable for large reasoning models; faster convergence but can be unstable [30].                                                                                                                                                                   |
| **REINFORCE Variants (e.g., ReMax, RLOO)**    | Reduces dependence on value models                                                     | Simpler policy gradient methods; estimates baselines through highest probability actions or multi-trajectory sampling [6].                                                                                                                                                                                                                                                                      |
| **Direct-RLAIF (d-RLAIF)**                    | Circumvents explicit RM training                                                       | Rewards directly obtained from off-the-shelf LLM during RL phase; reduces complexity & resource needs; addresses "RM staleness" [27].                                                                                                                                                                                                                                                            |
| **Rejection Sampling Fine-tuning**            | Converts RL into selection & SFT task                                                  | Samples multiple outputs, selects best based on RM; used in models like Llama 2; simplifies policy optimization by reducing direct RL interaction [28].                                                                                                                                                                                                                                        |
| **SRPO (Offline RLHF)**                       | Transforms complex min-max optimization into supervised learning                       | Aims for greater stability & ease of implementation for offline preference datasets; robust to distribution shifts; avoids online complexity of PPO [16].                                                                                                                                                                                                                                       |
| **ORPO**                                      | Unifies SFT & preference alignment, no separate RM/reference model, no SFT pre-warming | Adds odds-ratio-based penalty to standard SFT NLL loss; efficient differentiation between preferred & non-preferred responses; avoids complexity of multi-stage pipelines [29].                                                                                                                                                                                                                      |

The complexity and resource intensiveness of traditional reinforcement learning (RL) policy optimization algorithms, particularly Proximal Policy Optimization (PPO), have driven the development of more streamlined and robust alternatives for aligning Large Language Models (LLMs). These novel methods aim to reduce the number of models required, simplify hyperparameter tuning, and improve training stability, thereby making RL-based fine-tuning more accessible and efficient [4,24].

One significant advancement is **Direct Preference Optimization (DPO)**, which substantially simplifies the policy optimization process compared to PPO-based Reinforcement Learning from Human Feedback (RLHF) [19]. DPO achieves this by directly optimizing a policy against preference data through a supervised objective, thereby obviating the need for a separate reward model and complex sampling strategies inherent in PPO [13,19]. This architectural simplification leads to a more efficient and stable training procedure, requiring fewer models and simplifying hyperparameter tuning [19,29]. DPO is mathematically equivalent to RLHF but with a notably simpler implementation that directly leverages human annotation feedback [24]. Variants such as Pairwise DPO and Odds Ratio Preference Optimization (ORPO) further exemplify this trend. ORPO, for instance, avoids the necessity for supervised fine-tuning (SFT) pre-warming, reward models, or reference models by introducing a penalty based on odds ratios to the standard SFT negative log-likelihood loss, enabling efficient differentiation between preferred and non-preferred responses [29].

**Kahneman-Tversky Optimization (KTO)** offers another simplification by reducing the complexity of feedback acquisition. Unlike DPO, which typically requires comparative preferences, KTO operates effectively with only single desirable or undesirable feedback signals. This simplifies the feedback process, making it easier and more cost-effective to implement without compromising model performance [24].

The **Rank Responses to Align Language Models with Human Feedback (RRHF)** paradigm represents an alternative strategy to PPO for policy optimization in LLM alignment. RRHF's core mechanism involves scoring sampled responses from various sources using the logarithm of conditional probabilities and then aligning these probabilities with human preferences via a ranking loss function [4]. This approach offers several operational advantages over PPO, such as requiring only 1 to 2 models during tuning, which is fewer than PPO's standard implementation, leading to simpler coding and reduced resource overhead [4]. Furthermore, RRHF demonstrates robust alignment with human preferences without necessitating complex hyperparameter tuning, achieving performance comparable to PPO in both reward model scores and human labeling [4]. A critical characteristic of RRHF is its function as a "best-of-n learner," where its performance is highly dependent on the quality of sampled responses [4]. This highlights a shift towards methods that leverage sophisticated sampling for efficient learning.

Beyond these prominent methods, a broader trend towards simplified policy optimization is evident across various approaches:
*   **Critic-Free Algorithms**: Group Relative Policy Optimization (GRPO) stands out as a critic-free alternative that avoids the significant computational overhead associated with auxiliary critic models in PPO [6,30]. GRPO relies solely on final, sequence-level rewards or Monte Carlo group-wise normalization for baseline estimation, enhancing scalability and practicality, especially for large reasoning models [6,30]. It has been successfully applied in DeepSeek-R1 for reasoning capabilities and Perception-R1 for optimizing multimodal LLM visual perception [17,22].
*   **REINFORCE Variants**: Algorithms like REINFORCE, ReMax, and RLOO simplify architectural requirements by reducing dependence on value models, estimating baselines through highest probability actions or multi-trajectory sampling [6,27].
*   **Direct-RLAIF (d-RLAIF)**: This technique streamlines the RLHF process by circumventing explicit reward model training. Instead, rewards are directly obtained from an off-the-shelf LLM during the reinforcement learning phase, reducing complexity and resource needs [11,27].
*   **Rejection Sampling Fine-tuning**: Employed in models like Llama 2, this approach simplifies policy optimization by sampling multiple outputs and selecting the best one based on a reward model. This effectively converts an RL problem into a selection and supervised fine-tuning task [28].
*   **Offline RLHF with Supervised Learning**: SRPO transforms complex, adversarial min-max optimization into a supervised learning task for offline preference datasets, aiming for greater stability and ease of implementation, even surpassing DPO in robustness to distribution shifts for offline settings [16].
*   **Frameworks and Optimizations**: Tools like the TRL library simplify the deployment and configuration of RL algorithms, including PPO, through modular design and user-friendly features, while LPPO optimizes training samples for efficiency by intelligently managing their influence based on learning progress [2,32]. Moreover, methods like Ghost Attention in Llama 2 simplify multi-turn consistency by distilling system-level instructions into the model's attention mechanism [28]. The concept of "RL's Razor" suggests that on-policy RL inherently finds KL-minimal solutions, simplifying the balance between new task learning and old knowledge retention by preferring policies close to the base model [3].

Concurrently, efforts to simplify the reward acquisition process, such as the CARD framework for automated and refined reward function design, indirectly contribute to simplified policy optimization by providing higher quality and more stable reward signals [5]. Similarly, the use of "endogenous rewards" that are training-free and do not require expensive human annotation also reduce the overall complexity of RL fine-tuning [20].

In summary, the field is clearly trending towards more streamlined and less resource-intensive policy optimization techniques for LLMs. Methods like DPO, KTO, and RRHF, alongside critic-free algorithms and direct-RLAIF, exemplify a collective effort to overcome the complexities of PPO by minimizing model dependencies, simplifying feedback mechanisms, and enhancing training stability and efficiency, ultimately leading to more robust and scalable LLM alignment.
## 5. RL for Enhancing Specific LLM Capabilities
This chapter explores the pivotal role of Reinforcement Learning (RL) in augmenting specific, distinct capabilities of Large Language Models (LLMs) beyond mere general alignment. While initial applications of RL, such as Reinforcement Learning from Human Feedback (RLHF), primarily focus on aligning LLM outputs with human preferences and values, a more specialized application of RL is emerging to tackle complex, domain-specific challenges where objective performance metrics are paramount [1,5,7,18]. This necessitates a shift from subjective preference optimization to objective task performance, driving LLMs towards more intelligent, adaptive, and agentic behaviors across diverse tasks.

The common motivations for employing RL in these specialized contexts stem from the inherent limitations of traditional supervised learning or human-centric alignment methods. For complex reasoning tasks, traditional methods often depend on costly, unscalable, and potentially biased human-annotated examples, hindering autonomous improvement and intrinsic self-correction [22,29]. Similarly, in multimodal understanding, existing MLLMs frequently exhibit deficiencies in fine-grained perception tasks like precise object localization or OCR, lacking the necessary nuance and accuracy, which human feedback alone cannot effectively resolve [8,17]. Furthermore, for LLMs to become proficient tool users and autonomous agents, they require the ability to learn optimal strategies for selecting, invoking, and interpreting results from external tools, a capability difficult to instill through static training data [5].

To address these motivations, a set of shared methodological principles differentiates these advanced RL applications from general preference alignment. A critical distinction lies in **reward engineering**, which moves towards verifiable, objective, and often rule-based signals rather than subjective human preferences. For enhancing reasoning, approaches like Reinforcement Learning with Verifiable Rewards (RLVR) utilize objective correctness (e.g., mathematical answers, passing test cases for code) as direct reward signals, as exemplified by DeepSeek-R1 and SCoRe [18,22]. In multimodal perception, frameworks like Perception-R1 employ meticulously designed rule-based reward functions that combine format adherence with task-specific metrics such as Intersection over Union (IoU) for localization, Euclidean distance for counting, or Levenshtein distance for OCR, leveraging visual ground truth [1]. For tool interaction, dynamic, feedback-driven reward design frameworks like CARD leverage LLMs themselves to generate and refine reward function code, utilizing trajectory preference evaluation to reduce reliance on extensive human labeling or full RL training iterations [5]. Simpler outcome-based reward functions are also effective for learning tool use, as seen in SEARCH-R1 for search engine interaction [7].

Another differentiating principle is the **design of the RL environment** and the agent's interaction within it. For reasoning, the environment allows for autonomous exploration and evolution of diverse reasoning paths, enabling models to discover complex problem-solving strategies, including self-verification and reflection, as demonstrated by the "epiphany moment" in DeepSeek-R1-Zero [22]. In multimodal perception, the environment is structured to optimize an internal "perception policy" that governs the MLLM's strategy for extracting, understanding, and logically processing fine-grained visual details [1]. For tool use, external tools are explicitly integrated into the RL environment, enabling multi-turn interactions and learning optimal strategies for their invocation and utilization, as seen in SEARCH-R1 and various "Agentic Tasks" [7,15]. Optimization algorithms such as Group Relative Policy Optimization (GRPO) are recurrently applied across these domains, guiding learning without explicit constraints on intermediate human-like reasoning steps, thereby fostering autonomous strategy discovery [1,22].

Collectively, these RL applications are critical in pushing LLMs towards more intelligent, adaptive, and agentic behaviors. They enable the emergence of Large Reasoning Models (LRMs) that excel in complex problem-solving, equip Multimodal LLMs with highly precise visual perception capabilities, and transform LLMs into proficient tool-using agents capable of planning, reflection, and self-correction in dynamic environments [1,5,7,22]. This marks a significant evolution in LLM capabilities, moving from general language understanding to specialized mastery of complex cognitive and interactive tasks.
### 5.1 Enhancing Reasoning Abilities and the Emergence of Large Reasoning Models (LRMs)
A critical challenge in advancing Large Language Models (LLMs) lies in their ability to perform intrinsic self-correction, particularly without reliance on external input or extensive human annotation. This capability is paramount for complex scientific domains such as mathematics and programming, where precision and logical consistency are indispensable [22]. Traditional methods for enhancing LLM reasoning often depend on costly, unscalable, and potentially biased human-annotated reasoning examples, which limits their potential for autonomous improvement [7,22,29].

To address this, the SCoRe method presents a notable approach to improve LLM self-correction. It employs a two-stage online Reinforcement Learning (RL) framework that leverages entirely self-generated data and sophisticated reward shaping techniques to optimize performance and mitigate the risk of "behavior collapse" [18]. This methodology has demonstrated significant improvements in self-correction, achieving a 4.4% increase in mathematics (a 15.6% improvement over the base model on the MATH dataset) and a 12.2% increase in programming (a 9.1% improvement on the HumanEval dataset). By minimizing reliance on human annotation through self-generated data, SCoRe pushes LLMs towards more autonomous reasoning improvement [18].

A significant paradigm shift in enhancing LLM reasoning capabilities is exemplified by DeepSeek-R1, which ushered in the era of Large Reasoning Models (LRMs) by utilizing a "pure reinforcement learning" approach [23,29]. This approach minimizes reliance on human annotation for reasoning examples, instead enabling autonomous exploration and evolution of diverse reasoning paths [22]. DeepSeek-R1 is a prominent instance of Reinforcement Learning with Verifiable Rewards (RLVR), where objective and automatically verifiable reward signals—such as the correctness of a mathematical answer or the passing of test cases for code—are used for training [30,32]. This stands in contrast to Supervised Fine-tuning (SFT), which often suffers from overfitting to training data and can lead to significant forgetting of existing knowledge when learning new reasoning skills [3]. RL-trained models, particularly those leveraging RLVR, exhibit superior generalization to novel problems [30].

Central to DeepSeek-R1's methodology is the Group Relative Policy Optimization (GRPO) technique, which guides the model's learning process [7,8,10,22,23,32]. GRPO allows the model to learn effective reasoning strategies by receiving reward signals based solely on the correctness of the final prediction, without explicit constraints or rewards for intermediate human-like reasoning steps [22,23]. This decoupling from human-prescribed intermediate steps empowers the model to autonomously discover and evolve its own complex reasoning behaviors, including self-verification and reflection [22]. The efficacy of GRPO has been further demonstrated in other contexts, such as Perception-R1 for enhancing visual perception in Multimodal Large Language Models (MLLMs), underscoring its versatility in improving core model capabilities across modalities [1,17].

A fascinating observation during the training of DeepSeek-R1-Zero was the emergence of an "epiphany moment," where the model began to frequently use "wait" to indicate self-monitoring and internal reflection [22]. This emergent pattern is interpreted as a significant sign of developing sophisticated, self-correcting cognitive abilities, signifying a move towards more advanced problem-solving strategies [22]. DeepSeek-R1-Zero, trained solely with large-scale RL from DeepSeek-V3-Base, achieved a pass@1 score of 77.9% (86.7% with self-consistency decoding) on the AIME 2024 benchmark, substantially outperforming the average human participant [22,29]. This impressive performance, without relying on Supervised Fine-tuning (SFT) from human-annotated examples, highlights the potential of RL to unlock and refine intrinsic reasoning skills [32].

The success of models like DeepSeek-R1 has paved the way for the emergence of LRMs, which shift the focus from merely aligning models with human preferences to directly improving performance on complex reasoning tasks such as mathematics, programming, and logical deduction [30]. Other notable LRMs include OpenAI's o1, which optimizes for complex reasoning through extensive internal thought processes (Chain-of-Thought, CoT) and can continuously improve with additional RL training and increased "thinking time" during inference [23,29]. Kimi-k1.5, while multimodal, also emphasizes optimizing policy optimization to guide robust reasoning, including "long-short RL" to transfer long-chain reasoning to smaller models [29]. Similarly, DeepSeek-V2 employs a two-stage RL strategy with a "reasoning alignment" focus [29]. The "Implicit Trial-and-Error Search" paradigm, driven by RL, enables models to perform self-evaluation, detect errors, backtrack, and apply corrections within the reasoning process, fostering a more robust and adaptable reasoning ability, as seen in models like Marco-O1, O1 Journey, Slow Thinking with LLMs, and rStar-Math [6]. Frameworks like ROLL have also demonstrated significant improvements in complex reasoning through RLVR pipelines [15]. These developments collectively underscore RL's foundational role in transforming LLMs into powerful LRMs with enhanced logical thinking and problem-solving capabilities [26].
### 5.2 Empowering Multimodal Perception
Multimodal Large Language Models (MLLMs), such as GPT-4, Gemini, and Kimi-k1.5, have demonstrated significant advancements in processing and generating content across various modalities, including integrating vision and language to perform general visual question answering and complex reasoning tasks [14,29,30]. These models, often enhanced through Reinforcement Learning from Human Feedback (RLHF) and optimized feedback loops, exhibit strong capabilities in understanding and generating natural language in nuanced scenarios, and are continuously evolving to support more sophisticated multimodal interactions, including agentic applications [15,29]. However, despite their proficiency in broad visual comprehension, a notable distinction exists between these general MLLM vision capabilities and the specialized requirement for precise visual perception.

Existing MLLMs frequently exhibit deficiencies when confronted with fine-grained visual perception tasks. These limitations manifest in crucial areas such as precise object localization, accurate counting of multiple objects, and Optical Character Recognition (OCR) in complex layouts, where current MLLMs often struggle with the requisite nuance and precision [1,8,17]. For instance, while an MLLM might correctly identify the presence of a cat in an image, it typically lacks the ability to precisely delineate its ears, accurately count its whiskers, or understand its intricate interactions with other objects within the scene [8]. This indicates a fundamental gap between holistic scene understanding and granular visual analysis.

To address these limitations, the Perception-R1 (PR1) framework is introduced as a post-training solution specifically engineered to enhance the fine-grained visual perception capabilities of existing MLLMs, exemplified by its application to models like Qwen2-VL-Instruct-2B [1,8,17]. Perception-R1 approaches visual understanding as a Reinforcement Learning problem, focusing on optimizing an internal "perception policy" within the MLLM [1,8]. This policy governs the MLLM's internal strategy for: (1) extracting and comprehensively understanding relevant visual details; (2) executing precise logical operations based on this visual understanding, such as comparing object positions, identifying specific instances, and recognizing textual elements; and (3) generating outputs in the exact required format, including boundary box coordinates, object counts, or transcribed text [1,8,17].



The optimization of this perception policy is achieved through a rule-based Reinforcement Learning technique known as Group Relative Policy Optimization (GRPO) [1,8,17]. GRPO, a method previously applied in contexts such as DeepSeek-R1, guides the MLLM to "see smarter" by iteratively refining its ability to interpret visual information with greater accuracy and specificity. This involves learning to better discern subtle visual cues, execute more precise logical inferences, and produce outputs that align perfectly with task requirements, thereby bridging the gap between general understanding and detailed perception [1,8,17].

A critical component of Perception-R1's efficacy lies in its meticulously designed reward engineering, which is inherently rule-based and tailored specifically for visual tasks, leveraging the direct and quantifiable nature of visual ground truth [1,8,17]. The total reward function is composed of two primary elements:
1.  **Format Reward**: This component assigns a score based on whether the model's output adheres to the expected structural format. For example, an output conforming to a predefined structure, such as ``, receives a +1 point, while an incorrect format is penalized with -1 [1,8,17].
2.  **Answer Reward**: This measures the correctness of the perception using task-specific metrics. For visual localization tasks like RefCOCO, the Intersection over Union (IoU) between the predicted and ground truth bounding boxes serves as the reward. In visual counting tasks, such as PixMo-Count, the reward is based on the Euclidean distance between predicted and ground truth points, after redefining the task as point detection. For OCR tasks like PageOCR, the Levenshtein distance (edit distance) between the predicted and ground truth text is utilized [1,8,17].

The overall reward $R$ is calculated as the sum of the format reward ($R_{\text{format}}$) and the answer reward ($R_{\text{answer}}$): $$R = R_{\text{format}} + R_{\text{answer}}$$ [17].

A significant innovation in Perception-R1 is its approach to handling complex multi-instance scenarios in visual tasks, such as object detection and counting, through **bipartite matching** [1,8,17]. This method addresses the challenge of accurately aligning predicted results with ground truth annotations when multiple objects are present. It involves treating the predicted results and the ground truth objects as two distinct sets of points. Potential rewards (e.g., IoU) are then calculated for every possible pairing between a predicted item and a ground truth item. The Hungarian algorithm is subsequently applied to find the optimal match between these two sets, which maximizes the total reward, thereby ensuring accurate learning signals are provided for complex multi-object perception [1].

Empirically, Perception-R1 has demonstrated breakthrough performance, becoming the first pure multimodal open-source LLM to surpass 30 AP on the COCO2017 validation set [1,8]. It significantly outperforms its original MLLM counterparts and often exceeds the performance of specialized "expert" models across a diverse range of tasks, including visual grounding, OCR, visual counting, and object detection. This underscores the critical role of Reinforcement Learning, specifically through frameworks like Perception-R1, in advancing MLLMs from general multimodal understanding to highly precise and accurate visual perception capabilities [1,8,17]. Further advancements in RL-driven multimodal capabilities are also being explored in related areas, such as enhancing general manipulation skills in robotics through RL, as seen with OpenVLA-7B [3].
### 5.3 LLM-driven Tool Use and Agentic Behavior
The effective design of reward functions is paramount in reinforcement learning (RL), yet traditional reward engineering presents substantial challenges, including high human effort, the potential for sub-optimal or unexpected outcomes, and the inherent limitations of prior learning-based approaches [5]. For instance, inverse RL typically necessitates high-quality demonstrations, while preference-based RL demands extensive preference labels [5]. Current LLM-based reward generation methods, while promising, often incur high costs due to reliance on human feedback, numerous LLM queries, or iterative RL training for refinement [5].

To address these limitations, LLM-driven solutions offer enhanced efficiency and automation. 

The CARD framework exemplifies this by introducing a Coder-Evaluator paradigm for dynamic, feedback-driven reward design [5]. Within CARD, an LLM-based Coder autonomously generates reward function code based on environmental descriptions and task goals, and further verifies its execution, implicitly demonstrating tool use through interaction with a code execution environment [5]. The Evaluator component provides dynamic feedback to refine this code, primarily utilizing Trajectory Preference Evaluation (TPE). TPE assesses the current reward function through trajectory preferences, obviating the need for full RL training in every iteration and significantly reducing computational costs [5]. If the reward function fails TPE, preference feedback—derived from trajectory pairs—is provided to the Coder. Conversely, if successful, detailed process feedback (e.g., changes in trajectory return and sub-rewards during RL training) and trajectory feedback (parameter specifics for successful and failed trajectories) are furnished. This comprehensive feedback loop allows the Coder to grasp overall training trends and the impact of individual reward components [5]. This iterative process of generation, verification, and dynamic refinement underscores how RL, in the context of CARD, empowers LLMs to exhibit agentic behavior by autonomously learning and optimizing complex domain-specific code to achieve desired objectives, effectively treating code generation and execution as a form of tool interaction [5]. Similar approaches, such as Eureka and Text2Reward, leverage LLMs to automatically generate and optimize reward code from natural language, particularly for complex skills in robotics, integrating external libraries and supporting iterative human feedback, thus defining "tools" for various RL tasks [29].

Beyond reward design, RL plays a critical role in training LLMs to become proficient "tool users" and agents capable of learning optimal strategies for selecting, invoking, and interpreting results from external tools [5]. This ability to interact with external tools is a key component of sophisticated task planning and agentic behavior [14,34]. "Agentic Tasks" are an important domain where RL trains LLMs to operate as intelligent agents, autonomously interacting with tools such as calculators, various APIs, web navigation and search, and graphical user interfaces (GUIs) to complete complex assignments [30]. The framework ROLL, for instance, natively supports Agentic RL for multi-environment and multi-role agent-environment interactions, enabling LLMs to learn and utilize tools across diverse scenarios [15]. Case studies, such as Sokoban, FrozenLake, and WebShop, demonstrate significant improvements in multi-round decision-making, spatial planning, and efficient task completion under complex natural language instructions, indicating enhanced operational efficiency and instruction understanding [15]. Furthermore, the "emergence of tool use" in models like Llama 2-Chat, where they spontaneously utilize external tools in a 0-shot manner, even without explicit training, suggests that alignment techniques like RLHF can implicitly foster the discovery and application of tool interaction strategies [28]. However, developing such capabilities is challenging, as evidenced by DeepSeek-R1's current limitations in structured output and the inability to effectively use search engines and calculators to enhance its performance [22].

A notable application of RL in equipping LLMs with tool interaction capabilities is the SEARCH-R1 framework, which focuses on enabling effective interaction with search engines [7]. SEARCH-R1 innovatively treats the search engine itself as an integral part of the RL environment, facilitating multi-turn interactions between the LLM and the search tool [7]. Key mechanisms include "retrieved token masking" to ensure stable RL training and the use of a simple outcome-based reward function [7]. This approach allows the LLM to learn autonomously how to generate search queries and integrate real-time retrieved information into its reasoning process, overcoming the traditional limitation where LLMs struggled to learn optimal interaction strategies with search engines [7]. This framework has demonstrated significant performance improvements across various question-answering datasets, highlighting the efficacy of RL in enhancing LLM tool utilization through straightforward outcome-based rewards [7]. The ability of RL-trained models to plan, reflect, and self-correct errors represents a foundational step towards sophisticated LLM-driven agentic behaviors [23].
## 6. Methodological Deep Dive: Reward Engineering, Optimization, and Data Efficiency
The successful integration of Reinforcement Learning (RL) with Large Language Models (LLMs) hinges on a sophisticated interplay of methodological components: reward engineering, policy optimization algorithms, and data efficiency techniques. This chapter provides a detailed examination of these three critical areas, analyzing how innovations within each collectively address the inherent challenges of RL-driven LLM training. 

![Interdependencies in RL-driven LLM Training](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/GaX7ouKtDKEdirHRqYwcR_Interdependencies%20in%20RL-driven%20LLM%20Training.png)

The discussion highlights the fundamental interdependencies between these areas, such as how the design of robust reward models directly necessitates the development of resilient optimization algorithms, and how both are significantly constrained by the availability of high-quality data and computational resources. An overarching trend towards more automated, robust, and scalable solutions is observable across all methodological aspects [1,4,5,18,20,24,32].

Reward engineering serves as the foundational element, providing the crucial learning signals that guide LLM behavior and align it with desired outcomes [10]. This field has largely bifurcated into two paradigms: preference-based and verifiable reward mechanisms [23]. Preference-based methods, including Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF), aim to align LLMs with subjective human values and preferences, often utilizing comparative judgments [29,35]. However, these approaches face challenges related to human subjectivity, annotation costs, and potential "reward hacking" [10,24]. Innovations such as Direct Preference Optimization (DPO) and its variants directly optimize policies using preference data, bypassing the need for an explicit reward model [13,19]. Furthermore, the emergence of Endogenous Reward Models (EndoRM) suggests that reward signals can be inherently recovered from LLM logits, offering a training-free alternative [20], while Self-Evolved Reward Learning (SER) promotes iterative self-annotation to reduce external supervision [31]. In contrast, verifiable reward mechanisms (RLVR) leverage objective, ascertainable ground truth, particularly effective for tasks demanding factual accuracy and logical soundness, such as mathematics, programming, and fine-grained multimodal perception [1,22]. These rule-based rewards minimize human involvement, offering scalability and precision [30]. Dynamic feedback and reward shaping techniques, exemplified by SCoRe and CARD, further enhance reward efficacy by guiding self-correction and enabling LLMs to generate and refine reward function code [5,18]. The choice between these reward paradigms is task-dependent, often requiring a blend to achieve both alignment and factual accuracy.

The effectiveness of these reward signals is critically dependent on robust policy optimization algorithms, which dictate how LLMs learn from feedback to refine their generation capabilities [10]. A primary distinction lies between critic-based algorithms, such as Proximal Policy Optimization (PPO), and critic-free methods like Group Relative Policy Optimization (GRPO) and DPO [30]. PPO, widely adopted for RLHF, provides stability but incurs high computational overhead and can suffer from mode collapse [6,24]. Variants like Advantage-Induced Policy Alignment (APA) seek to mitigate these issues [12]. DPO offers a simplified, offline alternative by converting the RL problem into a supervised fine-tuning task, enhancing efficiency and stability by directly optimizing the policy with preference data [19]. GRPO, a critic-free algorithm, excels in domains like reasoning and multimodal perception by generating multiple outputs and updating policies based on relative comparisons using rule-based rewards [17,30]. Other notable algorithms include REINFORCE and RRHF, which provide alternative means of leveraging feedback without PPO's complexity, and LPPO, a sample-centric framework that enhances policy optimization within the RLVR paradigm [32]. These diverse algorithms reflect an ongoing effort to balance fine-grained control with computational scalability, addressing challenges like catastrophic forgetting and stability in LLM training [3].

Complementing reward engineering and policy optimization, data generation and efficiency techniques are paramount for practical and scalable RL-driven LLM training. The initial reliance on costly and time-consuming human feedback for reward model training spurred innovations in leveraging self-generated or synthetic data [9,29]. This evolution progresses from human-intensive annotation, through synthetic data generation using powerful LLMs (e.g., GPT-4 in RLAIF for DPO) [13,27], to purely self-generated data scenarios where models generate their own training data and reward signals with minimal human input, as seen in DeepSeek-R1, SCoRe, and SER [18,22,31]. Beyond data sources, architectural and algorithmic optimizations significantly enhance efficiency. Techniques like Trajectory Preference Evaluation (TPE) in the CARD framework avoid full RL training loops [5], while sample-centric methods such as LPPO and ROLL optimize the utility of high-quality data through intelligent sampling and weighting [15,32]. Furthermore, Parameter-Efficient Fine-Tuning (PEFT) and quantization techniques, like LoRA, reduce computational memory requirements, making large model training accessible on more modest hardware [2,29]. These advancements collectively aim to overcome the computational intensity and data demands that are inherent to RL-LLM integration.

The interdependencies among these methodological areas are profound. Sophisticated reward models, particularly those leveraging verifiable mechanisms (RLVR), necessitate equally robust and often specific policy optimization algorithms like GRPO to effectively process objective signals [1,22]. Conversely, the computational demands of advanced optimization algorithms like PPO drive the need for more efficient data generation strategies, leading to the development of AI-generated feedback and self-supervision techniques [24,27]. The push for data efficiency often leads to simplified optimization approaches, as exemplified by DPO, which directly optimizes policies from preference data, reducing the need for complex sampling and separate reward models [19]. Collectively, these innovations illustrate a concerted effort towards building more automated and scalable solutions that can effectively align LLMs with diverse objectives, whether subjective human preferences or objective factual correctness. This continuous evolution in reward engineering, optimization algorithms, and data efficiency is fundamental to addressing the complex challenges and unlocking the full potential of RL in advancing LLM capabilities.
### 6.1 Advanced Reward Engineering and Modeling
Reward engineering is a pivotal component in the application of Reinforcement Learning (RL) to Large Language Models (LLMs), as it provides the crucial learning signals that guide model behavior and alignment. 

**Comparison of Reward Engineering Paradigms**

| Aspect            | Preference-Based Rewards                                                     | Verifiable Reward Mechanisms (RLVR)                                                      |
| :---------------- | :--------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------- |
| **Primary Goal**  | Align with human values, preferences, subjective quality                     | Achieve objective correctness, logical soundness, factual accuracy                        |
| **Data Source**   | Human feedback (rankings, comparisons); AI-generated preferences (RLAIF)     | Objective ground truth; external verifiers (e.g., test cases, IoU, Levenshtein)          |
| **Data Collection** | Costly, time-consuming human annotation; prone to subjectivity & inconsistency | Automated, scalable; minimal human involvement                                           |
| **Objectivity**   | Subjective, dependent on annotator judgment                                  | Highly objective, based on ascertainable facts                                           |
| **Applicability** | Open-ended generation, dialogue systems, creativity, nuanced values          | Mathematics, programming, fine-grained multimodal perception (e.g., OCR, localization)   |
| **Strengths**     | Captures subtle aspects of quality, reflects human intent                    | Scalability, precision, reduced annotation burden, robust for precision tasks            |
| **Weaknesses**    | High cost, bias, reward hacking, RM staleness                                | Limited to tasks with clear ground truth; cannot assess subjective quality or nuance     |
| **Examples**      | RLHF, RLAIF, DPO, EndoRM, SER                                                | DeepSeek-R1, Perception-R1, CARD, LPPO                                                   |

This section synthesizes the diverse strategies for reward engineering, specifically differentiating between preference-based and verifiable reward mechanisms, and analyzing their goals, implications for data collection, objectivity, applicability, strengths, and weaknesses for various LLM-RL tasks [1,5,7,8,10,13,16,17,18,20,22,23,29,30,31].

The landscape of reward engineering can be broadly categorized into two primary paradigms: **preference-based rewards** and **verifiable rewards**. Preference-based methods, exemplified by Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF), aim primarily at aligning LLMs with human values, preferences, and subjective quality judgments. In this paradigm, human evaluators provide explicit feedback, such as rankings or choices between outputs, to train a reward model [29,35]. This process necessitates the construction of rich preference datasets, which can be costly and complex to acquire, facing challenges like human subjectivity, potential for conflicting annotations, and susceptibility to "reward hacking" [10,24]. To mitigate these issues, advancements include direct optimization algorithms like DPO [13,19] and DPOP, which directly optimize the policy using preference data. Furthermore, generative reward modeling, particularly RLAIF, leverages powerful AI systems (e.g., GPT-4) to generate preferences or scores, significantly reducing the reliance on extensive human feedback [25,29]. Concepts such as Endogenous Reward Models (EndoRM) propose that reward signals are inherently present within LLMs, recoverable from their latent representations [20], while Self-Evolved Reward Learning (SER) provides a framework for iterative self-improvement through self-annotation, gradually reducing the need for external supervision [31].

In contrast, **verifiable reward mechanisms (RLVR)** operate on the principle of objective, ascertainable ground truth, primarily targeting tasks where correctness can be precisely evaluated. These mechanisms are crucial for enhancing reasoning capabilities and achieving logically sound, factually accurate outcomes [17,22,23]. RLVR leverages explicit rules and external verifiers to compute direct and reliable reward signals, often binary (correct/incorrect), in domains such as mathematics, programming, and fine-grained multimodal perception [1,17,22]. For instance, frameworks like Perception-R1 employ rule-based rewards based on quantifiable metrics like Intersection over Union (IoU) for visual localization [1]. The primary advantage of RLVR lies in its scalability, objectivity, and reduced annotation burden, as it minimizes human involvement in feedback generation, leading to accelerated training and robust performance in tasks demanding precision [22,30].

**Comparing and contrasting** these two paradigms reveals their distinct strengths and applicability. Preference-based rewards are indispensable for tasks requiring subjective judgment, creativity, or alignment with nuanced human values where objective correctness is ill-defined (e.g., open-ended generation, dialogue systems). Their strength lies in capturing subtle aspects of quality that rule-based systems cannot. However, their weakness stems from the inherent subjectivity and resource-intensive nature of data collection [10]. Conversely, verifiable rewards excel in tasks with clear, objective ground truth, making them highly effective for logical reasoning, factual accuracy, and problem-solving (e.g., mathematical proofs, code generation, object detection) [1,22]. Their strength is their objectivity, scalability, and precision, but their limitation is the inability to assess subjective quality or generate nuanced responses where external verification is absent [23].

The evolution of reward engineering for LLMs demonstrates a continuous effort to move from resource-intensive human-centric annotations towards more automated, AI-driven, and even intrinsic reward mechanisms. Dynamic feedback and reward shaping techniques, such as SCoRe, further enhance model performance by guiding self-correction and combining multiple distinct reward signals to form richer composite rewards [18]. The CARD framework illustrates how LLMs can dynamically generate and refine reward function code, integrating verification steps for higher quality signals [5]. Ultimately, the choice and design of reward mechanisms are critical and task-dependent, often requiring a blend of preference-based strategies for alignment and verifiable methods for robust performance in tasks demanding factual accuracy and logical coherence. The ongoing research endeavors aim to develop more efficient, scalable, and robust reward systems that can effectively leverage both subjective preferences and objective ground truth to push the boundaries of LLM capabilities.
#### 6.1.1 Preference-based and Generative Reward Modeling
Reward engineering stands as a cornerstone in the application of Reinforcement Learning (RL) to Large Language Models (LLMs), dictating the learning signals that guide model behavior. This domain has witnessed the evolution of diverse strategies, including preference-based, rule-based, dynamic feedback, and generative reward types, each offering distinct advantages and limitations across various LLM-RL tasks.

**Preference-based Reward Modeling** forms the foundation of Reinforcement Learning from Human Feedback (RLHF), where human evaluators provide explicit preferences—such as rankings or choices between outputs—to train a reward model [29,35]. The construction of rich preference datasets is paramount for the generalization performance of these reward models [24]. Notable efforts in this direction include datasets like Skywork-Reward, comprising 80,000 high-quality preference pairs, and TÜLU-V2-mix, designed for diverse instruction-following and multi-turn dialogue tasks [29]. Llama 2, for instance, trains two independent reward models for helpfulness and safety using over 1 million human binary comparisons, employing a modified binary ranking loss that incorporates a margin component to encourage larger score differences for more distinct preferences [28]. This approach, while effective, faces inherent challenges such as the high cost and complexity of collecting high-quality human feedback, the strong subjectivity and potential for conflicts in human annotations, and susceptibility to "reward hacking" [6,9,10,24]. The DPO (Direct Preference Optimization) algorithm and its variants, such as DPOP, address some of these issues by directly optimizing the policy using preference data, thereby bypassing the explicit training of a separate reward model [13,19]. DPOP introduces a modified loss function that ensures the actor model's log-probability ratio for chosen responses remains positive relative to a reference model, preventing performance collapse during alignment:
$$L_{DPOP} = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left$$
Other methods like RRHF [4] and SRPO [16] also leverage ranking losses from human preferences to align LLMs directly, offering more robust ways to utilize preference data without strict dependence on a separate reward model or specific behavior policies.

**Generative Reward Modeling** represents a paradigm shift, leveraging the generative capabilities of LLMs to create feedback or reward signals, thereby reducing reliance on extensive human feedback [29]. A prominent instance is Reinforcement Learning from AI Feedback (RLAIF), where powerful AI systems (e.g., GPT-4) generate preferences or scores [25,29]. Datasets like UltraFeedback, with over 1 million GPT-4 feedback annotations, utilize Chain-of-Thought (CoT) reasoning to enhance quality [29,33]. Techniques for robust AI preference labeling include careful prompt engineering with detailed preambles and CoT rationales, as well as strategies to mitigate position bias by averaging inferences from swapped response orders [27,33]. Beyond generating preferences, LLMs can act as "reward designers," generating exploration goals, directly outputting reward values (e.g., RDLM), or even optimizing executable reward *code* (e.g., Eureka, Text2Reward) [14,29,34]. The CARD framework exemplifies this by using an LLM to generate reward function code, complemented by dynamic feedback for iterative refinement [5]. Self-rewarding mechanisms such as Self-Refined LM and SRLM allow LLMs to generate and iteratively refine reward functions or evaluate their own outputs, creating preference pairs for self-improvement [29].

A revolutionary idea posits that reward signals are *inherent* within LLMs through **Endogenous Reward Models (EndoRM)** [20]. This concept suggests that a powerful generalist reward model is latently present in any language model trained via standard next-token prediction. EndoRM is recovered by interpreting LLM logits as soft Q functions and applying the inverse soft Bellman operator. It is notable for being training-free and not requiring expensive human annotation, inheriting the base LLM's strong instruction-following capabilities [20]. Complementing this, **Self-Evolved Reward Learning (SER)** is a practical framework for iterative self-improvement [31]. SER reduces the need for extensive human feedback by enabling the reward model to generate its own labels for unannotated data ("self-annotation") and iteratively refine its judgment capabilities. This process initiates with a small human-annotated subset, then the model generates scores for additional data, filters these self-annotated data based on confidence, and retrains the reward model using a pairwise loss function [31].

**Rule-based Reward Modeling** offers an alternative for tasks with clear, quantifiable ground truth. This approach explicitly defines reward functions based on objective metrics, providing precise learning signals. It is particularly effective in complex domains like multimodal perception, as demonstrated by the Perception-R1 framework [1,8,17]. Perception-R1 employs a reward function with a binary **format reward** (e.g., +1 for correct bounding box format) and an **answer reward** based on task-specific metrics such as Intersection over Union (IoU) for visual localization, Euclidean distance for visual counting, and Levenshtein distance for OCR [1,8,17]. For multi-instance scenarios (e.g., object detection), advanced techniques like **bipartite graph matching** (specifically the Hungarian algorithm) are crucial. This method treats predicted results and ground truth as two sets, calculates potential rewards for all pairs, and finds the optimal match to maximize the total reward, ensuring accurate learning signals [1,8,17]. Other examples include using string matching for correctness in reasoning tasks [7] or simple binary signals for mathematical correctness [22,32]. While objective and precise, a limitation is that simple outcome-based rewards might not fully capture the quality of intermediate reasoning steps [7].

**Dynamic Feedback and Reward Shaping** further enhance reward engineering. Reward shaping, as seen in the SCoRe method, guides models toward self-correction by optimizing performance across multiple attempts [18]. It involves combining various distinct reward signals, such as correctness, adherence to format, or response length, to form a richer composite reward [30]. Dense rewards, which provide feedback at each step of the reasoning process, offer more precise guidance compared to sparse, outcome-based rewards, albeit at a higher acquisition cost [6,30].

In summary, the evolution of reward engineering for LLMs reflects a shift from costly human-centric annotations to more automated, AI-driven, and even inherent reward mechanisms. Preference-based methods, particularly with advancements like DPO and RLAIF, strive to reduce annotation burdens while aligning with human values. Rule-based rewards offer precise, objective signals for tasks with clear ground truth, exemplified by multimodal perception. The emergent concept of endogenous reward models, along with iterative self-improvement frameworks like SER, promises further breakthroughs by tapping into the intrinsic knowledge of LLMs themselves, pushing towards more efficient and scalable alignment strategies.
#### 6.1.2 Verifiable Reward Mechanisms (RLVR)
Verifiable Reward Mechanisms (RLVR) signify a transformative paradigm in training large language models (LLMs), fundamentally shifting reward signal generation from subjective human preferences to objective, ascertainable ground truth [23,30]. This approach is predicated on explicit rules and factual correctness, enabling the automated and highly scalable computation of direct and reliable reward signals [30].

The primary advantage of RLVR lies in its capacity to deliver objective and scalable reward signals, particularly in domains where the correctness of an outcome can be precisely verified. These domains encompass tasks such as mathematics, programming, and fine-grained visual perception [1,17,22,23]. For mathematical problems, the reward is intrinsically linked to the correctness of the final answer, often manifesting as a binary signal (correct/incorrect) [22,32]. Similarly, for programming tasks, rewards are determined by whether the generated code successfully passes all specified test cases or compiler checks [23,30]. In multimodal contexts, frameworks like Perception-R1 employ rule-based rewards derived from quantifiable metrics such as Intersection over Union (IoU) for visual localization, Euclidean distance for visual counting, and adherence to specific output formats [1,17]. While not explicitly termed RLVR, methods like SCoRe implicitly leverage this mechanism by enhancing self-correction based on the verifiable accuracy of outputs in mathematical and programming contexts [18]. Furthermore, the CARD framework incorporates an internal verification step to ensure the syntactic correctness and executability of generated reward functions, thereby contributing to higher quality reward signals [5]. This inherent objectivity minimizes reliance on human annotation, leading to reduced costs and accelerated training, as evidenced by the consistent accuracy improvements observed in models trained with the ROLL framework, including Qwen2.5-7B-Base and Qwen3-30B-A3B-Base [15]. Similarly, RLS-Razor utilizes binary "success/failure" rewards for tasks with verifiable outcomes, further affirming the efficacy of this approach [3].

RLVR fundamentally contrasts with preference-based reward modeling, which typically relies on subjective human judgments and comparative feedback to train a reward model. While preference-based methods are indispensable for aligning LLMs with human values, ethics, and aesthetic preferences, RLVR distinguishes itself by excelling in tasks that demand precise, logically sound, and factually correct outcomes [17,22,23]. Its strengths reside in providing unambiguous learning signals, thereby circumventing the complexities and potential for reward hacking often associated with more elaborate or subjective scoring schemes [32]. By leveraging external verifiers, RLVR substantially reduces the annotation burden and associated operational costs, enabling models to explore novel reasoning paths beyond those easily amenable to human annotation [22,30].

Despite its significant advantages, RLVR exhibits inherent limitations when applied to tasks lacking a clear, objectively ascertainable ground truth. Subjective domains, such as creative content generation, opinion formation, or nuanced conversational interactions, cannot be readily assessed by rule-based verifiers [23]. In such contexts, the absence of explicit factual correctness or quantifiable metrics poses substantial challenges to defining verifiable rewards, thereby constraining the direct applicability of RLVR. While some research explores internal or "endogenous" reward mechanisms within LLMs that operate without external verification, these differ from the externally verifiable, objective signals central to RLVR [20].

The advent of RLVR has been pivotal for the development of Large Reasoning Models (LRMs), enabling LLMs to acquire and significantly enhance complex reasoning capabilities. Models such as DeepSeek-R1 and OpenAI's o1 exemplify this trajectory, employing "pure reinforcement learning" to advance reasoning performance on challenging benchmarks like AIME and competitive programming platforms [22,26,29]. These models demonstrate how RLVR facilitates the acquisition of long-chain reasoning, planning, reflection, and self-correction by providing direct, objective feedback on the correctness of intermediate steps or final solutions [6,10,23]. The success of frameworks like LPPO in bolstering mathematical reasoning through a straightforward yet powerful binary verifiable reward further underscores RLVR's efficacy in improving reasoning without relying on extensive supervised fine-tuning [32]. Ultimately, RLVR empowers models to evolve and achieve stable, consistent accuracy improvements in reasoning-intensive tasks, thereby playing a critical role in expanding the frontiers of LLM capabilities toward more sophisticated cognitive functions [15].
### 6.2 Policy Optimization Algorithms and Frameworks
Policy optimization algorithms and frameworks are central to the effective integration of Reinforcement Learning (RL) with Large Language Models (LLMs), dictating how LLMs learn from feedback to refine their generation capabilities [10,34]. These methodologies are specifically tailored to address unique challenges such as automated reward design, intrinsic self-correction, and multimodal perception, providing advantages in iterative refinement, constraining unwanted behaviors, and optimizing policies through relative comparisons.



**Comparison of Key Policy Optimization Algorithms**

| Algorithm / Method                     | Type               | Key Idea                                                                                               | Advantages                                                                                                                                                                                             | Limitations / Challenges                                                                                                                                                                                                                                                                 |
| :------------------------------------- | :----------------- | :----------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Proximal Policy Optimization (PPO)** | Critic-based       | Integrates Actor & Critic; limits policy divergence with clipped objective & KL penalty.                 | Stability, widely adopted in RLHF, good for general alignment (e.g., InstructGPT, GPT-4, Llama 2).                                                                                                      | High computational overhead (multiple models), hyperparameter sensitivity, training complexity, poor sample efficiency, mode collapse, instability, robustness issues (OOD) [4, 12, 19, 24].                                                                                               |
| **Direct Preference Optimization (DPO)** | Offline, Critic-free | Directly optimizes policy from preference data (chosen/rejected pairs) via supervised objective.         | Simpler, more stable, computationally efficient (no separate RM, fewer models, no online sampling); widely adopted (e.g., Llama 3, Qwen 2, Phi-3).                                                  | Exploration limitations (OOD), insufficient fine-grained optimization, potential for overfitting/performance collapse, task-dependency [13, 16].                                                                                                                                         |
| **Group Relative Policy Optimization (GRPO)** | Critic-free        | Generates multiple outputs, evaluates with rule-based reward, updates based on relative comparisons.     | Computationally efficient (no critic), scalable for large reasoning models, fosters autonomous strategy discovery (e.g., DeepSeek-R1, Perception-R1); faster convergence.                                | Can exhibit instability ("reward collapse"), may need careful reward engineering to avoid shortcuts [7, 30].                                                                                                                                                                            |
| **Rank Responses to Align Language Models with Human Feedback (RRHF)** | Critic-free        | Scores sampled responses, aligns probabilities with human preferences via ranking loss.                  | Requires 1-2 models (fewer than PPO), robust alignment without complex hyperparameter tuning, simplified coding, reduced resource overhead, "best-of-n learner" [4].                                   | Performance highly dependent on quality of sampled responses.                                                                                                                                                                                                                            |
| **Advantage-Induced Policy Alignment (APA)** | Critic-based       | Uses squared error loss with estimated advantages instead of PPO's clipped objective.                    | Enhanced stability, improved sample efficiency, prevents mode collapse, augments response diversity; superior performance compared to PPO [12].                                                           | Still requires an independent reward model for evaluation.                                                                                                                                                                                                                               |
| **Self-Improving Robust Preference Optimization (SRPO)** | Offline, Critic-free | Min-max objective for continuous self-improvement; solution mathematically independent of behavior policy. | Robust to OOD tasks & policy shifts, superior generalization (e.g., +15% WR vs DPO on XSum OOD), prevents degradation of prior SFT/pre-training [16].                                                     | Complex min-max optimization.                                                                                                                                                                                                                                                         |
| **Learning-Progress Policy Optimization (LPPO)** | Sample-centric     | Combines Prefix-Guided Sampling and Learning-Progress Weighting for efficient sample utility.            | Enhances reasoning, faster convergence, higher performance; improves generalization, reduces regressions, preserves solution diversity across models and architectures (e.g., GRPO) [32].             | Requires specialized sampling and weighting mechanisms.                                                                                                                                                                                                                               |

A fundamental distinction in policy optimization for LLMs lies between critic-based and critic-free algorithms [30]. **Critic-based algorithms**, exemplified by Proximal Policy Optimization (PPO), integrate an auxiliary "Critic" model alongside the primary "Actor" (the LLM) [30]. The Critic evaluates the quality of intermediate states or individual generation steps, offering fine-grained, often token-level, guidance to the Actor [30]. While this provides strong, precise feedback, it introduces substantial computational overhead due to the need to maintain an additional Critic model, particularly challenging for very large models [30].

In contrast, **critic-free algorithms**, such as Group Relative Policy Optimization (GRPO), operate without an auxiliary Critic model [30]. These methods typically rely on the final, sequence-level reward and employ sophisticated mathematical techniques, like group relative ranking, to implicitly attribute this overall score back to individual generation steps [30]. This approach is inherently simpler, more computationally efficient, and more scalable, rendering it a mainstream choice for training large reasoning models [30]. The "on-policy" nature of algorithms like GRPO and REINFORCE has been empirically linked to reduced catastrophic forgetting and superior retention of old tasks, in contrast to offline methods [3].

**Proximal Policy Optimization (PPO)** stands as a cornerstone algorithm in RL for LLMs, widely adopted for Reinforcement Learning from Human Feedback (RLHF) due to its stability and efficiency in limiting policy divergence during updates [6,24]. It is integral to the fine-tuning of prominent models like InstructGPT and GPT-4, and plays a role in Llama 2's iterative RLHF process, often incorporating a Kullback-Leibler (KL) divergence penalty to prevent drastic deviations from the original policy and enhance stability [20,28,29]. However, PPO is recognized for its high training complexity, substantial resource consumption [6,24], and susceptibility to issues like mode collapse, instability, and poor sample efficiency [12,19]. To mitigate these, variants and improvements have emerged, such as the enhancements in Starling-7B's RLAIF process which address stability and robustness, and Advantage-Induced Policy Alignment (APA) which specifically targets PPO's mode collapse and instability by using a squared error loss function based on estimated advantages [12,29]. PPO is also supported by advanced frameworks like TRL and ROLL, which provide efficient and scalable training environments [2,15].

**Direct Preference Optimization (DPO)** offers a simplified alternative to PPO by directly optimizing the policy using preference data, thereby bypassing the need for a separate reward model and converting the RL problem into a supervised fine-tuning task [19,24,29]. This offline method reduces model complexity (requiring only two models compared to PPO's four) and avoids the sampling overhead associated with online RL, leading to more efficient and stable optimization, though it may face challenges in fine-grained optimization [6,13]. Notable LLMs like Llama 3 and Phi-3 leverage DPO [29]. Variants include DPOP, which enhances stability by modifying the loss function to maintain positive log-probability ratios [13], Reward-aware Preference Optimization (RPO) which utilizes the policy network's implicit reward to prevent overfitting [29], and Odds Ratio Preference Optimization (ORPO) which unifies alignment by adding an odds-ratio-based penalty to the SFT loss, negating the need for reward or reference models [29].

**Group Relative Policy Optimization (GRPO)** is a prominent critic-free algorithm, extensively applied in challenging LLM domains such as reasoning and multimodal perception [23,30]. Its core mechanism involves generating multiple outputs for a given input, evaluating each with a rule-based reward function, and then updating the policy based on relative comparisons of these rewards [1,8,17]. Outputs performing better than average receive a positive "advantage," while those worse receive a negative one, facilitating an iterative refinement process that increases the likelihood of high-reward generations [17]. GRPO has been successfully employed in DeepSeek-R1 to incentivize reasoning [22] and in Perception-R1 for fine-grained visual perception in multimodal LLMs [17]. While it offers faster convergence, it can exhibit instability, such as "reward collapse," as observed in LLaMA3.2-3B-Instruct training [7]. GRPO is also a supported algorithm within the ROLL framework [15].

Other significant policy optimization algorithms and frameworks include:
*   **REINFORCE**: A simpler policy gradient method that reduces dependence on value models. Variants like ReMax and RLOO leverage multi-trajectory sampling, and REINFORCE++ is integrated into the ROLL framework [6,15]. A modified REINFORCE with a KL divergence term is also used to penalize deviation from the initial policy, preventing reward hacking [27].
*   **RRHF (Ranking Reward Human Feedback)**: This novel algorithm aligns language models with human feedback without requiring PPO's complexity. It simplifies the process to 1-2 models, achieving robust alignment through scoring sampled responses via log-conditional probabilities and applying a ranking loss, effectively acting as a "best-of-n learner" [4].
*   **LPPO (Learning-Progress and Prefix-guided Optimization)**: A sample-centric framework designed to enhance policy optimization within the RLVR paradigm [32]. LPPO combines Prefix-Guided Sampling, an online data augmentation method for challenging problems, with Learning-Progress Weighting, which dynamically adjusts sample influence based on learning progress. This framework consistently improves the performance of various policy-gradient learners, including GRPO [32].
*   **Coder-Evaluator Framework**: Focuses on automated reward design rather than direct policy optimization. It generates and refines reward functions ($R$) that are then utilized by standard RL agents to learn policies, facilitating iterative refinement without human feedback or redundant LLM queries [5].
*   **Multi-round Online RL (SCoRe)**: This method addresses intrinsic self-correction through a two-stage training process. Stage I initializes a model to reduce "behavior collapse," while Stage II uses reward shaping to jointly optimize performance and encourage true self-correction strategies [18].
*   **Kahneman-Tversky Optimization (KTO)**: An innovative, cost-effective method based on human decision research, using single desirable/undesirable feedback instead of comparative preferences, demonstrating significant performance gains [24].
*   **SRPO (Self-improving Robust Preference Optimization)**: Introduces a min-max objective to jointly optimize self-improvement and generative strategies, providing robustness to behavior policy changes, contrasting with offline methods like DPO and IPO [16].

While Q-Learning and general Actor-Critic methods are recognized as fundamental RL algorithms, their specific detailed applications in the provided digests concerning LLM-RL integration are less pronounced compared to PPO or GRPO [10]. The evolution of these algorithms and frameworks reflects a concerted effort to overcome the complexities and instabilities inherent in aligning LLMs with desired behaviors and human preferences, balancing fine-grained control with computational scalability.
### 6.3 Data Generation and Efficiency
The application of Reinforcement Learning (RL) to Large Language Models (LLMs) is fundamentally constrained by significant computational intensity and demanding data requirements. A core challenge lies in the high cost, time consumption, and scalability limitations associated with collecting high-quality human feedback for reward model training [9,10,29,35]. Human annotations are prone to inconsistencies, subjectivity, and cognitive biases, which can compromise data quality and model performance [9]. Beyond data annotation, the RL training process itself, particularly algorithms like PPO, incurs substantial computational resources due to its online nature, requiring continuous generation of completions, multiple forward passes, and often maintaining multiple model copies (e.g., active and reference models) [2,13]. This computational burden translates into considerable GPU memory requirements and energy consumption, exemplified by models like Llama 2, which involved millions of GPU hours during pre-training [28]. The inherent sample inefficiency of traditional RL algorithms further exacerbates these issues, necessitating a large number of samples to learn effective strategies [12,18].

To address these challenges, researchers have developed various approaches to enhance data efficiency and reduce computational overhead. A prominent strategy involves leveraging **self-generated or synthetic data** to reduce reliance on expensive human annotation. 

![Evolution of Data Generation for RL-LLMs](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/JEGL5pRRL7ZsMxU9AS05B_Evolution%20of%20Data%20Generation%20for%20RL-LLMs.png)

The evolution in data generation paradigms demonstrates a clear shift:

1.  **From Human-Intensive Annotation**: Early RLHF approaches heavily depended on human evaluators to generate preference datasets, such as the millions of binary comparisons collected for Llama 2 or the data used for InstructGPT [28,29,35]. While this enabled rapid collection of comparative data and reduced the cognitive burden on annotators by asking for rankings instead of generation, the underlying cost remained substantial [35].

2.  **To Synthetic Data Generation (e.g., GPT-4 for DPO)**: Advanced LLMs are increasingly employed to generate synthetic data, significantly scaling data collection and reducing costs. Reinforcement Learning from AI Feedback (RLAIF) exemplifies this, replacing human preference labeling with AI-generated labels, which can be over 10 times cheaper and faster [11,27,33]. Methods like UltraFeedback use GPT-4 to generate large feedback annotations, and Magpie employs aligned LLMs for self-synthesis of instruction-response pairs [29]. For instance, DPO can utilize synthetic datasets created by GPT-4 for chosen and rejected responses, thereby circumventing costly human annotation for specific alignment tasks [13]. LLMs serving as "Generators" can produce diverse samples, simulate complex environments, and create self-supervised training data, thereby augmenting datasets and facilitating exploration [14,34].

3.  **And Purely Self-Generated Data (e.g., DeepSeek-R1)**: The frontier of data efficiency involves models generating their own training data and reward signals with minimal or no external human input. DeepSeek-R1, for example, employs "pure reinforcement learning" with a reward signal based solely on the correctness of the final prediction, significantly minimizing reliance on human annotation and achieving remarkable cost efficiency (e.g., $294,000 for its core DeepSeek-R1-Zero model) [22,26,29]. Similarly, SCoRe utilizes "completely self-generated data" for training, eliminating external human-annotated datasets, though this can still entail substantial computational demands [18]. The Self-Evolved Reward Learning (SER) method further enhances this by enabling the reward model to perform "self-annotation" on unlabeled examples, effectively expanding the training dataset with high-confidence samples and achieving comparable performance with a fraction of human-annotated data [31]. The concept of "Self-Evolution" promotes systems that iteratively generate and utilize their own data for continuous improvement across tasks, CoT, and reasoner/evaluator training [6]. Rule-based reward engineering, such as in Perception-R1, which leverages quantifiable ground truth in visual tasks, also minimizes reliance on human preference labels [1,8,17]. The "Endogenous Reward Model (EndoRM)" offers a training-free reward signal derived from internal LLM knowledge, eliminating the need for expensive human preference datasets entirely [20].

Beyond data generation, several architectural and algorithmic innovations contribute to efficiency:

*   **Avoiding Full RL Training Loops**: Trajectory Preference Evaluation (TPE) within the CARD framework allows the Evaluator to assess reward function quality using trajectory preferences, bypassing the need for computationally intensive full RL training at every iteration [5]. This method also incorporates reward introspection and dynamic feedback to skip unnecessary training steps and refine rewards without redundant LLM queries.
*   **Sample-Centric Optimization Techniques**: The Learning-Progress Policy Optimization (LPPO) framework shifts to a "sample-centric" approach, maximizing the utility of a small set of high-quality reasoning data [32]. It employs Prefix-Guided Sampling (PG-Sampling) for online data augmentation, generating partial expert solutions for challenging problems to guide model exploration. Learning-Progress Weighting (LP-Weighting) dynamically re-weights samples based on learning progress, concentrating resources on active learning areas and accelerating convergence. LPPO also features online data curation, dynamically filtering out overly easy or consistently unsolved samples. Other frameworks like ROLL utilize sample-level scheduling mechanisms, including dynamic sampling and EarlyStopping, to enhance training efficiency and resource utilization [15]. RRHF improves data efficiency by leveraging responses from diverse sources (model's own, other LLMs, human experts), focusing on sampling quality over quantity, and reducing computational overhead compared to PPO [4].
*   **Algorithmic and System-Level Optimizations**:
    *   **Advantage-Induced Policy Alignment (APA)** addresses the poor sample efficiency of PPO by providing more stable control and preventing mode collapse, allowing for more effective learning from fewer samples [12].
    *   **Direct Preference Optimization (DPO)**, as an offline method, inherently reduces computational costs by eliminating the continuous data sampling required by PPO and removing the need for a separate reward model, leading to a simpler and more efficient training pipeline [13,19].
    *   **Parameter-Efficient Fine-Tuning (PEFT)**, particularly LoRA (Low-Rank Adaptation), significantly reduces GPU memory requirements by fine-tuning low-rank adapter matrices instead of the entire LLM, making training feasible on more modest hardware [2,16,29]. Techniques like 8-bit matrix multiplication (LLM.int8()) further reduce model size and memory footprint.
    *   Specialized RL frameworks and techniques, such as ORPO, DeepSeek-V2's GRPO (Group Relative Policy Optimization), and ChatGLM-RLHF's use of model parallelism and fused gradient descent, specifically aim to reduce training costs and improve efficiency [29]. SEARCH-R1's retrieved token masking technique optimizes only LLM-generated tokens, contributing to stable training and reducing reliance on large-scale supervised data [7]. RLS-Razor introduces a method to predict and manage catastrophic forgetting using forward Kullback-Leibler (KL) divergence $$D_{KL}(P_{\text{new task}} || P_{\text{base}})$$, enabling better data efficiency by identifying suitable supervisory distributions and preventing performance degradation on old tasks [3].

These advancements have significant implications for the scalability and practical deployment of RL-based LLM solutions. The move towards AI-generated and self-generated data vastly improves scalability by circumventing the bottleneck of human annotation, allowing for the rapid expansion of training datasets [27]. Efficiency-focused algorithms and infrastructure optimizations make it possible to train larger models or fine-tune existing ones with fewer resources, democratizing access to powerful RL-LLM techniques, as demonstrated by the ability to fine-tune 20B LLMs on a single consumer GPU using PEFT and quantization [2].

However, these considerations also highlight crucial trade-offs between efficiency and performance. While AI labeling is cheaper and faster, RLAIF's performance improvement can reach a "bottleneck" compared to human feedback, suggesting that AI-generated data might lack certain nuanced "additional input features" inherent in human judgments [33]. Using larger AI labelers can produce higher quality preferences but at an increased computational cost [27]. Similarly, DPO's effectiveness can be unstable if the alignment goal is too specific, potentially leading to overfitting [13]. Methods like SCoRe, despite their self-sufficiency in data generation, still demand considerable computational resources and time, emphasizing a trade-off between autonomous data generation and overall computational efficiency [18]. The integration of these diverse strategies aims to strike an optimal balance, paving the way for more scalable, cost-effective, and robust RL-based LLM systems.
## 7. Practical Frameworks and Systems for RL-driven LLM Training
This section offers a comprehensive overview of the practical infrastructure and methodological considerations crucial for the effective training and deployment of Reinforcement Learning (RL)-driven Large Language Models (LLMs). The integration of RL with LLMs presents unique computational, scalability, and usability challenges, necessitating the development of specialized architectures, frameworks, and tools [2,14,15]. This chapter delineates how these specialized systems are engineered to address the immense resource demands and algorithmic complexities inherent in large-scale RL-LLM development.

The first sub-section, "Efficient and Scalable RL Training Frameworks," delves into the engineering solutions and infrastructural advancements that enable the practical training of RL-driven LLMs. It examines specialized frameworks such as ROLL, which leverages multi-role distributed architectures, advanced parallelism techniques (e.g., MegatronCore's 5D parallelism), modular component design, and elastic resource scheduling to facilitate efficient and stable training across diverse hardware configurations [15]. Furthermore, this sub-section explores critical memory optimization strategies, including 8-bit precision loading and Parameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation (LoRA), which significantly reduce the computational and memory footprint, making large-scale LLM fine-tuning more accessible even on consumer-grade hardware [2,16,29]. Other advancements, such as Fully Sharded Data Parallel (FSDP) and Mixture-of-Experts (MoE) architectures, are also discussed as essential components for managing memory and enhancing scalability [28,29]. Despite these innovations, the sub-section also acknowledges the persistent challenges related to general infrastructure and the unceasing demand for computing resources, highlighting the continuous need for resource-efficient solutions [23,26,28].

Building upon the technological foundations laid by these frameworks, the second sub-section, "Real-world Applications and Case Studies," illustrates the tangible impact and diverse deployments of RL-driven LLMs across various practical domains. It showcases how these advanced systems have significantly enhanced LLMs' reasoning capabilities in complex tasks such as mathematics and programming (e.g., DeepSeek-R1, SCoRe) [7,18,22]. A major focus is placed on LLM alignment with human preferences and safety standards through techniques like Reinforcement Learning from Human Feedback (RLHF), which has become a predominant strategy for commercially successful models like ChatGPT and Llama 2 [9,29,35]. Innovations like Reinforcement Learning from AI Feedback (RLAIF) and Self-Evolved Reward Learning (SER) are presented as scalable alternatives to address the high costs and labor intensity of traditional RLHF [27,31]. Furthermore, the sub-section explores breakthroughs in multimodal perception (e.g., Perception-R1) and the emergence of robust agentic behaviors in complex interaction scenarios, as demonstrated by frameworks like ROLL in business innovation and CARD in robotics [1,5,15]. This section collectively discusses the trade-offs between custom solutions and general-purpose frameworks, underscoring the prevailing trends towards modular, distributed, and resource-efficient systems that are vital for both academic research and industrial application. It highlights the lessons learned from deploying these systems, emphasizing the continuous evolution required to push the boundaries of RL applications in LLMs towards greater accessibility, intelligence, and broader impact [14].
### 7.1 Efficient and Scalable RL Training Frameworks
The integration of Reinforcement Learning (RL) with Large Language Models (LLMs) has necessitated the development of specialized training frameworks that address the inherent challenges of large-scale model optimization. The primary motivations for these frameworks stem from the enormous computational demands, memory footprint, and complex algorithmic requirements associated with training LLMs, often comprising billions or even hundreds of billions of parameters. Achieving high efficiency, scalability, and user-friendliness becomes critical to democratize access and accelerate research in this rapidly evolving field [15,23,26].



**Key Features of Efficient & Scalable RL Training Frameworks**

| Feature Category      | Specific Feature                                                                     | Description & Impact                                                                                                                                                                                                                                                                                                                                                                                           |
| :-------------------- | :----------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Architecture & Parallelism** | **Multi-Role Distributed Architecture** (e.g., ROLL, Ray-based)                        | Distributes computational load across multiple nodes/roles, enhancing throughput and fault tolerance [15].                                                                                                                                                                                                                                                                                                 |
|                       | **5D Parallelism** (e.g., MegatronCore in ROLL)                                      | Combines Data, Tensor, Pipeline, Sequence, and Expert Parallelism for optimal utilization of heterogeneous hardware, reducing training costs and interruption risks on large clusters [15].                                                                                                                                                                                                               |
|                       | **Mixture-of-Experts (MoE) Architectures** (e.g., DeepSeek-V2, Zephyr 141B-A39B)      | Economically efficient by activating only a subset of parameters per token during training/inference, enhancing scalability for very large models [29].                                                                                                                                                                                                                                                     |
|                       | **Fully Sharded Data Parallel (FSDP)** (e.g., Llama 2)                               | Manages memory demands by sharding model states across GPUs, crucial for large models; can introduce slowdowns during generation [28].                                                                                                                                                                                                                                                                     |
| **Memory Optimization** | **8-bit Precision Loading / Quantization** (e.g., LLM.int8())                        | Drastically reduces GPU memory footprint, enabling training of models (e.g., 20B LLMs) on single consumer-grade GPUs [2].                                                                                                                                                                                                                                                                                  |
|                       | **Parameter-Efficient Fine-Tuning (PEFT)** (e.g., LoRA)                              | Freezes pre-trained weights, injects small, trainable low-rank adapter matrices. Significantly reduces trainable parameters & memory consumption, making fine-tuning feasible on modest hardware [2, 16, 29].                                                                                                                                                                                                 |
| **Algorithmic & Process** | **Elastic Resource Scheduling** (e.g., ROLL)                                         | Allows flexible allocation of computational resources, optimizing utilization and cost [15].                                                                                                                                                                                                                                                                                                               |
|                       | **Modular Component Design** (e.g., ROLL)                                            | Simplifies pipeline development & debugging; supports on-demand component combination; enables seamless switching between backend engines (vLLM, SGLang, Megatron-Core, DeepSpeed) [15].                                                                                                                                                                                                                      |
|                       | **Sample-level Scheduling & Dynamic Sampling** (e.g., ROLL)                          | Improves training efficiency & resource utilization by managing prompt samples during generation and adapting sampling strategies (e.g., EarlyStopping) [15].                                                                                                                                                                                                                                               |
|                       | **Efficient RL Algorithms** (e.g., optimized GRPO, DPO, RRHF)                        | Algorithms tailored for efficiency, reducing model dependencies, hyperparameter sensitivity, and data demands compared to traditional PPO [4, 19, 29, 30].                                                                                                                                                                                                                                                  |
| **Tools & Platforms** | **Integrated Libraries & Platforms** (e.g., TRL, PEFT, Qianfan, DeepSpeed hybrid engines) | Provides user-friendly features, modular design, and advanced optimizations (e.g., fused gradient descent, model parallelism) to simplify deployment and configuration of RL algorithms, making LLM optimization more accessible [2, 15, 24, 29].                                                                                                                                                         |

Specialized frameworks like ROLL (Reinforcement Learning Optimization for Large-scale Learning) exemplify efforts to overcome these hurdles. ROLL is an open-source RL training framework meticulously engineered for efficient, scalable, and user-friendly deployment, capable of supporting LLMs ranging from billions to over 600 billion parameters [15]. Its architectural designs incorporate several innovative features. For advanced parallelization, ROLL leverages Ray-based multi-role distributed architecture and integrates MegatronCore's 5D parallelism (Data Parallelism, Tensor Parallelism, Pipeline Parallelism, Sequence Parallelism, Expert Parallelism). This comprehensive parallelism strategy facilitates efficient heterogeneous hardware utilization and stable training on clusters scaling from single machines to thousands of GPUs, significantly reducing training costs and mitigating interruption risks for technical pioneers [15]. Elastic resource scheduling further enhances efficiency by allowing flexible resource allocation. The framework also boasts modular component design, including modules like Rollout Scheduler and AutoDeviceMapping, which simplify pipeline development and debugging, support on-demand component combination, and enable seamless switching between backend engines such as vLLM, SGLang, Megatron-Core, and DeepSpeed [15]. Beyond core infrastructure, ROLL offers multi-task and agentic RL capabilities, a diverse range of RL strategy configurations (e.g., PPO, GRPO, Reinforce++), and sample-level scheduling with dynamic sampling, which significantly improves training efficiency and resource utilization [15]. Such comprehensive platforms, alongside others like the "Qianfan Large Model Development and Service Platform" which integrates advanced RL algorithms and efficient techniques like "DeepSpeed hybrid engines", are crucial for enabling easier LLM optimization and upgrades [24].

Memory optimization techniques are paramount for fine-tuning large models within realistic hardware constraints. One effective strategy involves loading models in 8-bit precision, which drastically reduces the GPU memory footprint, enabling models that would typically require high-end, multi-GPU setups to be trained on single consumer-grade GPUs [2]. Complementing this, Parameter-Efficient Fine-Tuning (PEFT) frameworks, particularly Low-Rank Adaptation (LoRA), have emerged as indispensable tools. LoRA works by freezing the pre-trained model weights and injecting small, trainable rank-decomposition matrices into each layer of the network. During fine-tuning, only these low-rank adaptation matrices are updated, rather than the entire model, significantly reducing the number of trainable parameters and memory consumption [2]. For instance, the SRPO framework utilizes LoRA with a rank of 16 and an alpha of 32 in its experiments, demonstrating its practical application in managing computational and memory requirements during LLM alignment training [16]. Similarly, Hermes 3 employs LoRA adapters for DPO training to substantially decrease GPU memory consumption [29]. The `trl` and `peft` libraries facilitate this process by providing an integrated, practical framework for efficient and scalable RL-driven LLM training, even in memory-constrained environments [2]. Other advanced techniques, such as Fully Sharded Data Parallel (FSDP) employed in Llama 2 during PPO training, are also crucial for managing memory demands, though they can introduce complexities like significant slowdowns during the generation phase [28]. Model parallelism combined with fused gradient descent, as seen in ChatGLM-RLHF, further contributes to scalable and stable training [29]. Moreover, Mixture-of-Experts (MoE) architectures, utilized in models like DeepSeek-V2 and Zephyr 141B-A39B, offer economic efficiency in both training and inference by activating only a subset of parameters per token, thereby enhancing scalability for very large models [29].

Despite these advancements, the field continues to grapple with general infrastructure challenges and an unceasing demand for increased computing resources for scaling RL in Large Reasoning Models (LRMs) [23,26]. The training of models like Llama 2, which leverages substantial computational infrastructure including Meta's Research SuperCluster (RSC) and NVIDIA A100 GPUs, highlights the immense resource investment required [28]. The carbon footprint associated with such training further underscores the need for energy-efficient solutions [28]. While frameworks like LPPO optimize resource *utilization* through sample-centric strategies and demonstrate adaptability across various model scales and architectures, the baseline computational demands remain high, often requiring multiple high-end GPUs [32]. The emphasis on "good scalability" in systems like Perception-R1 for future large-scale multimodal LLM applications reinforces the ongoing requirement for efficient underlying implementations and robust infrastructure for applying techniques like GRPO without prohibitive resource demands [8]. The continuous evolution of these frameworks and optimization techniques is therefore essential to push the boundaries of RL applications in LLMs towards greater accessibility and intelligence.
### 7.2 Real-world Applications and Case Studies

**Real-world Impact of RL-Driven LLMs**

| Application Domain                    | Key LLM Capabilities Enhanced                                     | Examples / Case Studies                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Real-world Impact / Benefits                                                                                                                                                                                                                                                        |
| :------------------------------------ | :---------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Reasoning & Problem-Solving**       | Complex reasoning, self-correction, logical deduction             | **DeepSeek-R1**: State-of-the-art in math, programming (AIME 2024 score from 15.6% to 77.9%, > human average). **SCoRe**: Improves self-correction in math & programming (4.4% in math, 12.2% in programming). **SEARCH-R1**: Enhances LLMs for complex reasoning requiring real-time external knowledge. **RLVR**: Proven benefits for coding tasks. **O1 class studies**: Advanced reasoning via self-evolution [7, 18, 22, 30]. | Enables LLMs to excel in challenging cognitive tasks, leading to more intelligent question-answering, AI assistants, and automated problem-solving; reduced reliance on costly human-annotated examples.                                                                          |
| **Alignment & Safety**                | Adherence to human preferences, helpfulness, harmlessness, honesty | **RLHF (ChatGPT, Llama 2, GPT-4, Gemini)**: Predominant strategy for commercial models. **InstructGPT**: Smaller models outperform larger ones in human preference after RLHF. **RLAIF**: Comparable/superior performance to RLHF at reduced cost/scalability. **SER**: Achieves comparable performance with 15% of human data. **XuanYuan-6B**: RLHF in finance [9, 24, 27, 29, 31, 35]. | Critical for user acceptance & commercial viability; makes LLMs safe, ethical, and reliable; mitigates biases & harmful content; scalable alignment solutions for diverse applications.                                                                                             |
| **Multimodal Perception**             | Fine-grained visual understanding (localization, counting, OCR)   | **Perception-R1**: First open-source MLLM to surpass 30 AP on COCO2017 validation; superior performance on RefCOCO, PageOCR, PixMo-Count; outperforms specialized "expert" models [1, 8, 17].                                                                                                                                                                                                                | Enables precise visual analysis beyond general comprehension; potential for deployment in tasks demanding high visual accuracy (e.g., medical imaging, autonomous driving, industrial inspection).                                                                                   |
| **Agentic Behavior & Tool Use**       | Strategic decision-making, external tool invocation, planning     | **ROLL (Taotian Group)**: Enhanced LLM performance in business innovation; improved success rates in Sokoban (13.3% to 35.2%), FrozenLake (12.9% to 23.8%), WebShop (37% to >85%); reduced interactive actions (7 to 4). **CARD**: LLM-driven dynamic reward design for robotics, matching/surpassing human-oracle performance [5, 15].                                                               | Transforms LLMs into autonomous agents capable of complex interaction, multi-round decision-making, and leveraging external resources (e.g., robotics, energy management, recommendation systems); improved operational efficiency.                                               |

Reinforcement Learning (RL) has driven significant advancements in Large Language Models (LLMs), moving them beyond research prototypes to practical deployments across diverse industries and problem settings. These real-world applications demonstrate improved performance in domain-specific tasks, address critical challenges, and offer valuable lessons for system integration and user acceptance.

A primary area of impact is the enhancement of **reasoning capabilities** in LLMs. Models like DeepSeek-R1 have achieved state-of-the-art performance in complex reasoning tasks, including mathematics, programming contests, and graduate-level STEM problems. DeepSeek-R1-Zero, for instance, dramatically increased its pass@1 score on the AIME 2024 benchmark from 15.6% to 77.9%, and further to 86.7% with self-consistency decoding, surpassing the average human participant [22]. The SCoRe method further illustrates this by significantly improving self-correction in mathematical problem-solving on the MATH dataset and programming on HumanEval [18]. For tasks requiring real-time external knowledge, SEARCH-R1 enhances LLMs' ability to handle complex reasoning, proving highly practical for intelligent question-answering systems and AI assistants [7]. The success of Reinforcement Learning with Verifiable Rewards (RLVR) in enhancing LLMs' reasoning, particularly in coding tasks due to inherent verifiability, highlights its practical benefits [30]. Furthermore, numerous "O1 class studies," including Marco-O1, OpenR/O1-Coder, and Kimi-k1.5, leverage RL-driven training to achieve advanced reasoning through self-evolution frameworks [6].

Another crucial application domain is **LLM alignment with human preferences and safety standards**. RL from Human Feedback (RLHF) has emerged as the predominant strategy for aligning both commercial and open-source LLMs, including highly influential models like ChatGPT and Llama 2 [9]. InstructGPT demonstrated that even smaller models (1.3B parameters) could outperform much larger ones (175B parameters) in human preference evaluations after RLHF, underscoring the practical value of alignment over sheer model size [29]. Models such as GPT-4 and Gemini employ iterative RLHF to ensure appropriate response behavior and align with human needs across complex real-world applications [29,35]. To address the scalability challenges and costs associated with human feedback, methods like RLAIF (Reinforcement Learning from AI Feedback) have been developed, achieving comparable or superior performance to RLHF in tasks such as summarization and dialogue generation, but at significantly reduced cost and increased scalability [25,27]. Similarly, the Self-Evolved Reward Learning (SER) method has shown that comparable performance to full human-annotated datasets can be achieved with as little as 15% of the data, as validated on Llama 3 and Mistral 7B models, significantly reducing human supervision requirements [31].

RL-driven LLMs are also making breakthroughs in **multimodal perception**. The Perception-R1 framework applies rule-based RL, specifically Group Relative Policy Optimization (GRPO), to enhance Multimodal LLMs (MLLMs) for fine-grained visual perception tasks [1]. This framework has demonstrated superior performance on standard visual perception benchmarks such as RefCOCO for visual grounding, PageOCR for OCR, PixMo-Count for visual counting, and COCO2017 for object detection, even surpassing specialized models and becoming the first pure multimodal open-source LLM to exceed 30 AP on COCO2017 validation [8,17]. These empirical validations suggest strong potential for real-world deployment in areas demanding precise visual understanding.

Beyond cognitive tasks, RL is enabling **agentic behavior and complex interaction scenarios**. The ROLL framework, deployed within the Taotian Group, provides robust technical support for business innovation by enhancing LLM performance across multiple tasks and demonstrating strong robustness and generalizability in agent interaction scenarios [15]. For instance, Qwen2.5-7B-base and Qwen3-30B-A3B-base models saw their overall accuracy increase by 2.89x and 2.30x, respectively, without model collapse. In agent interaction tasks, ROLL significantly improved success rates in environments like Sokoban (validation success rate from 13.3% to 35.2%), FrozenLake (validation success rate from 12.9% to 23.8%, with effective action ratio increasing from 69.1% to 88.8%), and WebShop (success rate from 37% to over 85%), concurrently reducing the average number of interactive actions per round from 7 to 4 [15]. These improvements signify enhanced multi-round decision-making, spatial planning, and efficient task completion. Furthermore, the CARD framework utilizes LLM-driven dynamic feedback for reward design in robotics, consistently outperforming or matching baselines on tasks from Meta-World and ManiSkill2, even surpassing human-oracle performance in 3 out of 12 tasks [5]. Such capabilities pave the way for real-world applications in robotics, autonomous driving, energy management, and medical recommendation systems [14].

In specific **domain-specific deployments**, RLHF has been successfully implemented in the financial sector using the XuanYuan-6B model, resulting in improved performance for financial applications through optimized training and preference datasets [24].

**Lessons Learned and Challenges:**
The journey from research prototypes to production systems has illuminated several commonalities and challenges. The widespread adoption of RLHF by major AI labs for models like Llama 2 underscores its critical role in achieving high user acceptance and commercial viability, emphasizing the need for robust alignment strategies [9,28]. However, the high computational cost and the labor-intensive nature of human feedback in traditional RLHF pose significant challenges for scalability. The development of RLAIF and SER directly addresses these, offering more efficient and accessible methods for LLM alignment [27,31]. Performance tuning and accessibility have also been advanced, with methods demonstrating the feasibility of RLHF for 20B parameter LLMs on consumer-grade hardware (e.g., a single 24GB NVIDIA 4090 GPU), significantly lowering the barrier to entry for researchers and developers [2]. The provision of "Responsible Use Guides" and open-source models like Llama 2 and DeepSeek-R1 exemplify efforts to facilitate safe deployment and foster broader research and development in practical applications [1,22,28]. The successful deployment of frameworks like ROLL in internal business scenarios demonstrates effective system integration, showing how RL can enhance cross-domain multi-task performance and ensure robust, generalizable agent interactions [15].
## 8. Challenges and Limitations of RL in LLMs
The integration of Reinforcement Learning (RL) with Large Language Models (LLMs) has ushered in unprecedented capabilities but is simultaneously confronted by a formidable array of challenges and inherent limitations. 

![Interconnected Challenges of RL in LLMs](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/-A89Cy0jx0VT1k7IdRlmN_Interconnected%20Challenges%20of%20RL%20in%20LLMs.png)

These impediments span multiple dimensions, from the foundational aspects of data acquisition and reward specification to the intricate technicalities of computational efficiency, model stability, and ethical governance. Collectively, these challenges significantly impede the development of truly autonomous, reliable, and safe RL-driven LLMs, necessitating innovative solutions across a wide spectrum of research [3,9,33].

A primary hurdle lies in the **Data Collection and Reward Design Complexity**. The predominant reliance on human feedback, particularly within Reinforcement Learning from Human Feedback (RLHF), introduces substantial scalability and efficiency bottlenecks [9,28,33]. The process of obtaining high-quality human preference labels is inherently costly, time-consuming, and difficult to scale, leading to a critical shortage of data for training robust reward models [25,31]. Furthermore, human preferences are often subjective, inconsistent, and context-dependent, leading to data conflicts, strong subjectivity, and biases that are challenging to encapsulate consistently in reward functions [9,28,34]. The debate between "Process Rewards," which are difficult to annotate but ensure reasoning faithfulness, and "Outcome Rewards," which are easier to scale but risk "reward hacking," highlights the fundamental difficulty in designing effective and aligned reward signals [22,30]. Moreover, the prohibitive cost of expert annotation and the risk of reward hacking, where models exploit shortcuts to maximize rewards without genuine improvement, further complicate the landscape, driving the need for more objective and scalable reward mechanisms [6,22,24].

These data and reward challenges are deeply intertwined with significant **Computational Costs and Model Training Stability** issues. Algorithms like Proximal Policy Optimization (PPO), commonly used in RLHF, are known for their hyperparameter sensitivity and demand for multiple models (actor, critic, reward, reference), leading to "huge computational overhead" and poor sample efficiency [4,28,30]. The sheer scale of LLMs, exemplified by pre-training costs reaching millions of GPU hours and substantial energy consumption, renders the integration of RL exceptionally resource-intensive [10,22,28]. Training instability, often manifested as PPO's sensitivity or performance collapse in certain DPO variants, poses additional hurdles, making consistent and reliable model improvement difficult [12,13]. While approaches like Reinforcement Learning from AI Feedback (RLAIF) aim to mitigate data collection costs and scalability limitations, they introduce new concerns such as reward model staleness and performance plateaus, indicating that cost-effective scalability often comes with trade-offs [11,27,33].

Compounding these issues are critical concerns regarding **Robustness, Generalization, and Catastrophic Forgetting**. RL-fine-tuned LLMs often exhibit vulnerabilities in out-of-distribution (OOD) scenarios, with traditional RLHF methods lacking robustness when faced with shifts in task or behavior policy [13,16]. A major obstacle in continuous improvement is catastrophic forgetting, where fine-tuning for new tasks overwrites previously acquired knowledge, leading to a degradation of capabilities [3]. While "RL's Razor" suggests RL's tendency to preserve old capabilities better than Supervised Fine-tuning (SFT) by converging to minimal KL divergence solutions, effective mechanisms are still required to explicitly prevent knowledge loss during ongoing learning [3,29]. Furthermore, the phenomenon of "mode collapse" in algorithms like PPO can lead to a significant reduction in the diversity of generated outputs, causing models to converge to a limited set of responses rather than exploring the full spectrum of desirable behaviors [12]. Adversarial vulnerabilities and susceptibility to "jailbreak attacks" highlight a lack of robustness against manipulated inputs and underscore the need for models that generalize safely and reliably across diverse, and potentially malicious, inputs [9,28].

Finally, the application of RL to LLMs raises profound **Ethical Considerations, Safety Alignment, and Human Values**. The core objective of RLHF is to align LLM behavior to be helpful, harmless, and honest, mitigating biases and toxicity [9,35]. However, defining universal human preferences for alignment is exceptionally challenging due to the inherent subjectivity, cultural variations, and inconsistencies in human values [13,23]. The potential for bias transfer from AI evaluators, the risk of data poisoning from malicious annotators, and the ethical implications of models exhibiting overconfidence even when incorrect, all threaten the trustworthiness and safety of RL-driven LLMs [9,27]. Moreover, enhanced LLM reasoning capabilities, particularly in open-source models, raise concerns about misuse for generating misinformation, hateful content, or operationally feasible plans for malicious objectives [22,27]. Addressing these challenges requires robust safety alignment mechanisms, extensive red-teaming, and a multi-pronged approach that balances helpfulness with safety, recognizing the trade-offs involved, and ensures responsible deployment in a complex global society [28].

In summary, the journey towards truly autonomous, reliable, and safe RL-driven LLMs is paved with interconnected challenges. The limitations in scalable and consistent data collection, the complexity of reward design, the immense computational demands and training instabilities, the issues of robustness, generalization, and catastrophic forgetting, and the critical ethical and safety alignment concerns all demand innovative, holistic solutions. Overcoming these fundamental barriers is essential for realizing the full potential of RL in enhancing LLM capabilities responsibly and beneficially.
### 8.1 Data Collection and Reward Design Complexity
The reliance on human feedback as the primary source for Reinforcement Learning from Human Feedback (RLHF) introduces significant limitations, fundamentally impeding the scalability and efficiency of Large Language Model (LLM) alignment efforts. 

**Challenges in Data Collection & Reward Design**

| Challenge Type             | Specific Problems                                                                                               | Causes / Impact                                                                                                                                                                                                                                                                                                              | Mitigation / Alternative Approaches (with own challenges)                                                                                                                              |
| :------------------------- | :-------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Human Feedback Bottleneck** | High Cost, Time, & Scalability Limitations                                                                      | Collecting large volumes of high-quality human preference labels is expensive, time-consuming, and difficult to scale, leading to data shortage for robust RM training [9, 11, 20, 25, 28, 33, 35].                                                                                                                             | RLAIF (AI-generated feedback), Direct-RLAIF, Self-Generated Data (DeepSeek-R1, SCoRe), DPO (offline optimization) [18, 19, 22, 27, 33].                                              |
| **Subjectivity & Inconsistency** | Data Conflicts, Strong Subjectivity, Bias                                                                       | Human preferences are subjective, context-dependent, vary among annotators, leading to lower inter-rater reliability, conflicting feedback, and cognitive biases (fatigue, attention decay) [9, 13, 24, 28]. Risk of data poisoning from malicious annotators [9].                                                                 | Careful prompt engineering, Chain-of-Thought for AI labelers, multi-dimensional preference datasets, specialized bias mitigation efforts (OffsetBias), expert policies for high-stakes domains [27, 29]. |
| **Reward Function Design** | Difficulty in Defining Effective Rewards ("Process vs. Outcome")                                                | Process rewards ensure faithfulness but are hard to annotate. Outcome rewards are scalable but risk "reward hacking" (model fabricating illogical steps for score) [30]. Traditional reward engineering is effort-intensive, leads to suboptimal functions [5, 6, 9, 22].                                                          | Verifiable Reward Mechanisms (RLVR) for objective tasks, LLM as Reward Designer (CARD, Eureka), dynamic feedback, reward shaping [1, 5, 18, 29, 30, 32].                             |
| **Model Alignment Issues** | Reward Model Staleness & Distribution Shift                                                                     | RM accuracy degrades quickly as policy generates out-of-distribution responses from RM's training data; necessitates frequent, costly RM updates [27, 28].                                                                                                                                                                   | Direct-RLAIF (no separate RM), SRPO (robust to policy shifts) [16, 27].                                                                                                                |
|                            | Conflicting Objectives (e.g., Helpfulness vs. Safety)                                                           | Designing a single effective reward model for multiple, potentially conflicting goals is complex [28].                                                                                                                                                                                                                               | Multi-objective alignment frameworks, separate reward models for different objectives (e.g., Llama 2 helpfulness/safety RMs) [28, 29].                                               |

The process of collecting high-quality human preference labels presents a critical bottleneck due to its inherent cost, time consumption, and restricted scalability [9,11,20,25,28,33,35]. Training a high-quality reward model, a central component of RLHF, necessitates large volumes of human-annotated data, which is both expensive and time-consuming to acquire, thus limiting its scalability and introducing potential inconsistencies [31]. Methods like DeepSeek-R1 attempt to minimize this dependency by deriving reward signals solely from the correctness of final predictions through a "pure reinforcement learning" approach, thereby avoiding the complexities of human preference modeling [22]. Similarly, SCoRe mitigates data collection challenges by utilizing self-generated data, reducing reliance on costly human annotations [18]. Some integration methods, however, still require "large-scale supervised data," indicating persistent challenges in data collection for such paradigms [7]. Even approaches like RRHF, which simplify policy optimization, acknowledge that their performance is highly related to "sampling quality," reflecting an indirect form of data quality and collection complexity [4]. Direct preference optimization (DPO) aims to circumvent explicit reward model design and reduce data collection overhead by directly optimizing the policy using preference data [19].

Designing effective reward functions for complex and open-ended LLM tasks poses additional inherent difficulties, primarily because human preferences are often subjective, inconsistent, and context-dependent [9,14,28,34]. A core debate exists between "Process Rewards" and "Outcome Rewards" [30]. While process rewards—which incentivize correct thought processes—ensure reasoning faithfulness, their annotation is exceptionally difficult and costly. Conversely, outcome rewards, which merely reward the final correct result, are easier to scale but carry the risk of the model "fabricating" illogical intermediate steps to achieve the correct answer [30]. Traditional reward engineering demands extensive human knowledge and effort, often resulting in suboptimal reward functions and unintended behaviors [5]. In self-evolution frameworks, challenges persist in generating diverse and complex tasks, ensuring accurate self-evaluation and correction, and overcoming limitations in reward modeling generalization and accuracy, particularly in addressing issues like "overthinking and underthinking" [6]. Even with simpler reward mechanisms, such as SEARCH-R1's reliance on final result evaluation, capturing the quality of intermediate reasoning steps often remains inadequate [7]. When LLMs act as "Reward Designers," they can infer human preferences, but accurately capturing and robustly translating these preferences into reliable reward signals remains a significant hurdle, potentially introducing misinterpretations or biases from the LLM itself [14,34].

The quality of the final aligned LLM is fundamentally contingent upon the quality of its reward model, which in turn relies on expensive and difficult-to-scale human annotation [20,31]. The underlying causes of these problems are multifaceted.
Firstly, **data conflicts and inconsistencies** arise from various factors. Human annotations are often subjective, varying among annotators, especially for nuanced preferences, leading to lower inter-rater reliability [9,13,24,28]. Cognitive traps such as fatigue, attention decay, and common misconceptions further compromise feedback quality during prolonged annotation sessions [9]. The difficulty in conveying expected model behavior and establishing consistent judgment criteria among evaluators exacerbate these issues [13].
Secondly, **strong subjectivity** in human annotations contributes to potential data conflicts and inconsistent feedback [24]. Human goals, intents, and preferences are fluid and context-dependent, making them challenging to encapsulate consistently in numeric values or loss functions [9]. This includes potential biases in annotation content standards and the risk of "data poisoning," where annotators deliberately provide incorrect feedback [9,28].
Thirdly, the **prohibitive cost of expert annotation** is a recurrent issue, making large-scale data collection economically unfeasible [22,24,25]. This financial barrier is a major reason why alternatives like RLAIF have emerged, though they introduce their own set of challenges, including reward model staleness, position bias in LLM labelers, suboptimal prompting techniques, bias transfer, and qualitative deficiencies [27,33]. Reward model accuracy can also degrade quickly without fresh preference data reflecting new model output distributions, necessitating frequent, costly updates [28]. Moreover, sometimes helpfulness and safety can be **conflicting objectives**, complicating the design of a single effective reward model [28].
Finally, **reward hacking** is a significant concern, where models learn to exploit shortcuts or "overfit to the reasoner" to maximize rewards without genuinely improving their underlying problem-solving abilities [6,9,10,22]. This phenomenon is analogous to "Goodhart’s Law," where a measure ceases to be a good measure when it becomes a target [28].

In contrast, certain domains or reward structures offer a more objective ground truth. For instance, visual perception tasks often possess direct, quantifiable ground truth, which facilitates the design of rule-based reward functions [1]. This stands in stark contrast to the subjective and costly nature of preference-based reward modeling prevalent in language tasks. Similarly, DeepSeek-R1's focus on the correctness of final predictions in reasoning tasks, or LPSO's use of a binary "mathematical correctness" metric, represents a move towards more objective, verifiable rewards, thereby simplifying reward design and mitigating human subjectivity in specific contexts [22,32]. Even on-policy RL methods, such as GRPO, can naturally achieve implicit KL minimization with simple binary rewards (e.g., success/failure), suggesting that method mechanics can intrinsically bias solutions without complex reward function design for controlling policy drift [3]. This highlights a fundamental distinction between tasks where objective performance metrics are readily available and those heavily dependent on nuanced, often inconsistent, human preferences.
### 8.2 Computational Costs and Model Training Stability
The application of Reinforcement Learning (RL) to Large Language Models (LLMs) is hampered by several overarching challenges, primarily the continuous struggle with obtaining consistent and high-quality feedback, maintaining training stability, and ensuring the scalability of these complex models [24,33]. RLHF-trained models, despite their exceptional performance, frequently exhibit unpredictable errors, hallucinations, and biases, which implicitly points to difficulties in achieving robust training stability and reliable feedback mechanisms [9]. Furthermore, the high cost of data collection in RLHF is a notable concern [35].



**Computational & Stability Challenges in RL-LLM Training**

| Challenge Category          | Specific Problems                                                                                                   | Causes / Impact                                                                                                                                                                                                                                                                                                                                                     |
| **Memory Demands**        | Substantial GPU Memory Needs                                                            | LLMs with >10B parameters require significant GPU RAM (e.0.g., 40GB for float32 model). AdamW optimizer adds memory. PPO needs two model copies (actor & reference), further increasing demands (e.g., A100 GPUs) [2].                                                                                                                                                                                                                                                                                                                                                                  | 8-bit quantization, PEFT (LoRA) reduce memory footprint. MoE architectures decrease active parameters. FSDP shards model states [2, 16, 28, 29].                                                                                                                                                                                                                                                              |
| **Computational Overhead** | High Training Complexity & Resource Consumption (PPO)                                   | PPO's multi-model architecture & iterative online nature incur significant overhead. Poor sample efficiency means more interactions. Llama 2 pre-training: 3.3M GPU hours, 539 tons CO2e. DeepSeek-R1-Zero: $294,000 cost [6, 10, 12, 14, 22, 24, 28, 29, 30, 34].                                                                                                                                                                                                                                                                                                    | RLAIF (AI-generated feedback) reduces cost and speed. Direct-RLAIF bypasses RM training. RRHF needs fewer models. DPO simplifies pipeline. Critic-free algorithms (GRPO). Frameworks like ROLL for scalable training. PEFT/quantization. Optimized sampling (LPPO) [4, 11, 15, 19, 27, 30, 32, 33]. |
| **Training Instability**   | PPO Sensitivity & Instability                                                           | PPO notoriously sensitive to hyperparameters, prone to instability, especially during generation phase (e.g., 20x slowdown). Can lead to mode collapse, poor sample efficiency [4, 12, 19, 28].                                                                                                                                                                                                                                                                                                                                                                  | DPOP (modified DPO loss) for stability. APA (squared error loss) for stability & diversity. Direct-RLAIF (addresses RM staleness). RRHF (robust alignment without complex tuning). Optimized GRPO (DeepSeek-V2). Regularization constraints [4, 12, 13, 27, 29].                                      |
| **Feedback Quality**       | Cost of High-Quality Feedback                                                           | Human feedback is costly & complex. RLAIF has "performance plateau" (lacks novel input features), and "RM staleness" (reward signal degrades as policy evolves) [9, 27, 33].                                                                                                                                                                                                                                                                                                                                                                      | RLAIF (cost reduction), Direct-RLAIF (removes RM staleness), Endogenous Rewards (training-free), SER (self-annotation), RLVR (objective metrics) [1, 5, 20, 22, 31].                                                                                                                         |
| **Scalability for LRMs**   | Fundamental Difficulties in Scaling                                                     | Continuous performance improvements require more computational power & "thinking time." Fundamental difficulties arise in meeting demands for large reasoning models [23, 26, 28].                                                                                                                                                                                                                                                                                                                                                                          | Frameworks like ROLL designed for large-scale, distributed training. MoE architectures. Efficient fine-tuning techniques. Ongoing algorithmic innovations (e.g., LPPO) [15, 29, 32].                                                                                                               |

A significant portion of these technical complexities arises from the policy optimization step, particularly when using algorithms like Proximal Policy Optimization (PPO). PPO is known for its sensitivity to hyperparameters and the requirement for multiple models—typically an actor, a critic, a reward model, and a reference model—which complicates the training process and impedes scalability to larger parameter counts [4,19,28]. This multi-model architecture leads to "huge computational overhead," especially for critic-based algorithms like PPO, as they necessitate maintaining an auxiliary critic model to provide fine-grained feedback [30]. The "poor sample efficiency" of PPO also contributes to higher computational costs, demanding more interactions or data to reach desired performance levels [12]. Consequently, implementing and scaling PPO for LLM alignment results in "high training complexity and resource consumption" [6,24].

Beyond algorithmic intricacies, the field grapples with substantial "computational costs" and "scalability limitations" [10,11,27]. Training RL-driven LLMs incurs substantial expenses, exemplified by Llama 2's pre-training alone, which consumed 3.3 million GPU hours and generated 539 tons of CO2 equivalent emissions [28]. Even optimized efforts, such as DeepSeek-R1-Zero's training cost of $294,000, while significantly lower than typical multi-million dollar expenditures, still highlight the inherent financial burden [22]. Such demanding requirements underscore the challenge of integrating RL with LLMs, leading to "significant computational requirements" for these enhanced systems [14,34].

The underlying causes for these challenges are multifaceted. Scaling RL applications to Large Reasoning Models (LRMs) introduces fundamental difficulties related to computational resources and the ability of algorithms to meet these demands [23]. Continuous performance improvements often necessitate increased computational power during RL training and allocating more "thinking time" during inference, driving an incessant demand for computing resources [28]. PPO instability is a pervasive issue [12,19], with the generation phase within PPO leading to considerable slowdowns (e.g., approximately 20 times) [28]. Moreover, large LLMs require substantial GPU memory, with AdamW optimizers adding significant memory footprint, and PPO demanding two model copies, making large-scale training "challenging" and often necessitating "several high-end 80GB A100 GPUs" for initial supervised fine-tuning [2]. The implicit costs of complex feedback loops, particularly the increased complexity in human feedback collection, further slow down the process and escalate overall expenses [9]. Furthermore, the method of Self-Correcting RL (SCoRe) requires extensive computational resources and time, demanding a large number of samples to learn effective strategies, which can pose challenges when processing large-scale datasets [18].

To address these hurdles, researchers are exploring various strategies. Reinforcement Learning from AI Feedback (RLAIF) presents a "potential solution to the scalability limitations of RLHF" by reducing reliance on extensive human data collection [11,25]. RLAIF can achieve a "good level" of performance more quickly, potentially lowering initial computational costs [33], and AI labeling can be over 10 times cheaper than human annotation [27]. Direct-RLAIF (d-RLAIF) further reduces computational burdens by circumventing reward model training [11]. However, RLAIF exhibits a "performance plateau," where improvements diminish with increased AI-generated data, suggesting scalability limitations in continuous performance gains due to a lack of novel "input features" provided by human feedback [33]. The "RM staleness" issue in RLAIF, where the reward signal becomes less reliable as the policy evolves, also impacts stability [27].

Alternative algorithms also aim to mitigate PPO's limitations. RRHF significantly reduces computational and stability challenges by requiring only 1 to 2 models and offering robust alignment without complex hyperparameter tuning [4]. Similarly, Direct Preference Optimization (DPO) simplifies the training pipeline, requiring only two models and avoiding costly data sampling, leading to a more stable and efficient optimization process [19]. Variants like DPOP further address the instability of naive DPO, which can lead to "performance collapse" and issues like incoherence, by ensuring the stability of the chosen response's log-probability ratio [13]. Approaches like "endogenous reward" eliminate the need for training a separate reward model, significantly reducing computational burden [20]. The Self-Evolving Reward (SER) method, while using a modified PPO, implicitly contributes to more effective RL training by improving reward signal quality, though it does not directly address PPO's computational or stability aspects [31]. Furthermore, frameworks like ROLL are explicitly designed for "efficient, scalable" training, capable of handling models with over 600 billion parameters on thousands of GPUs, thereby "greatly reducing training costs and interruption risks" and achieving stable accuracy improvements without "model collapse" [15]. Techniques such as 8-bit quantization and Parameter-Efficient Fine-Tuning (PEFT) like LoRA reduce memory requirements, but often introduce a trade-off with overall training speed [2]. Overall, the persistent challenges of computational cost and model training stability underscore the need for continuous algorithmic innovation and resource management strategies in the domain of RL for LLMs.
### 8.3 Robustness, Generalization, and Catastrophic Forgetting
The robustness and generalization capabilities of Large Language Models (LLMs) fine-tuned with Reinforcement Learning (RL) are critical for their deployment in diverse, real-world scenarios, while catastrophic forgetting poses a significant challenge to their continuous improvement. 

**Robustness, Generalization & Forgetting Challenges**

| Challenge Type             | Specific Problems & Symptoms                                                              | Impact                                                                                                                                                                                                                                                                                                                               | Mitigation / Solutions                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| :------------------------- | :---------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Robustness & Generalization** | Vulnerability to Out-of-Distribution (OOD) Scenarios                                    | Traditional RLHF (PPO, DPO) often lacks robustness to OOD tasks or shifts in behavior policy. DPO has "exploration limitations" on OOD prompts. Can degrade utility of preference data & undo prior SFT/pre-training [13, 16].                                                                                                         | **SRPO**: Mathematically robust, independent of behavior policy, superior OOD generalization (e.g., +15% WR vs DPO on XSum). **ROLL**: "Excellent robustness & practicality" with stable cross-task transfer. **Direct-RLAIF**: Addresses RM staleness for robust signal. **LPPO**: Improves generalization & robustness. **RL/RLVR**: Stronger generalization than SFT [15, 16, 27, 30, 32]. |
|                            | Adversarial Vulnerabilities & Jailbreak Attacks                                           | RLHF models exhibit unexpected errors, hallucinations, biases, vulnerable to manipulated inputs and jailbreaks [9]. Lacks robustness against targeted adversarial prompts.                                                                                                                                                           | Extensive red-teaming (e.g., Llama 2 with 350+ experts). Use of metrics for adversarial prompts. Risk control systems for LLMs like DeepSeek-R1. Ongoing safety alignment mechanisms [22, 28].                                                                                                                                                                                                                                                                 |
|                            | Applying Language-centric RL to Visual Tasks                                              | Direct application of language-centric RL to MLLMs often doesn't yield expected benefits, leading to robustness issues in multimodal contexts [1, 17].                                                                                                                                                                              | Tailored enhancement strategies like Perception-R1 using rule-based RL and GRPO, specifically designed for fine-grained visual perception tasks.                                                                                                                                                                                                                                               |
| **Catastrophic Forgetting** | New Task Learning Overwrites Old Knowledge                                                | Fine-tuning for new tasks can overwrite previously acquired knowledge, leading to degradation of capabilities [3]. Observed in iterative fine-tuning processes.                                                                                                                                                                      | **"RL's Razor"**: RL tends to converge to KL-minimal solutions, preserving old capabilities better than SFT. **SRPO**: Reduces dependence on behavior policy. **ChatGLM-RLHF**: Regularization constraints. **LPPO**: Prevents neglect of challenging samples. **Llama 2**: Incorporates "best samples" from previous iterations [3, 16, 28, 29, 32]. |
| **Mode Collapse**          | Reduced Diversity of Generated Outputs                                                    | Models converge to a limited set of responses rather than exploring the full spectrum of desirable behaviors, impacting creativity and flexibility [12].                                                                                                                                                                               | **APA**: Ensures performance improvement "without collapsing to deterministic output," maintains diversity. **RPO**: Prevents "unwanted generation styles." **Llama 2**: Controlled management of output diversity. **LPPO**: Preserves rich solution diversity [12, 28, 29, 32].                                                                                                              |

Traditional RLHF methods, such as Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO), exhibit identified vulnerabilities in out-of-distribution (OOD) scenarios. For instance, existing RLHF methods often display optimality that is strongly dependent on the task, lacking robustness when faced with OOD tasks or shifts in the behavior policy [16]. DPO, in particular, raises concerns regarding "exploration limitations," suggesting that DPO-trained LLMs might fail on OOD prompts due to their offline nature and implicit reward definitions [13].

In contrast, frameworks like Self-Referential Policy Optimization (SRPO) are explicitly designed to overcome these limitations. SRPO is introduced as an offline RLHF framework that offers mathematically proven and empirically validated robustness, achieving a solution "completely independent of the behavior policy $$\mu$$" and "completely robust to task variations" [16]. Empirical results on the XSum dataset demonstrated that SRPO 5-rev achieved a 15% higher win rate compared to DPO, showcasing its superior robustness and generalization in unseen task distributions [16]. Other methods also enhance robustness and generalization; for example, the ROLL framework demonstrates "excellent robustness and practicality" with stable performance and good cross-task transferability [15]. Direct-RLAIF (d-RLAIF) addresses reward model staleness by continuously querying an off-the-shelf LLM for rewards, thereby maintaining a robust and up-to-date reward signal that better adapts to the policy's evolving distribution [27]. LPPO, through its Learning-Progress Weighting (LP-Weighting) component, improves generalization on both in-domain and OOD problems and increases robustness by reducing performance regressions [32]. Similarly, the Self-Evolved Reward Learning (SER) method improves reward model robustness and generalization by iteratively generating and filtering its own training data, making it more adaptable to new data and robust against initial biases [31]. DeepSeek-R1's multi-stage training also aims to maintain broad behavior and alignment across diverse reasoning tasks, suggesting an effort towards robust generalization [22]. RL itself has been observed to possess stronger generalization capabilities compared to Supervised Fine-tuning (SFT), with research indicating that "RL generalizes, SFT memorizes" by being less prone to overfitting and more effective on new or OOD tasks [30]. This is further supported by the TÜLU-V2-mix dataset, designed to improve LLM generalization and execution across multiple domains [29]. However, generalization remains a significant challenge within self-evolution frameworks, with concerns about overfitting to specific tasks and reward hacking behaviors that exploit reward shortcuts, impacting model robustness and generalizability [6]. Challenges also arise in applying RL methods from the language domain directly to visual tasks, leading to robustness issues in multimodal LLMs [1,17].

Catastrophic forgetting emerges as a major obstacle in continuously improving LLMs. When fine-tuning LLMs, previous knowledge and capabilities can be overwritten as the model adapts to new tasks. A key insight, termed "RL's Razor," posits that when multiple policies can solve a new task, RL tends to converge to solutions with the minimal Kullback-Leibler (KL) divergence from the original base model [3]. This mechanism allows RL to preserve old capabilities more effectively compared to SFT, which is more prone to knowledge loss [3]. The empirical law of forgetting further links the degree of forgetting to the forward KL divergence ($$D_{KL}(P_{\text{new task}} || P_{\text{base}})$$ ) estimated on new task samples, proposing it as a powerful predictive metric where smaller KL divergence correlates with less forgetting [3]. SRPO implicitly addresses catastrophic forgetting by reducing the dependence of the solution on the behavior policy, thereby preventing degradation that could "undo SFT/pre-training" [16]. Similarly, ChatGLM-RLHF incorporates "regularization constraints to prevent catastrophic forgetting" during large-scale RL training [29]. LPPO's LP-Weighting component explicitly prevents the "complete neglect of persistently challenging samples" and mitigates potential catastrophic forgetting by retaining previously acquired knowledge [32]. In Llama 2, issues akin to catastrophic forgetting observed in earlier rejection sampling fine-tuning (e.g., forgetting how to rhyme) were mitigated by incorporating "best samples" from all previous iterations, a method common in continuous learning within RL literature [28]. However, DPO's "performance collapse," leading to incoherent and repetitive outputs, can also indicate a form of catastrophic forgetting or mode collapse [13].

"Mode collapse" in PPO is a significant limitation impacting the diversity of generated outputs. This phenomenon causes the model to converge to a limited set of responses rather than exploring the full distribution of desired behaviors [12]. While the provided digests do not explicitly detail mode collapse as a PPO limitation, the problem of diversity degradation is implicitly linked to it. Advantage-Induced Policy Alignment (APA) directly addresses this by ensuring performance improvement "without collapsing to deterministic output," thereby maintaining diversity and preventing overly narrow generation capabilities [12]. The initial DPO loss, especially when alignment goals are too specific, might also limit diversity if not carefully managed, potentially leading to repetitive or incoherent outputs [13]. RPO aims to prevent models from adopting "unwanted generation styles," indicating a concern for controlling output distribution and avoiding undesirable modes [29]. Llama 2's RLHF pipeline demonstrates a nuanced approach to diversity, where creative prompts allow for greater diversity, while factual prompts are steered towards consistent responses, suggesting a controlled management of output diversity rather than a complete collapse [28]. LPPO's PG-Sampling and LP-Weighting components are also shown to preserve rich solution diversity, with accuracy gains accompanied by "sustained improvements in pass@3" [32].

Beyond mode collapse, RLHF-trained models are susceptible to adversarial vulnerabilities and general diversity degradation. These models can exhibit unexpected errors, hallucinations, and biases [9]. They are vulnerable to adversarial attacks, including "unusual jailbreaks" that bypass safety safeguards [9]. This indicates a lack of robustness and generalization against manipulated inputs. Extensive red-teaming and the use of metrics like $$\gamma$$ during the Llama 2 development process were crucial for identifying and mitigating such adversarial prompts, including techniques like positive framing and creative writing requests designed to circumvent safety filters [28]. While PPO generally maintains stability across different LLM architectures, some RL algorithms like GRPO have shown instability leading to "reward collapse" in certain models, highlighting varying degrees of robustness across different RL approaches [7]. RRHF, in contrast, is presented as a robust method that achieves human preference alignment "without complex hyperparameter tuning," simplifying the process and implying enhanced stability [4]. The continuous self-correction mechanism within the Self-Evolved Reward Learning (SER) framework implicitly offers robustness against biases and limitations of static datasets, aiding in more adaptable reward models [31]. Furthermore, the theoretical performance gains achieved by RL with endogenous rewards (EndoRM) over imitation learning suggest enhanced learning stability and generalization, preventing degradation over longer sequences and demonstrating "scalable robustness" across diverse preferences [20]. Addressing these issues is paramount for developing reliable and broadly applicable LLMs.
### 8.4 Ethical Considerations, Safety Alignment, and Human Values
The application of Reinforcement Learning (RL) to Large Language Models (LLMs), particularly through Reinforcement Learning from Human Feedback (RLHF), profoundly impacts their ethical behavior and alignment with human intentions [13,24]. 

**Ethical Considerations & Safety Alignment**

| Aspect / Challenge           | Specific Ethical/Safety Issues                                                              | Impact / Concerns                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Mitigation / Strategies                                                                                                                                                                                                                                                                                                                                                               |
| :--------------------------- | :------------------------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Defining Human Values**    | Subjectivity, Inconsistency & Cultural Variation                                            | Defining universal human preferences for alignment is challenging due to inherent subjectivity, cultural differences, and inconsistencies in human values. Leads to difficulties in establishing consistent evaluation criteria and balancing human/AI evaluators [13, 23, 34].                                                                                                                                                                                                 | Context-aware alignment, multi-objective frameworks to balance competing values, cross-cultural and multilingual alignment strategies, open science initiatives for broader input [28, 29].                                                                                                                                                                                          |
| **Bias & Fairness**          | Bias Transfer from AI Evaluators, Data Poisoning, Inherited Prejudices                        | AI-generated feedback can amplify biases from training data or judging models. Malicious annotators can poison data. Leads to unfair or harmful outputs in sensitive domains (e.g., medicine, law) [9, 20, 27].                                                                                                                                                                                                                                                                    | Deriving rewards from intrinsic LLM objectives (less biased). Systematic bias mitigation (OffsetBias). Human experts for high-stakes applications. Rigorous evaluation criteria. Red-teaming [20, 27, 28, 29].                                                                                                                                                                        |
| **Misuse & Harmful Content** | Generating Misinformation, Hateful Content, Malicious Plans                                 | Enhanced LLM reasoning capabilities can be misused for generating convincing misinformation, hateful content, or operationally feasible plans for malicious objectives. Open-source models are vulnerable to fine-tuning that bypasses safety [22, 27].                                                                                                                                                                                                                               | Integrated "risk control systems" (DeepSeek-R1). Extensive red-teaming (Llama 2). Rule-Based Reward Models (RBRM) for harmful content. Safe RLHF variants. Robust safety alignment mechanisms [22, 28, 29, 35].                                                                                                                                                                   |
| **Trust & Reliability**      | Overconfidence when Incorrect, Lack of Transparency                                         | RLHF models can exhibit overconfidence even when incorrect, raising ethical questions about trustworthiness and human susceptibility to misleading information [9]. Lack of transparency in decision-making process.                                                                                                                                                                                                                                                              | Ensuring LLM output certainty correlates with correctness. Explainability in LLM reasoning. Robust safety alignment mechanisms, responsible deployment strategies. Iterative evaluation with human safety assessments [9, 28, 34].                                                                                                                                       |
| **Safety-Hapfulness Trade-off** | "Over-cautiousness" / "False Refusals"                                                      | Fine-tuning for safety can sometimes lead to models being overly cautious, resulting in "false refusals" (e.g., refusing benign requests) or reduced helpfulness, impacting user experience [28].                                                                                                                                                                                                                                                                                   | Research into optimal balance points; nuanced reward models; contextual safety filters; iterative refinement based on user feedback to minimize false refusals while maintaining security [28].                                                                                                                                                                                |
| **Societal Impact**          | Ethical & Legal Issues from LLM-Enhanced RL Systems, Pursuit of ASI                         | Application of LLM-enhanced RL raises broad ethical and legal concerns. The pursuit of Artificial Superintelligence (ASI) through enhanced RL scalability underscores profound ethical considerations [14, 23].                                                                                                                                                                                                                                                                          | Collaborative efforts from AI community, transparency, open-source initiatives, policymaker engagement, responsible deployment strategies, continuous auditing and governance [15, 28].                                                                                                                                                                                            |

The overarching goal is to shape LLM behavior to be helpful, harmless, and honest, thereby enforcing desirable traits and mitigating undesirable ones such as bias and toxicity [10,13,23,35]. This alignment is a primary strategy for making LLMs safe and ethically compliant in their interactions [4,9].

RLHF significantly contributes to enhancing the safety and ethical alignment of LLMs through various mechanisms. Models like GPT-4 leverage Rule-Based Reward Models (RBRM) to encourage effective rejection of harmful content, while Kimi-k1.5 designs high-quality RL prompt sets to guide models towards robust reasoning and risk mitigation [29]. DeepSeek-R1's final RL stage explicitly aims to align the model with human preferences, usefulness, and safety [29]. A critical component of this process is the integration of safety-specific data and annotations, as evidenced by preference datasets like HelpSteer2 which include "safety" as a multi-dimensional annotation, and efforts like OffsetBias which address safety biases in reward models [29]. Llama 2, for instance, employs a dedicated process for collecting safety-specific data, including adversarial prompts and safe demonstrations for supervised safety fine-tuning and RLHF, defining risk categories such as illegal activities and hate speech [28]. Furthermore, variants like "Safe RLHF" are actively being developed to specifically address these issues [35].

Despite its effectiveness, defining universal human preferences for RL-driven alignment presents substantial challenges. The evaluation of RLHF models is complex, requiring consistent criteria for desired model behavior and addressing biases arising from evaluator subjectivity [13]. The use of AI evaluators, while scalable, is critically examined for its sensitivity to prompt variations and potential inability to accurately mimic true human preferences, highlighting the difficulty in establishing universal values and unbiased evaluations [13]. Moreover, the subjectivity of human feedback, and the potential for malicious annotators to provide incorrect feedback, pose significant safety concerns, potentially leading to data poisoning [9]. This can inadvertently lead to unintended consequences or "reward hacking," where models optimize for the reward signal in ways that do not align with true human intent. LLMs trained with RLHF can also exhibit overconfidence even when incorrect, which raises ethical questions about trustworthiness and human susceptibility to misleading information [9]. The challenge of aligning LLMs with complex human values is compounded by the tendency of RLAIF methods to inherit style biases and inherent prejudices from the judging models themselves [20].

Broader ethical implications and safety risks extend beyond internal alignment. The application of LLM-enhanced RL systems introduces various ethical and legal issues that demand careful consideration [14]. One significant risk is bias transfer, where AI-generated feedback can amplify existing biases from off-the-shelf LLMs used for labeling, potentially causing harm in sensitive domains like medicine or law [27]. For such high-stakes applications, human experts assigning preferences according to strict policies are often considered the gold standard [27]. The reduced barriers to aligning LLMs also raise concerns about their misuse for malicious purposes, such as generating convincing misinformation or hateful content [27]. Models with enhanced reasoning capabilities, like DeepSeek-R1, carry potential ethical risks including susceptibility to "jailbreak attacks" for generating dangerous content and creating operationally feasible plans for malicious objectives [22]. Open-source models, in particular, are vulnerable to fine-tuning that could bypass inherent safety measures [22].

To mitigate these risks, a multi-pronged approach to safety and ethical deployment is essential. For instance, DeepSeek-R1 demonstrates that combining its intrinsic safety with "risk control systems" can elevate its safety to an excellent standard [22]. Llama 2 utilizes extensive red-teaming with over 350 experts from diverse backgrounds to proactively identify and mitigate risks and biases through an iterative process [28]. Iterative evaluation loops with human safety evaluations using a Likert scale further refine models [28]. While pre-training data often contains biases, fine-tuning aims to mitigate these downstream, though a trade-off between helpfulness and safety, sometimes leading to "over-cautiousness" or "false refusals," is acknowledged [28]. Responsible deployment strategies, emphasizing community collaboration and transparency, are crucial for addressing risks and potential misuse of powerful LLMs [28]. Interestingly, Reinforcement Learning from AI Feedback (RLAIF) has shown promise in safety alignment, outperforming RLHF in harmlessness in some contexts (88% vs. 76% harmless rate) [27].

The societal impacts of LLM-enhanced RL extend to fundamental challenges in defining universal human values and cross-cultural considerations [13,23,34]. Ensuring LLM-enhanced RL systems are obedient, harmless, and align with diverse societal values remains a significant challenge [34]. The pursuit of Artificial Superintelligence (ASI) through enhanced RL scalability further underscores the profound ethical considerations and the imperative for responsible development [23]. Future research should focus on developing more robust and transparent safety alignment mechanisms that can effectively navigate the complexities of human preferences, cultural diversity, and the potential for malicious use. This includes exploring novel approaches or supplementary safety measures beyond current RLHF paradigms to ensure responsible and beneficial deployment of advanced LLMs [9].
## 9. Future Directions and Open Research Questions

![Key Future Research Directions for RL in LLMs](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/IKlO7ldLnJL8qnuf4EnNe_Key%20Future%20Research%20Directions%20for%20RL%20in%20LLMs.png)

This chapter synthesizes the most promising and critical avenues for future research in Reinforcement Learning (RL) for Large Language Models (LLMs), directly building upon the challenges identified previously [9,10,14,24]. The discussion is organized to present innovative, actionable, and cross-disciplinary solutions that aim to overcome current limitations and propel the field towards more autonomous, robust, and ethically aligned LLM agents. This overview emphasizes a long-term vision for RL-driven LLMs, highlighting their potential for transformative impact across various domains and the imperative for collaborative research efforts [14,23].

A primary direction lies in the development of **more autonomous and adaptive LLM agents**, capable of continuous learning and self-improvement with minimal explicit human intervention [5,30]. This trajectory necessitates advancing hybrid feedback mechanisms, combining human and AI strengths (e.g., RLAIF as a scalable alternative to RLHF), and developing endogenous reward systems to reduce reliance on external supervision [20,27]. Crucially, research must focus on creating more robust and sample-efficient algorithms that mitigate catastrophic forgetting, thereby enabling "long-lived agents" to continuously acquire new skills [3,24]. The pursuit of self-evolution and continual learning, exemplified by self-rewarding systems and frameworks like Self-Evolved Reward Learning (SER), will empower LLMs to autonomously evaluate and refine their outputs [22,29,31]. Furthermore, enhancing dynamic adaptation, intelligent tool use, and the development of model-based RL for sophisticated planning are vital steps towards truly autonomous agents, especially in multimodal contexts for complex tasks [7,17,28,30].

Simultaneously, **algorithmic and architectural innovations** are crucial to address current limitations and unlock new capabilities in RL for LLMs [26,34]. Key areas include verifier-guided training, where external or internal mechanisms evaluate and refine model outputs, and the design of multi-objective alignment frameworks to balance competing objectives like helpfulness and harmlessness [5,9,22]. The exploration of hybrid RL algorithms, which integrate diverse paradigms, alongside specialized approaches like Continual Reinforcement Learning (CRL), Model-based Reinforcement Learning (MBRL), and Reinforcement Learning for Pre-training (RLfP), will foster intrinsically capable and adaptable LLMs [10,30]. Theoretical frameworks, such as "RL's Razor," are essential for understanding and controlling forgetting in continuous learning scenarios [3]. Additionally, developing advanced frameworks for efficiency, interpretability, and robustness – encompassing adaptive compute allocation, context-aware and multi-modal reward models, theoretical justifications for simplified alignment methods like RRHF, and automated data generation for policy learning – will drive the field forward [4,17,22,31,32].

Finally, the development and deployment of RL for LLMs demand a paramount focus on **ethical AI and responsible deployment** to navigate inherent complexities and societal impacts [9]. This entails continuous research into enhancing safety and alignment, moving beyond potentially biased external feedback towards intrinsic reward signals and systematic bias mitigation strategies like OffsetBias [20,29]. A robust, multi-pronged safety approach is indispensable, emphasizing transparency, explainability, ongoing red-teaming, and the development of "Safe RLHF" variants and risk control systems to prevent vulnerabilities such as "jailbreak attacks" [22,28]. Achieving an optimal balance between safety and helpfulness, reducing "false refusals" while ensuring security, remains a critical area [28]. The proposed "Hybrid Alignment for Safety and Reasoning in LRMs" aims to integrate verifiable rewards with safety alignment, ensuring that capability enhancement is balanced with stringent requirements for safety and scalability [10,14]. Ultimately, the responsible development of LLMs necessitates a concerted, collaborative effort from the broader AI community, fostering transparency, open-source initiatives, and collective problem-solving among all stakeholders to ensure that increasingly powerful agents remain aligned with human values and safety [15,28]. These interconnected research frontiers collectively chart a course towards truly transformative, autonomous, and ethically sound RL-driven LLMs.
### 9.1 Towards More Autonomous and Adaptive LLM Agents
The trajectory of Reinforcement Learning (RL) for Large Language Models (LLMs) is converging towards the development of increasingly autonomous and adaptive agents, capable of continually learning and self-improving in dynamic environments with minimal explicit human intervention [5,30]. This ambitious goal necessitates a multi-faceted research agenda, spanning advancements in feedback mechanisms, algorithmic robustness, self-evolution capabilities, and the expansion of RL's application across novel and complex tasks [24,34].

A primary avenue for future research lies in the exploration of **hybrid feedback mechanisms** that judiciously combine human and AI strengths, thereby optimizing feedback resources [27]. Concepts such as using AI-generated feedback (RLAIF) as an initial "warm-up" policy before refinement with human feedback (RLHF), or leveraging AI to collect significantly more feedback due to its lower cost, are promising directions [27]. Indeed, the efficacy of AI-generated feedback in replacing or augmenting human feedback for RL fine-tuning has been demonstrated, indicating a path toward more self-sufficient alignment processes [11,25]. Further optimizing human feedback resources by training AI tools for automated feedback generation is critical, potentially involving sophisticated mechanisms for dynamically generating and selecting optimal responses from diverse sources [4,14]. This includes developing a "dynamic, promptable judge" through an endogenous reward mechanism within LLMs, which reduces reliance on explicit human supervision for reward design and fosters intrinsic self-evaluation [20]. Addressing the inherent subjectivity and potential for harmful human goals in feedback also requires better mechanisms for autonomous oversight or adaptive learning from diverse intentions, alongside methods for discerning and conveying uncertainty, which are crucial for creating robust and reliable AI systems [9]. The continuous refinement of RLHF variants, such as RLAIF, Safe RLHF, and Pairwise DPO, also contributes to this goal [35].

Developing **more robust and sample-efficient algorithms** is essential for enhancing LLM autonomy [24]. Future work needs to improve the generalizability of these methods to broader reasoning domains and diverse tasks, moving beyond specialized applications like mathematical reasoning [32]. Reducing the dependency on expert solutions for challenging problems by enabling autonomous generation of effective hints or partial solutions is another key area [32]. Furthermore, mitigating catastrophic forgetting is fundamental for creating "long-lived agents" that can continuously learn new skills while preserving previously acquired knowledge, facilitating sustained and effective learning over extended periods [3,30].

The trajectory towards autonomy is strongly driven by advancements in **self-evolution and continual learning**. Self-rewarding systems, such as Self-Refined LM, Self-Rewarding Language Models (SRLM), and Generative Judge (Con-J), empower LLMs to autonomously evaluate their outputs, generate feedback, and iteratively improve, significantly reducing reliance on external supervision [29]. DeepSeek-R1's pioneering use of pure reinforcement learning with an emphasis on "self-evolution" highlights the potential for LLMs to enhance complex cognitive functions with minimal external guidance, including advanced reasoning behaviors like self-verification and reflection [22,29]. Methods like SCoRe, which enable LLMs to self-correct using entirely self-generated data, represent substantial steps towards autonomy, though their applicability to more complex tasks such as natural language understanding requires further exploration [18]. The Self-Evolved Reward Learning (SER) framework aims to refine how reward models judge their own "learning state" and dynamically adjust data filtering strategies, optimizing the self-evolution process and fostering more diverse response generation [31]. Similarly, SRPO’s self-improvement mechanism, where models recursively refine outputs based on learned preferences, points towards self-sufficient and intelligent systems [16].

Another critical direction is the development of agents capable of **dynamic adaptation and intelligent tool use**. This involves fostering the ability for LLMs to dynamically adjust their retrieval strategies based on uncertainty in their current state or knowledge, enabling them to intelligently decide when and how to query external tools [7]. The observation of emergent tool use capabilities, such as calculator integration, even from zero-shot alignment, underscores the potential for training agents that can effectively interact with and leverage external tools [22,28]. LLMs' capacity to generate and verify executable code, as seen in frameworks like CARD, further suggests their potential for active interaction with environments beyond mere text generation [5]. Furthermore, LLMs should be able to continuously adapt to dynamic environmental changes through sustained learning, similar to how Llama 2-Chat can dynamically rescale temperature based on prompt type, indicating a capacity for fine-grained, context-dependent behavioral adaptation [14,28].

The development of **Model-based Reinforcement Learning** for LLMs is crucial, enabling agents to construct an internal model of the "world" or "environment" to facilitate deeper planning and more sophisticated autonomous behavior [30]. This involves exploring how AI feedback mechanisms, such as RLAIF, can be adapted to model-based RL settings where both human and assistant are modeled by LLMs, leading to more sophisticated generative interaction models [27]. Additionally, fostering robust internal models of time and knowledge organization will reduce reliance on explicit prompting or fine-tuning for temporal reasoning [28].

Finally, future research should focus on **Hierarchical and Multi-modal LLM-RL Agents for Complex Tasks**. Extending the self-evolution paradigm to embodied intelligent agents requires addressing challenges in multimodal data understanding, redefining Chain-of-Thought (CoT) formats for multimodal reasoning, and reducing the cost of environmental interaction [6]. The progress in autonomous and adaptive Multimodal LLMs (MLLMs) for advanced visual understanding, exemplified by Perception-R1, lays a critical foundation for intelligent perception AI systems [8,17]. This involves optimizing their "perception policy" through RL, reducing reliance on explicit human supervision for fine-grained visual tasks and striving for true visual "epiphany" [1,17]. The emphasis on enhancing agent capabilities in multimodal contexts, such as supporting "Qwen2.5 VL Agentic RL," further underscores this direction [15].

Ultimately, realizing the vision of highly autonomous and adaptive LLM agents requires sustained efforts in exploring scalable strategies to extend RL towards Artificial General Intelligence (AGI) and even Artificial SuperIntelligence (ASI) [23,26]. Fostering community contributions through open-source initiatives is crucial for advancing LLM capabilities and safety, ensuring collaborative progress towards more beneficial and capable AI systems [15,28]. These collective endeavors promise to unlock the full potential of LLMs as intelligent, self-evolving entities.
### 9.2 Algorithmic and Architectural Innovations
The continuous advancement of Reinforcement Learning (RL) for Large Language Models (LLMs) necessitates significant algorithmic and architectural innovations to address current limitations and unlock new capabilities [14,26,34]. Key emerging directions include verifier-guided training and multi-objective alignment frameworks, alongside the potential of hybrid RL algorithms [10].

**Verifier-Guided Training and Multi-Objective Alignment**
Verifier-guided training represents a critical innovation for enhancing LLM reasoning by leveraging external or internal mechanisms to evaluate and refine model outputs. DeepSeek-R1 champions this approach through pure reinforcement learning, positing that robust verifiers, combined with advanced RL techniques, could enable machines to surpass human capabilities in evaluable tasks [22]. A central challenge lies in improving "marking efficiency" and dynamic compute allocation to prevent "overthinking" on simpler problems [22]. Furthermore, the Coder-Evaluator Framework introduced by CARD exemplifies automated verifier-guided training, wherein an LLM-based Coder generates and verifies reward code, while an Evaluator provides dynamic feedback, notably through Trajectory Preference Evaluation (TPE) without requiring full RL training at each iteration [5]. This framework employs distinct types of dynamic feedback (preference, process, and trajectory) to iteratively refine the LLM, suggesting a move towards more sophisticated and automated verification mechanisms [5].

Closely related are multi-objective alignment frameworks, which are essential for navigating the complex trade-offs inherent in LLM development, such as balancing helpfulness, harmlessness, and honesty. While explicit examples are nascent, the inherent challenges of existing RLHF paradigms, including scalability limitations and the need for fundamental architectural innovations, underscore the necessity for such frameworks [9,10,34]. These frameworks aim to optimize multiple, potentially conflicting, objectives simultaneously.

**Hybrid RL Algorithms and Specialized RL Paradigms**
The development of hybrid RL algorithms holds significant promise for combining the strengths of different RL paradigms [10]. This involves exploring new algorithms tailored for language-based environments and designing novel architectures that better facilitate the interaction between LLM components and traditional RL elements [34]. For instance, future work could explore hybrid feedback mechanisms that integrate various feedback types for improved performance [33].

Beyond hybrid approaches, specialized RL paradigms offer distinct advantages:
*   **Continual Reinforcement Learning (CRL)**: CRL is crucial for enabling LLMs to acquire new knowledge and tasks without suffering from catastrophic forgetting of previously learned capabilities [30]. Research into mitigating catastrophic forgetting, as observed during iterative fine-tuning processes, will benefit from integrating principles from continual learning [28].
*   **Model-based Reinforcement Learning (MBRL)**: MBRL empowers LLMs to construct internal world models, facilitating advanced planning, reasoning, and hypothetical scenario generation [30]. While not explicitly detailed in digests, the need for improved reasoning and planning capabilities implies a role for MBRL.
*   **Reinforcement Learning for Pre-training (RLfP)**: Integrating RL principles into the foundational pre-training phase of LLMs, rather than solely during fine-tuning, could imbue intrinsic reasoning capabilities from the ground up [30]. This "from-the-ground-up" approach aims to build a more robust and capable base model.

**"RL's Razor" and Forgetting Control**
The "RL's Razor" principle provides a theoretical framework for designing post-training methods that explicitly control KL divergence (KL drift) to minimize forgetting of old capabilities [3]. This involves proactively integrating KL monitoring during training, utilizing on-policy RL's natural design for conservative policy drift, and treating KL shift from the base model as a primary constraint in continuous learning and online deployment [3]. Theoretical investigation into the representational mechanisms by which KL drift causes the destruction of old capabilities is also a critical future direction [3]. For example, the theory suggests that RL's convergence to KL-minimal optimal solutions can explain its effectiveness in retaining prior knowledge [3].

**Advanced Frameworks for Efficiency, Interpretability, and Robustness**
Future research also focuses on developing **Adaptive Compute Allocation for Reasoning** and **Sample-Efficient and Interpretable LLM-RL Frameworks for Data Generation and Policy Learning**. Adaptive compute allocation, as suggested by dynamic compute allocation in verifier-guided training and adaptive resource allocation strategies in large-scale RL, is vital for optimizing performance and efficiency [15,22,23].

Within sample-efficient and interpretable frameworks, several key components are being explored:
*   **Context-aware and Multi-modal Reward Models**: These models are crucial for providing fine-grained, accurate feedback, especially in complex reasoning or multi-modal tasks. Perception-R1, for instance, demonstrates the effectiveness of adapting Group Relative Policy Optimization (GRPO) with rule-based reward engineering for fine-grained MLLM visual tasks, suggesting the need for more sophisticated context-aware and multi-modal reward models [17]. Similarly, CARD's dynamic feedback mechanisms offer context-aware insights, and the concept of Endogenous Reward Models (EndoRM) proposes a training-free and promptable judge by interpreting LLM logits as soft Q functions [5,20]. Advanced reward modeling methods are sought to offer better generalization and accuracy, addressing issues like "overthinking and underthinking" [6].
*   **Theoretical Justifications for Simplified Methods**: A deeper understanding of why simpler policy optimization algorithms, such as RRHF, can perform comparably to more complex PPO-based approaches is crucial [4]. Theoretical foundations, such as the mathematical proof of SRPO's robustness to behavior policy changes or APA's theoretical justification for its squared error loss function, are vital for developing stable, sample-efficient, and robust alignment algorithms [12,16]. This theoretical grounding also validates the effectiveness of RL in LLMs by recovering internal reward functions [20].
*   **Automated Data Generation and Policy Learning**: Innovations in generating diverse and complex tasks and data are needed to enhance LLMs' self-evaluation and self-correction capabilities [6]. This includes refining learning state recognition, developing automated data filtering strategies, and enhancing generative diversity during self-annotation [31]. Algorithms must address exploration limitations, especially for out-of-distribution prompts, and improve policy optimization beyond the challenges of DPO and PPO, seeking more efficient, stable, and less data-intensive paradigms [13,18,24]. Approaches like LPPO, which focuses on sample-centric optimization, demonstrate the potential for fine-grained control to improve reasoning abilities across various models and architectures [32].

In summary, the trajectory of algorithmic and architectural innovations in RL for LLMs points towards increasingly sophisticated, efficient, and robust methods that integrate verifiers, multi-objective considerations, and theoretically grounded simplifications to achieve more capable and aligned artificial intelligence [4,9,10,13,17,23,34].
### 9.3 Ethical AI and Responsible Deployment
The development and deployment of Large Language Models (LLMs) necessitate a paramount focus on ethical AI and responsible strategies to navigate their inherent complexities and societal impacts. A core tenet of this endeavor is the continuous research into enhancing safety and alignment, building upon existing efforts such as those in Reinforcement Learning from Human Feedback (RLHF) [9]. While RLHF is a primary strategy for aligning LLMs with human objectives and motivations, enabling them to avoid harmful outputs and become more capable and useful, it also presents challenges related to ethical implications, including human subjectivity, potential for malicious intent, and the capacity for LLMs to mislead evaluators [9,24,35]. Therefore, proactive and multi-faceted measures are indispensable to mitigate biases, ensure fairness, and comprehensively address the broad societal ramifications of these advanced AI systems [34].

Addressing biases requires moving beyond potentially prejudiced external feedback mechanisms. For instance, approaches that derive rewards from an LLM's intrinsic pre-training objectives can offer a less biased alignment signal, thereby reducing the inheritance of style biases and inherent prejudices from external evaluation models [20]. Furthermore, systematic research like OffsetBias is crucial for explicitly mitigating various biases in reward models, contributing significantly to the development of fair and impartial AI systems [29]. The challenges extend to the very methodologies used for evaluation; fundamental questions persist regarding how to effectively communicate expected model behavior, establish consistent and unbiased evaluation criteria, and appropriately balance human and AI evaluators, given the observed variance and sensitivity of AI-based assessments [13]. Moreover, as LLMs gain global reach, future work must prioritize cross-cultural and multilingual alignment strategies to ensure equitable and safe deployment across diverse linguistic and cultural contexts [28].

A robust, multi-pronged safety approach is essential, emphasizing transparency and explainability in LLM behaviors. This involves not only ongoing red-teaming efforts, especially with non-English prompts and diverse attack vectors, but also the development of "Safe RLHF" variants specifically designed to enhance safety and mitigate risks [28,35]. With increasingly capable models such as DeepSeek-R1 [22], the focus must extend to integrated "risk control systems" that prevent vulnerabilities like "jailbreak attacks," the generation of harmful content, and malicious fine-tuning of open-source models [22]. Techniques such as ORPO and DPO exemplify direct interventions to control undesirable behaviors by preventing models from adopting "unwanted generation styles" and guiding them away from "unwanted behaviors" [29]. Achieving an optimal balance between safety and helpfulness, thereby reducing instances of "false refusals" without compromising security, remains a critical area for research [28]. The need for transparency and explainability is paramount to building trust and facilitating responsible use, especially concerning safety-critical decisions [28,34].

To further advance this field, researchers have proposed "Hybrid Alignment for Safety and Reasoning in LRMs," which integrates verifiable rewards with safety alignment, alongside a comprehensive multi-pronged safety approach that foregrounds transparency and explainability [14]. This hybrid model aims to provide "new ideas" for enhancing complex reasoning abilities, as demonstrated by the advancements in models like DeepSeek-R1, while simultaneously embedding human values and ethical considerations [22,34]. The future development of RL-driven LLMs mandates balancing capability enhancement with stringent requirements for "safety and scalability" [10]. Simpler and more robust alignment methods, such as RRHF, can democratize access to alignment efforts, enabling a broader community to contribute to building ethically aligned AI systems [4].

Ultimately, the responsible development and deployment of LLMs demand a concerted, collaborative effort from the broader AI community. Meta's commitment to open science and fostering community contributions underscores the importance of transparency, collaboration, and collective problem-solving among academic researchers, civil society, policymakers, and industry stakeholders [15,28]. This open-source spirit facilitates shared responsibility and accelerates progress in addressing the complex risks and societal impacts of LLMs. As the pursuit of highly capable AI systems continues, ensuring that the development of increasingly powerful agents remains aligned with human safety and values, reminiscent of the foundational goals of early RLHF, will be paramount [23]. This collaborative ecosystem is vital for auditing, governing, and continually improving the ethical posture and responsible utility of LLMs.
## 10. Conclusion
This survey has elucidated the transformative impact of Reinforcement Learning (RL) on Large Language Models (LLMs), charting a significant evolution from initial alignment paradigms to advanced, increasingly autonomous, and specialized methodologies. The journey underscores not only substantial achievements but also persistent challenges in the pursuit of more intelligent, adaptable, and robust LLM systems.

Initially, Reinforcement Learning from Human Feedback (RLHF) emerged as a foundational technique, pivotal in aligning LLMs with human preferences, enabling models like ChatGPT and Llama 2 to generate helpful, honest, and safe outputs [28,35]. Despite its efficacy in reducing human annotation burden and yielding consistent reward models, RLHF faces inherent limitations, including the high cost, subjectivity, and inconsistency of human feedback, alongside the complexity and instability of traditional RL algorithms like Proximal Policy Optimization (PPO) [9,24].

In response to these challenges, the field has rapidly advanced towards more scalable and efficient solutions. Reinforcement Learning from AI Feedback (RLAIF) has gained prominence, leveraging powerful LLMs to generate feedback comparable to human feedback, and in some instances, even surpassing it [11,29]. Simultaneously, simplified alignment techniques such as Direct Preference Optimization (DPO), which converts RL into a supervised fine-tuning task, and Ranking-based Reward for Human Feedback (RRHF), which avoids complex PPO algorithms, have significantly streamlined the alignment process, enhancing stability and computational efficiency [4,19]. Further innovations, including Advantage-Induced Policy Alignment (APA) and Self-Improving Robust Preference Optimization (SRPO), address PPO's limitations and improve generalization and robustness [12,16].

A profound shift has occurred towards reducing human dependency through leveraging endogenous reward mechanisms and self-improving frameworks. Research demonstrating the existence of universal reward models within LLMs, leading to training-free reward recovery and self-improvement, exemplifies this trend [20]. Methods like Self-Evolved Reward Learning (SER) and Google DeepMind's SCoRe facilitate iterative self-improvement using self-generated data, drastically reducing the reliance on extensive human annotation for complex reasoning tasks [18,31]. The CARD framework further automates reward engineering via LLM-driven generation and refinement of reward function code [5].

Furthermore, outcome-based RL, such as Reinforcement Learning with Verifiable Rewards (RLVR), has proven highly effective in improving stepwise reasoning abilities. Approaches like DeepSeek-R1, driven by objective, verifiable reward signals based on final answer correctness, have unlocked emergent advanced reasoning behaviors like self-verification and reflection, achieving state-of-the-art performance in mathematics, programming, and STEM problems [10,22]. Specific application successes, including SEARCH-R1 for enhanced external knowledge retrieval and Perception-R1 for improving multimodal LLM visual perception, showcase RL's versatility in expanding LLM capabilities towards becoming Large Reasoning Models (LRMs) [7,17]. Accessibility enhancements, such as PEFT and 8-bit quantization for cost-effective RLHF, and robust frameworks like ROLL for scalable multi-task RL, further democratize these advancements [2,15].

Looking forward, the future of RL for LLMs presents critical research frontiers. Algorithmic innovation and robustness remain paramount, requiring more stable, sample-efficient policy optimization methods that address issues like training instability, mode collapse, and catastrophic forgetting [3,12]. Enhanced autonomy and self-improvement will continue to be a central theme, focusing on leveraging intrinsic reward mechanisms and refining techniques for advanced self-evolution and system generalization [20,31]. Scalability, efficiency, and infrastructure also demand ongoing attention, with efforts directed towards optimizing training speed, resource utilization, and token usage in complex tasks [5,18]. Beyond general alignment, future work will delve into specialized applications, multimodal capabilities, and advanced reasoning, exploring continual RL for lifelong knowledge retention and model-based RL for sophisticated planning [17,30]. Finally, ethical AI and responsible deployment will remain a critical area, emphasizing safety alignment, bias mitigation, and ensuring LLMs adhere to human values across diverse contexts [28,29].

In conclusion, RL continues to be an indispensable driver of LLM development, propelling both alignment with human preferences and the enhancement of complex reasoning capabilities. The trajectory from traditional RLHF to more autonomous, efficient, and outcome-focused methods signifies a continuous innovation cycle. This progressive reduction in reliance on extensive human intervention addresses long-standing challenges in scalability, cost, and stability, ultimately unlocking the full potential of LLMs as intelligent, adaptable, and robust systems and paving the way towards Artificial General Intelligence (AGI) and beyond [23,26].
### 10.1 Summary of Key Findings
The integration of Reinforcement Learning (RL) has fundamentally transformed the capabilities of Large Language Models (LLMs), significantly enhancing their alignment with human preferences and their performance across complex tasks [14,29]. 

![Evolution of RL for LLM Alignment and Capability Enhancement](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/fgg27OErlEr7uiRqCWUjc_Evolution%20of%20RL%20for%20LLM%20Alignment%20and%20Capability%20Enhancement.png)

This survey highlights a critical journey from foundational alignment techniques to advanced, more autonomous, and specialized methods, underscoring both the triumphs and persistent challenges in the field.

Initially, Reinforcement Learning from Human Feedback (RLHF) emerged as the dominant paradigm for aligning LLMs, enabling models like ChatGPT and Llama 2 to generate helpful, honest, and safe outputs, follow complex instructions, and avoid harmful content [10,28,35]. RLHF's advantages include reducing human annotation burden through response ranking and yielding more consistent reward models compared to automatic metrics [35]. However, RLHF inherently faces significant limitations, including the high cost, subjectivity, and inconsistency of human feedback, leading to potential data poisoning and reward hacking [9]. Furthermore, the complexity and instability of traditional RL algorithms like Proximal Policy Optimization (PPO), along with challenges in ensuring data annotation consistency and model stability, pose hurdles for widespread adoption and scalability [24,29].

In response to RLHF's dependency on costly human annotation, Reinforcement Learning from AI Feedback (RLAIF) has emerged as a promising, scalable alternative [29]. RLAIF leverages powerful LLMs to generate feedback, achieving performance comparable to RLHF across tasks such as summarization, helpful dialogue, and harmless dialogue generation, and in some cases even outperforming it [11,27]. Key advancements in RLAIF include demonstrations of LLM "self-improvement," where an AI labeler derived from the same model can still effectively enhance performance, and the introduction of direct-RLAIF (d-RLAIF), which simplifies the process by directly using LLM feedback as a reward signal without explicit reward model training [11,27]. While RLAIF can rapidly achieve good performance and mitigate scalability challenges, it has been observed to reach a performance plateau as it does not introduce novel information, unlike RLHF which can continuously improve with increased human data [33].

Beyond the evolution of feedback mechanisms, significant efforts have focused on simplifying existing RL processes and developing innovative approaches that reduce human dependency. Methods like Ranking-based Reward for Human Feedback (RRHF) offer a simplified learning paradigm that achieves robust alignment without the need for complex PPO algorithms or multiple models, instead using log-conditional probabilities and ranking loss [4]. Direct Preference Optimization (DPO) further streamlines alignment by converting the RL problem into a supervised fine-tuning task, eliminating the need for a separate reward model and complex sampling, thereby offering a more stable and computationally efficient approach [19,24]. Advantage-Induced Policy Alignment (APA) addresses PPO's limitations such as mode collapse and instability by using a novel squared error loss based on estimated advantages, leading to more robust LLM alignment [12]. Furthermore, Self-Improving Robust Preference Optimization (SRPO) introduces a framework for complete robustness to task variations and out-of-distribution shifts through a min-max objective, demonstrating superior generalization compared to DPO [16].

A significant trend involves innovative approaches that dramatically reduce human dependency. The concept of "endogenous rewards" posits that a universal reward model is potentially present in any LLM trained via standard next-token prediction [20]. This discovery allows for training-free reward recovery and has been shown to outperform existing LLM-as-a-judge methods, leading to self-improvement effects in LLMs fine-tuned with these internal rewards [20]. Similarly, Self-Evolved Reward Learning (SER) enables reward models to iteratively improve through self-generated data, drastically reducing the reliance on extensive human annotation and achieving performance comparable to models trained with 100% human-annotated data using only 15% of the original data [31]. Google DeepMind's SCoRe method introduces multi-round online RL for enhancing intrinsic self-correction in LLMs, leveraging "completely self-generated data" to achieve substantial performance gains in mathematics and programming, further reducing reliance on external human feedback for complex reasoning tasks [18]. The CARD framework takes this a step further by using an LLM-driven approach for iterative generation and refinement of reward function code, largely removing the need for human feedback in reward engineering and demonstrating performance matching or exceeding human-designed "oracle" rewards [5].

Crucially, outcome-based RL, such as Reinforcement Learning with Verifiable Rewards (RLVR), has emerged as a powerful paradigm for significantly improving stepwise reasoning abilities in LLMs [10,30]. DeepSeek-R1 exemplifies this by using a "pure reinforcement learning" approach driven by objective, verifiable reward signals based solely on final answer correctness, thereby minimizing reliance on expensive human-annotated reasoning examples [22,26]. This method has unlocked emergent advanced reasoning behaviors like self-verification and reflection, leading to state-of-the-art performance in mathematics, programming, and STEM problems [22]. LPPO, another sample-centric RLVR framework, combines Prefix-Guided Sampling and Learning-Progress Weighting to achieve faster convergence and higher performance in mathematical reasoning [32].

Specific application successes further illustrate RL's versatility. SEARCH-R1 employs RL to train LLMs to autonomously generate search queries and interact with search engines, significantly enhancing complex reasoning tasks requiring real-time external knowledge retrieval and outperforming traditional RAG methods [7]. In the multimodal domain, Perception-R1 demonstrates that rule-based RL, particularly Group Relative Policy Optimization (GRPO), can dramatically improve the fine-grained visual perception capabilities of Multimodal LLMs (MLLMs), addressing deficiencies in object localization, counting, and OCR through meticulously designed rule-based reward engineering [1,8,17]. These advancements highlight how RL transforms LLMs into Large Reasoning Models (LRMs) capable of planning, reflecting, and self-correcting [23,30].

Furthermore, the field has seen advancements in making RL for LLMs more accessible and robust. Techniques integrating PEFT and 8-bit quantization enable cost-effective RLHF fine-tuning of large LLMs on consumer-grade GPUs, broadening community engagement [2]. Frameworks like ROLL provide scalable and user-friendly infrastructure for multi-task and agentic RL, demonstrating significant performance improvements and robustness without model collapse [15]. Reinforcement Learning has also been shown to significantly reduce catastrophic forgetting compared to supervised fine-tuning, attributed to its inherent bias towards "KL-minimal solutions" [3].

In summary, RL continues to be an indispensable tool for LLM development, driving both alignment with human preferences and the enhancement of complex reasoning capabilities. While RLHF remains a dominant force, the landscape is rapidly evolving towards more efficient, autonomous, and outcome-focused methods like RLAIF, endogenous rewards, self-evolved learning, and RLVR, progressively reducing reliance on extensive human intervention and addressing long-standing challenges in scalability, cost, and stability. The journey from traditional RLHF to these advanced, more autonomous alignment methods underscores a continuous innovation cycle aimed at unlocking the full potential of LLMs as intelligent, adaptable, and robust systems.
### 10.2 Future Outlook
The landscape of Reinforcement Learning (RL) for Large Language Models (LLMs) is rapidly evolving, presenting both persistent challenges and promising emerging directions that will shape future research and development [10]. This section outlines critical areas for future investigation, serving as a roadmap for researchers committed to advancing RL-driven LLM development and reinforcing the potential for more robust, efficient, and intelligent language models [10,31].

A primary area for future work lies in **algorithmic innovation and robustness** for policy optimization. Current methods, particularly within the RLHF paradigm, face significant hurdles such as training instability, mode collapse, and difficulties in ensuring consistent evaluation and generalization [9,12,13]. Future research must focus on developing more stable, sample-efficient, and robust algorithms, exploring alternative loss functions and policy update mechanisms that prioritize both performance and behavioral diversity in LLM outputs [12,28,29]. The success of simplified, robust algorithms like RRHF suggests a promising avenue for efficient LLM alignment, emphasizing the optimization of sampling quality and source diversity for preference learning [4]. Additionally, addressing the challenge of catastrophic forgetting, where new knowledge displaces old, requires a deeper understanding of representational mechanisms and enhanced precision in KL divergence estimation, potentially through "offline but approximate on-policy" algorithms [3].

The pursuit of **enhanced autonomy and self-improvement** for LLMs stands out as a critical emerging direction. Research increasingly points towards LLMs capable of self-evolving their reasoning abilities, particularly in tasks verifiable by reliable mechanisms, potentially leading to machines surpassing human capabilities in such domains [22]. This shift necessitates exploring internal dynamics of LLMs to leverage intrinsic reward mechanisms, thereby reducing reliance on costly and potentially biased external human or AI feedback [20]. Future work will concentrate on developing more advanced self-evolution modes, improving system generalization, and refining techniques for judging the model's learning state to automatically adjust data filtering and encourage diverse responses during self-evolution [6,16,31]. Refining reward mechanisms to better capture the quality of intermediate reasoning steps, especially for complex tasks involving tool use and dynamic retrieval, will be paramount [7].

**Scalability, efficiency, and infrastructure** remain perennial challenges. The computational resource demands of advanced RL techniques like SCoRe, particularly for large-scale datasets and complex tasks, require further investigation into efficient RL algorithms, optimized reward shaping, and adaptive self-generation processes [18]. Efforts must continue to improve training speed, particularly in multi-GPU environments, and to develop new tools and strategies to overcome these bottlenecks [2]. The development of robust frameworks like ROLL, with plans for advanced features and community involvement, highlights the ongoing commitment to adapting to emerging LLM and RL research trends [15]. Furthermore, optimizing token usage in LLM-driven reward design frameworks, like CARD, is crucial for enhancing scalability in long-horizon tasks [5].

Beyond general alignment, future work will delve into **specialized applications and multimodal capabilities**. In multimodal LLMs, the quest for "true visual 'epiphany'" remains an open area, requiring continued research into adapting RL to the unique characteristics of visual tasks and scaling these methods to increasingly complex visual understanding challenges [1,8,17]. Enhancing tool-use capabilities, improving "marking efficiency," and extending reasoning abilities to a wider range of models are critical for the progression towards more autonomous and adaptive LLM agents [22,26,28]. This includes exploring Continual Reinforcement Learning for lifelong knowledge retention, Model-based Reinforcement Learning for advanced planning, and integrating RL into LLM pre-training for optimized performance [30].

Finally, **ethical AI and responsible deployment** will remain a critical area. Continuous efforts in safety alignment, bias mitigation, and ensuring LLMs adhere to human values while performing complex tasks are essential, especially as models become more autonomous [9,22,28,29]. This involves refining alignment techniques to balance helpfulness and safety, mitigating "false refusals," and expanding efforts to diverse cultural contexts and non-English languages [28]. Transparency, open science, and collaborative efforts with the wider AI community are vital for navigating the complexities of responsible development and deployment [28]. Ultimately, the ongoing development of RL in LLMs aims to advance towards Artificial General Intelligence (AGI) or even Artificial SuperIntelligence (ASI) by continuously reviewing its development, re-examining future directions, and exploring scalable strategies [23,26].

## References

[1] 强化学习突破多模态感知极限：Perception-R1 论文与代码开源 [https://baijiahao.baidu.com/s?id=1831071441950645488&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1831071441950645488&wfr=spider&for=pc) 

[2] 24GB显卡微调20B LLMs：RLHF与PEFT的巧妙结合 [https://www.bilibili.com/read/cv22581486](https://www.bilibili.com/read/cv22581486) 

[3] RL's Razor：强化学习如何更少遗忘旧能力 [https://www.zhihu.com/question/1934907181452485781/answer/1951594839557776005](https://www.zhihu.com/question/1934907181452485781/answer/1951594839557776005) 

[4] RRHF：无需PPO即可实现人类反馈的语言模型对齐 [https://arxiv.org/abs/2304.05302](https://arxiv.org/abs/2304.05302) 

[5] CARD: LLM驱动的动态反馈式奖励设计框架 [https://www.sciencedirect.com/science/article/abs/pii/S0950705125011104](https://www.sciencedirect.com/science/article/abs/pii/S0950705125011104) 

[6] 从自我进化视角解析LLM推理能力技术演进路径 [http://ir.hit.edu.cn/2025/0321/c19589a364801/page.htm](http://ir.hit.edu.cn/2025/0321/c19589a364801/page.htm) 

[7] Search-R1：让大模型学会“检索+推理”的新范式 [https://www.51cto.com/aigc/4749.html](https://www.51cto.com/aigc/4749.html) 

[8] 强化学习驱动多模态LLM视觉感知新突破 [http://mt.sohu.com/a/891641768_121207965](http://mt.sohu.com/a/891641768_121207965) 

[9] RLHF的挑战与对策 [https://bdtechtalks.com/2023/09/04/rlhf-limitations/](https://bdtechtalks.com/2023/09/04/rlhf-limitations/) 

[10] 大型语言模型强化学习技术技术综述 [http://www.paperreading.club/page?id=321233](http://www.paperreading.club/page?id=321233) 

[11] RLAIF: AI反馈驱动的强化学习，媲美人类反馈 [https://arxiv.org/abs/2309.00267](https://arxiv.org/abs/2309.00267) 

[12] APA: 改进LLM与人类偏好对齐的新方法 [https://arxiv.org/abs/2306.02231](https://arxiv.org/abs/2306.02231) 

[13] PyTorch实现GPT-2 DPO训练：方法对比与改进 [https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247650788&idx=2&sn=345cd67770c27f65be1a0395e8392b65&chksm=e9ef846fde980d79f22db8c19c788d0ef438df7386065d4bf216bbcaca23cc01b995b796c673&scene=27](https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247650788&idx=2&sn=345cd67770c27f65be1a0395e8392b65&chksm=e9ef846fde980d79f22db8c19c788d0ef438df7386065d4bf216bbcaca23cc01b995b796c673&scene=27) 

[14] 赵俊华教授团队在大模型增强强化学习研究方面取得重要进展 [https://sse.cuhk.edu.cn/article/1867](https://sse.cuhk.edu.cn/article/1867) 

[15] 淘天携手爱橙开源强化学习框架ROLL，赋能百亿至千亿参数大模型训练 [https://www.thepaper.cn/newsDetail_forward_31039751](https://www.thepaper.cn/newsDetail_forward_31039751) 

[16] SRPO：提升AI与人类偏好对齐的鲁棒性框架 [https://baijiahao.baidu.com/s?id=1801672190108060512&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1801672190108060512&wfr=spider&for=pc) 

[17] 强化学习赋能多模态LLM：Perception-R1突破视觉感知极限 [https://roll.sohu.com/a/891641358_610300](https://roll.sohu.com/a/891641358_610300) 

[18] DeepMind SCoRe：LLM自我纠正新突破，性能显著提升 [https://developer.aliyun.com/article/1634599](https://developer.aliyun.com/article/1634599) 

[19] DPO算法：LLM优化新篇章 [https://www.bilibili.com/read/mobile?id=30512787](https://www.bilibili.com/read/mobile?id=30512787) 

[20] 周志华团队：LLM内部存在通用奖励模型，RL可有效提升LLM性能 [https://baijiahao.baidu.com/s?id=1836524052345369684&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1836524052345369684&wfr=spider&for=pc) 

[21] RLAIF：AI反馈强化学习的有效性与RLHF相当 [https://baijiahao.baidu.com/s?id=1780896645812385055&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1780896645812385055&wfr=spider&for=pc) 

[22] DeepSeek-R1：强化学习驱动大模型推理能力的Nature封面论文 [https://mp.weixin.qq.com/s?__biz=MzU1MzMxMzcyMg==&mid=2247785885&idx=1&sn=703adac4b77386da468f414a0c323c32&chksm=fa04c17709a45a4844c3fe2180a006b628840065b8b8f295ecb1ae60670f404c26218fe76812&scene=27](https://mp.weixin.qq.com/s?__biz=MzU1MzMxMzcyMg==&mid=2247785885&idx=1&sn=703adac4b77386da468f414a0c323c32&chksm=fa04c17709a45a4844c3fe2180a006b628840065b8b8f295ecb1ae60670f404c26218fe76812&scene=27) 

[23] 顶级团队发布RL超全综述，探索通往超级智能之路 [https://baijiahao.baidu.com/s?id=1843139108318040300&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1843139108318040300&wfr=spider&for=pc) 

[24] LLM强化学习算法深度解析：PPO、DPO、KTO [https://developer.baidu.com/article/detail.html?id=3361502](https://developer.baidu.com/article/detail.html?id=3361502) 

[25] RLAIF：利用AI反馈提升人类反馈强化学习的效率 [http://www.paperreading.club/page?id=181003](http://www.paperreading.club/page?id=181003) 

[26] 大型语言模型强化学习推理能力综述 [https://hub.baai.ac.cn/paper/1ab2a193-3952-45ee-8e54-68f2ea7552aa](https://hub.baai.ac.cn/paper/1ab2a193-3952-45ee-8e54-68f2ea7552aa) 

[27] RLAIF vs. RLHF: AI Feedback as a Scalable Alternative to Human Feedback [https://arxiv.org/html/2309.00267v3](https://arxiv.org/html/2309.00267v3) 

[28] Llama 2 论文中译版——开放式基础和微调聊天模型 [https://www.cnblogs.com/ms27946/p/llama-2-open-foundation-and-fine-tuned-chat-models-zh.html](https://www.cnblogs.com/ms27946/p/llama-2-open-foundation-and-fine-tuned-chat-models-zh.html) 

[29] 强化学习增强大型语言模型（LLMs）综述：中文翻译（1-5章） [https://blog.csdn.net/weixin_51606521/article/details/146102296](https://blog.csdn.net/weixin_51606521/article/details/146102296) 

[30] 推理模型RL综述：强化学习如何赋能大模型推理能力 [https://blog.csdn.net/acccc1122/article/details/151659499](https://blog.csdn.net/acccc1122/article/details/151659499) 

[31] 自进化奖励学习：AI语言模型的自我提升之道 [https://blog.csdn.net/weixin_36829761/article/details/143494848](https://blog.csdn.net/weixin_36829761/article/details/143494848) 

[32] LPPO: 提升LLM推理能力的样本中心优化框架 [https://arxiv.org/html/2507.06573v1](https://arxiv.org/html/2507.06573v1) 

[33] RLAIF: 用AI反馈替代人类反馈，加速强化学习模型训练 [https://blog.csdn.net/codename_cys/article/details/133466677](https://blog.csdn.net/codename_cys/article/details/133466677) 

[34] LLM增强强化学习：概念、分类与方法综述 [https://blog.csdn.net/c_cpp_csharp/article/details/140610510](https://blog.csdn.net/c_cpp_csharp/article/details/140610510) 

[35] RLHF：人类反馈强化学习的故事：起源、动机、技术及现代应用 [https://blog.csdn.net/XianxinMao/article/details/136780314](https://blog.csdn.net/XianxinMao/article/details/136780314) 

