# A Survey on Vision-Language-Action Models

# 0. A Survey on Vision-Language-Action Models

## 1. Introduction

![Motivations and Challenges of Vision-Language-Action Models](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/3AiS_79e-U5K2JypDTZv9_Motivations%20and%20Challenges%20of%20Vision-Language-Action%20Models.png)

Vision-Language-Action (VLA) models represent a paradigm shift in artificial intelligence, aiming to bridge the critical gap between abstract AI understanding and concrete physical interaction. By extending the capabilities of large-scale foundation models—including Large Language Models (LLMs), Vision Foundation Models (VFMs), and Vision-Language Models (VLMs)—to the physical world, VLAs unify perception, natural language understanding, and embodied action within a single computational framework [25,32]. This survey provides a comprehensive roadmap for VLA models, clarifying foundational concepts, summarizing developmental progress, identifying inherent challenges, and charting future research directions [25,28].

Historically, AI research progressed within fragmented, unimodal systems for vision, language, and action, which struggled with integration and generalization in complex real-world environments [28]. The emergence of Vision-Language Models (VLMs) marked a crucial step by aligning visual and linguistic modalities, enabling sophisticated multimodal understanding but notably lacking the capacity for physical action [25,32]. VLA models address this "integration gap" by enabling direct physical control, transforming AI from passive observation to active, embodied interaction [18,29]. Pioneering efforts like Google DeepMind's RT-2 demonstrated how large-scale pre-trained VLMs could be adapted for robot control by treating actions as text tokens [20]. Subsequent advancements have continued to refine VLA capabilities, tackling limitations in 3D understanding, reasoning, memory, and generalization through models like InstructVLA, MemoryVLA, and UniVLA [5,21,31].

This survey synthesizes insights from a diverse range of scholarly works, including foundational reviews, specialized surveys, and primary research articles introducing novel models, datasets, and evaluation methodologies [25]. It leverages existing meta-survey resources and community-curated lists, such as "awesome_vla_for_robotics_资源汇总" and "具身智能年度影响力论文盘点", to structure analysis and identify major research trends [16,30]. A central analytical framework for this survey is action tokenization, which systematically categorizes existing VLA research based on how action tokens are formulated and organized [27,32]. This approach offers a nuanced understanding of current VLA capabilities and highlights areas for future innovation.

The structure of this survey progresses logically from foundational principles to advanced applications and future directions. Initially, we define VLA models and trace their historical evolution, emphasizing the critical role of multimodal integration and language-based tokenization strategies [28,32]. Subsequent sections delve into core architectural components, examining various paradigms such as monolithic and hierarchical models, and discussing data-efficient learning frameworks and model acceleration techniques [6,28]. We categorize existing VLA research through lenses such as core architectural modules, key model examples (e.g., InstructVLA, DreamVLA, GraspVLA, AutoVLA, UAV-VLA, OpenVLA, DexVLA, ThinkAct), and the properties of training data and simulation environments, including evaluation methodologies and specific applications like robotic manipulation and autonomous driving [2,6,7,10,11,12,14,15,17,29]. Finally, we critically analyze the major challenges impeding the evolution of VLA models, such as data scarcity, computational inefficiency, architectural imbalances leading to semantic-physical mismatches, and limitations in long-horizon planning and generalization [3,9,27,28,29,31]. Promising future research avenues, including memory mechanisms, 4D perception, and efficient adaptation, are identified to inspire further investigations [6]. Through this structured analysis, the survey aims to foster the development of robust, efficient, and ethically sound VLA systems.
### 1.1 Motivation and Significance of VLA Models
Vision-Language-Action (VLA) models represent a transformative paradigm in artificial intelligence, aiming to bridge the critical gap between abstract AI understanding and concrete physical interaction by extending the capabilities of large-scale foundation models (LLMs, VFMs, VLMs) to the physical world [27,32]. This burgeoning field is propelled by the ambition to unify perception, natural language understanding, and embodied action within a single computational framework [25,28], thereby addressing longstanding limitations of traditional robotic and AI approaches. The recent surge in research, marking 2024 as a pivotal year for embodied intelligence, underscores the growing significance of VLA models in advancing this domain [16].

Historically, AI systems in robotics operated in fragmented silos, with distinct vision, language, and action modules that struggled to collaborate effectively or adapt to novel, unpredictable scenarios [28,35]. Traditional computer vision models, often task-specific and data-intensive, lacked the semantic understanding to translate visual information into meaningful actions, while Large Language Models (LLMs) excelled at textual reasoning but were incapable of perceiving or interacting with the physical world [28]. Robotic action systems, typically reliant on hand-designed policies or laborious reinforcement learning, faced challenges with high costs, scalability, and generalization, particularly in unstructured and novel environments [6,28]. This "integration gap" between modalities and the limitations of rigid, predefined tasks led to a lack of comprehensive world knowledge, including dynamic, spatial, and semantic information [2]. Furthermore, conventional autonomous driving systems were prone to error accumulation between modules and struggled with complex rule design and covering extreme scenarios [8,19]. VLA models are specifically designed to overcome these shortcomings by integrating these functionalities, enabling intelligent agents to perceive environments, comprehend complex instructions, and execute purposeful actions dynamically [25].



![Motivations and Challenges of Vision-Language-Action Models](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/wUAniMLFV4qAggLBjMxaH_Motivations%20and%20Challenges%20of%20Vision-Language-Action%20Models.png)

The development of VLA models is motivated by several key objectives. Firstly, they aim for **enhanced environmental understanding** and **generalization**. By leveraging the powerful perceptual and cognitive capabilities of foundation models, VLAs enable agents to solve open-ended tasks expressed via open-vocabulary instructions in complex, open-world physical environments [32]. This promises zero-shot generalization, scalability, and few-shot adaptability to new tasks and environments, improving robustness in real-world scenarios [4,10,30,34]. For instance, models like DreamVLA integrate world knowledge forecasting to establish a robust perception-prediction-action loop, improving generalization and reasoning in robot manipulation [2]. Similarly, InstructVLA aims to preserve the powerful language reasoning capabilities of VLMs while enhancing action generation, addressing the "catastrophic forgetting" problem often observed during fine-tuning on limited robot data [5,14].

Secondly, VLA models aspire to facilitate more **intuitive human-AI interaction**. By processing language and visual inputs, these models enable robots to understand complex human instructions and intentions, paving the way for intuitive user-robot interfaces [18,34]. The UAV-VLA system, for example, simplifies human-drone interaction by allowing natural language commands to generate complex flight paths and execute autonomous tasks [15].

Thirdly, VLA models are seen as a critical step towards achieving **general-purpose intelligence** and **Embodied AI**. They are designed to imbue robots with autonomy, allowing them to interpret high-level instructions, generalize to unseen objects, reason about complex spatial relationships, and execute sophisticated multi-stage tasks that demand integrated perception, reasoning, and action [26,35]. The Microsoft Research call for joint projects explicitly highlights the motivation to develop more capable and generalist AI agents for complex, dynamic, and diverse real-world scenarios, emphasizing VLA model architecture design as crucial for these goals [24]. Projects like RT-2 aim to create single models capable of both low-level action control and leveraging internet-scale semantic reasoning, addressing core challenges of scalability and generalization in robotics [20].

Despite this immense promise, current VLA models face several significant challenges. A critical limitation is the pervasive **data scarcity** for real-world robot interaction, which is costly and labor-intensive to collect [29]. Efforts like GraspVLA explore large-scale synthetic action data to mitigate this issue, along with sim-to-real gaps [10]. Another major concern is **computational inefficiency** (labeled as #computational inefficiency), as many advanced VLA models are "super smart but slow," hindering their practical deployment due to high computational loads and slow processing speeds [9]. Furthermore, existing models often exhibit an **architectural imbalance** (labeled as #ArchitecturalImbalance), where VLM components remain somewhat disconnected from the embodied, sensorimotor context needed for robotic action, leading to a #Semantic-PhysicalMismatch [29]. The lack of a comprehensive understanding of action tokens (labeled as #actiontoken_challenge) also significantly hinders effective VLA development and future research directions [27].

A fundamental conflict arises from the reliance on 2D inputs, which leads to a #ModalityGap and limits the seamless integration with the rich dynamics and geometric properties of the 3D physical world [3]. This often results in a #PlanningLimitation, as current models lack the capacity for imagination about future scenarios and explicit reasoning processes necessary for long-horizon planning and self-correction in complex embodied AI tasks [3,12,31]. The non-Markovian nature of many manipulation tasks further highlights the #long_horizon_tasks limitation, as traditional VLAs often overlook temporal dependencies [31]. Moreover, challenges like the "catastrophic forgetting" of VLM capabilities during fine-tuning on robot data, as addressed by InstructVLA, point to a #GeneralizationLimitation where models struggle to retain broad understanding while achieving precise action [14]. Finally, the fragmentation caused by diverse robot morphologies and the high cost of action annotation present a challenge for scalability and generalization across different robotic systems, a problem UniVLA addresses by proposing a universal, task-centric latent action language [21]. The growing concern for privacy and security of user-specific data (labeled as #PrivacyConcern, #SecurityRisk) in distributed training also poses a barrier to wider adoption, as addressed by FedVLA [23].

In summary, VLA models are poised to play a crucial role in advancing Embodied AI and robotics by enabling robots to "see, think, and act" in a unified, intelligent manner, overcoming the rigidity and limited generalization of traditional approaches [18,35]. While the initial promise of achieving truly generalist capabilities is immense, the field currently grapples with significant challenges related to data, computational efficiency, architectural coherence, and robust generalization. These challenges set the stage for subsequent detailed discussions on architectural designs, training methodologies, and evaluation benchmarks in VLA research.
### 1.2 Evolution from Vision-Language Models (VLMs) and Historical Context



The evolution of artificial intelligence towards integrated Vision-Language-Action (VLA) models represents a profound shift from earlier, fragmented approaches. Historically, AI research progressed largely within isolated modalities: computer vision models excelled at image recognition (e.g., AlexNet for CNNs) [34], natural language processing models mastered text understanding and generation (e.g., BERT, ChatGPT for LLMs) [32,34], and action systems focused on motion control through reinforcement learning or hand-crafted policies (e.g., DQN, AlphaGo) [25,28,34]. While individually successful, these unimodal systems struggled significantly with integration and generalization, particularly when confronted with complex, dynamic, real-world environments, leading to brittle and inflexible performance [25,28].

The emergence of multimodal AI, specifically Vision-Language Models (VLMs), marked a critical intermediate step. Pioneering efforts like CLIP (2022) and Flamingo (2022) achieved breakthroughs by aligning visual and linguistic modalities through contrastive learning on vast internet-scale datasets [25,28,32]. This enabled impressive capabilities such as zero-shot object recognition, visual question answering, and image captioning, laying a crucial foundation for subsequent advancements [6,34]. VLMs leveraged the powerful pre-trained knowledge inherent in large language models (LLMs) and vision foundation models (VFMs), simplifying model architectures and enhancing reasoning for complex tasks [27,32]. Models like LLaVA, InternVL, Qwen-VL, and Prismatic VLMs, often combining vision encoders (e.g., DINOv2, SigLIP) with LLMs (e.g., Llama 2, Gemma), exemplified this trend, providing sophisticated multimodal understanding and reasoning capabilities [11,16,22,26,32].

Despite their powerful understanding and reasoning, VLMs faced a conspicuous **"integration gap"**: their inherent inability to generate or execute coherent physical actions based on multimodal inputs [6,25,26,28,30,34,35]. Unlike general AI models such as GPT or even multimodal models like GPT-4V, which primarily operate within the digital domain, VLA models' core distinction lies in their capacity to *directly control physical actions* [18,32]. This ability to transition from passive observation and planning to active, embodied physical interaction is the innovation VLA models bring, bridging the disconnect between high-level human instructions and low-level physical robot actions [6,19,29].

The paradigm shift to VLA models began around 2021-2022, primarily driven by the advancements in LLMs and VLMs. A pioneering effort was Google DeepMind's Robotic Transformer 2 (RT-2) [20,25,26,28,30,34]. RT-2 demonstrated how large-scale pre-trained VLMs (e.g., PaLM-E, PaLI-X) could be directly applied to robot control by treating robot actions as text tokens and co-training on both internet-scale vision-language data and real robot trajectories [16,20,26]. This effectively enabled the VLM to "absorb" robot control as another language task, significantly enhancing generalization and semantic reasoning for novel objects and instructions compared to earlier, modular systems like RT-1 [20,26]. Early VLA methods thus introduced action tokens on top of VLMs, enabling joint learning from visual, language, and trajectory data, thereby improving robot generalization and multi-step reasoning [28].

The evolution within VLA models has continued to address various limitations of early approaches. For instance, while VLMs demonstrate strong general domain image understanding, they often struggle with 3D and spatial relations, representing a "modality gap" for 3D environment understanding [33]. Models like 3D-Generalist and 3D-VLA have emerged to enhance spatial understanding by using multi-view images and integrating 3D-based LLMs or generative world models, moving beyond 2D inputs and direct perception-to-action mapping [3,33]. Similarly, OpenDriveVLA and AutoVLA address the "modality gap" in driving tasks, where 2D-trained VLMs might generate "hallucinations" (physically infeasible actions), by incorporating traditional Bird's Eye View (BEV) perception or tokenizing actions into a codebook to eliminate VLM hallucinations [17].

Another critical "integration gap" identified is the lack of explicit reasoning and long-term memory in many existing VLA approaches. End-to-end models often struggle with multi-step planning and adaptation to diverse task variations [12]. ThinkAct and DreamVLA have evolved VLA by integrating multimodal LLMs for high-level reasoning and comprehensive world knowledge, moving beyond simple image-based forecasting to align VLA models with human-like abstract reasoning chains [2,12]. Furthermore, the challenge of "catastrophic forgetting of pre-trained vision-language capabilities" when adapting VLMs to action generation has been addressed by models like InstructVLA. This framework employs a novel instruction-tuning paradigm to jointly optimize textual reasoning and action generation, ensuring that flexible reasoning from VLMs is maintained while achieving precise action generation, contrasting with approaches that dilute VLM understanding during action-oriented fine-tuning [5,14]. MemoryVLA further enhances VLA capabilities by explicitly incorporating memory, addressing the "amnesia" issue in prior models that lacked temporal reasoning crucial for sustained interaction in dynamic environments [31].

The push for generalization has also led to innovations in action representation. UniVLA represents an evolution by tackling the "integration gap" at the action level, aiming for a universal, task-centric latent action language rather than embodiment-specific actions, allowing for cross-embodiment and cross-view action execution [21]. DexVLA, OpenVLA, and GraspVLA leverage pre-trained VLMs and internet-scale data to generalize to novel objects and environments, adapting broad knowledge for specific robotic control tasks like grasping [10,11,29]. Systems like UAV-VLA demonstrate this evolution in specialized domains, combining VLMs and LLMs for natural language-driven path and action generation for aerial robots [15].

In summary, the evolution from isolated unimodal AI systems to integrated VLA models has been propelled by the need to bridge the "integration gap" – specifically, the inability of VLMs to directly control physical actions. This transition has been significantly enabled by the rapid advancements in large language models and vision-language models, with pioneering efforts like RT-2 demonstrating the potential for unified perception, reasoning, and control. Subsequent VLA developments continue to refine these capabilities, addressing limitations in 3D understanding, reasoning, memory, and generalization, moving towards truly embodied and intelligent agents capable of interacting with the physical world [28].
### 1.3 Types of Literature Reviewed and Survey Methodology
The comprehensive academic survey on Vision-Language-Action (VLA) models synthesizes insights from a diverse range of scholarly works, employing a systematic methodology to ensure breadth and depth in its analysis. The literature encompassed includes foundational review papers, specialized surveys, meta-surveys or resource compilations, primary research articles introducing novel models, datasets, or evaluation methodologies, and comparative performance evaluations. The temporal span of this review primarily covers recent advancements, with particular attention to works published in the last three years, as highlighted by existing foundational surveys such as [25,28].

The inclusion criteria for this survey prioritize papers that directly contribute to the understanding, development, or evaluation of VLA models. This includes primary research presenting new model architectures (e.g., InstructVLA [5,14], MemoryVLA [31], DreamVLA [2], GraspVLA [10], AutoVLA [7], UAV-VLA [15], RT-2 [20], OpenVLA [11], DexVLA [29], ThinkAct [12], UniVLA [21]), novel datasets (e.g., VLA-IT [5,14], SynGrasp-1B [10], UAV-VLPA-nano-30 [15]), innovative training strategies (e.g., embodied curriculum learning for DexVLA [29], VLA-IT [5]), and new evaluation methodologies or benchmarks (e.g., SimplerEnv-Instruct [5,14], MultiNet v0.2 [4]). Papers focused solely on general Vision-Language Models (VLMs) or robotic control without explicit integration of both modalities for action generation were generally excluded unless they presented foundational concepts critical for VLA development. Non-academic publications, such as calls for proposals [24] or blog posts unless they explicitly announced a relevant survey, were excluded from direct analysis, though they occasionally informed the broader context, such as the announced "A Survey on Vision-Language-Action Models for Autonomous Driving" [13].

A critical aspect of this survey's methodology involves leveraging existing survey papers and meta-surveys to structure the analysis and identify major trends. Surveys like [6,26] provide a taxonomy-oriented review for large VLM-based VLA models in robotic manipulation, addressing inconsistencies and fragmentation in the field. Another survey offers a systematic analysis of 102 VLA models, categorizing architectures into End-to-End Unified, Modular Fusion, and Hierarchical Planning, alongside an evaluation of 26 datasets and 12 simulation platforms [35]. The foundational reviews [25,28] summarize advancements across five thematic pillars, including conceptual foundations, architectural innovations, and challenges like data efficiency and generalization. This survey complements and extends prior work, particularly for embodied AI, where previous surveys focused on other facets such as foundation models or LLMs in robotics [34].

Meta-survey resources, such as "awesome_vla_for_robotics_资源汇总" and "具身智能年度影响力论文盘点", play a crucial role in contextualizing individual paper contributions and identifying significant research directions. The "awesome_vla_for_robotics_资源汇总" acts as a comprehensive list of resources, categorizing papers by major application areas (e.g., Manipulation, Navigation, HRI, Autonomous Driving) and key technical contributions (e.g., Model Architectures, Action Representation & Generation, Learning Paradigms). This resource helps in identifying clusters of research activity and dominant approaches. For instance, models like RT-1, RT-2 [20], PaLM-E, and OpenVLA [11] are frequently highlighted in such compilations as influential primary research papers. The "具身智能年度影响力论文盘点" provides expert-curated lists of influential papers, offering an "unofficial ranking" that reflects the community's perception of impact and innovation.

The synthesis process involved identifying commonalities and contrasting differences across various studies. For example, a common thread across many primary research papers is the introduction of new model architectures aimed at unifying multimodal reasoning and action generation, often accompanied by novel datasets and evaluation benchmarks [5,14,29]. Significant differences emerge in architectural paradigms (End-to-End Unified, Modular Fusion, Hierarchical Planning [35]) and in how action tokens are represented and utilized, a critical aspect systematically reviewed in [27,32]. These surveys also distill the strengths and limitations of different action token types, identifying areas for improvement.

Debates and unique contributions in the literature often revolve around overcoming challenges such as data efficiency, generalization, and real-time performance. For instance, the development of FedVLA [23] introduces a novel privacy-preserving federated learning framework, addressing data-sharing challenges. The focus on zero-shot generalization capabilities, as evaluated by benchmarks like SimplerEnv-Instruct [5] and MultiNet v0.2 [4], underscores a critical challenge in VLA models. Furthermore, efforts to accelerate VLA models, such as the EfficientVLA framework [9], highlight the ongoing pursuit of practical and deployable solutions.

By integrating these diverse types of literature, this survey categorizes and analyzes the selected content based on core contributions (e.g., new models, datasets, benchmarks, training strategies, optimization techniques), application domains (e.g., robotic manipulation, autonomous driving, UAVs), and methodological approaches. This structured analysis enables a comprehensive understanding of the current state of VLA models, allowing for a critical appraisal of their strengths, weaknesses, and limitations, and ultimately identifying promising future research directions.
### 1.4 Scope and Organization of the Survey
This survey is meticulously designed to provide a comprehensive roadmap for Vision-Language-Action (VLA) models, systematically clarifying foundational concepts, summarizing developmental progress, identifying inherent challenges, and charting future research directions [25,28]. Its overarching goal is to integrate current understanding, critically appraise limitations, and propose viable solutions to foster the development of robust, efficient, and ethically sound VLA systems [25].

A key value proposition of this survey lies in its adoption of action tokenization as a central analytical framework [27,32]. This approach systematically categorizes and interprets existing VLA research by focusing on how action tokens are formulated and organized, which is identified as a primary distinguishing design choice among VLA models [27,32]. For instance, various approaches define action tokens as language descriptions, code, affordances, trajectories, goal states, latent representations, raw actions, or reasoning components [27,32]. By distilling the advantages and limitations of each token type, this framework offers a nuanced understanding of current VLA capabilities and pinpoints areas necessitating further innovation [32]. This perspective allows for a systematic outlook on the broader evolution of VLA models, highlighting underexplored directions and guiding research toward general-purpose intelligence [27].

The logical flow of this survey is structured to build a comprehensive understanding of VLA models, progressing from foundational principles to advanced applications and future trends. Initially, we will define VLA models and trace their historical evolution, emphasizing the critical role of multimodal integration mechanisms and language-based tokenization strategies [28]. This foundational section will also establish VLA as the next frontier, following the evolution of Large Foundation Models (LFMs), Vision Foundation Models (VFMs), and Vision-Language Models (VLMs) [32].

Subsequently, the survey will delve into core architectural components and underlying technologies, exploring various paradigms such as monolithic models (including single-system and dual-system designs) and hierarchical models that decouple planning from execution via interpretable intermediate representations [6,26,34]. This includes an examination of architectural innovations and data-efficient learning frameworks, alongside parameter-efficient modeling techniques and model acceleration methods aimed at improving performance while reducing computational costs [28]. For example, InstructVLA demonstrates a unified framework and two-stage training paradigm that allows general-purpose robots to accurately understand human instructions and execute complex physical actions, demonstrating a robust approach for intuitive and controllable robot operations [14].

Further sections will categorize existing VLA research through lenses such as core architectural modules, key model examples, and the properties of training data and simulation environments [35]. This includes a discussion of evaluation methodologies, as exemplified by papers that systematically compare models like OpenDriveVLA and AutoVLA using quantitative metrics to assess performance in autonomous driving contexts [17]. Specific applications, such as robotic manipulation [6,26,34] and autonomous driving [13,17], will be explored, highlighting how VLA models address critical challenges like VLM "hallucinations" in these domains [17].

The survey will then consolidate architectural traits, operational strengths, and the datasets and benchmarks supporting VLA model development [6,26]. It will address advanced domain integrations, including reinforcement learning, training-free optimization, learning from human videos, and world model integration [6]. Finally, we will critically analyze the major challenges impeding the evolution of VLA from specialized models to generalist agents, including inference bottlenecks, safety issues, high computational demands, limited generalization, and ethical implications [28,35]. Promising future research avenues, such as memory mechanisms, 4D perception, efficient adaptation, and multi-agent cooperation, will be identified to inspire subsequent investigations [6]. This structured progression ensures that readers gain a comprehensive and nuanced understanding of the VLA landscape, from its theoretical underpinnings to its practical implications and future potential.
## 2. Foundational Concepts and Core Components of VLA Models
Vision-Language-Action (VLA) models represent a rapidly evolving paradigm in artificial intelligence, designed to equip intelligent agents with the capability to perceive, comprehend, and interact with the physical world [28,32]. These models bridge the gap between digital intelligence and embodied interaction by jointly processing visual observations and natural language instructions to generate executable actions in dynamic environments [26,34]. The motivation behind the emergence of VLA models is rooted in the increasing demand for AI systems that can move beyond purely perceptual or conversational tasks to actively engage with and manipulate the physical world, positioning them firmly within the domain of Embodied AI [18,35]. A significant trend within this field is the development of 'large VLM-based' VLAs, which leverage the extensive knowledge embedded in pre-trained Vision-Language Models (VLMs) to enhance understanding, reasoning, and generalization capabilities for robotic action generation [26].

At their core, VLA architectures are predicated on three fundamental components: **Vision Encoders**, **Language Understanding Modules**, and **Action Decoders**, each contributing uniquely to the agent's ability to interpret human intent and execute tasks [25,30]. The genesis of these architectures is deeply influenced by the success of Large Language Models (LLMs) and VLMs, which serve as powerful backbones for processing multimodal data [22,34]. Vision encoders, acting as the agent's "eyes," process raw visual inputs (e.g., images, 3D point clouds, 4D spatiotemporal data) into meaningful feature representations, evolving from traditional Convolutional Neural Networks (CNNs) to Vision Transformers (ViTs) and advanced Vision Foundation Models (VFMs) like CLIP, DINOv2, and SAM [34,35]. These advancements enable richer scene understanding and spatial reasoning. Concurrently, Language Understanding Modules, often built upon powerful LLMs and VLMs such as Llama 2 or PaLM-E, interpret natural language commands, infer nuanced intent, and facilitate complex reasoning, including task decomposition and generating "reasoning tokens" or "cognitive tokens" that guide subsequent actions [30,31]. Finally, Action Decoders and Policies translate this integrated understanding into executable control signals for the embodied agent. These range from low-level joint commands to high-level movement primitives, employing diverse methodologies from autoregressive Transformers (e.g., RT-2) to diffusion-based policies, which offer smoother and more robust action generation [25,26].

A cornerstone of VLA model functionality lies in the **integration and alignment strategies** between these components. Techniques like cross-attention, embedding concatenation, and unified tokenization (where visual, language, and action data are treated as sequences within a single Transformer framework) are crucial for establishing a shared, semantically aligned latent space, facilitating tight semantic grounding and context-aware reasoning [20,26]. Architecturally, VLA models can adopt monolithic designs (single end-to-end model) or hierarchical designs (separating high-level reasoning from low-level execution), each with trade-offs in interpretability and seamless information flow [6,18].

Despite significant progress in model capabilities, the advancement and real-world deployment of VLA models are intrinsically linked to the capabilities and limitations of underlying robotic **hardware**. Challenges persist in achieving human-level dexterity, integrating sensitive multimodal sensors (e.g., tactile), and overcoming the diversity and isolation of robot embodiments [24,32]. Furthermore, the computational intensity of VLA models, especially for real-time inference on resource-constrained robotic platforms, necessitates continuous innovation in hardware-aware optimizations such as compiler-level graph rewriting, quantization techniques (e.g., INT8), and the development of specialized hardware accelerators for efficient processing [9,28].

VLA models represent a critical and specialized category within the broader field of **Embodied AI**, directly addressing the objective of developing generalist agents capable of operating and interacting intelligently within complex physical environments [18,30]. They enable agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic scenarios, as demonstrated in robotic manipulation, autonomous driving, and aerial robotics [15,26]. However, VLA models currently face limitations in achieving full generalization, abstraction, adaptability, and coordination in unstructured real-world settings, with most evaluations confined to simplified laboratory environments [18,32]. Overcoming data scarcity, improving cross-embodiment transferability, and bridging the gap between specialized instruction-following and open-ended interaction remain key challenges for the continued evolution of VLA models towards truly general-purpose embodied intelligence [29].
### 2.1 Defining Vision-Language-Action Models
Vision-Language-Action (VLA) models represent a burgeoning paradigm in artificial intelligence, designed to bridge the gap between digital intelligence and physical world interaction. While the core concept of integrating visual perception, language understanding, and action generation is widely acknowledged, the precise definition and characteristics of VLA models exhibit nuanced variations across the literature, reflecting the field's rapid evolution and diverse application domains [25,26,27,28,32,34].



![Defining Vision-Language-Action (VLA) Models](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/Gk-KiW47EV4lZm9IrDPyM_Defining%20Vision-Language-Action%20%28VLA%29%20Models.png)



![Defining Vision-Language-Action (VLA) Models](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/n5M6wTedfEUM0Q2_egisI_Defining%20Vision-Language-Action%20%28VLA%29%20Models.png)

A general definition of VLA models posits them as multimodal AI systems capable of jointly processing visual inputs, interpreting natural language, and generating executable actions in dynamic environments [25,28]. This foundational understanding underscores their role in enabling general-purpose agents to follow human instructions and perform tasks in complex, unstructured physical or digital settings [22,32]. Specifically, VLA models are characterized by their ability to interpret multimodal instructions, perform reasoning based on fused visual and linguistic cues, and ultimately generate actions [25,28]. These actions can range from low-level robot control signals to high-level symbolic plans or even specific output tokens for digital environments [27,28].

The motivation behind coining the term "VLA" and its emergence is primarily driven by the increasing need for AI systems that can actively interact with the physical world, moving beyond purely perceptual or conversational capabilities [18,35]. This positions VLA models firmly within the domain of Embodied AI, distinguishing them from generative AI models like ChatGPT which lack direct physical interaction [18,34]. The term gained significant prominence with the introduction of models such as RT-2, which were explicitly defined as Vision-Language-Action models, unifying pre-trained Vision-Language Model (VLM) reasoning with robot control capabilities [20,30,34]. This shift represented a critical advancement, extending the intelligence of digital foundation models into the physical world [27,32].

While the general definition provides a broad scope, a significant subset of VLA research focuses on 'large VLM-based' VLAs. These models are specifically defined as systems that leverage a large Vision-Language Model (VLM) to understand visual observations and natural language instructions, and subsequently perform reasoning processes that directly or indirectly serve robotic action generation [6,26]. A key characteristic of these models is their ability to exploit the vast knowledge embedded in large VLMs, pre-trained on extensive image-text datasets, to interpret complex commands and generalize to novel objects and environments [26,29]. Papers like InstructVLA explicitly aim to preserve the flexible reasoning of large VLMs while achieving high manipulation performance, addressing concerns such as "catastrophic forgetting" during fine-tuning on robotic data [5,14]. This distinction highlights an architectural reliance on pre-existing large-scale multimodal models as a backbone for embodied control.

Common characteristics of VLA models, whether general or VLM-based, include their end-to-end processing nature, aiming to integrate sensing, reasoning, and acting within a unified framework [8,25,28,35]. They typically consist of a visual module for scene understanding, a language module for instruction comprehension and reasoning, and an action module for generating specific movements [9]. Multimodal fusion techniques, such as cross-attention or concatenated embeddings, are crucial for aligning sensory observations with textual instructions, enabling semantic grounding, context-aware reasoning, and temporal planning [25,28]. This approach distinguishes them from traditional modular visuomotor pipelines, advocating for integrated processing [30,35].

Several papers emphasize the concept of "action tokens" as a distinguishing factor, where VLA models generate a chain of tokens that progressively encode grounded and actionable information, ultimately leading to executable actions [20,27]. This perspective highlights the importance of action representation and the interface between high-level reasoning and low-level control. For instance, UniVLA proposes a "task-centric latent action language" to unify action interpretation across diverse robot embodiments, addressing the limitation of varying physical forms and annotation costs [21]. Other specific applications, such as 3D-Generalist, define action generation in terms of outputting action code in a Domain-Specific Language for crafting 3D environments, showcasing the versatility of VLA beyond purely physical robotics [33].

In summary, VLA models are fundamentally defined by their ability to process visual and linguistic inputs to produce actions in embodied tasks. The critical distinction lies in the explicit role of large pre-trained Vision-Language Models (VLMs) within their architecture, particularly for robotic manipulation, where the VLM's knowledge and reasoning capabilities are leveraged and adapted for physical control. Common motivations include extending AI to the physical world, enabling general-purpose agents, and achieving end-to-end control. The ongoing research into "Vision-Language-Action model architecture design" signifies an active and important area, continuously refining how these core components are integrated and optimized [24]. While the broad definition encompasses various agents (physical robots, GUI agents, autonomous vehicles), the emphasis on leveraging large VLMs for robust reasoning and generalization in embodied interaction marks a significant trend in the field [6,7,19,22].
### 2.2 Core Architectural Elements and Modality Integration

![Core Architectural Elements and Modality Integration in VLA Models](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/zjOxKlgtYCDUye5skVP0C_Core%20Architectural%20Elements%20and%20Modality%20Integration%20in%20VLA%20Models.png)


![Core Architectural Elements and Modality Integration in VLA Models](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/pUO7euxW-063wS5Q5fLGP_Core%20Architectural%20Elements%20and%20Modality%20Integration%20in%20VLA%20Models.png)

Vision-Language-Action (VLA) models represent a paradigm shift in embodied AI, enabling intelligent agents to perceive, understand, and interact with the physical world through a confluence of visual observations, linguistic instructions, and generated actions. At their core, VLA architectures are predicated on three fundamental components: Vision Encoders, Language Understanding Modules, and Action Decoders, each playing a distinct yet interconnected role in translating abstract human intent into concrete physical behaviors [8,25,26,28,30,34]. This section provides a high-level overview of these components, their origins, and crucially, the diverse strategies employed to integrate and align them, setting the stage for a more detailed examination in the subsequent subsections.

The genesis of VLA architectures is deeply rooted in the remarkable successes of Large Language Models (LLMs) and Vision-Language Models (VLMs), which have demonstrated advanced capabilities in textual and multimodal reasoning respectively [20,22,34]. These pre-trained foundation models serve as powerful backbones, significantly influencing the design and capabilities of VLA systems.

**Vision Encoders** are responsible for state perception, acting as the "eyes" of the embodied agent. They process raw visual inputs—ranging from images and video frames to 3D point clouds and 4D spatiotemporal data—into meaningful feature representations [25,28,34]. These representations capture essential information about objects, scenes, spatial relationships, and environmental affordances, forming the basis for intelligent decision-making. Historically, Convolutional Neural Networks (CNNs) were dominant, but the field has rapidly embraced Vision Transformers (ViTs) and large Vision Foundation Models (VFMs) like CLIP, DINO/DINOv2, and SAM, due to their enhanced ability to capture global contexts and robust generalizable features from vast pre-training datasets. The evolution towards 3D and 4D spatial encoders further underscores the need for richer, more comprehensive environmental understanding, particularly in complex domains like autonomous driving and robotic manipulation.

**Language Understanding Modules**, often built upon powerful LLMs and VLMs, serve as the "ears and thinking" component, interpreting natural language instructions and facilitating complex reasoning. Their primary role is to process human commands, infer nuanced intent, and translate abstract directives into concrete, actionable plans [30,35]. This involves capabilities such as decomposing high-level tasks into subtasks, comprehending contextual information, and leveraging vast semantic knowledge acquired during large-scale pre-training. Architecturally, various transformer-based designs (encoder-only, decoder-only, encoder-decoder) are employed, reflecting a progression from simple linguistic processing to sophisticated reasoning mechanisms, including "Chain-of-Thought" processes and the generation of "reasoning tokens" or "cognitive tokens" that guide subsequent actions [7,26,29,31].

**Action Decoders and Policies** are tasked with generating actions, effectively serving as the agent's "limbs and dexterity." They translate the integrated visual and linguistic understanding into executable control signals or sequences for the embodied agent. These can range from low-level joint commands to high-level movement primitives [25,28,30]. Key methodologies include autoregressive Transformers (e.g., RT series), which generate actions token by token, and diffusion-based policies (e.g., in MemoryVLA and DreamVLA), which leverage iterative denoising for smoother, more robust action generation and inherent multi-modality [2,25,31,35]. A recurring theme is the hierarchical decoupling of high-level planning (often mediated by LLMs) from low-level motion control, where VLA models often output high-level instructions that are then translated into precise movements by independent controllers [6,12,18]. The evolution in action decoding also highlights a debate regarding the optimal level of action tokenization, balancing the leveraging of VLM generalizability with the need for precise and reliable physical control [17].

The cornerstone of VLA model functionality lies in the **integration and alignment strategies** between these core components. The goal is to embed visual observations and linguistic instructions into a shared, semantically aligned latent space, facilitating tight semantic grounding and reducing inter-module semantic drift [26]. Common mechanisms for achieving this multimodal fusion include:
1.  **Cross-attention**: Enabling different modalities to attend to each other's features, allowing for context-aware fusion [25,28,35].
2.  **Embedding Concatenation**: Directly combining the feature vectors from different encoders, often followed by projection layers to align their dimensions [29].
3.  **Unified Tokenization**: Treating visual, language, and even action data as sequences of tokens within a single Transformer framework, allowing for end-to-end processing and shared representations [20,25,26]. This approach, epitomized by models like RT-2, extends the VLM's inherent semantic understanding directly to action generation by treating robot actions as another form of text token [20].

Different VLA models conceptualize this integration in varying ways, often falling into monolithic or hierarchical architectural paradigms [6]. Monolithic designs aim for a single end-to-end model that processes all modalities and directly outputs actions, often leveraging large pre-trained VLMs as a unified backbone [5,14]. Hierarchical designs, conversely, explicitly separate high-level reasoning and planning (e.g., generating subtasks or intermediate representations) from low-level execution [6,12,18]. This modularity can enhance interpretability and allow for specialized components, although it introduces challenges in seamless information flow between layers. Notably, reasoning is often integrated as an explicit intermediate step, informing action generation, whether through LLM-induced chain-of-thought, visual reasoning, or multimodal reasoning before action planning [7,14,25,28,34].

Across all components, a significant challenge and ongoing area of research is **computational efficiency**. VLA models inherently deal with vast amounts of multimodal data, leading to high computational loads. Innovations such as efficient visual token filtering (EfficientVLA), optimized language module architectures (EfficientVLA), action chunking, parallel decoding, and specialized compression techniques like FAST for action generation, are actively being explored to mitigate these bottlenecks while preserving or enhancing performance [9]. The continuous refinement of these core architectural elements and their integration strategies is pivotal for advancing the capabilities of VLA models toward more generalized, robust, and efficient embodied intelligence.
#### 2.2.1 Vision Encoders

**Evolution and Types of Vision Encoders in VLA Models**

| Type          | Description                                                    | Key Examples / Models                                                                      |
|---------------|----------------------------------------------------------------|--------------------------------------------------------------------------------------------|
| **Traditional CNNs** | Capture local features & hierarchical patterns; foundational.       | ResNet (TransDiffuser, R3M, BCZ), EfficientNet (RT-1)                                      |
| **Vision Transformers (ViT)** | Treat images as sequences; capture global context, advanced scene understanding. | VIMA, Octo, MVP, VC-1, Voltron, RPT, I-JEPA, Theia, DexVLA                               |
| **Vision Foundation Models (VFMs)** | Pre-trained on vast internet-scale data for robust, generalizable features. | CLIP, SigLIP (CLIPort, EmbCLIP, CoW, OpenVLA, MemoryVLA, RDT), DINO/DINOv2 (UniVLA, InstructVLA, OpenVLA, MemoryVLA), SAM/SAM 2, OWL-ViT |
| **3D/4D Spatial Encoders** | Go beyond 2D to incorporate richer spatial & temporal information for 3D/4D scenes. | OpenDriveVLA (BEV), PointNet++ (SpatialVLA, BridgeVLA), 3D-Generalist, 3D-VLA           |
| **Specialized/Optimized** | Address specific needs like efficiency or multimodal fusion.     | Theia (distills multiple VFMs), EfficientVLA (visual token filtering)                     |



**Evolution and Types of Vision Encoders in VLA Models**

| Type          | Description                                                    | Key Examples / Models                                                                      |
|---------------|----------------------------------------------------------------|--------------------------------------------------------------------------------------------|
| **Traditional CNNs** | Capture local features & hierarchical patterns; foundational.       | ResNet (TransDiffuser, R3M, BCZ), EfficientNet (RT-1)                                      |
| **Vision Transformers (ViT)** | Treat images as sequences; capture global context, advanced scene understanding. | VIMA, Octo, MVP, VC-1, Voltron, RPT, I-JEPA, Theia, DexVLA                               |
| **Vision Foundation Models (VFMs)** | Pre-trained on vast internet-scale data for robust, generalizable features. | CLIP, SigLIP (CLIPort, EmbCLIP, CoW, OpenVLA, MemoryVLA, RDT), DINO/DINOv2 (UniVLA, InstructVLA, OpenVLA, MemoryVLA), SAM/SAM 2, OWL-ViT |
| **3D/4D Spatial Encoders** | Go beyond 2D to incorporate richer spatial & temporal information for 3D/4D scenes. | OpenDriveVLA (BEV), PointNet++ (SpatialVLA, BridgeVLA), 3D-Generalist, 3D-VLA           |
| **Specialized/Optimized** | Address specific needs like efficiency or multimodal fusion.     | Theia (distills multiple VFMs), EfficientVLA (visual token filtering)                     |

Vision encoders constitute a fundamental component within Vision-Language-Action (VLA) models, serving as the initial gateway for processing raw visual inputs and extracting meaningful features essential for subsequent language understanding, action planning, and execution [8,25,28,30,34]. These encoders transform diverse visual modalities, including camera images, video frames, RGB-D frames, LiDAR point clouds, and even satellite imagery, into compact and informative representations [8,15,25,28,30]. Their primary role encompasses tasks such as object recognition, scene understanding, spatial reasoning, object localization, and perceiving object attributes like color and ripeness, directly influencing overall VLA performance by providing critical information about the current state and affordances of the environment [18,25,28,34,35].

Historically, Convolutional Neural Networks (CNNs) have been a cornerstone for deep feature extraction from raw pixels in VLA models, exemplified by architectures like ResNet (e.g., ResNet34 in TransDiffuser, ResNet-50 in R3M, BCZ, VIP, DINOv2) and EfficientNet (e.g., FiLM-conditioned EfficientNet in RT-1) [8,17,27,34,35]. These models are adept at capturing local features and hierarchical patterns within images. However, the emergence of Vision Transformers (ViT) has marked a significant shift, offering enhanced capabilities, particularly in understanding complex scenes and object interactions [35]. ViT-based architectures, by treating images as sequences of visual tokens or patches, overcome limitations of traditional CNNs in capturing global visual relationships [32,35]. This allows VLA models to discern higher-level contextual information, such as "apple on the plate" or "kitchen countertop," which is vital for sophisticated tasks [35]. Many contemporary VLA models, including VIMA, Octo, MVP, VC-1, Voltron, RPT, I-JEPA, Theia, and DexVLA, leverage ViT as their primary visual backbone, or in conjunction with CNNs [27,29,34]. The choice of ViT also facilitates easier alignment with the Transformer-based architectures commonly used in subsequent VLM components due to their shared attention mechanism [29].

Beyond foundational CNN and ViT architectures, advancements in visual representation learning have led to the widespread adoption of powerful Vision Foundation Models (VFMs) as vision encoders, significantly contributing to VLA performance. These models, often pre-trained on vast internet-scale image-text datasets, provide robust and generalizable representations:
*   **CLIP (Contrastive Language-Image Pretraining)** and its successor **SigLIP** are widely adopted for their ability to learn robust, generalizable image representations through natural language supervision on massive datasets, demonstrating impressive zero-shot transfer capabilities and excelling in visual-language alignment [25,28,30,32,34]. CLIP, often using a ViT-B backbone, is integrated into models like CLIPort, EmbCLIP, and CoW [34]. SigLIP is frequently utilized, sometimes in a frozen state, to maintain semantic consistency established during pre-training, as seen in RDT and in dual-encoder setups for OpenVLA and MemoryVLA [11,16,19,27,30,31].
*   **DINO/DINOv2** learn rich, general-purpose visual features through self-supervised learning, proving effective for fine-grained tasks like semantic segmentation and depth estimation. They capture in-depth semantic understanding, matching similar regions across different objects [32]. DINOv2 is a prominent choice, used in UniVLA to transform raw pixels into patch-level semantic features, and in InstructVLA, where it is augmented with a FiLM module for improved spatial perception and context alignment [14,32,34]. Like SigLIP, DINOv2 is often employed in a frozen state to preserve pre-trained semantic consistency [28]. OpenVLA and MemoryVLA notably fuse features from DINOv2 and SigLIP, with DINOv2 often capturing geometric and spatial relationships while SigLIP handles visual-language alignment [11,19,31].
*   **SAM/SAM 2 (Segment Anything Model)** provide promptable segmentation capabilities, enabling the generation of valid segmentation masks based on various prompts [32]. Grounded SAM is used for fixture segmentation in 3D environment generation [33].
*   **OWL-ViT** is utilized for extracting object representations and grounding language instructions into object-centric visual inputs [32].
Other notable specialized encoders include R3M, MVP, VIP, VC-1, Voltron, RPT, and I-JEPA, which focus on aspects like temporal relationships in videos, masked autoencoding (MAE) for reconstruction, and language-vision alignment through specific pretraining objectives [34]. Theia stands out by distilling information from multiple VFMs (ViT, CLIP, SAM, DINOv2, Depth-Anything) into a single model, fusing segmentation, depth, and semantics to achieve superior performance with less data and smaller models [34].

A significant trend in vision encoders for VLA models is the move beyond 2D image processing to incorporate richer spatial and temporal information. This includes the use of 3D spatial encoders to map multi-view images or point clouds into a unified 3D feature space, enhancing understanding of 3D scenes [8]. Examples include OpenDriveVLA's Bird's Eye View (BEV) perception frontend for structured 3D environmental understanding, and the processing of "3D perception" in models capable of predicting "goal images and point clouds" [3,17]. Research efforts, including calls for proposals, emphasize "3D and computer vision" and "3D human-object-environment reconstruction and understanding" for embodied AI [24]. Advanced models incorporate PointNet++ for encoding object-centric 3D point clouds, or reconstruct 3D point clouds from RGB-D images and fuse them with 2D features, as seen in SpatialVLA and BridgeVLA [26]. Furthermore, 4D perception integrates spatiotemporal reasoning by overlaying sampled motion-point trajectories (TraceVLA) or integrating 3D coordinates into visual features with memory banks for historical keyframes (4D-VLA, ST-VLA) [26]. These advancements empower VLA models with stronger capabilities in handling complex road environments, pedestrian identification, object tracking, and grasping tasks [8,10].

Many prominent VLA models leverage these advanced vision encoders. RT-2 utilizes the vision encoders embedded within large pre-trained Vision-Language Models (VLMs) like PaLI-X and PaLM-E, which have learned rich visual features from billions of image-text pairs [16,20]. Similarly, models aspiring to "GPT-4V level capabilities" or serving as "pioneering open-source alternatives to GPT-4o" (e.g., LLaVA, InternVL Family) inherently rely on sophisticated vision encoders for high-level visual understanding [22]. UniVLA analyzes frame-by-frame video changes to quantify motion and filter background noise, while EfficientVLA addresses the computational burden by employing a visual token filtering strategy, reducing visual processing workload by 78% by selecting only task-relevant and diverse tokens from an initial 256 to 56 [9,21]. This highlights an ongoing challenge in vision encoders: efficiently processing vast visual information while preserving essential details for decision-making. Future directions also involve encoding other sensory inputs like tactile and auditory features, aligning them with visual sequences for multimodal fusion in models like VTLA and VLAS [26]. The continuous evolution of vision encoders, from traditional CNNs to advanced self-supervised and multimodal Transformer-based architectures, is central to unlocking greater capabilities in VLA models for embodied intelligence.
#### 2.2.2 Language Understanding Modules

**Role and Types of Language Understanding Modules in VLA Models**

| Aspect                | Description                                                                    | Key Examples / Models                                                                                                           |
|-----------------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|
| **Primary Function**  | Process natural language commands, infer intent, map to context, perform reasoning for action generation/policy formulation. | Interpret abstract commands ("tidy desk"), fine-grained directives ("left of plate"), contextual instructions (driving).        |
| **Core Capabilities** | Task decomposition, semantic understanding, contextual comprehension, reasoning (Chain-of-Thought). | "Identify apple -> identify red plate -> plan grasping trajectory" (decomposition).                                            |
| **Architectures (LLM/VLM)** | Transformer-based models, leveraging scale and pre-training.                     | Llama 2 (OpenVLA, MemoryVLA), PaLI-X, PaLM-E (RT-2), LLaMA (MemoryVLA), Gato, T5 (VIMA, Octo, RDT), BERT (DistilBERT in DexVLA) |
| **Reasoning Mechanisms** | Break down problems, generate intermediate plans, guide actions, contextual understanding. | Chain-of-Thought (3D-Generalist, AutoVLA), Reasoning Tokens (DexVLA), Cognitive Tokens (MemoryVLA), Embodied Reasoning Plans (ThinkAct) |
| **Optimizations**     | Enhance efficiency and reduce redundancy.                                        | EfficientVLA (redundant layer removal, 41% param reduction), Visual Instruction Tuning (LLaVA)                                  |



**Role and Types of Language Understanding Modules in VLA Models**

| Aspect                | Description                                                                    | Key Examples / Models                                                                                                           |
|-----------------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|
| **Primary Function**  | Process natural language commands, infer intent, map to context, perform reasoning for action generation/policy formulation. | Interpret abstract commands ("tidy desk"), fine-grained directives ("left of plate"), contextual instructions (driving).        |
| **Core Capabilities** | Task decomposition, semantic understanding, contextual comprehension, reasoning (Chain-of-Thought). | "Identify apple -> identify red plate -> plan grasping trajectory" (decomposition).                                            |
| **Architectures (LLM/VLM)** | Transformer-based models, leveraging scale and pre-training.                     | Llama 2 (OpenVLA, MemoryVLA), PaLI-X, PaLM-E (RT-2), LLaMA (MemoryVLA), Gato, T5 (VIMA, Octo, RDT), BERT (DistilBERT in DexVLA) |
| **Reasoning Mechanisms** | Break down problems, generate intermediate plans, guide actions, contextual understanding. | Chain-of-Thought (3D-Generalist, AutoVLA), Reasoning Tokens (DexVLA), Cognitive Tokens (MemoryVLA), Embodied Reasoning Plans (ThinkAct) |
| **Optimizations**     | Enhance efficiency and reduce redundancy.                                        | EfficientVLA (redundant layer removal, 41% param reduction), Visual Instruction Tuning (LLaVA)                                  |

The efficacy of Vision-Language-Action (VLA) models hinges critically on their capacity for robust language understanding, predominantly facilitated by Large Language Models (LLMs) and Vision-Language Models (VLMs). These modules serve as the core interpreters of natural language instructions, enabling VLA systems to translate abstract human intent into concrete, actionable steps [30,35].

A primary function of these language understanding modules is to process natural language commands, mapping them to the current context and performing intricate reasoning for action generation or high-level policy formulation [6,30]. This involves not only understanding human speech (e.g., "take the red cup to the kitchen") but also inferring the nuanced intent behind instructions, such as distinguishing "take" from "push" [18]. LLMs empower VLA models to interpret complex human instructions, encompassing both abstract commands like "tidy the desk" and fine-grained directives such as "place the cup to the left of the plate" [35]. Furthermore, they are crucial for comprehending contextual instructions in domains like autonomous driving, for instance, "turn right at the second traffic light ahead" or "reduce speed to below 5 km/h when pedestrians are detected" [8]. The ability to decompose high-level instructions into executable subtask sequences (e.g., "identify apple -> identify red plate -> plan grasping trajectory") is a key contribution of LLMs to VLA planning capabilities [34,35].

The effectiveness of language understanding in VLA models is profoundly impacted by the scale and pre-training of the underlying LLMs. Models such as Llama 2 (e.g., 7 billion parameters in OpenVLA) [11,19], PaLI-X (up to 55 billion parameters), and PaLM-E (12 billion parameters) [20] leverage vast semantic knowledge acquired from pre-training on extensive text and image-text datasets [6,20]. This large-scale pre-training allows them to exhibit emergent capabilities in symbolic understanding and reasoning, essential for guiding robot actions [20]. Techniques like Visual Instruction Tuning, exemplified by LLaVA, further enhance LLM flexibility by converting image-text pairs into conversational samples, enabling comprehension of abstract or open-ended instructions and generalization to novel multimodal scenarios [22,26].

Architecturally, VLA models incorporate various types of transformer-based language models. Encoder-only architectures, such as BERT variants like DistilBERT used for language embedding extraction during pre-training [29], excel at deep contextual understanding of input text. Decoder-only models, including LLaMA (e.g., LLaMA-7B in MemoryVLA, Llama 2 in OpenVLA) and Gato (1.2 billion parameters), are particularly strong in text generation and sequential processing [11,31]. Encoder-decoder architectures, exemplified by T5 (used in VIMA, Octo, and RDT), combine the strengths of both, offering robust capabilities for both understanding and generating language tokens, which is critical for complex VLA tasks [27]. The evolution from recurrent neural networks to these Transformer architectures has significantly advanced NLP capabilities within VLA systems [34].

Beyond fundamental language processing, LLMs contribute significantly to reasoning capabilities. The "Chain-of-Thought" approach, integrated into models like 3D-Generalist and AutoVLA, allows LLMs to break down problems into sequential steps, generating pseudocode or intermediate reasoning plans before actual action code [7,33]. This shift from purely reactive control to deliberative decision-making is further evidenced by models generating "reasoning tokens" that act as implicit high-level policies, guiding action generation for long-horizon tasks [26,29]. ThinkAct, for instance, employs a multimodal LLM specifically trained to generate "embodied reasoning plans" from multimodal instructions [12]. MemoryVLA introduces a "cognitive token" at the end of a sequence, which encapsulates the robot's high-level understanding of the task's context and semantic intent, leveraging the LLM's pre-trained knowledge [31].

While the integration of LLMs offers substantial advantages, research also explores optimizations and specialized applications. EfficientVLA, for example, addresses potential redundancies in language modules by identifying and removing layers that do not significantly alter information content. This optimization has been shown to reduce a language module's parameters by 41% and computational load by 78%, while maintaining or improving task execution [9]. Other specialized applications include PaLM-E's method of injecting continuous sensor data into a pre-trained LLM for sequential manipulation planning [30] and 3D-VLA's use of a 3D-based LLM for planning in embodied environments by interacting with a generative world model [3]. The continuous development of advanced LLMs like GPT-4o and GPT-4.1, often integrated with vision, underscores the ongoing pursuit of "GPT-4V level capabilities" in VLA systems [4,22].

In summary, language understanding modules, primarily built upon diverse LLM and VLM architectures, are indispensable for VLA models. They facilitate the nuanced interpretation of human instructions, bridge the gap between abstract commands and concrete actions, and provide critical reasoning capabilities that enable complex task planning and execution. The scale, pre-training, and architectural design of these models significantly determine their efficacy in translating human intent into effective robot behaviors.
#### 2.2.3 Action Decoders and Policies

**Action Decoding Strategies and Efficiency in VLA Models**

| Strategy / Type         | Description                                                                     | Key Examples / Models                                                                        | Optimizations / Features                                                     |
|-------------------------|---------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|
| **Autoregressive Transformers** | Generate action sequences token by token, often treating actions as text tokens. | RT-1, RT-2, OpenVLA, UniVLA (latent actions), DecisionTransformer, TrajectoryTransformer, Gato | Prone to error accumulation, can be slow. UniVLA: 10 Hz inference (12-step). |
| **Diffusion-Based Policies** | Iterative denoising process to model conditional action distributions.           | CogACT, GR00T N1, π0π0, GR-3, DexVLA, SkillDiffuser, TinyVLA, EnerVerse, InstructVLA, MemoryVLA, GraspVLA | Smoother, robust actions; inherent multi-modality.                            |
| **MLP/Token Predictors** | Simpler direct regression or classification for actions.                        | RoboFlamingo (independent MLP head), BCZ                                                     | Often for direct, independent action generation.                             |
| **LLMs as Decoders**    | Fine-tuned LLMs for language-conditioned robotic tasks.                         | (General category, specific models vary)                                                     | Leveraging pre-trained language understanding.                               |
| **Hierarchical Decoupling** | High-level planning (LLM) generates intermediate representations; low-level policy executes. | Groot N1, HAMSTER (trajectory keypoints), ReKep (keypoint optimization), A0A0 (contact points), 3D-VLA (goal images/point clouds) | Improves interpretability, modularity, safety.                               |
| **Efficiency Optimizations** | Reduce computational load and latency for real-time control.                    | Action Chunking (CogACT, π0, 50 Hz control), Parallel Decoding (GR00T N1, 100-120 Hz control, 2.5x faster), Early-Exit Decoding, Compressed Action Tokenization (FAST, 15x accel, 200 Hz), EfficientVLA (caching, 80% reduction) | Critical for real-time deployment and high-frequency control.                |



**Action Decoding Strategies and Efficiency in VLA Models**

| Strategy / Type         | Description                                                                     | Key Examples / Models                                                                        | Optimizations / Features                                                     |
|-------------------------|---------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|
| **Autoregressive Transformers** | Generate action sequences token by token, often treating actions as text tokens. | RT-1, RT-2, OpenVLA, UniVLA (latent actions), DecisionTransformer, TrajectoryTransformer, Gato | Prone to error accumulation, can be slow. UniVLA: 10 Hz inference (12-step). |
| **Diffusion-Based Policies** | Iterative denoising process to model conditional action distributions.           | CogACT, GR00T N1, π0π0, GR-3, DexVLA, SkillDiffuser, TinyVLA, EnerVerse, InstructVLA, MemoryVLA, GraspVLA | Smoother, robust actions; inherent multi-modality.                            |
| **MLP/Token Predictors** | Simpler direct regression or classification for actions.                        | RoboFlamingo (independent MLP head), BCZ                                                     | Often for direct, independent action generation.                             |
| **LLMs as Decoders**    | Fine-tuned LLMs for language-conditioned robotic tasks.                         | (General category, specific models vary)                                                     | Leveraging pre-trained language understanding.                               |
| **Hierarchical Decoupling** | High-level planning (LLM) generates intermediate representations; low-level policy executes. | Groot N1, HAMSTER (trajectory keypoints), ReKep (keypoint optimization), A0A0 (contact points), 3D-VLA (goal images/point clouds) | Improves interpretability, modularity, safety.                               |
| **Efficiency Optimizations** | Reduce computational load and latency for real-time control.                    | Action Chunking (CogACT, π0, 50 Hz control), Parallel Decoding (GR00T N1, 100-120 Hz control, 2.5x faster), Early-Exit Decoding, Compressed Action Tokenization (FAST, 15x accel, 200 Hz), EfficientVLA (caching, 80% reduction) | Critical for real-time deployment and high-frequency control.                |

Action decoders and policies in Vision-Language-Action (VLA) models serve as the critical interface between abstract multimodal understanding and concrete physical execution, translating fused visual, linguistic, and contextual features into actionable control signals or sequences for embodied agents [6,25,30,32,34,35]. These actions can range from low-level control commands, such as joint angle updates, torque values, or end-effector velocities, to higher-level movement primitives and trajectories [25,28,30]. The sophistication of these decoders directly impacts a robot's dexterity, precision, and ability to operate within physical constraints.

A primary dichotomy in action generation methodologies is between **autoregressive Transformers** and **diffusion-based policies** [25,35]. Autoregressive Transformers, exemplified by models like the RT series (RT-1, RT-2) and OpenVLA, generate action sequences token by token [25,26]. RT-2 uniquely casts robot actions as text tokens and integrates them into the same training corpus as natural language, effectively treating robot control as another language generation task [16,25,26]. OpenVLA, while employing autoregressive decoding, has also been compared against diffusion policies, indicating a performance distinction [11]. UniVLA, a Transformer-based model, generates sequences of "task-centric latent actions," acting as a "conductor" for robot movements by predicting subsequent latent actions based on integrated image features, language instructions, and learned discrete action "notes" [21]. The stepwise prediction mechanism of autoregressive models can, however, be prone to error accumulation over long sequences and may yield less smooth action trajectories compared to generative methods [35].

In contrast, **diffusion-based policies**, often implemented as Diffusion Transformers (DiT), leverage an iterative denoising process to model the conditional distribution over future actions [2,25,31,35]. This iterative mechanism helps generate smoother, more noise-resistant action trajectories and intrinsically captures the multi-modality inherent in robot action spaces, producing diverse and plausible actions rather than a single average output [31,35]. Prominent examples include CogACT and GR00T N1, which adopt DiT as low-level action models in dual-system architectures for faster policy steps and adaptive action ensemble algorithms [26]. Other notable diffusion models include π0π0 and GR-3, which utilize flow-matching architectures for strong zero-shot generalization and unifying vision-language understanding with trajectory learning [16,26]. DexVLA features a VLM planner paired with a diffusion-based action policy for complex tasks [26], while SkillDiffuser employs a low-level diffusion policy to realize actions from subtasks [26]. Similarly, TinyVLA and EnerVerse integrate diffusion policy decoders for precise robot actions and real-time prediction [16]. InstructVLA, an end-to-end VLA model, employs a Transformer-based action expert module supervised by a flow matching mechanism to translate high-level features into precise control signals [14]. MemoryVLA further enhances diffusion with a DiT conditioned on memory-augmented tokens, incorporating dual attention layers (cognitive and perceptual) to guide action planning and precise control during a 10-step DDIM denoising process [31]. GraspVLA also employs flow-matching-based action generation for robotic grasping [10].

Beyond these dominant paradigms, other approaches exist. Some models use simpler **MLP/Token Predictors** (e.g., RoboFlamingo's independent MLP action head generating stacked actions in one pass, or BCZ using an MLP [26,27]) or **direct regression** to continuous control signals, though often implicitly [26]. The adaptation of **Large Language Models (LLMs) as decoders**, fine-tuned on robot data, represents another strategy for language-conditioned robotic tasks [34]. Furthermore, **Reinforcement Learning Transformers** like DecisionTransformer (DT) and TrajectoryTransformer (TT) model Markov Decision Processes as autoregressive sequential data, providing a foundation for action generation, with Gato extending this to multi-modal/task/embodiment settings [27,34].

To ensure that generated actions comply with robotic kinematics and physical constraints, decoders translate abstract intentions into precise, executable movements such as SE(3) end-effector poses, 6-dimensional hand joint spaces, or 14-dimensional bimanual configurations [29,31]. In autonomous driving, this involves generating specific control commands like steering angle, acceleration, and braking pressure [8]. For UAVs, path planning algorithms like Dynamic Time Warping (DTW) and K-Nearest Neighbors (KNN) are used to convert geographical coordinates and task goals into flight paths [15]. A common observation is that VLA models often output high-level commands, with the detailed, low-level motion planning for joint torques or precise trajectories typically handled by separate, independent controllers [18]. This **hierarchical decoupling** is seen in models where a high-level planner generates interpretable intermediate representations (e.g., keypoints, subtasks, programs), and a low-level policy then accepts these representations to generate executable action sequences [26,32]. Examples include HAMSTER predicting trajectory keypoints, ReKep using DINOv2 and SAM for keypoint proposals, and A0A0 predicting contact points as embodiment-agnostic affordance representations [26]. Motion Planning Modules are frequently used to convert high-level action tokens into raw actions, often involving constraint optimization or path planning [32].

The evolution in action decoding strategies has progressed from simpler tokenization methods to highly sophisticated approaches. Early action tokenization often involved discretizing continuous action spaces into a fixed set of tokens for autoregressive prediction [30]. This has advanced to more complex methods like **diffusion policies**, which have gained prominence for their ability to model complex action distributions [30]. Advanced variants include the billion-parameter **Scale Diffusion Policy (ScaleDP)** in DexVLA, which features a multi-head architecture for effective cross-embodiment learning across diverse robot morphologies [29]. Similarly, RDT-1B scales the Diffusion Transformer head to 1 billion parameters specifically for bimanual manipulation [16,27], and GR00T N1 employs a 0.86 billion parameter diffusion transformer for real-time motor control [27]. In 3D-VLA, embodied diffusion models are aligned with a 3D-based LLM to predict goal images and point clouds, which then guide concrete control signals as a goal-conditioned policy [3].

However, these sophisticated methods introduce trade-offs in complexity, computational cost, and performance. While diffusion models offer smoother trajectories and better handling of stochasticity, their iterative denoising process can be computationally intensive [35]. To mitigate this, significant innovations in efficiency have emerged:
*   **Action Chunking**, seen in CogACT and π0, abstracts multi-step operations into single tokens, reducing inference steps [28,30]. π0 significantly improves control frequency to 50 Hz, a substantial leap from RT-2's 5 Hz [27].
*   **Parallel Decoding architectures**, such as in GR00T N1, predict multiple action tokens simultaneously, reducing end-to-end latency by 2.5x for 7-DOF robot arms at 100 Hz control [28]. GR00T N1 achieves a fast 120 Hz control frequency [27].
*   **Early-Exit Decoding** further enhances efficiency [30].
*   **Compressed action tokenization (FAST)** converts continuous action outputs into frequency-domain tokens, achieving a 15x inference acceleration for a 300M parameter diffusion head and supporting 200 Hz high-frequency control with minimal precision loss [28].
*   **EfficientVLA** introduces a caching mechanism to reuse computational results during diffusion-based action generation, reducing computational load by 80% while preserving accuracy [9]. UniVLA achieves 10 Hz inference by outputting a 12-step action sequence in one go [21].

A debate exists regarding the optimal level of action tokenization. While RT-2 tokenizes actions as text, OpenDriveVLA and AutoVLA propose action tokenization, with AutoVLA using a "codebook," to mitigate VLM hallucinations and ensure more reliable action generation [17]. This highlights a tension between leveraging general language capabilities of VLMs for action generation and ensuring the precision and reliability required for physical robot control. Overall, the field is moving towards more sophisticated, efficient, and robust action decoding strategies that can translate diverse multimodal inputs into precise, physically compliant, and dexterous robot movements across a wide range of tasks and embodiments. The continuous exploration of hierarchical architectures, advanced diffusion models, and computational optimizations underscores the importance of efficient and accurate action generation in the advancement of VLA models for embodied AI [24].
### 2.3 Hardware Considerations for Embodied VLA

**Hardware Challenges and Solutions for Embodied VLA Models**

| Category           | Challenges                                                                        | Solutions / Trends                                                                                              |
|--------------------|-----------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| **Robotic Hardware** | Lack of human-level dexterity (gripper-based limits), lack of sensitive multimodal sensors (tactile), diversity & isolation of embodiments, semantic-physical mismatch (e.g., force). | Unified interfaces (UMI, UniVLA for cross-embodiment), advanced sensory integration (tactile, force, audio), general-purpose robots (Franka, WidowX250), human-robot skill transfer (HumanPlus). |
| **Computational Hardware** | Computational intensity (high-dim embeddings, real-time inference), latency, memory usage, resource-constrained robotic platforms. | Compiler-level graph rewriting (TensorRT-LLM), quantization (INT8), specialized hardware accelerators (sparse tensor ops, fused kernels), 'TinyVLA' models, efficient VLA frameworks (EfficientVLA). |
| **Data Acquisition** | High cost & labor-intensive real-world data collection (teleoperation).           | Synthetic data generation, sim-to-real transfer techniques, human demonstrations, shared datasets (Open X-Embodiment). |



**Hardware Challenges and Solutions for Embodied VLA Models**

| Category           | Challenges                                                                        | Solutions / Trends                                                                                              |
|--------------------|-----------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| **Robotic Hardware** | Lack of human-level dexterity (gripper-based limits), lack of sensitive multimodal sensors (tactile), diversity & isolation of embodiments, semantic-physical mismatch (e.g., force). | Unified interfaces (UMI, UniVLA for cross-embodiment), advanced sensory integration (tactile, force, audio), general-purpose robots (Franka, WidowX250), human-robot skill transfer (HumanPlus). |
| **Computational Hardware** | Computational intensity (high-dim embeddings, real-time inference), latency, memory usage, resource-constrained robotic platforms. | Compiler-level graph rewriting (TensorRT-LLM), quantization (INT8), specialized hardware accelerators (sparse tensor ops, fused kernels), 'TinyVLA' models, efficient VLA frameworks (EfficientVLA). |
| **Data Acquisition** | High cost & labor-intensive real-world data collection (teleoperation).           | Synthetic data generation, sim-to-real transfer techniques, human demonstrations, shared datasets (Open X-Embodiment). |

The advancement of Vision-Language-Action (VLA) models is intrinsically linked to the capabilities and limitations of underlying robotic hardware, underscoring a fundamental "Triad of Progress: Model, Data, and Hardware" [32]. While VLA models demonstrate increasing sophistication, the realization of general-purpose embodied agents remains constrained by the current state of robotic platforms, particularly in areas of dexterity, sensing, and computational efficiency.

Several studies highlight the practical applicability of VLA models on standard robotic hardware. For instance, InstructVLA and MemoryVLA have been successfully deployed and validated through real-world experiments on common manipulators such as the WidowX250 and Franka robots [14,31]. Other models, including DreamVLA and InstructVLA, also demonstrate applicability to physical systems in real-world environments and tasks [2,5]. DexVLA further showcases compatibility with diverse robot morphologies, including Franka with grippers or dexterous hands, and bimanual UR5e and AgileX systems, which implies varying control policy complexities and challenges in cross-embodiment learning [29]. Efforts like the Universal Manipulation Interface (UMI) and HumanPlus focus on providing unified hardware and software systems for data collection and transferring human demonstrations to robot policies, illustrating a push towards standardizing hardware interaction for data acquisition [16].

Despite these successes, current robotic hardware presents significant limitations that constrain VLA research and deployment. A primary concern is the **lack of human-level dexterity**, with most research confined to simplified lab settings and predominantly gripper-based manipulation, thereby limiting task complexity and generality [30,32]. The absence of sensitive, full-coverage tactile sensors further restricts robots' ability to interact subtly and effectively with the physical world, necessitating advancements in multimodal-sensory intelligence, including touch, force, and audio feedback [24,32]. The **diversity and isolation of embodiments** also pose a challenge, as different robot platforms often have varying capabilities and lack common interfaces, hindering generalization and transferability of learned skills [21,32]. UniVLA addresses this by seeking a unified latent action representation that is cross-embodiment and cross-view [21]. Furthermore, a **semantic-physical mismatch** can arise where VLA models' intended actions do not perfectly translate to the robot's physical execution due to limitations in fine-grained control or perception, such as a robot applying excessive force despite an instruction to "gently push a balloon" [18]. The high cost and physical constraints associated with collecting raw action data via teleoperation and real robot interaction also limit scalability [27].

Beyond mechanical limitations, computational hardware constraints are critical for VLA model deployment. Deploying VLA models on resource-constrained robotic platforms raises significant concerns regarding latency, memory usage, and sustained operation [26,28]. High-dimensional visual embeddings, for instance, demand substantial memory bandwidth (e.g., 1.2 GB/s for 400 512-dim visual tokens), frequently exceeding the capacity of edge AI chips [28]. Real-time inference, crucial for high-frequency control, can become a major bottleneck, as evidenced by RT-2's computational costs [16].

To address these computational challenges, several hardware-aware optimizations and solutions are being explored. These include compiler-level graph rewriting and kernel fusion (e.g., NVIDIA TensorRT-LLM) to leverage target hardware features such as tensor cores, fused attention, and pipelined memory transfers, which can reduce inference latency by 30% and energy consumption by 25% on GPUs like RTX A2000 [28]. Quantization techniques, such as INT8 quantization, are employed to maintain high task success rates while enabling efficient continuous control loops on edge modules like Jetson Orin, even with strict power budgets [11,25]. The development of "TinyVLA" models, with less than 1 billion parameters, demonstrates near-state-of-the-art performance and real-time inference capabilities on operational benchmarks, paving the way for large-scale deployment in resource-constrained scenarios [25,28]. Furthermore, the call for "Next-Generation Programmable Hardware for AI Systems" and optimized hardware accelerators supporting sparse tensor operations and fused vision-language kernels points to a future where specialized hardware will enable sustained high TOPS computation at lower power, essential for embedded platforms in mobile robots and UAVs [24,28]. The continuous advancement of vehicle hardware is also recognized as crucial for the future commercialization of VLA in domains like autonomous driving [8].

In summary, while current VLA models show promising deployment on existing robotic platforms, their full potential is hindered by limitations in hardware dexterity, sensor capabilities, and computational efficiency. Overcoming these challenges necessitates a concerted effort in developing more versatile, dexterous, and computationally optimized robotic hardware, allowing for more robust and general-purpose VLA applications and facilitating the co-evolution of model, data, and hardware [32].
### 2.4 Relationship to Embodied AI

![VLA's Role and Limitations in Advancing Embodied AI](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/rZoYDsJRaWvV3lcchRvBB_VLA%27s%20Role%20and%20Limitations%20in%20Advancing%20Embodied%20AI.png)


![VLA's Role and Limitations in Advancing Embodied AI](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/JYESEtZBP7l6VDTwx_955_VLA%27s%20Role%20and%20Limitations%20in%20Advancing%20Embodied%20AI.png)

Vision-Language-Action (VLA) models represent a critical and specialized category within the broader field of Embodied AI, directly addressing its core objectives of developing generalist agents capable of operating and interacting intelligently within complex physical environments [18,25,26,28,30,34,35]. While Embodied AI encompasses a wide range of topics, VLAs provide concrete mechanisms for agents to act in and interact with their environment based on perceived visual and linguistic cues, distinguishing themselves from purely cognitive or generative AI paradigms [28,30,32,34]. The pursuit of "Embodied AI towards open-ended tasks and environments" signifies the crucial role of VLA capabilities for flexible and adaptive operation in real-world complexities [24].

VLA models are instrumental in pushing the boundaries of Embodied AI by integrating advanced perception and reasoning with direct action generation. They achieve this by enabling agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments [2,12,34]. This integration unifies visual perception, language interpretation, and physical action generation into a single framework, often through end-to-end learning, allowing AI agents and robots to autonomously execute tasks in dynamic, real-world scenarios [25,28,35]. Specific applications such as robotic manipulation [6,23,26,27], autonomous driving [17,19,28], and aerial robotics with UAVs [15] highlight the transformative potential of VLAs to bridge the gap between high-level reasoning and low-level physical interaction. Projects like InstructVLA aim to facilitate intuitive human-robot interaction by enabling robots to "listen, think, and act accurately" based on human language [5,14]. Similarly, RT-2 contributes by making robots understand and execute complex natural language commands and generalize to novel situations [20]. Even in digital environments, models like Cradle and ShowUI extend Embodied AI principles by empowering agents with strong reasoning and control capabilities for computer tasks, highlighting the conceptual breadth of "embodiment" [22].

Several advancements underscore the progress in VLA's contribution to Embodied AI. MemoryVLA, for instance, addresses the challenge of non-Markovian tasks by integrating a dual-memory system, enabling robots to leverage past experiences for future actions, thereby moving beyond purely reactive systems [31]. UniVLA tackles the generalization problem across different robot embodiments by learning "task-centric latent actions" from vast multimodal data, aiming for a universal action representation [21]. GraspVLA further advances zero-shot generalization and scalability for physical interaction tasks by training on synthetic data, reducing reliance on extensive real-world supervision [10]. Furthermore, the development of EfficientVLA aims to enhance computational efficiency, facilitating the transition of VLA models from laboratory settings to practical real-world applications [9]. The focus on 3D representations, as seen in 3D-VLA and frameworks for generating "Simulation-Ready 3D environments," further strengthens embodied agents' ability to perceive, reason, and plan in complex 3D physical worlds [3,33].

Despite these significant advancements and the promise of VLA models, they currently fall short in fully achieving the ultimate goals of Embodied AI, particularly regarding their limited generalization capabilities and insufficient abstraction, adaptability, and coordination in complex real-world scenarios [18]. One major limitation is that most evaluations for VLA models remain confined to simplified laboratory settings, which are far from the requirements for general-purpose embodied agents operating in everyday environments [32]. The real world presents inherent unstructuredness, dynamic interventions, cluttered spaces, and occlusions, posing orders of magnitude greater challenges than digital environments or constrained benchmarks [32]. Issues like data scarcity and poor cross-embodiment transferability continue to hinder the development of truly generalist agents [29,32]. While VLA models leverage general visual and language capabilities from foundation models, endowing them with action capabilities, the complexity of physical interaction still demands substantial advances to move from specialized instruction-following tasks to truly open-ended scenarios in open-world physical environments [18,32]. Achieving seamless and robust interaction in diverse, unstructured environments, and exhibiting the abstraction, adaptability, and coordination seen in human intelligence, remains a key challenge for the continued evolution of VLA models within Embodied AI.
## 3. Architectural Paradigms and Innovations
The development of Vision-Language-Action (VLA) models is profoundly shaped by their underlying architectural paradigms, which dictate how multimodal information is processed, decisions are made, and actions are ultimately executed [6,28]. These architectures represent the fundamental design choices that enable VLAs to bridge the gap between abstract human commands and concrete robotic movements. Broadly, VLA architectures can be categorized into two primary philosophies: monolithic (end-to-end) and hierarchical, each offering distinct advantages and confronting unique challenges. Beyond these foundational structures, a continuous stream of advanced architectural concepts further refines and extends the capabilities of VLA models, addressing specific limitations and pushing towards more robust, intelligent, and generalizable embodied agents.

**Monolithic (End-to-End) Architectures** represent a unified approach where visual perception, language understanding, and action generation are integrated into a single, cohesive system [8,35]. These models operate on an "image input, instruction output" closed-loop principle, directly mapping raw sensory inputs to motor commands without distinct intermediate modules [8,28]. Their primary strength lies in architectural simplicity and direct data-driven learning, which can minimize information loss and simplify the control pipeline. Prominent examples like the RT-series (RT-1, RT-2, RT-2-X) exemplify this paradigm by fine-tuning pre-trained Vision-Language Models (VLMs) to directly output action tokens, aiming for seamless integration of reasoning and execution [20,28]. However, this unified approach often struggles with explicit reasoning, multi-step planning, and ensuring physically coherent actions, leading to potential issues with interpretability and safety in complex or unforeseen scenarios [7,35].

In contrast, **Hierarchical Architectures** explicitly decouple high-level planning from low-level execution, often drawing inspiration from human cognitive dual-process theories [6,28,34]. These systems typically feature a slower, reasoning-intensive planner (System 2) responsible for complex decision-making and goal decomposition, and a fast-reacting controller (System 1) for real-time action execution. The core advantage of this decoupling is enhanced interpretability, modularity, and generalization through the use of explicit intermediate representations, such as language plans, visual plan latents, or goal states [12,26,32]. Models like NVIDIA's Groot N1 exemplify this by integrating a high-level VLM for contextual reasoning with a low-level diffusion transformer for precise motor control [27,28]. This approach facilitates multi-time scale reasoning and improved safety, particularly for long-horizon and intricate tasks.

Beyond these fundamental structural choices, **Advanced Architectural Concepts** are continuously being developed to address specific challenges and enhance VLA capabilities across both monolithic and hierarchical designs. Key innovations include:
*   **Memory Mechanisms:** Essential for handling non-Markovian tasks and long-horizon planning, allowing models like MemoryVLA to maintain context through dual-memory systems for immediate and long-term recall [6,31].
*   **World Models:** Integrate explicit knowledge about environmental dynamics, spatial relationships, and semantics, moving beyond pure observational learning to enable better planning and prediction. DreamVLA, for instance, incorporates comprehensive world knowledge forecasting, while models like Genie simulate future visual dynamics [2,32].
*   **3D Perception and Generative Models:** Overcome the limitations of 2D understanding by explicitly integrating 3D sensory information, crucial for grounded spatial reasoning and physical interaction. 3D-VLA employs a 3D-based LLM and generative world model to reason and plan in 3D environments, while OpenDriveVLA utilizes Bird's Eye View (BEV) perception for rich 3D context [3,17].
*   **Sophisticated Action Generation:** Techniques like diffusion models are increasingly adopted for smoother and more robust action generation, as seen in models like Octo and DexVLA [29,35].
*   **Adaptive and Efficient Mechanisms:** Innovations such as Mixture-of-Experts (MoE) in InstructVLA and FedVLA enable adaptive and efficient processing by dynamically activating specialized modules for reasoning and operation, improving computational efficiency and adaptability [14,23].
*   **Self-Correction and Robustness:** Mechanisms like those in SC-VLA feature hybrid execution loops with diagnostic and corrective strategies to reduce task failure rates, highlighting a move towards more fault-tolerant systems [28].

The ongoing evolution of VLA architectures reflects a concerted effort to build more intelligent, robust, and generalizable embodied agents. This involves carefully balancing the trade-offs between integration and modularity, addressing temporal and spatial reasoning challenges, and incorporating advanced learning paradigms to enable seamless, intuitive, and reliable interaction with the physical world [26,32].
### 3.1 Monolithic (End-to-End) Architectures

**Monolithic (End-to-End) VLA Architectures: Characteristics, Pros, and Cons**

| Aspect         | Description                                                                  | Advantages                                                                      | Limitations                                                                                             | Key Examples                                                      |
|----------------|------------------------------------------------------------------------------|---------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------|
| **Core Idea**  | Single, unified system integrates perception, language, and action; "image input, instruction output" loop. | Architectural simplicity, direct data-driven learning, reduced information loss, simplified pipeline.    | Lack explicit reasoning, struggle with multi-step planning, opaque "black box" (safety, interpretability). | RT-series (RT-1, RT-2, RT-2-X), OpenVLA, InstructVLA, AutoVLA, ShowUI |
| **Output**     | Directly maps raw sensory inputs to motor commands/action tokens.            | Efficient data utilization for control strategies, leverages VLM capabilities. | Prone to physically infeasible actions, semantic-physical mismatch, modality gap.                         |                                                                   |
| **Types**      | Single-system (fully integrated) or Dual-system (VLM + action expert).       | Transfers semantic knowledge, seamless integration.                             | Computational inefficiency for long-horizon tasks, high inference latency.                             | Single: RT-series, OpenVLA. Dual: DexVLA (VLM+diffusion expert).  |



**Monolithic (End-to-End) VLA Architectures: Characteristics, Pros, and Cons**

| Aspect         | Description                                                                  | Advantages                                                                      | Limitations                                                                                             | Key Examples                                                      |
|----------------|------------------------------------------------------------------------------|---------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------|
| **Core Idea**  | Single, unified system integrates perception, language, and action; "image input, instruction output" loop. | Architectural simplicity, direct data-driven learning, reduced information loss, simplified pipeline.    | Lack explicit reasoning, struggle with multi-step planning, opaque "black box" (safety, interpretability). | RT-series (RT-1, RT-2, RT-2-X), OpenVLA, InstructVLA, AutoVLA, ShowUI |
| **Output**     | Directly maps raw sensory inputs to motor commands/action tokens.            | Efficient data utilization for control strategies, leverages VLM capabilities. | Prone to physically infeasible actions, semantic-physical mismatch, modality gap.                         |                                                                   |
| **Types**      | Single-system (fully integrated) or Dual-system (VLM + action expert).       | Transfers semantic knowledge, seamless integration.                             | Computational inefficiency for long-horizon tasks, high inference latency.                             | Single: RT-series, OpenVLA. Dual: DexVLA (VLM+diffusion expert).  |

Monolithic, or end-to-end, Vision-Language-Action (VLA) architectures represent a fundamental paradigm in the field, characterized by their unified approach to processing multimodal inputs and directly generating actions [6,25,28]. These models integrate visual perception, language understanding, and action decision-making within a single, cohesive system, bypassing the need for distinct, human-designed intermediate modules [8,35]. The core principle is an "image input, instruction output" closed loop, wherein raw sensory inputs are mapped directly into motor commands through a unified network [8,28]. The meta-survey literature frequently classifies these as a primary architectural approach for large VLM-based VLA models [6,34].

A primary strength of monolithic VLA models lies in their architectural simplicity and direct mapping capabilities, which can significantly reduce information loss between modules and simplify the overall control pipeline [8,30,35]. This approach leverages efficient data utilization by directly learning optimal control strategies from multimodal inputs, circumventing manually designed algorithms [8]. Examples like ShowUI, an end-to-end model for GUI agents and computer use, exemplify this direct mapping without explicit intermediate planning stages [22]. Similarly, OpenVLA, a unified system combining a language model (Llama 2) with visual encoders (DINOv2, SigLIP), directly produces policies for visuomotor control, demonstrating high efficiency with 7B parameters, outperforming larger models like RT-2-X (55B) in some contexts [11,28]. InstructVLA further highlights this strength by achieving joint optimization of textual reasoning and action generation within a single VLM backbone, ensuring that multimodal reasoning directly informs action planning [5,14]. Early fusion models, such as EF-VLA, also demonstrate improved performance and generalization on compositional manipulation tasks by preserving semantic consistency through early integration of image-text embeddings [25]. This integrated learning paradigm facilitates superior versatility, dexterity, and generalizability in complex environments, surpassing limitations of earlier deep reinforcement learning methods [34].

Despite these advantages, monolithic architectures present inherent limitations, particularly concerning explicit reasoning, multi-step planning, and adaptation to complex variations [12,28,35]. A significant challenge is their propensity to produce physically infeasible actions, stemming from the lack of explicit, interpretable reasoning within their "black box" nature [6,7,8,28,35]. This opacity also increases the difficulty in safety verification for decision errors in edge scenarios [8]. Furthermore, these models can suffer from #semantic-physical mismatch and #modalitygap problems, where the linguistic interpretation of an object may diverge from the visual perception and the intended action, leading to erroneous executions [18]. They often struggle with the complexities of the real world beyond fixed patterns learned from training data, limiting their generalization capabilities [18]. Directly generating raw control commands for long-term tasks can lead to prohibitively high context length, computational cost, and inference latency, presenting issues like #ComputationalInefficiency and #LongHorizonTasks [27]. The ideal of "raw actions" for end-to-end learning is often limited by data availability [32].

Several models exemplify the monolithic approach. The **RT-series (RT-1, RT-2, RT-2-X)** are prominent examples, converting robot control into an autoregressive sequence prediction task by fine-tuning pre-trained VLMs to output action tokens [16,26,28,30]. RT-2, notably, demonstrated a 63% performance improvement when dealing with new objects and was described as "the largest model to date for end-to-end robot control" [20,28]. However, this direct action output contrasts with earlier high-level planning VLMs like PaLM-E, which only interpreted commands into primitives, highlighting the ongoing challenge of unifying high-level reasoning with low-level execution within a single framework [20].

In the context of autonomous driving, **AutoVLA** stands out as an end-to-end VLA model that unifies semantic reasoning and action generation within a single autoregressive framework [7]. It directly performs semantic reasoning and trajectory planning from raw visual inputs and language instructions, aiming to mitigate physically infeasible action outputs and address overly complex model structures or unnecessarily long reasoning processes [7]. While early end-to-end autonomous driving VLA models often underperformed traditional segmented approaches (e.g., Driving Scores below 60 compared to ReasonNet's 79.95), recent models like AutoVLA (DS 78.84) and ORION (DS 77.74) have shown substantial improvement, closely rivaling or even exceeding segmented methods [17]. AutoVLA also combats VLM hallucinations by tokenizing actions into a codebook combined with rules, and OpenDriveVLA addresses the "modality gap" by integrating traditional BEV perception to generate structured 3D environmental tokens for LLMs [17].

Within the monolithic paradigm, survey papers categorize models into **single-system** and **dual-system** designs [6]. Single-system models, such as the RT series and OpenVLA, fully integrate environmental comprehension and action generation, aiming to transfer semantic knowledge from large VLMs to robotic tasks through a unified model. These models typically employ autoregressive decoding to predict action tokens sequentially, but this can lead to slow inference speeds, prompting efforts in architectural optimization (e.g., RoboMamba, MoLe-VLA) and inference acceleration (e.g., parallel decoding) [26]. RoboMamba, for instance, integrates a visual encoder with an efficient Mamba language model for improved robot reasoning and manipulation within an end-to-end framework [16,26]. Dual-system models, on the other hand, employ a VLM backbone for slower, reflective reasoning (System 2) and an action expert for faster, reactive behaviors (System 1), exchanging information via latent representations. This design seeks to combine reactive speed with deliberate accuracy, although their internal reasoning often remains opaque within latent spaces [26]. Examples such as GraspVLA integrate autoregressive perception and flow-matching-based action generation into a unified Chain-of-Thought process, indicating a trend toward structured reasoning within end-to-end architectures [10]. Similarly, $\pi_{0.5}$ unifies the planner and controller, first predicting high-level semantic subtasks before generating continuous low-level actions, showcasing a hybrid approach to address multi-step planning within an end-to-end framework [32].

While monolithic VLA models offer compelling advantages in seamless integration, reduced latency, and simplified pipelines, they continue to grapple with fundamental challenges related to explicit reasoning, interpretability, and the generation of physically coherent actions, especially in real-time and complex scenarios [7,20]. The ongoing research trend involves architectural and algorithmic advancements to imbue these systems with more structured reasoning capabilities and overcome limitations like computational inefficiency for long-horizon tasks, moving towards more robust and generalizable end-to-end intelligent agents [2,15,26].
### 3.2 Hierarchical Architectures

**Hierarchical VLA Architectures: Design Principles, Advantages, and Challenges**

| Aspect                 | Description                                                                  | Advantages                                                                                                  | Challenges                                                                                             | Key Examples                                                                    |
|------------------------|------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------|
| **Core Idea**          | Decouples high-level planning (System 2, slower) from low-level execution (System 1, faster). | Enhanced interpretability, modularity, better generalization, multi-time scale reasoning, improved safety. | Designing robust intermediate representations, seamless coordination between levels, potential inference overhead. | Groot N1, ThinkAct, NaVILA, UAV-VLA, 3D-Generalist, Hi Robot, $\pi_{0.5}$, OpenDriveVLA |
| **Intermediate Representations** | Explicit, interpretable forms guiding actions.                             | Clear conceptual bridge between abstract goals and concrete actions.                                        |                                                                                                        | Language Plans/Motions, Visual/Geometric (Keypoints, Affordances), Goal States, Latent Representations, Structured Tokens (Map, Scene, Agent Tokens) |
| **Primary Goal**       | Manage complexity, facilitate long-horizon tasks, provide explicit reasoning. | Easier debugging, flexible adaptation to new scenarios.                                                     |                                                                                                        |                                                                                 |



**Hierarchical VLA Architectures: Design Principles, Advantages, and Challenges**

| Aspect                 | Description                                                                  | Advantages                                                                                                  | Challenges                                                                                             | Key Examples                                                                    |
|------------------------|------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------|
| **Core Idea**          | Decouples high-level planning (System 2, slower) from low-level execution (System 1, faster). | Enhanced interpretability, modularity, better generalization, multi-time scale reasoning, improved safety. | Designing robust intermediate representations, seamless coordination between levels, potential inference overhead. | Groot N1, ThinkAct, NaVILA, UAV-VLA, 3D-Generalist, Hi Robot, $\pi_{0.5}$, OpenDriveVLA |
| **Intermediate Representations** | Explicit, interpretable forms guiding actions.                             | Clear conceptual bridge between abstract goals and concrete actions.                                        |                                                                                                        | Language Plans/Motions, Visual/Geometric (Keypoints, Affordances), Goal States, Latent Representations, Structured Tokens (Map, Scene, Agent Tokens) |
| **Primary Goal**       | Manage complexity, facilitate long-horizon tasks, provide explicit reasoning. | Easier debugging, flexible adaptation to new scenarios.                                                     |                                                                                                        |                                                                                 |

Hierarchical architectures represent a fundamental paradigm in Vision-Language-Action (VLA) models, explicitly decoupling high-level planning from low-level execution [6,34]. This design principle, often drawing inspiration from human cognitive dual-process theory, posits two complementary subsystems: a slower, reasoning-intensive planner (System 2) for complex decision-making and a fast-reacting controller (System 1) for real-time action execution [25,28]. The core advantage of this decoupling lies in its ability to manage complexity by using interpretable intermediate representations to guide actions, thereby enhancing interpretability, modularity, and generalization across diverse tasks and environments [26,30,32,34].

A prominent example of this dual-system framework is NVIDIA's Groot N1, which integrates a high-level autoregressive VLM (Eagle-2) operating at 10 Hz for contextual reasoning and planning (System 2) with a low-level diffusion transformer operating at 120 Hz for smooth, real-time motor control (System 1) [27,28]. The System 2 component is responsible for task planning, skill composition, and decomposing long-term goals (e.g., "clean the table") into atomic subtasks, while System 1 ensures precise execution. This layered architecture facilitates multi-time scale reasoning and improves safety, particularly in scenarios demanding both rapid reactions and deep strategic thought [28]. Empirically, Groot N1 demonstrated a 17% increase in success rate and a 28% reduction in collision failure rate compared to monolithic models in multi-stage home operation benchmarks [25,28]. Notably, these two models are tightly integrated and jointly trained end-to-end, suggesting a sophisticated approach to mitigate potential pitfalls of independent module optimization [27].

ThinkAct further exemplifies a sophisticated hierarchical approach through its "reinforced visual latent planning" [12,30]. In ThinkAct, a multimodal Large Language Model (LLM) generates "embodied reasoning plans" for high-level abstract reasoning. These plans are then compressed into a "visual plan latent," serving as an interpretable intermediate representation that conditions a downstream action model for execution [12]. A critical technical solution for improving planning quality and adaptability in ThinkAct is the integration of reinforcement learning, which optimizes the generation of these embodied reasoning plans and visual plan latents using "action-aligned visual rewards based on goal completion and trajectory consistency" [12]. This reinforcement mechanism directly addresses the challenge of ensuring coherence between high-level plans and low-level execution, leading to benefits such as few-shot adaptation, long-horizon planning, and self-correction in complex tasks [12]. The explicit learning of interpretable intermediate plans, as seen in ThinkAct, offers a significant advantage over purely reactive or implicitly hierarchical models by providing a clear conceptual bridge between abstract goals and concrete actions.

Hierarchical models leverage various forms of intermediate representations to decouple planning and execution. These can be broadly categorized:
*   **Language-based Representations**: High-level VLMs often generate "language plans" for goals or subtasks, which are then translated into "language motions" for low-level physical actions [32]. Models like CogACT and NaVILA utilize LLM planners to generate sub-goals that guide a low-level controller [28]. UAV-VLA separates high-level natural language instruction parsing (using GPT), visual target identification (via VLM on satellite images), and subsequent low-level path generation [15]. Program-based methods, such as Code as Policies and ProgPrompt, employ LLMs to translate language instructions into Python code snippets for high-level action plans, which then parameterize low-level robot APIs [26,32]. Other subtask-based approaches decompose high-level implicit instructions into step-by-step textual commands for a low-level policy, including models like PaLM-E and HiRobot [26].
*   **Visual and Geometric (Keypoint/Affordance) Representations**: These involve the prediction of salient points for gripper interaction, waypoints, or actionable spatial constraints. CoPa uses a coarse-to-fine grounding pipeline to identify interaction regions and refine them into spatial constraints for motion planning [32]. ReKep formalizes manipulation as a constraint optimization problem over tracked keypoints, planning SE(3) subgoals [32]. Models like MoManipVLA and RoboPoint directly predict manipulation targets, while A0A0 predicts contact points as embodiment-agnostic affordance representations for action policies [26].
*   **Goal State and Latent Representations**: Hierarchical models frequently use high-level generative models to synthesize desired goal states, which then condition lower-level models to produce actions [32]. VLP, for instance, generates and scores multiple candidate goal videos to devise optimal long-term strategies for subtasks [32]. InstructVLA, despite its end-to-end presentation, features an internal modularity where the VLM generates "latent action vectors" that serve as abstract guidance for an independent action expert module [14]. Similarly, UniVLA employs "task-centric latent actions" as a "unified musical score" to guide low-level execution [21]. The generative world model in 3D-VLA supports a hierarchical approach by "depicting imagination about future scenarios to plan actions accordingly," effectively decoupling future state prediction from direct action generation [3].
*   **Structured Tokens**: OpenDriveVLA utilizes traditional Bird's Eye View (BEV) perception to generate distinct "Map Token," "Scene Token," and "Agent Token." These structured tokens are then unified before inputting into an LLM, providing a pre-processed 3D environmental understanding that guides the LLM's comprehension and prevents "hallucinations" [17].

While hierarchical architectures are widespread, some models opt for a more monolithic approach. For instance, RT-2 aims to overcome explicit separation by training a single model to perform both multimodal reasoning and direct action generation, integrating these capabilities into a unified structure [20]. However, even "traditional segmented end-to-end" models like ReasonNet inherently imply a hierarchical separation of perception, planning, and control modules [17]. Furthermore, hybrid architectures, combining VLAs with traditional rule engines or Model Predictive Control (MPC), also demonstrate a form of hierarchy where the VLA handles high-level decision-making while classical components provide low-level control or safety overrides [8].

The strengths of hierarchical VLA models are multifaceted: they offer enhanced interpretability through explicit intermediate outputs, improved modularity allowing independent development or replacement of components, and potentially better generalization across diverse tasks by abstracting high-level reasoning from low-level mechanics [26,28]. This paradigm also facilitates easier debugging by localizing issues to specific layers and enables more flexible adaptation to new scenarios. However, challenges include the complexity of designing robust intermediate representations and ensuring seamless coordination between hierarchical levels. The overhead of processing these multiple stages could also impact real-time performance, though approaches like DeeR-VLA address this by employing dynamic reasoning mechanisms that adjust computational depth based on task complexity, reducing costs while maintaining success rates [18]. Despite these challenges, the ability of hierarchical architectures to manage complex, long-horizon tasks and offer enhanced safety makes them a dominant and evolving architectural choice in VLA research.
### 3.3 Advanced Architectural Concepts

**Key Advanced Architectural Concepts in VLA Models**

| Concept                  | Description                                                                  | Purpose / Benefit                                                                    | Key Examples / Models                                                                                                                              |
|--------------------------|------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|
| **Memory Mechanisms**    | Store and retrieve past observations/actions (dual-memory, sparse memory).   | Handle non-Markovian tasks, long-horizon planning, maintain context.                 | MemoryVLA (Working Memory, PCMB), EnerVerse, Octo, BUMBLE, ReflectVLM                                                                             |
| **World Models**         | Integrate explicit knowledge of environmental dynamics, spatial relations.     | Beyond pure observation, enables better planning, prediction, "physical common sense". | DreamVLA (world knowledge forecasting), Genie, FLIP, WorldVLA, World4Omni, 3D-VLA                                                                 |
| **3D Perception & Generative Models** | Overcome 2D limitations by integrating 3D sensory info; predict future states. | Grounded spatial reasoning, physical interaction, dynamic 3D representations.        | 3D-VLA (3D-based LLM, generative world model), OpenDriveVLA (BEV), TransDiffuser, LEO, NeRF, 3D Gaussian Splatting, Occupancy-Language-Action (OccLLaMA) |
| **Sophisticated Action Generation** | Advanced techniques for smoother, robust, and efficient action generation.   | Improved control, multi-modality handling, better dexterity.                         | Diffusion models (Octo, DexVLA, RDT-1B, π0, InstructVLA, MemoryVLA, GraspVLA), Flow Matching, Action Chunking, Parallel Decoding, Early-Exit Decoding, FAST |
| **Adaptive & Efficient Mechanisms** | Dynamic activation of specialized modules, optimized processing.             | Improved computational efficiency, adaptability, preserving pre-trained capabilities. | Mixture-of-Experts (MoE) (InstructVLA, FedVLA), LoRA (OpenVLA, DexVLA), EfficientVLA (token filtering, caching), Model Compression, Lightweighting |
| **Self-Correction & Robustness** | Mechanisms for diagnosing failures, generating corrective strategies, refining outputs. | More fault-tolerant systems, adaptive to unforeseen circumstances, robust to errors.  | SC-VLA, ThinkAct (reinforced planning), 3D-Generalist (iterative refinement), iRe-VLA, ReflectVLM                                                  |


**Key Advanced Architectural Concepts in VLA Models**

| Concept                  | Description                                                                  | Purpose / Benefit                                                                    | Key Examples / Models                                                                                                                              |
|--------------------------|------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|
| **Memory Mechanisms**    | Store and retrieve past observations/actions (dual-memory, sparse memory).   | Handle non-Markovian tasks, long-horizon planning, maintain context.                 | MemoryVLA (Working Memory, PCMB), EnerVerse, Octo, BUMBLE, ReflectVLM                                                                             |
| **World Models**         | Integrate explicit knowledge of environmental dynamics, spatial relations.     | Beyond pure observation, enables better planning, prediction, "physical common sense". | DreamVLA (world knowledge forecasting), Genie, FLIP, WorldVLA, World4Omni, 3D-VLA                                                                 |
| **3D Perception & Generative Models** | Overcome 2D limitations by integrating 3D sensory info; predict future states. | Grounded spatial reasoning, physical interaction, dynamic 3D representations.        | 3D-VLA (3D-based LLM, generative world model), OpenDriveVLA (BEV), TransDiffuser, LEO, NeRF, 3D Gaussian Splatting, Occupancy-Language-Action (OccLLaMA) |
| **Sophisticated Action Generation** | Advanced techniques for smoother, robust, and efficient action generation.   | Improved control, multi-modality handling, better dexterity.                         | Diffusion models (Octo, DexVLA, RDT-1B, π0, InstructVLA, MemoryVLA, GraspVLA), Flow Matching, Action Chunking, Parallel Decoding, Early-Exit Decoding, FAST |
| **Adaptive & Efficient Mechanisms** | Dynamic activation of specialized modules, optimized processing.             | Improved computational efficiency, adaptability, preserving pre-trained capabilities. | Mixture-of-Experts (MoE) (InstructVLA, FedVLA), LoRA (OpenVLA, DexVLA), EfficientVLA (token filtering, caching), Model Compression, Lightweighting |
| **Self-Correction & Robustness** | Mechanisms for diagnosing failures, generating corrective strategies, refining outputs. | More fault-tolerant systems, adaptive to unforeseen circumstances, robust to errors.  | SC-VLA, ThinkAct (reinforced planning), 3D-Generalist (iterative refinement), iRe-VLA, ReflectVLM                                                  |

Advanced architectural concepts in Vision-Language-Action (VLA) models extend beyond basic input-output mappings, focusing on enhancing capabilities for complex, real-world robotic tasks. These innovations address fundamental limitations such as handling temporal dependencies, integrating comprehensive environmental understanding, and improving generalization.

A critical motivation for advanced VLA architectures is the inherently non-Markovian nature of many real-world robotic tasks [31]. Unlike simpler tasks where the current state is sufficient for decision-making, long-horizon tasks require models to recall past observations and actions to maintain context and plan coherently. This necessitates the integration of memory mechanisms into VLA models [6,24,32]. MemoryVLA, for instance, introduces a dual-memory mechanism inspired by human cognition. It comprises a **Working Memory** for immediate context, formed by perceptual tokens (e.g., from DINOv2 and SigLIP) and a cognitive token (from LLaMA-7B) [31]. Complementing this is a **Perceptual-Cognitive Memory Bank (PCMB)**, serving as a long-term memory with dual streams for low-level visual details and high-level semantic information. This PCMB uses cross-attention with temporal positional encoding for context-dependent retrieval, gated fusion for adaptive balancing of historical and current information, and an intelligent consolidation mechanism (cosine similarity-based merging) to manage capacity and reduce redundancy [31]. Other models also integrate memory; EnerVerse utilizes a "sparse memory mechanism" to enhance long-range task planning [16], and Octo proposes a "memory-augmented Transformer" for long-term decision-making across diverse scenarios [28]. Similarly, BUMBLE and ReflectVLM incorporate memory and reflection to manage interdependent subtasks and plan in complex environments [32]. The general trend points toward "Long-term memory modeling in agentic AI systems" as a crucial research direction [24,26].

Another significant advancement is the explicit integration of "world knowledge" to overcome the limitations of purely image-based forecasting, which often lacks a deeper understanding of physical dynamics and semantic context [2]. DreamVLA addresses this by explicitly incorporating "comprehensive world knowledge forecasting," encompassing dynamic, spatial, and semantic information, acquired through "dynamic-region-guided prediction" [2]. Its architectural innovations include a block-wise structured attention mechanism that disentangles representations by masking mutual attention among dynamic, spatial, and semantic information. Additionally, it uses a diffusion-based transformer for action decoding, which aids in inverse dynamics modeling by disentangling action representations from shared latent features [2]. In contrast, RT-2 integrates world knowledge by leveraging the extensive pre-training of Vision-Language Models (VLMs) like PaLI-X and PaLM-E on vast image-text datasets, transferring their reasoning capabilities directly to robotic control through co-fine-tuning [20]. UniVLA learns a "task-centric latent action language" from video data, quantifies visual changes into discrete "notes" using VQ-VAE, and filters them with language instructions, effectively creating a "unified musical score" for actions that implicitly integrates dynamic, spatial, and semantic knowledge [21]. GraspVLA employs a "unified Chain-of-Thought process" to integrate reasoning, implicitly understanding dynamic and semantic information to mitigate sim-to-real gaps and enable open-vocabulary generalization [10]. The broader concept of "World Model Construction" focuses on optimizing the simulation of physical laws to anticipate dynamic changes in complex scenes, such as predicting liquid trajectories or maintaining stability under external forces [18]. Research areas such as "Robotic world models and neural simulators" and "Learning foundation world models" highlight the increasing interest in predictive modeling of the environment [24].

World models are increasingly recognized as essential components in VLA architectures, enabling models to learn and leverage internal representations of environmental dynamics for enhanced planning and prediction [6,34]. They encode vast physical common sense and can predict visual goal states to ground affordance generation and trajectories [32]. Examples include Genie and Genie 2, which simulate future visual dynamics based on action sequences, and FLIP, which incorporates a world model built from videos, encompassing dynamics, action, and value modules for model-based planning [32]. Other models like WorldVLA use autoregressive action world models to predict visual outcomes, while World4Omni generates subgoal images to guide low-level policies [26]. The ability to predict future states is also central to "dynamics learning," which involves understanding both forward dynamics ($s_{t+1} \leftarrow f_{fwd}(s_t, a_t)$) and inverse dynamics ($a_t \leftarrow f_{inv}(s_t, s_{t+1})$) [34]. Models like Vi-PRoM, MaskDP, MIDAS, SMART, PACT, VPT, and GR-1 contribute to this domain, with DreamVLA also enabling inverse dynamics modeling [2,34].

Addressing the limitations of 2D VLA models, which often struggle with spatial understanding and interaction in physical environments, 3D perception and generative world models are being integrated. 3D-VLA notably integrates a "3D-based large language model (LLM)" for inherent processing and reasoning about 3D sensory information, a significant advancement over 2D-centric approaches [3,32]. This model incorporates a "generative world model" to "imagine about future scenarios to plan actions," learning environmental dynamics and action consequences. To operationalize this, 3D-VLA aligns "embodied diffusion models" with the LLM to predict "goal images and point clouds," serving as target states for future actions [3]. In a similar vein, OpenDriveVLA overcomes the lack of 3D spatial and temporal understanding by employing a Bird's Eye View (BEV) perception frontend to generate hierarchical tokens (Map, Scene, Agent Tokens) that provide rich, grounded 3D environmental context to the LLM, thereby preventing "hallucinations" [17]. TransDiffuser further exemplifies advanced multimodal sensor integration with excellent LiDAR and camera fusion combined with a denoising diffusion decoding module [17]. The importance of "4D perception," which considers 3D scenes evolving over time, is highlighted as crucial for robust and adaptive manipulation, integrating depth, point clouds, and multi-modal inputs [6,26]. LEO also enhances 3D reasoning by incorporating 3D datasets [27]. Technologies like NeRF and 3D Gaussian Splatting (3D-GS) are being used to extract rich 3D information from videos and generate dynamic 3D representations [34]. Furthermore, "Unified 3D and video generation models" are key research areas for understanding and synthesizing dynamic environments [24], and Occupancy-Language-Action models (e.g., OccLLaMA) integrate 3D scene understanding directly with action planning [28].

Beyond these core areas, several other architectural innovations contribute to the sophistication of VLA models. Diffusion strategies are increasingly dominating action generation, offering smoother and more noise-resistant action trajectories compared to traditional autoregressive models [35]. Models like Octo and DexVLA utilize Diffusion Transformers, with DexVLA introducing a "Billion-Parameter Diffusion Expert" (Scale Diffusion Policy) with a multi-head output for cross-embodiment learning [29]. RDT-1B is another "diffusion foundation model" for bimanual manipulation [16]. $\pi_0$ employs a "flow matching architecture" with pre-trained VLMs, demonstrating improved dexterity, generalization, and robustness, alongside a significantly higher control frequency compared to models like RT-2 [16,27]. InstructVLA also utilizes a flow matching model for efficient action generation [14]. These "Embodied Diffusion Models" can predict goal states or function as action decoders [32].

Mixture-of-Experts (MoE) mechanisms are emerging as a powerful concept for adaptive and efficient processing. InstructVLA incorporates an MoE within its LLM backbone, allowing flexible switching between reasoning and operation modes by adaptively adjusting weights of different "expert" LoRA modules. A "scale head" module predicts gating coefficients for weighted fusion, preserving pre-trained capabilities and ensuring inference efficiency. Its optimized inference strategy includes greedy decoding for text and parallel processing of action queries, coupled with result caching for both language and latent actions, significantly speeding up inference [5,14]. Similarly, FedVLA introduces a "Dual Gating Mixture-of-Experts (DGMoE)" mechanism, where both input tokens and self-aware experts adaptively decide their activation, leading to improved computational efficiency [23].

Modularity and scalability are enhanced through techniques like LoRA, used in OpenVLA, and plug-in designs such as DexVLA's "plug-in diffusion experts," facilitating rapid adaptation to new robots or tasks [35]. Furthermore, multimodal fusion is deepening beyond just vision and language to incorporate tactile signals (e.g., TLA model) and force control, improving precision in contact-intensive tasks [35]. Microsoft's Magma model, for instance, combines "Sets of Marks (SoM)" and "Tracks of Marks (ToM)" to convert visual-language data into actionable tasks, thereby enhancing spatial reasoning and action planning [18].

Advanced VLA models are also distinguished by sophisticated reasoning and planning capabilities. ThinkAct, with its dual-system architecture, generates "embodied reasoning plans" and "visual plan latents" to facilitate long-horizon planning and support "self-correction behaviors" [12]. Cradle's design emphasizes "strong reasoning abilities, self-improvement, and skill curation" [22]. Self-correction mechanisms, such as SC-VLA, feature a hybrid execution loop with fast inference and a slower correction path, querying internal LLMs or external experts to diagnose failures and generate corrective strategies, reducing task failure rates by 35% in complex environments [28]. Similarly, the 3D-Generalist framework incorporates self-correction by iteratively refining its 3D environment generation through rendering, inspection, and error identification (e.g., via CLIP scores) [33]. The ultimate goal is a transition "from VLA Models to VLA Agents," which are proactive systems augmenting perception-action with cognitive architectures for memory, exploration, planning, and reflection, moving towards more complex, bidirectional, and graph-structured topologies [32].
## 4. Training Strategies and Data Management
The development and deployment of effective Vision-Language-Action (VLA) models hinge critically on sophisticated training strategies and robust data management practices [25,28]. This section provides a comprehensive overview of the methodologies employed to acquire, process, and leverage diverse data sources, alongside the advanced learning paradigms that enable VLA models to bridge the gap between perception, language understanding, and embodied action. The theoretical framework guiding these efforts encompasses the strategic integration of foundational knowledge, the progressive acquisition of skills, and the optimization of computational resources, all while addressing practical concerns such as data scarcity, generalizability, and privacy.

A cornerstone of VLA model efficacy is the **data-centric approach**, which emphasizes the scale, diversity, and quality of training data [28]. Models are typically trained on a spectrum of data, ranging from vast internet-scale vision-language corpora (e.g., COCO, LAION-400M) for general semantic understanding, to diverse robot demonstration data (e.g., RoboNet, BridgeData, Open X-Embodiment) for grounding actions in the physical world [28,34]. The integration of human demonstrations, particularly in embodied forms (e.g., UMI, HumanPlus, EgoMimic), further enriches skill transfer and addresses the persistent challenge of **data scarcity** for real-world robot interactions [16]. To mitigate this, innovative data annotation and generation strategies, including synthetic data creation (e.g., UniSim, SynGrasp-1B) and automated data synthesis pipelines (e.g., ECoT, RAD), are crucial for expanding training datasets and enhancing robustness against diverse scenarios [10,28].

Complementing data acquisition, **instruction tuning** has emerged as a critical training paradigm, treating action generation as an instruction-following task to improve generalizability and mitigate catastrophic forgetting in large pre-trained models [14]. This involves curating datasets with diverse instruction types, such as scene captions, Q&A, and instruction rewriting, to refine models like InstructVLA in understanding and executing complex user intents [5].

A central theme in VLA training is the **integration of pre-trained Vision-Language Models (VLMs) and Large Language Models (LLMs)**, which form the architectural backbone for robust reasoning and perception [26,28]. Two primary strategies exist: direct fine-tuning of large, general-purpose VLMs (e.g., RT-2 leveraging PaLI-X or PaLM-E) to transfer web-scale knowledge to embodied tasks, or modular approaches that combine pre-trained LLMs with specialized visual encoders (e.g., OpenVLA using Llama 2 with DINOv2/SigLIP) [11,20]. A key innovation across both approaches is the alignment of robot "action tokens" with natural language "text tokens" to enable VLMs to interpret and generate physical actions [20].

To manage the substantial computational demands and data requirements of these large models, **efficient fine-tuning and adaptation strategies** are essential [6]. **Parameter-Efficient Fine-Tuning (PEFT)** techniques, notably Low-Rank Adaptation (LoRA), allow for adapting billion-parameter models with a fraction of trainable parameters, significantly reducing computational overhead and enabling deployment on consumer-grade hardware [25,28]. Beyond PEFT, **training-free acceleration frameworks** (e.g., EfficientVLA, FlashVLA) optimize inference speed by pruning redundant layers, filtering visual tokens, or caching features without altering model parameters [9]. Supervised Fine-Tuning (SFT) remains a foundational method, often combined with "self-improvement fine-tuning" and iterative refinement loops for enhancing model performance [33]. The integration of **Reinforcement Learning (RL)** into fine-tuning, particularly through hybrid RL-SFT frameworks and advanced reward shaping techniques (e.g., GRPO in AutoVLA, DPO in iRe-VLA), further optimizes VLA models for complex reasoning, planning, and efficient decision-making in dynamic environments [7,28].

**Curriculum learning and multi-stage training** provide structured approaches to skill acquisition, addressing challenges like data scarcity and long-horizon tasks [25]. Models often progress through stages, such as initial cross-embodiment pre-training, followed by embodiment-specific alignment and task-specific adaptation, as seen in DexVLA, to efficiently acquire complex dexterous manipulation skills [29]. Various architectures, including two-stage (e.g., BLIP-2, UniVLA, InstructVLA) and three-stage (e.g., Qwen-VL, JARVIS-VLA) models, as well as hierarchical and decoupled training strategies (e.g., ThinkAct), are employed to progressively build capabilities from general knowledge to specific embodied actions [21,32].

While **Imitation Learning (IL)**, particularly through Behavior Cloning, serves as a foundational paradigm for acquiring physical manipulation skills from expert trajectories (robot, human, and synthetic demonstrations), **Reinforcement Learning (RL) approaches** offer a dynamic alternative for learning through interactive trial-and-error, crucial for complex behaviors and long-horizon tasks where demonstrations are scarce [28,34]. Algorithms like GRPO (DeepSeek-R1, AutoVLA) and PPO (NaVILA) are utilized, often within hybrid training frameworks that combine RL in simulation with supervised fine-tuning to improve sample efficiency and stabilize policy updates [7,32]. Advanced reward shaping techniques, including VLM-based feedback and DPO, are vital for guiding RL agents effectively [26].

Finally, the increasing reliance on user-specific data necessitates a focus on **privacy-preserving and distributed learning**. Federated Learning (FL), exemplified by FedVLA, enables collaborative training across decentralized client devices without exchanging raw data, safeguarding sensitive information [23]. FedVLA employs distributed model training, task-aware representation learning, and adaptive expert selection with expert-driven aggregation to ensure both privacy and performance, showcasing a move towards ethical and secure VLA deployment [23].

Despite these advancements, significant challenges persist. Data scarcity, particularly for diverse real-world robot interactions, remains a bottleneck. Computational inefficiency in training and inference of large models requires continuous innovation in optimization techniques. The complexity of reward design in RL and the sim-to-real gap for transferring policies from simulation to real robots are ongoing research problems. Moreover, ensuring safety and reliability during exploration in real-world environments and enhancing the generalizability of VLA models across unseen tasks and embodiments are critical for broad applicability. Future research will likely focus on developing more sample-efficient RL algorithms, robust reward shaping, seamless integration of LLMs with RL, and further innovations in privacy-preserving and distributed training paradigms to unlock the full potential of VLA models in complex, unstructured environments.
### 4.1 Data-Centric Approaches and Instruction Tuning

**Data Sources and Instruction Tuning in VLA Model Training**

| Category               | Description                                                                    | Key Examples / Models / Datasets                                                                                     | Challenges / Solutions                                                                    |
|------------------------|--------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|
| **Internet Corpora**   | Large-scale vision-language and instruction-following data for pre-training VLMs. | COCO, LAION-400M, WIT, HowTo100M, WebVid, VQA, GQA, LAION-5B                                                        | Foundation for semantic knowledge and general representations.                            |
| **Robot Demonstrations** | Trajectory datasets from real robots or high-fidelity simulators.                | RoboNet, BridgeData, RT-X, Open X-Embodiment (OXE - 1M trajectories from 22 robots), OpenVLA (970k demos), RT-series (130k demos), CALVIN, DROID | Critical for grounding actions in physical world; OXE for generalization and cross-platform learning. |
| **Human Demonstrations** | Skill transfer from human videos/interactions.                                   | UMI (in-the-wild), ORION (single human videos), HumanPlus (humanoid learning), EgoMimic (egocentric human videos) | Enriches skill transfer, addresses data scarcity. EgoMimic: Human data often valuable.    |
| **Synthetic Data**     | Data generated in simulations for scalability and specific scenarios.            | UniSim (realistic 3D envs), SynGrasp-1B (billion-frame for grasping), EnerVerse, Cosmos (physics-based), ECoT, RAD, 3D-Generalist | Mitigates data scarcity, enhances robustness, helps bridge sim-to-real gap.             |
| **Instruction Tuning** | Paradigm treating action generation as instruction following.                    | LLaVA (Visual Instruction Tuning), InstructVLA (VLA-IT: 650k samples), VIMA (prompt-based control), 3D-VLA (3D embodied instruction tuning) | Improves generalizability, mitigates catastrophic forgetting, enhances understanding of user intent. |



**Data Sources and Instruction Tuning in VLA Model Training**

| Category               | Description                                                                    | Key Examples / Models / Datasets                                                                                     | Challenges / Solutions                                                                    |
|------------------------|--------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|
| **Internet Corpora**   | Large-scale vision-language and instruction-following data for pre-training VLMs. | COCO, LAION-400M, WIT, HowTo100M, WebVid, VQA, GQA, LAION-5B                                                        | Foundation for semantic knowledge and general representations.                            |
| **Robot Demonstrations** | Trajectory datasets from real robots or high-fidelity simulators.                | RoboNet, BridgeData, RT-X, Open X-Embodiment (OXE - 1M trajectories from 22 robots), OpenVLA (970k demos), RT-series (130k demos), CALVIN, DROID | Critical for grounding actions in physical world; OXE for generalization and cross-platform learning. |
| **Human Demonstrations** | Skill transfer from human videos/interactions.                                   | UMI (in-the-wild), ORION (single human videos), HumanPlus (humanoid learning), EgoMimic (egocentric human videos) | Enriches skill transfer, addresses data scarcity. EgoMimic: Human data often valuable.    |
| **Synthetic Data**     | Data generated in simulations for scalability and specific scenarios.            | UniSim (realistic 3D envs), SynGrasp-1B (billion-frame for grasping), EnerVerse, Cosmos (physics-based), ECoT, RAD, 3D-Generalist | Mitigates data scarcity, enhances robustness, helps bridge sim-to-real gap.             |
| **Instruction Tuning** | Paradigm treating action generation as instruction following.                    | LLaVA (Visual Instruction Tuning), InstructVLA (VLA-IT: 650k samples), VIMA (prompt-based control), 3D-VLA (3D embodied instruction tuning) | Improves generalizability, mitigates catastrophic forgetting, enhances understanding of user intent. |

The efficacy of Vision-Language-Action (VLA) models is profoundly rooted in data-centric methodologies, emphasizing the critical role of large-scale, diverse datasets and sophisticated training paradigms such as instruction tuning [28,35]. This section synthesizes the extensive literature on data strategies, highlighting their commonalities, distinctions, and inherent challenges.

A fundamental commonality across VLA research is the reliance on vast and varied datasets. These typically comprise two main categories: large-scale internet corpora and diverse robot demonstration data [28]. Internet corpora, such as image-text pairs (e.g., COCO, LAION-400M, WIT with 400 million pairs) and instruction-following data (e.g., HowTo100M, WebVid), serve to pre-train vision and language encoders, endowing models with foundational semantic knowledge and general object, action, and concept representations [28,34]. In contrast, diverse robot demonstration data, including robotic trajectory datasets from real robots or high-fidelity simulators (e.g., RoboNet, BridgeData, RT-X, Open X-Embodiment), are indispensable for teaching models to translate language and perception into executable actions [28,34]. Datasets like Open X-Embodiment (OXE) are particularly significant, aggregating over 1 million trajectories from 22 diverse robots across 21 institutions, fostering generalization and cross-platform learning [16,27]. OpenVLA, for instance, was trained on 970,000 real-world robot demonstrations, further underscoring the scale of data required [11,19]. The integration of human-robot interaction data, particularly human embodied data, is increasingly recognized as critical. Approaches like UMI and HumanPlus enable direct skill transfer from human demonstrations, with EgoMimic even suggesting that "adding 1 hour of additional hand data is more valuable than adding 1 hour of additional robot data" [16].

Despite the demonstrated benefits of large datasets, a significant challenge is **data scarcity** (#DataScarcity), especially for real-world robot interactions. Training a robot for complex tasks can demand thousands of specific demonstrations to cover diverse scenarios [18]. To mitigate this, researchers employ various data annotation and generation strategies. Novel data construction methods include annotating demonstrations with sub-step reasoning, as seen in DexVLA, where language descriptions are provided every five seconds for long-horizon tasks to guide action generation [29]. CoVLA combines manipulation trajectories with detailed natural language annotations and synchronized sensor streams [28]. Synthetic data generation offers a scalable alternative; UniSim generates realistic 3D environments to enhance robustness against rare edge cases, improving performance by over 20% [25,28]. Similarly, GraspVLA leverages SynGrasp-1B, a billion-frame synthetic dataset, to address data scarcity in robotic grasping [10]. Other efforts include EnerVerse's data engine pipeline using generative models and 4D Gaussian Splatting to enhance data quality and bridge the sim-to-real gap, and Cosmos World Foundation Model Platform's support for generating physics-based synthetic data [16]. Domain-Specific Languages (DSL) are also used to represent 3D environments flexibly for data generation [33]. Furthermore, automated data synthesis pipelines like ECoT and RAD structure reasoning and generate data from robot trajectories and action-free human videos for co-training [27]. UniVLA introduces a unique method to generate "task-centric latent actions" by analyzing frame-by-frame video changes, circumventing the high cost of explicit action annotation [21].

**Instruction tuning** emerges as a critical training paradigm for VLA models. It addresses issues like catastrophic forgetting, a common problem when fine-tuning large pre-trained models, and significantly improves model generalizability by treating action generation as instruction following [14]. This paradigm allows models to retain language understanding abilities inherited from pre-trained Vision-Language Models (VLMs) while enhancing their capacity for accurate action generation in embodied contexts [5,31]. LLaVA pioneered "Visual Instruction Tuning" by converting raw image-text pairs into conversational samples, achieving high-level capabilities comparable to GPT-4V [22,26]. InstructVLA further exemplifies this with its novel Vision-Language-Action Instruction Tuning (VLA-IT) framework, utilizing a curated 650,000-sample VLA-IT dataset [5,14]. This dataset includes diverse instruction tuning tasks such as scene captions, Q&A pairs, instruction rewriting, and context construction to enhance the model's understanding of user intent and improve response capabilities [14]. VIMA similarly uses "prompt-based" control for multi-task support, while 3D-VLA fine-tunes on a curated 3D embodied instruction tuning dataset of 2 million scene-language-action pairs [32,35]. FedVLA's "Instruction Oriented Scene-Parsing mechanism" also highlights the instruction tuning aspect for contextual understanding in privacy-preserving federated learning [23].

Multi-stage training approaches are commonly employed to optimize different model components and integrate diverse data sources effectively. Models often undergo initial pre-training on large-scale vision-language datasets, followed by fine-tuning on robot demonstration data using token-level autoregressive loss [25,28]. InstructVLA, for instance, features an initial action pre-training phase followed by a dedicated instruction tuning phase [14]. JARVIS-VLA adopts a three-stage fine-tuning strategy for Minecraft, progressing from text-only world knowledge to multimodal alignment and instruction-following imitation learning [27]. Curriculum learning is another form of multi-stage training, gradually introducing complexity from simple tasks to multi-step operations [26,28].

The balance between internet-scale pre-training for general knowledge and large-scale robot demonstration data for task-specific performance is critical. Internet-scale pre-training endows VLA models with a broad understanding of the world, essential for robust vision-language correlations [6,32]. However, this general knowledge must be grounded in physical actions through extensive robot data. For example, RT-2 leverages pre-trained VLMs (PaLI-X, PaLM-E) that are trained on billions of image-text pairs and then co-finetuned with 130,000 robot trajectories [16,20]. OpenVLA, despite having fewer parameters (7B) than RT-2 (55B), achieved a 16.5% improvement in task success rate through effective joint fine-tuning on diverse, large-scale robot data, demonstrating that judicious data integration can outweigh raw model size [11,28].

Different fine-tuning strategies play a crucial role in leveraging these diverse data sources. The **co-finetuning** strategy, notably employed by RT-2, involves jointly fine-tuning a Vision-Language Model on a mixture of vast web data and robot-specific data [20,28]. To balance the significant disparity in data scale, robot data is typically sampled with an increased weight in each training batch [20]. This approach aligns action tokens with text tokens, treating them as "multimodal sentences" within the training set, enabling the model to output robot motion instruction vectors and perform tasks not explicitly seen during pre-training [20]. RT-2-X further extends this by using co-finetuning on the OXE dataset to improve cross-robot skill transfer [26].

In contrast, **self-improvement fine-tuning** iteratively refines model generations. 3D-Generalist utilizes this strategy to train VLMs by generating text prompts, iteratively updating a 3D environment, and retaining candidate action sequences that yield high CLIP scores for supervised fine-tuning [33]. This iterative process enables the VLM to learn to generate more prompt-aligned 3D environments and create large-scale synthetic 3D data, supporting scalability [33]. Other self-improvement techniques include ReflectVLM's self-correcting loop and iRe-VLA's alternation between online Reinforcement Learning (RL) for collecting successful trajectories and Supervised Fine-Tuning (SFT) on the expanded dataset, progressively improving the model through iterative imitation and exploration [26,32]. These strategies highlight an emerging trend towards models actively generating and curating their own training data to enhance performance and overcome static dataset limitations [24].

In summary, the advancements in VLA models are intimately tied to robust data-centric approaches and innovative instruction tuning paradigms. The integration of internet-scale knowledge with real-world and synthetic robot demonstrations, coupled with sophisticated training methodologies like co-finetuning and self-improvement, continues to push the boundaries of embodied AI, albeit with persistent challenges related to data scarcity and the need for high-quality, diverse data [24].
### 4.2 Integration of Pre-trained Vision-Language Models

**Strategies for Integrating Pre-trained VLMs/LLMs into VLA Models**

| Strategy                          | Description                                                                                              | Rationales / Benefits                                                                              | Key Examples / Models                                                                                                           | Challenges                                                                                                   |
|-----------------------------------|----------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|
| **Direct Fine-tuning of VLMs**    | Use large, general-purpose VLMs as core policy; fine-tune directly for robotic control.                  | Maximizes transfer of broad semantic & visual understanding, robust reasoning.                     | RT-2 (PaLI-X, PaLM-E), GPT-4o (as base), Pi0 (PaliGemma), JARVIS-VLA (Qwen2-VL, LLaVA-NeXT), VIMA (T5), ThinkAct (multimodal LLM) | High computational costs, complexity, 'catastrophic forgetting' of VLM capabilities.                         |
| **Modular LLM + Visual Encoders** | Combine pre-trained LLMs with specialized visual encoders (often fused).                                 | Controlled model sizes, flexibility in visual feature extraction, leverages LLM for language.      | OpenVLA (Llama 2 + DINOv2/SigLIP), UniVLA (Prismatic-7B), MemoryVLA (LLaMA-7B + DINOv2/SigLIP), DexVLA (Qwen-2-VL + ViT + DistilBERT), CoVLA (CLIP + LLaMA-2), InstructVLA (Eagle2-2B + DINOv2) | Requires careful alignment, potential for 'catastrophic forgetting' if not handled well.                   |
| **Action Token Alignment**        | Adapt VLMs to generate action-oriented outputs by aligning robot actions with natural language tokens.   | Enables direct semantic reasoning, generalization, treating robot control as a language task.      | RT-2 (robot actions as text tokens), DexVLA (projection module), InstructVLA (MoE adaptation), AutoVLA (action codebook)     | Balancing language generalization with precise physical control, mitigating VLM 'hallucinations'.             |



**Strategies for Integrating Pre-trained VLMs/LLMs into VLA Models**

| Strategy                          | Description                                                                                              | Rationales / Benefits                                                                              | Key Examples / Models                                                                                                           | Challenges                                                                                                   |
|-----------------------------------|----------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|
| **Direct Fine-tuning of VLMs**    | Use large, general-purpose VLMs as core policy; fine-tune directly for robotic control.                  | Maximizes transfer of broad semantic & visual understanding, robust reasoning.                     | RT-2 (PaLI-X, PaLM-E), GPT-4o (as base), Pi0 (PaliGemma), JARVIS-VLA (Qwen2-VL, LLaVA-NeXT), VIMA (T5), ThinkAct (multimodal LLM) | High computational costs, complexity, 'catastrophic forgetting' of VLM capabilities.                         |
| **Modular LLM + Visual Encoders** | Combine pre-trained LLMs with specialized visual encoders (often fused).                                 | Controlled model sizes, flexibility in visual feature extraction, leverages LLM for language.      | OpenVLA (Llama 2 + DINOv2/SigLIP), UniVLA (Prismatic-7B), MemoryVLA (LLaMA-7B + DINOv2/SigLIP), DexVLA (Qwen-2-VL + ViT + DistilBERT), CoVLA (CLIP + LLaMA-2), InstructVLA (Eagle2-2B + DINOv2) | Requires careful alignment, potential for 'catastrophic forgetting' if not handled well.                   |
| **Action Token Alignment**        | Adapt VLMs to generate action-oriented outputs by aligning robot actions with natural language tokens.   | Enables direct semantic reasoning, generalization, treating robot control as a language task.      | RT-2 (robot actions as text tokens), DexVLA (projection module), InstructVLA (MoE adaptation), AutoVLA (action codebook)     | Balancing language generalization with precise physical control, mitigating VLM 'hallucinations'.             |

The integration of pre-trained Vision-Language Models (VLMs) and Large Language Models (LLMs) forms a cornerstone of modern Vision-Language-Action (VLA) architectures, enabling them to leverage vast internet-scale knowledge for robust reasoning and perception [6,25,26,28,30,34,35]. This foundational capability allows VLAs to bridge the semantic gap between visual perception and natural language understanding, interpreting complex visual scenes with textual descriptions and generalizing to unseen objects and scenarios [25,26,28]. The field actively explores various strategies for effectively integrating these powerful models into embodied AI systems, aligning with the broader research theme of adapting foundation models to diverse robotics tasks [24].

Two primary strategies for integrating pre-trained VLMs into VLA models have emerged, each with distinct rationales and trade-offs concerning model size, complexity, and knowledge transfer. The first approach involves directly leveraging large, general-purpose VLMs as the core policy. Models such as RT-2 [16,20,25,26,27,28,30] utilize web-scale pre-trained VLMs like PaLI-X (with 5B and 55B parameters) and PaLM-E (with 12B parameters) as powerful backbones. These VLMs are co-trained or fine-tuned directly for robotic control, aiming to transfer their extensive capabilities in reasoning and perception, derived from billions of image-text pairs, to the embodied domain [20]. This direct integration strategy maximizes the transfer of broad semantic and visual understanding inherent in these large models, enabling dramatically stronger semantic comprehension for robotic tasks [26]. Other examples include GPT-4o, which serves as a base model for scene-level policies, leveraging its strong semantic reasoning [33], and $\pi_0$ series models (Pi0 Base, Pi0 FAST) that are initialized from PaliGemma [4,16,27,32]. Similarly, JARVIS-VLA is fine-tuned from pre-trained VLM models like Qwen2-VL or LLaVA-NeXT [27], and VIMA employs a pre-trained T5 model [27]. ThinkAct also trains a multimodal LLM to generate embodied reasoning plans [12].

The second prominent strategy combines pre-trained LLMs with fused or specialized visual encoders. OpenVLA exemplifies this approach by building on a Llama 2 language model and a visual encoder that fuses pre-trained features from DINOv2 and SigLIP [11,16,19,26,28,30,34]. This modular design leverages the extensive knowledge of LLMs for language understanding while benefiting from specialized visual encoders, often with frozen weights, to maintain the semantic consistency established during their pre-training on vast image datasets [28]. UniVLA, based on the Prismatic-7B visual language model, also processes image features and natural language instructions, integrating them with learned latent action "notes" into a Transformer architecture [21]. MemoryVLA similarly employs a 7B parameter Prismatic VLM backbone, along with DINOv2 and SigLIP as vision encoders and LLaMA-7B for language understanding [31]. Other instances include DexVLA, which uses the Qwen-2-VL 2B model as its base VLM and a ViT architecture for vision, with DistilBERT for language embeddings [29], and CoVLA, which integrates CLIP for visual semantic alignment and LLaMA-2 for instruction embedding [28]. InstructVLA builds its core architecture upon an Eagle2-2B backbone and utilizes a DINOv2 vision encoder for robust visual features [14].

A critical innovation across both strategies is the mechanism for adapting VLMs to generate action-oriented outputs, primarily by aligning robot "action tokens" with natural language "text tokens." In RT-2, for instance, this involves restricting the model's output vocabulary to a defined set of "action tokens," which are then treated as analogous to natural language text tokens during the co-fine-tuning process [16,20,26,28,30]. This promotes direct semantic reasoning and generalization, allowing the VLM to "absorb" robot control as another language task [26]. Other models adapt this concept; DexVLA uses a projection module (two linear layers with LayerNorm) to align action tokens from the VLM with the input requirements of a diffusion-based action expert [29], while InstructVLA employs a mixture-of-experts adaptation to align action tokens with text tokens [5]. For driving tasks, AutoVLA tokenizes actions into a codebook, using rules to eliminate VLM hallucinations, rather than relying on text output for actions [17].

While both integration strategies effectively leverage pre-trained knowledge, they present distinct trade-offs. Directly fine-tuning large general-purpose VLMs (e.g., PaLI-X 55B in RT-2) offers deep integration of web-scale knowledge but entails significant computational costs and complexity. Conversely, modular approaches, such as those combining LLMs with fused visual encoders (e.g., Llama 2 with DINOv2/SigLIP in OpenVLA), can offer more controlled model sizes and potentially greater flexibility in visual feature extraction. However, a notable challenge in adapting these models for action is the "catastrophic forgetting of pre-trained vision-language capabilities" when fine-tuned on robot-specific data [5]. InstructVLA addresses this by integrating standard VLM corpora into its VLA-IT training, ensuring the preservation of flexible VLM reasoning [5]. Furthermore, VLA models often encounter inherent limitations in 3D spatial and temporal understanding, especially in complex domains like autonomous driving, which can lead to "hallucinations" [17]. To mitigate this, OpenDriveVLA employs a strategy of pre-processing visual data into structured tokens (Map, Scene, Agent Tokens) before feeding them to the VLM, thereby providing high-quality, grounded 3D environmental understanding [17]. This reflects a broader trend toward enhancing VLM grounding and robustness in embodied contexts.
### 4.3 Efficient Fine-tuning and Adaptation

**Efficient Fine-tuning and Adaptation Methods for VLA Models**

| Method                                | Description                                                                        | Benefits                                                                                                 | Key Examples / Models                                                                                                               |
|---------------------------------------|------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|
| **Parameter-Efficient Fine-Tuning (PEFT)** | Adapt billion-parameter models with a fraction of trainable parameters.            | Reduces computational overhead (GPU usage, training time), enables deployment on consumer hardware.        | LoRA (OpenVLA, Pi-0 Fast, InstructVLA, RoboBrain), `mlx-vlm` (Macs)                                                                   |
| **Training-Free Acceleration Frameworks** | Optimize model execution without altering parameters (plug-and-play).              | Boosts inference speed, prunes redundancy, reduces memory footprint, enables edge deployment.            | EfficientVLA (prunes layers, filters tokens, caches features), FlashVLA, VLA-Cache, SP-VLA, PD-VLA, Lightweighting, Model Compression |
| **Supervised Fine-Tuning (SFT)**      | Foundational adaptation method, often with iterative refinement.                   | Acquires specific skills, improves performance with quality data, iterative improvement.                   | RoboMamba (minimal params), Pi-0 (fine-tuning on high-quality data), DexVLA (embodied curriculum learning), RT-2 (co-fine-tuning), 3D-Generalist (self-improvement) |
| **Reinforcement Learning (RL) in Fine-tuning** | Optimize performance for complex reasoning/planning through interaction.         | Guides plan generation, improves planning performance/efficiency, enables adaptive reasoning, few-shot adaptation. | AutoVLA (GRPO-based RL fine-tuning), ThinkAct (RL-guided plan generation with visual rewards), iRe-VLA (RL+SFT with DPO), Groot N1 (constrained optimization) |



**Efficient Fine-tuning and Adaptation Methods for VLA Models**

| Method                                | Description                                                                        | Benefits                                                                                                 | Key Examples / Models                                                                                                               |
|---------------------------------------|------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|
| **Parameter-Efficient Fine-Tuning (PEFT)** | Adapt billion-parameter models with a fraction of trainable parameters.            | Reduces computational overhead (GPU usage, training time), enables deployment on consumer hardware.        | LoRA (OpenVLA, Pi-0 Fast, InstructVLA, RoboBrain), `mlx-vlm` (Macs)                                                                   |
| **Training-Free Acceleration Frameworks** | Optimize model execution without altering parameters (plug-and-play).              | Boosts inference speed, prunes redundancy, reduces memory footprint, enables edge deployment.            | EfficientVLA (prunes layers, filters tokens, caches features), FlashVLA, VLA-Cache, SP-VLA, PD-VLA, Lightweighting, Model Compression |
| **Supervised Fine-Tuning (SFT)**      | Foundational adaptation method, often with iterative refinement.                   | Acquires specific skills, improves performance with quality data, iterative improvement.                   | RoboMamba (minimal params), Pi-0 (fine-tuning on high-quality data), DexVLA (embodied curriculum learning), RT-2 (co-fine-tuning), 3D-Generalist (self-improvement) |
| **Reinforcement Learning (RL) in Fine-tuning** | Optimize performance for complex reasoning/planning through interaction.         | Guides plan generation, improves planning performance/efficiency, enables adaptive reasoning, few-shot adaptation. | AutoVLA (GRPO-based RL fine-tuning), ThinkAct (RL-guided plan generation with visual rewards), iRe-VLA (RL+SFT with DPO), Groot N1 (constrained optimization) |

The development and deployment of Vision-Language-Action (VLA) models necessitate robust strategies for efficient fine-tuning and adaptation, primarily to address the high computational demands and data requirements associated with training large models from scratch [6,24]. This sub-section analyzes various techniques employed to make VLA models more adaptable and computationally accessible, thereby lowering the barrier to entry for VLA research and real-world deployment [30,35].

A central innovation in efficient VLA adaptation is **Parameter-Efficient Fine-Tuning (PEFT)**, with Low-Rank Adaptation (LoRA) emerging as a prominent technique [26,32]. LoRA adapters inject small, trainable low-rank decomposition matrices into frozen Transformer layers, enabling the fine-tuning of billion-parameter VLA models with only a fraction of the original parameters [25,28]. For instance, OpenVLA effectively utilizes LoRA adapters (approximately 20 million parameters) to fine-tune a 7-billion parameter backbone model on consumer GPUs within 24 hours, reducing GPU computation by 70% compared to full backpropagation, and achieving strong generalization in multi-task environments [25,28]. Similarly, Pi-0 Fast employs merely 10 million adapter parameters to maintain continuous 200Hz control on a static backbone with minimal accuracy loss [25,28]. InstructVLA integrates LoRA modules as "experts" within a Mixture-of-Experts (MoE) framework, employing a two-stage training strategy where an action pre-training stage fine-tunes 650 million parameters and a subsequent instruction tuning stage fine-tunes only 220 million parameters via new language LoRA modules, significantly reducing computational overhead [5,14]. RoboBrain also leverages A-LoRA and T-LoRA modules for identifying interactable regions and predicting trajectory waypoints [26]. The accessibility of LoRA is further highlighted by tools like `mlx-vlm`, which facilitate VLM inference and fine-tuning on consumer-grade hardware like Macs [22]. DexVLA employs LoRA to adapt to different robot morphologies, such as single-arm or dual-arm configurations, demonstrating its utility in transferring knowledge to novel physical embodiments without extensive task-specific fine-tuning [35].

In contrast to parameter-efficient fine-tuning, **training-free acceleration frameworks** optimize VLA model execution without altering the model's parameters, functioning as "plug-and-play" solutions [9]. EfficientVLA exemplifies this approach, acting as a training-free acceleration and compression framework that prunes redundant language layers, filters task-relevant visual tokens, and caches intermediate features to significantly boost running speed and enable real-world deployment [9,26]. Other notable training-free methods include FlashVLA, which uses a trigger mechanism to skip full decoding, selectively reusing or pruning visual tokens; VLA-Cache, which reuses cached key-value representations of static tokens; SP-VLA, combining spatio-semantic token pruning with an action-aware scheduler; and PD-VLA, which reformulates autoregressive decoding as parallel fixed-point iteration for simultaneous token prediction [26]. Complementary techniques like Lightweighting and Model Compression (quantization, feature pooling) further reduce memory footprints, enabling efficient operation on embedded devices like robot GPUs [18]. These methods collectively allow for rapid prototyping and targeted enhancements, addressing the #computationalinefficiency issue by optimizing inference rather than training.

Beyond LoRA and training-free methods, **Supervised Fine-Tuning (SFT)** remains a foundational adaptation strategy. RoboMamba, for instance, acquires manipulation skills with minimal fine-tuning parameters (0.1% of the model) and in a short timeframe (20 minutes) once sufficient reasoning capability is established [16]. Similarly, $\pi_0$ exhibits strong zero-shot generalization but benefits from fine-tuning on high-quality data to achieve high performance in complex, multi-stage tasks [16,27]. DexVLA employs an embodied curriculum learning strategy across three stages, which significantly enhances efficiency and adaptation by reducing data needs by 60% compared to end-to-end training. This strategy involves pre-training a diffusion expert, followed by embodiment-specific alignment and task-specific adaptation using sub-step annotated language data, enabling few-shot learning for dexterous skills with fewer than 100 demonstrations, outperforming OpenVLA on specific tasks [29]. RT-2 employs a "co-fine-tuning" strategy that jointly trains VLMs on internet-scale web data and robot trajectories, increasing the sampling weight for robotics data to transfer general VLM knowledge to robotic control [20]. Furthermore, "self-improvement fine-tuning" strategies, such as the one used for the Scene-Level Policy, involve the VLM generating candidate actions and updating policy parameters via SFT based on high CLIP alignment scores, thereby enhancing visual grounding and reducing visual hallucinations [33].

The integration of **Reinforcement Learning (RL) into fine-tuning** represents a sophisticated approach to optimizing VLA model performance, particularly for complex reasoning and planning tasks. AutoVLA exemplifies a hybrid approach, using SFT to implement "fast thinking" (trajectory-only) and "slow thinking" (enhanced with chain-of-thought reasoning) modes. Subsequently, reinforcement fine-tuning with Group Relative Policy Optimization (GRPO) is applied to improve planning performance and efficiency, selectively reducing unnecessary reasoning in straightforward scenarios [7,26]. This combined methodology allows for adaptive reasoning, optimizes computational efficiency, and improves the physical feasibility of actions in dynamic environments like autonomous driving [7].

ThinkAct further illustrates the power of RL for high-level planning by leveraging it to guide the multimodal LLM's plan generation, focusing on "embodied reasoning plans" and "visual plan latents" [12]. Unlike traditional RL that directly optimizes action execution, ThinkAct's approach refines the planning module through strategically designed "action-aligned visual rewards" based on goal completion and trajectory consistency [12]. This iterative refinement significantly enhances reasoning capabilities and contributes to strong few-shot adaptation in VLA models, enabling more intelligent and adaptive high-level decision-making [12]. However, a critical challenge in such RL-driven planning tasks lies in the complex **reward design and exploration**, as specifying precise, dense rewards for high-level abstract goals can be difficult and prone to misalignment.

Hybrid RL-SFT training frameworks, such as iRe-VLA, alternate between reinforcement learning in simulation and supervised fine-tuning from human demonstrations to stabilize policy updates. This approach incorporates Direct Preference Optimization (DPO) for reward shaping and conservative Q-learning to mitigate extrapolation errors, leading to a 60% reduction in sample complexity compared to pure RL while retaining semantic consistency from language-conditioned priors [26,28]. Similarly, hierarchical architectures like Groot N1 combine LLMs for high-level task decomposition (planning) with low-level diffusion policies for execution, leveraging RL algorithms with constrained optimization, like SafeVLA's Lagrangian methods, to ensure safety while maximizing task success [28].

Collectively, these efficient fine-tuning and adaptation methods—ranging from parameter-efficient techniques like LoRA to training-free acceleration frameworks and sophisticated hybrid SFT/RL approaches—significantly lower the barrier to entry for VLA research and deployment. They enable VLA models to acquire specific skills, generalize to new tasks, and adapt to novel environments with reduced computational cost and data requirements [6,16]. This is evidenced by models like UniVLA achieving an 86.3% success rate with only 10% of demonstration examples and its pre-training requiring significantly fewer GPU hours compared to previous methods [21]. The impact extends to improved few-shot adaptability, as demonstrated by DexVLA, ThinkAct, and GraspVLA [10,12]. While these advancements enhance the generalizability and practical applicability of VLA models in real-world scenarios, challenges remain in robust reward design for complex high-level planning and ensuring seamless transfer across diverse embodiments and tasks.
### 4.4 Curriculum Learning and Multi-Stage Training

**Curriculum Learning and Multi-Stage Training Approaches in VLA Models**

| Type                                 | Description                                                                                | Key Stages / Techniques                                                                                     | Benefits                                                                                                   | Key Examples / Models                                                                                                                              |
|--------------------------------------|--------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|
| **Curriculum Learning**              | Gradually introduces complexity from simple tasks to multi-step operations.                | Progressive skill acquisition, structured learning.                                                         | Enhanced data efficiency, improved generalization, better handling of long-horizon tasks.                  | DexVLA (3 stages), BLIP-2 (2 stages), Qwen-VL (3 stages), JARVIS-VLA (3 stages), VLAS (3 stages), RT-2 (co-fine-tuning with sampling weight) |
| **Multi-Stage Training**             | Breaks down learning into distinct phases, often optimizing different components or data types. | Initial pre-training (general VL data), fine-tuning (robot demo data), instruction tuning.                   | Leverages large-scale pre-trained knowledge, acquires precise embodied skills, prevents catastrophic forgetting. | UniVLA (2 stages, disentangles latent action space), InstructVLA (Action Pre-training + Instruction Tuning), OE-VLA, Reinforced Planning, ConRFT |
| **Hierarchical/Decoupled Training**  | Trains components at different levels of abstraction (planner/controller).                   | High-level VLMs for language plans/subtasks, low-level policies for raw action execution.                   | Modularity, separate objectives/optimizations, manages complex, long-horizon tasks.                      | Hi Robot, $\pi_{0.5}$, ThinkAct (dual-system framework), GR00T N1 (data pyramid)                                                               |
| **Progressive Skill Acquisition & Iterative Refinement** | Emphasizes gradual learning and continuous improvement through feedback.             | Initial pre-training, iterative model refinement (self-improvement), incremental learning.                  | Adapts to new tasks without losing old skills, continuous performance enhancement.                          | $\pi_0$ (pre-training + fine-tuning), 3D-Generalist (iterative refinement with CLIP scores), iRe-VLA (RL+SFT), Incremental Learning (Edinburgh) |



**Curriculum Learning and Multi-Stage Training Approaches in VLA Models**

| Type                                 | Description                                                                                | Key Stages / Techniques                                                                                     | Benefits                                                                                                   | Key Examples / Models                                                                                                                              |
|--------------------------------------|--------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|
| **Curriculum Learning**              | Gradually introduces complexity from simple tasks to multi-step operations.                | Progressive skill acquisition, structured learning.                                                         | Enhanced data efficiency, improved generalization, better handling of long-horizon tasks.                  | DexVLA (3 stages), BLIP-2 (2 stages), Qwen-VL (3 stages), JARVIS-VLA (3 stages), VLAS (3 stages), RT-2 (co-fine-tuning with sampling weight) |
| **Multi-Stage Training**             | Breaks down learning into distinct phases, often optimizing different components or data types. | Initial pre-training (general VL data), fine-tuning (robot demo data), instruction tuning.                   | Leverages large-scale pre-trained knowledge, acquires precise embodied skills, prevents catastrophic forgetting. | UniVLA (2 stages, disentangles latent action space), InstructVLA (Action Pre-training + Instruction Tuning), OE-VLA, Reinforced Planning, ConRFT |
| **Hierarchical/Decoupled Training**  | Trains components at different levels of abstraction (planner/controller).                   | High-level VLMs for language plans/subtasks, low-level policies for raw action execution.                   | Modularity, separate objectives/optimizations, manages complex, long-horizon tasks.                      | Hi Robot, $\pi_{0.5}$, ThinkAct (dual-system framework), GR00T N1 (data pyramid)                                                               |
| **Progressive Skill Acquisition & Iterative Refinement** | Emphasizes gradual learning and continuous improvement through feedback.             | Initial pre-training, iterative model refinement (self-improvement), incremental learning.                  | Adapts to new tasks without losing old skills, continuous performance enhancement.                          | $\pi_0$ (pre-training + fine-tuning), 3D-Generalist (iterative refinement with CLIP scores), iRe-VLA (RL+SFT), Incremental Learning (Edinburgh) |

Curriculum learning and multi-stage training have emerged as critical paradigms for developing robust and efficient Vision-Language-Action (VLA) models, addressing challenges such as data scarcity, generalization across embodiments, and the complexity of long-horizon tasks [25,28,30]. This approach systematically introduces complexity, allowing models to progressively acquire skills and integrate capabilities, thereby enhancing learning efficiency and generalization.

A prime example of this strategy is **DexVLA**, which employs a three-stage "embodied curriculum learning strategy" to achieve improved generalization and cross-embodiment control [26,29]. This structured progression is critical for learning dexterous manipulation tasks:
1.  **Cross-Embodiment Pre-training**: This initial stage focuses on acquiring embodiment-agnostic motor skills, where only the diffusion expert is pre-trained using diverse cross-embodiment data, decoupled from the VLM. This stage, notably, is five times faster than pre-training the entire VLA model and utilizes sub-step annotated instructions to prevent models from overlooking crucial intermediate steps in long-horizon tasks [29].
2.  **Embodiment-Specific Alignment**: Following pre-training, the model is refined with embodiment-specific data (filtered to a single embodiment) to align abstract vision-language representations with the physical constraints of a particular robot. This involves jointly training the VLM, projection layer, and diffusion expert, while keeping the VLM’s visual encoder frozen, enabling proficiency in tasks like shirt folding and bin picking on in-domain objects [29].
3.  **Task-Specific Adaptation**: The final stage further refines the model for complex, dexterity-demanding downstream tasks. It leverages sub-step annotated language data as *intermediate language output*, compelling the model to generate these descriptions. This mechanism facilitates the learning of complex long-horizon tasks, such as laundry folding, without relying on an external high-level policy [29].

This staged approach by DexVLA offers significant advantages over monolithic end-to-end training, particularly in managing data scarcity and enabling efficient learning for complex, long-horizon tasks across diverse robot embodiments. It effectively reduces data needs by 60% compared to end-to-end training, highlighting its efficiency [29].

Beyond DexVLA, various VLA models leverage multi-stage training with different focuses:
*   **Two-Stage Training Architectures**: Many models adopt a two-stage approach. **BLIP-2** utilizes two stages to align frozen pre-trained image encoders with frozen Large Language Models (LLMs) [32]. Similarly, **UniVLA** employs a "two-stage orchestration" to disentangle latent action space into task-centric and task-irrelevant components. This strategy allows the model to efficiently learn compact and generalizable action representations, achieving performance comparable to OpenVLA with only 4.45% of the training time by focusing on a simpler pretraining target of the latent space compared to raw actions [21,32]. **InstructVLA** also features a two-stage process: "Action Pre-training" (Stage 1), where the model is pre-trained on diverse manipulation task data to generate concrete actions and language descriptions, involving approximately 650 million parameters. This is followed by "Instruction Tuning" (Stage 2), where the action expert is frozen, and the LLM backbone is fine-tuned to process complex instructions, with only the Mixture-of-Experts (MoE) module (approximately 220 million parameters) being trainable. This process reactivates VLM's multimodal reasoning capabilities, leading to a "Generalist" model [5,14]. **RT-2** integrates internet-scale vision-language data (e.g., VQA datasets) with robot trajectory data through a co-fine-tuning process after initial VLM pre-training. This allows RT-2 to leverage vast pre-trained knowledge for understanding and adapt it for specific embodied action tasks, thereby achieving cross-robot zero-shot capabilities [20,35]. Other two-stage strategies include **OE-VLA** for enhanced multimodal learning with interleaved images and videos, **Reinforced Planning** with Supervised Fine-Tuning (SFT) followed by reinforcement learning, and **ConRFT** combining offline behavioral cloning with online Human-in-the-Loop reinforcement learning [26].
*   **Three-Stage Training Architectures**: Beyond DexVLA, **Qwen-VL** uses a three-stage strategy for interleaved image-text understanding and visual grounding [32]. **JARVIS-VLA** similarly employs a three-stage fine-tuning process for adapting pre-trained VLMs to Minecraft tasks [27]. **VLAS** utilizes a three-stage fine-tuning process specifically for integrating voice commands and supporting personalized tasks [30].
*   **Hierarchical and Decoupled Training**: This involves training components at different levels of abstraction. For instance, **Hi Robot** and $\pi_{0.5}$ train high-level VLMs for language plans or subtasks, and low-level policies for raw action execution [32]. **ThinkAct** proposes a "dual-system framework" where a multimodal LLM for high-level planning is optimized separately, and its visual plan latents then condition a downstream action model, allowing for distinct objectives and optimizations [12]. **GR00T N1** uses a hierarchical architecture combined with a "data pyramid" comprising web/human video, synthetic simulation, and real-world data [27].
*   **Progressive Skill Acquisition and Iterative Refinement**: This approach emphasizes gradual learning and continuous improvement. **$\pi_0$** involves a pre-training phase on mixed datasets followed by fine-tuning on smaller, task-specific datasets [27]. The **3D-Generalist** framework utilizes a multi-stage approach for 3D world creation, progressing from panoramic environment generation to scene-level and then asset-level policy for iterative object placement and refinement [33]. **iRe-VLA** alternates between online reinforcement learning to collect successful trajectories and SFT on the expanded dataset, progressively improving the model through imitation and exploration [26]. Furthermore, **Incremental Learning** frameworks, such as those proposed by the University of Edinburgh, aim to solve "catastrophic forgetting" by allowing robots to continuously adapt to new tasks without losing previously acquired skills [18].

Commonalities across these approaches include the strategy of initially pre-training on general vision-language datasets (e.g., using masked language modeling) to acquire broad semantic understanding, followed by fine-tuning on robot demonstration data using token-level autoregressive loss to ground actions [25,28]. This approach unifies semantic priors with task execution data, enabling VLA models to generalize across tasks, domains, and embodiments [28]. Some models, like **GraspVLA**, employ "joint training on synthetic action data and Internet semantics data," which is a form of multi-modal training combining distinct data sources, though not explicitly described as a staged curriculum for progressive skill acquisition [10].

The primary strengths of curriculum learning and multi-stage training lie in their ability to enhance data efficiency, improve generalization and cross-embodiment transfer, and enable the tackling of complex, long-horizon tasks. By breaking down learning into manageable stages, models can leverage large-scale pre-trained knowledge while also acquiring precise embodied skills. However, a significant challenge remains in designing truly effective curricula for embodied AI. This involves determining the optimal sequence of tasks, the appropriate data distribution for each stage, and how to effectively transfer knowledge between stages, especially given the dynamic and diverse nature of real-world robot interactions. While approaches like MemoryVLA leverage pre-trained VLMs, not all explicitly detail subsequent multi-stage training or curriculum learning strategies, indicating a potential area for further research and explicit design in future VLA models [31].
### 4.5 Reinforcement Learning Approaches

**Reinforcement Learning in VLA Models: Approaches and Challenges**

| Aspect                | Description                                                                    | Key Algorithms / Techniques                                                                                              | Benefits                                                                           | Challenges                                                                                                        |
|-----------------------|--------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|
| **Primary Role**      | Learning through interactive trial-and-error, self-guided exploration.         | Optimizes performance directly through environment interaction, crucial for complex/long-horizon tasks.                  | Overcomes IL limitations (distributional shift, demo scarcity).                    | Sample inefficiency (vast data), high real-robot interaction cost, safety during exploration.                     |
| **Algorithms**        | Policy optimization methods.                                                   | GRPO (DeepSeek-R1, AutoVLA, Cosmos-Reason1, AlphaDrive, ManipLVM-R1), PPO (NaVILA).                                      | Enhances embodied reasoning and efficiency.                                        |                                                                                                   |
| **Hybrid Training**   | Combines RL with other paradigms (e.g., SFT, IL).                              | iRe-VLA (RL+SFT, DPO for reward, Conservative Q-Learning), ReWiND (offline IQL + online SAC), ConRFT (BC + QL + HITL) | Reduces sample complexity, stabilizes policy updates, leverages prior knowledge.   | Complex reward design, sim-to-real gap, safety for real-world exploration.        |
| **Reward Shaping**    | Techniques to guide RL agents effectively, especially with sparse rewards.       | VLM-based feedback (Grape, TGRPO), DPO, RPRM (predicts success likelihood).                                              | Addresses sparse reward issue, improves learning efficiency.                       | Difficulty in precise, dense reward engineering.                                  |
| **Advanced RL Concepts** | RL Transformers (DT, TT, Gato), RL for high-level plan generation (ThinkAct).  | Multi-modal/task/embodiment generalization, few-shot adaptation, long-horizon planning, self-correction.                 | Extends RL to complex sequential/multi-modal tasks.                                |                                                                                                   |
| **Safety Integration** | RL with constrained optimization.                                              | SafeVLA (Lagrangian methods)                                                                                             | Ensures safety while maximizing task success.                                      | Defining comprehensive safety constraints.                                        |



**Reinforcement Learning in VLA Models: Approaches and Challenges**

| Aspect                | Description                                                                    | Key Algorithms / Techniques                                                                                              | Benefits                                                                           | Challenges                                                                                                        |
|-----------------------|--------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|
| **Primary Role**      | Learning through interactive trial-and-error, self-guided exploration.         | Optimizes performance directly through environment interaction, crucial for complex/long-horizon tasks.                  | Overcomes IL limitations (distributional shift, demo scarcity).                    | Sample inefficiency (vast data), high real-robot interaction cost, safety during exploration.                     |
| **Algorithms**        | Policy optimization methods.                                                   | GRPO (DeepSeek-R1, AutoVLA, Cosmos-Reason1, AlphaDrive, ManipLVM-R1), PPO (NaVILA).                                      | Enhances embodied reasoning and efficiency.                                        |                                                                                                   |
| **Hybrid Training**   | Combines RL with other paradigms (e.g., SFT, IL).                              | iRe-VLA (RL+SFT, DPO for reward, Conservative Q-Learning), ReWiND (offline IQL + online SAC), ConRFT (BC + QL + HITL) | Reduces sample complexity, stabilizes policy updates, leverages prior knowledge.   | Complex reward design, sim-to-real gap, safety for real-world exploration.        |
| **Reward Shaping**    | Techniques to guide RL agents effectively, especially with sparse rewards.       | VLM-based feedback (Grape, TGRPO), DPO, RPRM (predicts success likelihood).                                              | Addresses sparse reward issue, improves learning efficiency.                       | Difficulty in precise, dense reward engineering.                                  |
| **Advanced RL Concepts** | RL Transformers (DT, TT, Gato), RL for high-level plan generation (ThinkAct).  | Multi-modal/task/embodiment generalization, few-shot adaptation, long-horizon planning, self-correction.                 | Extends RL to complex sequential/multi-modal tasks.                                |                                                                                                   |
| **Safety Integration** | RL with constrained optimization.                                              | SafeVLA (Lagrangian methods)                                                                                             | Ensures safety while maximizing task success.                                      | Defining comprehensive safety constraints.                                        |

Reinforcement Learning (RL) constitutes a pivotal paradigm in the development of Vision-Language-Action (VLA) models, offering a dynamic alternative to traditional imitation learning (IL) by enabling agents to learn through interactive trial-and-error and self-guided exploration [12,32]. This capability is crucial for VLA models to overcome the inherent limitations of IL, such as distributional shift and the difficulty of acquiring comprehensive expert demonstrations for all possible scenarios [28,34]. While IL relies on pre-recorded expert data, RL trains policies by optimizing performance through direct interaction with the environment, which is particularly advantageous for complex behaviors and long-horizon tasks where defining explicit reward functions or gathering demonstrations is challenging [26,28,34]. In contrast, models like MemoryVLA predominantly utilize an imitation learning paradigm coupled with diffusion models for action generation, focusing on supervised learning from demonstrations rather than explicit RL algorithms [31], and some reviews on embodied AI also primarily emphasize imitation learning [16].

A significant contribution of RL algorithms to VLA research lies in enhancing embodied reasoning and efficiency. Group Relative Policy Optimization (GRPO) emerges as a prominent algorithm in this context. For instance, DeepSeek-R1, a Large Foundation Model (LFM), acquires its reasoning capabilities through large-scale reinforcement learning based on GRPO [32]. In the domain of autonomous driving, AutoVLA integrates a GRPO-based reinforcement fine-tuning method to improve planning performance and efficiency by actively reducing unnecessary reasoning in straightforward scenarios [7,17]. Similarly, Cosmos-Reason1, a VLM, and AlphaDrive, an autonomous driving VLM, are trained with initial supervised fine-tuning (SFT) followed by GRPO-based RL exploration to achieve embodied reasoning and robust performance [27]. Other models like Reinforced Planning and ManipLVM-R1 also leverage GRPO for improved generalization and the prediction of affordance areas and trajectories [26]. Beyond GRPO, algorithms such as Proximal Policy Optimization (PPO) have been employed, as seen in NaVILA for training visual locomotion policies [32].

The integration of RL into VLA models often involves hybrid training paradigms and sophisticated reward shaping techniques to address inherent challenges. The iRe-VLA framework, for example, combines RL in simulation with supervised fine-tuning from human demonstrations to stabilize policy updates, leveraging Direct Preference Optimization (DPO) for reward model shaping and Conservative Q-Learning to prevent extrapolation errors. This hybrid approach significantly reduces sample complexity by 60% compared to pure RL [25,28]. Other hybrid methods include ReWiND, which integrates offline Implicit Q-Learning with online Soft Actor-Critic, and ConRFT, which combines offline behavior cloning and Q-learning with online human-in-the-loop intervention and RL losses [26]. Effective reward modeling is crucial, especially in environments with sparse rewards. VLA-RL trains a Robotic Process Reward Model (RPRM) to predict success likelihood, while ReWiND assigns higher rewards to states visually closer to the goal [26]. Furthermore, VLMs can be prompted to generate feedback-based reward signals, as demonstrated by Grape and TGRPO [26], and the use of Large Language Models (LLMs) with RL for efficient policy learning and system planning is an active research area [24]. Reinforcement Learning from Human Feedback (RLHF), a technique prominent in LLMs, is utilized by models like SEED to tackle sparse reward issues, and Reflexion proposes a verbal reinforcement learning framework using linguistic feedback instead of direct weight updates [34]. For safety-critical applications, RL algorithms augmented with constrained optimization, such as SafeVLA's Lagrangian methods, learn policies that maximize task success rates while strictly adhering to safety constraints, which is particularly valuable in tasks with sparse feedback like dynamic obstacle avoidance [25,28].

RL is recognized as an "advanced domain" for integrating large VLM-based VLA models, enhancing their learning and optimization for robust performance in robotic manipulation tasks [6]. The nature of RL trajectories aligns well with sequence modeling problems, leading to the development of RL Transformers like DecisionTransformer (DT), TrajectoryTransformer (TT), and Gato, which extend to multi-modal, multi-task, and multi-embodiment settings [34]. ThinkAct trains a multimodal LLM with RL to generate "embodied reasoning plans," guided by "action-aligned visual rewards based on goal completion and trajectory consistency" [12]. This enables capabilities such as few-shot adaptation, long-horizon planning, and self-correction, surpassing the limitations of purely imitation-based methods for complex reasoning [12]. Even seemingly non-RL approaches, such as the "self-improvement fine-tuning" in 3D Generalist, which uses CLIP scores as feedback to select optimal action sequences for SFT, implicitly mimic the iterative learning and improvement characteristic of RL, with the ultimate goal of generating environments for training RL policies [33].

Despite its advantages, RL approaches in VLA contexts face significant challenges. The primary weakness is **sample efficiency**, as pure RL often requires vast amounts of interaction data, leading to a high cost associated with real-robot interaction [6,28]. This translates to high reset costs and low interaction efficiency, particularly for online RL in real-world scenarios [26,32]. Ensuring **safety in real-world environments** during exploration remains a critical concern, especially given the exploratory nature of RL and the difficulty of defining precise reward functions in sparse feedback scenarios [28]. Moreover, the complexity of **effective reward shaping** continues to be a hurdle for optimizing complex behaviors [28].

Future directions for making RL more feasible and effective in VLA training include:
1.  **Developing more sample-efficient RL algorithms**: This involves advancing hybrid online-offline methods, leveraging large datasets for offline pre-training, and exploring model-based RL to reduce real-world interaction demands.
2.  **Enhancing reward shaping techniques**: Further research into VLM-based dense reward generation, inverse reinforcement learning, and advanced preference-based learning (like DPO) could alleviate the burden of manual reward engineering.
3.  **Improving safety and reliability**: Integrating formal verification methods, robust constrained RL, and explicit risk-aware planning into VLA models is crucial for deployment in safety-critical applications.
4.  **Bridging the sim-to-real gap**: Focusing on high-fidelity simulators and robust domain adaptation techniques will reduce reliance on expensive real-robot interactions, making RL more scalable.
5.  **Synergistic LLM-RL integration**: Further research into how LLMs can provide intelligent policy learning, dynamic planning, and verbal feedback for RL agents holds significant promise [24].

By addressing these challenges, RL can unlock the full potential of VLA models, enabling them to generalize more effectively, adapt to novel situations, and operate robustly in complex, unstructured environments.
### 4.6 Imitation Learning and Learning from Demonstrations

**Imitation Learning in VLA: Data Sources, Impact, and Challenges**

| Aspect / Source       | Description                                                                    | Benefits / Impact                                                                                                    | Key Examples / Models / Datasets                                                                                                                            | Limitations / Challenges                                                          |
|-----------------------|--------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|
| **Core Idea**         | Directly models action distribution from expert trajectories (state-action sequences). | Foundational for acquiring physical manipulation skills, effective when rewards are hard to define.                                | Behavior Cloning (BC) as dominant method.                                                                                                                   | Performance capped by demo quality, struggles with OOD scenarios.             |
| **Robot Demonstrations** | Extensive datasets of robotic trajectories for manipulation.                     | Acquires manipulation capabilities, learns common action patterns, adapts across platforms.                                | InstructVLA (VLA-IT: 650k demos), OpenVLA (970k demos), RT-series (130k demos), Open X-Embodiment (OXE: 1M trajectories), CALVIN, DROID, BridgeData V2 | Costly, labor-intensive to collect, specific to robot platform.                    |
| **Human Demonstrations** | Learning from human videos and interactions.                                   | Enriches skill transfer, narrows embodiment gap, effective for complex full-body skills.                             | UMI (in-the-wild), ORION (single videos), HumanPlus (humanoid learning), EgoMimic (egocentric human videos), UniVLA (human+robot videos), LAPA, VPDD, RAD | Data collection still has overhead, need for robust tracking (HaMeR).             |
| **Synthetic Data**    | Demonstrations generated in simulation environments.                             | Scalable alternative, mitigates data scarcity, enables domain randomization for sim-to-real.                         | GraspVLA (SynGrasp-1B), RoboPoint                                                                                                                            | Realism gap, may lack real-world complexities.                                    |
| **Key Contributions** |                                                                                | Addresses data scarcity, improves robot generalization (cross-robot, novel objects), enables complex real-world skills. | UniVLA (unified latent actions), ORION/GraspVLA (novel object generalization), HumanPlus (few-shot learning).                                               | Still needs diverse data, OOD robustness, optimal level of human/robot data. |



**Imitation Learning in VLA: Data Sources, Impact, and Challenges**

| Aspect / Source       | Description                                                                    | Benefits / Impact                                                                                                    | Key Examples / Models / Datasets                                                                                                                            | Limitations / Challenges                                                          |
|-----------------------|--------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|
| **Core Idea**         | Directly models action distribution from expert trajectories (state-action sequences). | Foundational for acquiring physical manipulation skills, effective when rewards are hard to define.                                | Behavior Cloning (BC) as dominant method.                                                                                                                   | Performance capped by demo quality, struggles with OOD scenarios.             |
| **Robot Demonstrations** | Extensive datasets of robotic trajectories for manipulation.                     | Acquires manipulation capabilities, learns common action patterns, adapts across platforms.                                | InstructVLA (VLA-IT: 650k demos), OpenVLA (970k demos), RT-series (130k demos), Open X-Embodiment (OXE: 1M trajectories), CALVIN, DROID, BridgeData V2 | Costly, labor-intensive to collect, specific to robot platform.                    |
| **Human Demonstrations** | Learning from human videos and interactions.                                   | Enriches skill transfer, narrows embodiment gap, effective for complex full-body skills.                             | UMI (in-the-wild), ORION (single videos), HumanPlus (humanoid learning), EgoMimic (egocentric human videos), UniVLA (human+robot videos), LAPA, VPDD, RAD | Data collection still has overhead, need for robust tracking (HaMeR).             |
| **Synthetic Data**    | Demonstrations generated in simulation environments.                             | Scalable alternative, mitigates data scarcity, enables domain randomization for sim-to-real.                         | GraspVLA (SynGrasp-1B), RoboPoint                                                                                                                            | Realism gap, may lack real-world complexities.                                    |
| **Key Contributions** |                                                                                | Addresses data scarcity, improves robot generalization (cross-robot, novel objects), enables complex real-world skills. | UniVLA (unified latent actions), ORION/GraspVLA (novel object generalization), HumanPlus (few-shot learning).                                               | Still needs diverse data, OOD robustness, optimal level of human/robot data. |

Imitation Learning (IL), often synonymous with learning from demonstrations, represents a foundational paradigm for training Vision-Language-Action (VLA) models, particularly in acquiring embodied experiences and physical manipulation skills [25,28]. This strategy is critical when defining explicit reward functions for reinforcement learning proves challenging, as IL directly models the action distribution from expert trajectories, which intrinsically lack explicit rewards [34]. These trajectories typically consist of state-action sequences, such as $\tau=(s_1,a_1,...,s_T,a_T)$ [34]. Behavior Cloning (BC), a common form of IL, is the dominant method for training VLAs, particularly through supervised learning techniques that train autoregressive policy decoders to predict action tokens based on fused visual-language-state embeddings [25,28,30].

The efficacy of imitation learning in VLA models heavily depends on the scale, diversity, and quality of the demonstration datasets [30]. These datasets are typically sourced from two main categories: robot demonstrations and human demonstrations, with synthetic data emerging as a valuable third.

**Robot Demonstrations:**
Many prominent VLA models leverage extensive robot demonstration datasets to acquire manipulation capabilities. InstructVLA, for instance, relies on the VLA-IT dataset, comprising approximately 650,000 human-robot interaction data entries with detailed language action annotations [5,14]. Its "Action Pre-training" stage trains an "action expert" to imitate latent actions derived from these demonstrations [14]. Similarly, OpenVLA is pretrained on 970,000 real-world robot operation data entries and has demonstrated superior performance over other from-scratch imitation learning methods like Diffusion Policy [11,19,26]. Other notable examples include RT-1 and RT-2, which were fine-tuned on a robot dataset containing 130,000 trajectories across over 700 tasks, teaching models to map visual observations and language commands to robot actions [20,27,28]. Octo and MemoryVLA have also been trained on the Open X-Embodiment dataset, which contains 800,000 robot demonstrations, facilitating cross-robot skill transfer through co-finetuning [26,28,31]. Datasets like BridgeData V2 (over 60,000 trajectories, including 50,000 human operations) and DROID (76,000 demonstrations) provide diverse robot demonstrations, allowing VLA models to learn common action patterns and adapt across different robot platforms [35]. These demonstrations are acquired through various methods such as kinesthetic teaching, teleoperation (e.g., Hi Robot [32]), or scripted policies [25].

**Human Demonstrations (Learning from Human Videos):**
A significant trend in VLA research is the direct integration of "learning from human videos," recognized as an advanced domain for VLM-based VLA models [6]. This approach enriches VLA models by transferring task-relevant knowledge and narrowing the embodiment gap [26]. Several innovative methods leverage human inputs:
*   **UMI (Universal Manipulation Interface)** tackles the "chicken or egg" problem in robot training by facilitating skill transfer from "in-the-wild human demonstrations" directly to deployable robot policies [16]. Its open-source nature promotes wider adoption and data collection.
*   **ORION** focuses on vision-based manipulation, extracting object-centric manipulation plans and deriving policies from "single human videos" captured even by mobile devices, enabling generalization to new objects and environments [16].
*   **HumanPlus** introduces a full-stack system for humanoid robots to learn complex autonomous and movement skills. It employs a real-time "shadowing system" where human operators control humanoids via a single RGB camera, and a Humanoid Imitation Transformer (HIT) that learns high-degree-of-freedom control with as few as 40 demonstrations, achieving high success rates [16].
*   **EgoMimic** scales imitation learning by utilizing "egocentric human videos paired with 3D hand tracking," treating human and robot data equally to learn unified policies. It notably finds that one hour of human hand data can be more valuable than one hour of robot data for long-horizon, single-arm, and bimanual manipulation tasks [16].
Other approaches include UniVLA, which learns task-centric latent actions from unlabeled human and robot videos to unify policy planning and synthesize a common "musical score" for actions, enabling skill transfer without explicit action annotations [21,26]. Methods like LAPA and VPDD utilize VQ-VAE quantized latent actions and discrete diffusion over unified video tokens, respectively, to enable transfer from actionless human videos and facilitate cross-domain knowledge transfer [26]. Furthermore, RAD synthesizes reasoning data from robot trajectories and easily accessible action-free human videos using HaMeR for tracking, facilitating co-training on both sources [27].

**Synthetic Demonstrations:**
Beyond real-world data, synthetic demonstrations offer a scalable alternative. GraspVLA, for example, is pretrained on "SynGrasp-1B," a billion-frame robotic grasping dataset derived from synthetic action data. This leverages simulated demonstrations for skill acquisition, with extensive domain randomization aiming to ensure transferability to real-world scenarios [10]. RoboPoint also constructs synthetic datasets specifically for instruction-tuning VLMs [32]. These methods highlight the potential of simulation environments to generate high-quality training data, implicitly acting as demonstrations [18].

**Impact and Contributions:**
Imitation learning methods significantly contribute to addressing key challenges in VLA research:
1.  **Addressing Data Scarcity**: Approaches like UMI's in-the-wild human demonstrations, ORION's ability to learn from single human videos, and HumanPlus's few-shot learning capabilities (e.g., from 40 demonstrations) directly mitigate the need for vast, robot-specific datasets [16]. Synthetic data generation also plays a crucial role in alleviating this constraint [10].
2.  **Improving Robot Generalization**: UniVLA's unified latent action representation enables cross-robot and cross-embodiment skill transfer by transcending specific hardware [21]. The Open X-Embodiment dataset similarly facilitates co-finetuning for cross-robot skill transfer [26]. Moreover, methods like ORION and GraspVLA's domain randomization enhance generalization to novel objects and environments [10,16]. EgoMimic's unified policy learning across human and robot data further underscores this, demonstrating improved performance in complex manipulation tasks [16].
3.  **Enabling Complex, Real-World Skills**: The detailed annotations in datasets like DexVLA, which include language descriptions of sub-steps every five seconds, help models learn disentangled action representations crucial for complex, long-horizon tasks [29]. HumanPlus's capacity to learn intricate autonomous and movement skills from human data, and RT-2's mastery of diverse robotic tasks like "Pick Object" and "Open Drawer," exemplify the ability of IL to instill complex behavioral repertoires in robots [16,20].

**Limitations and Challenges:**
Despite its strengths, imitation learning presents limitations. A primary concern is that performance is inherently capped by the quality and coverage of the provided demonstrations [30]. Models trained via IL often struggle with out-of-distribution scenarios not explicitly present in the training data, leading to a lack of robustness when faced with novel situations [30]. For instance, while UAV-VLA evaluates its performance against human expert flight paths, it primarily uses these as a benchmark rather than explicitly learning through imitation from them, indicating a potential gap in applying IL for such complex, real-time control [15]. The value distribution between human and robot data also presents an interesting debate, with EgoMimic suggesting that even a small amount of high-quality human hand data can outweigh larger volumes of robot data for certain tasks [16].

In conclusion, imitation learning from diverse sources—robot trajectories, human videos, and synthetic environments—remains a cornerstone for VLA model development. By systematically organizing and leveraging these demonstrations, researchers continue to advance the state-of-the-art in robot skill acquisition, striving to overcome data limitations and enhance generalization for complex, real-world applications.
### 4.7 Privacy-Preserving and Distributed Learning
The increasing reliance on user-specific data for training Vision-Language-Action (VLA) models has brought the critical issues of data privacy and security to the forefront [25]. Traditional centralized training paradigms, which aggregate sensitive user data onto a single server, inherently pose significant privacy risks, particularly in domains such as robotics, healthcare, and smart homes where VLA models are deployed [25]. The academic discourse acknowledges the urgent need for privacy-preserving solutions, including on-device processing, homomorphic encryption, and differential privacy, to safeguard user data during both training and inferencing phases [25]. While some reviews focus broadly on VLM-driven robot manipulation, they often omit detailed discussions on these crucial privacy aspects, highlighting a gap in the comprehensive understanding of VLA model development and deployment challenges [6].

To address these emerging challenges, federated learning (FL) has emerged as a promising distributed learning paradigm. FL enables the collaborative training of machine learning models across multiple decentralized client devices or organizations holding local data samples, without exchanging their raw data. This approach significantly mitigates privacy concerns by keeping sensitive data on client devices. 



Within the VLA domain, FedVLA represents a seminal contribution as the first federated VLA learning framework specifically designed to tackle the privacy and security challenges inherent in training VLA models on distributed, user-specific data [23].

FedVLA employs several technical mechanisms to ensure privacy preservation without compromising model performance. Central to its design is **distributed model training**, which allows VLA models to learn from diverse datasets across multiple clients while keeping the sensitive raw data localized and never sent to a central server [23]. This distributed approach inherently protects user privacy. Furthermore, FedVLA incorporates **task-aware representation learning** to facilitate efficient and privacy-preserving training by focusing on relevant features for specific tasks, thereby enhancing the utility of local data without over-exposing it [23].

A key innovation within FedVLA is its use of a Dual Gating Mixture-of-Experts (DGMoE) for **adaptive expert selection** [23]. In this mechanism, experts within the model adaptively activate based on the specific task patterns and data characteristics of individual clients. This allows for a more nuanced and specialized learning process where different experts contribute to different aspects of the learning task in a distributed setting, enhancing model capacity and flexibility [23]. Complementing this, FedVLA utilizes an **expert-driven aggregation strategy** at the federated server [23]. Instead of simple averaging, the aggregation of model updates from clients is intelligently guided by the activated experts. This targeted aggregation ensures that cross-client knowledge transfer is effective and privacy-preserving, as only aggregated, anonymized insights, rather than raw data or individual client updates, are shared [23]. The efficacy of FedVLA is evidenced by its achievement of task success rates comparable to those obtained through traditional centralized training, while critically maintaining robust data privacy [23].

The benefits of such distributed learning paradigms like FedVLA, particularly in sensitive application domains like robotics, are substantial when compared to traditional centralized training. Centralized training necessitates collecting and storing all user data in a single location, creating a single point of failure and a high-value target for security breaches. In contrast, distributed learning minimizes the risk of data exposure by preventing the centralization of raw data. This is particularly crucial for robotic systems operating in homes or industrial settings, where user interactions and environmental data can be highly personal and proprietary. By enabling VLA models to learn collaboratively from diverse, private datasets without direct data sharing, federated learning ensures that the ethical deployment of these advanced AI systems can proceed with greater assurance regarding data confidentiality and integrity [25]. This paradigm shift moves beyond merely acknowledging privacy concerns to actively architecting solutions that embed privacy by design into the VLA model development lifecycle.
## 5. Action Representation and Generation Mechanisms
Action Representation and Generation Mechanisms constitute a pivotal domain within Vision-Language-Action (VLA) models, directly addressing how these systems translate multimodal inputs—encompassing visual observations and linguistic instructions—into concrete, executable actions within embodied environments [25,35]. This section systematically explores the theoretical frameworks, diverse methodologies, and intricate interplay between various action representations and generation strategies, aiming to elucidate how high-level goals are transformed into precise, actionable sequences [2,18].

At its core, VLA action generation is conceptualized around "action tokens," which serve as semantically meaningful outputs that encapsulate actionable information derived from fused vision and language inputs [27,32]. These tokens are designed to progressively encode grounded and executable information, ultimately culminating in specific control commands for robotic execution, thereby effectively closing the perception-action loop [28].

This section commences with an examination of **Unified Frameworks for Action Generation and High-level Control**, which broadly categorize VLA models into Monolithic and Hierarchical architectures. Monolithic models, such as RT-2, are characterized by their integration of perception, reasoning, and control within a single, end-to-end pipeline, directly mapping multimodal semantics to low-level actions by treating robot-specific actions as analogous to text tokens [16,20]. Conversely, Hierarchical models, exemplified by ThinkAct, explicitly decouple high-level planning from low-level execution, often employing intermediate representations like subtasks or programs to guide independent controllers [6,12,26]. A significant cross-cutting trend in both architectural paradigms is the increasing adoption of diffusion models, recognized for their capability to generate complex conditional distributions over future actions, thereby contributing to smoother and more robust control sequences [2,29,31]. Furthermore, ongoing efforts, such as those demonstrated by UMI and UniVLA, focus on unifying action spaces to transcend embodiment-specific limitations and foster broader cross-robot generalizability [16,21]. The challenge of mitigating VLM "hallucinations" in critical applications is also addressed through strategies like integrating grounded contextual information (e.g., OpenDriveVLA) or employing explicit tokenization with rule-based mechanisms (e.g., AutoVLA) to ensure reliability and safety [17].

Subsequently, the section systematically dissects the diverse **Action Representation Formats and Decoding Strategies** employed across VLA models. It elaborates on a spectrum of action token types, each distinguished by its unique underlying mechanisms, inherent advantages, and associated trade-offs. These include:
*   **Language Descriptions**, which harness the advanced reasoning capabilities of Large Language Models (LLMs) and Vision-Language Models (VLMs) for high-level planning and instruction following, albeit confronting inherent challenges in precision for fine-grained control [27].
*   **Code**, which provides a structured and logical mechanism for control through executable programs or pseudocode, enabling precise and robust execution for complex, long-horizon tasks. However, this approach is often constrained by its API dependency and susceptibility to brittleness [32,33].
*   **Affordances**, which offer spatially grounded representations of task-specific, interaction-relevant object properties—such as keypoints, bounding boxes, or segmentation masks—thereby directly linking visual perception to functional interaction and promoting cross-platform generalization [26,32].
*   **Trajectories**, which explicitly delineate temporally ordered sequences of specific movements (e.g., point trajectories, visual trajectories, or continuous value sequences) essential for fine-grained control. While often generated by diffusion or flow-matching models, they grapple with issues of high-dimensionality and computational costs [2,9,27].
*   **Goal States**, which represent predicted future observations (e.g., images, point clouds, or videos) to guide planning and enhance robustness against transient execution errors. However, their effective utilization depends on high-quality generation and can incur significant inference latency [3,32].
*   **Latent Representations**, which provide compressed, abstract encodings of action semantics to promote efficiency and generalizability, particularly through innovations like task-centric latent actions exemplified by UniVLA. Nevertheless, they often face challenges related to explainability and potential misalignment with human intent [21,32].
*   **Raw Actions**, which entail a direct mapping from multimodal inputs to physical motor commands (e.g., joint angles, force/torque values). While leveraging the power of VLMs for end-to-end learning, this approach is constrained by data scarcity and typically exhibits poor cross-embodiment generalization [26,27,28].
*   **Reasoning**, which functions as a meta-token or an intermediate deliberative process—frequently externalized in natural language via Chain-of-Thought approaches—to enhance decision-making, improve interpretability, and facilitate long-horizon planning. Despite its benefits, reasoning can lead to increased inference time and presents grounding challenges [26,29,32,33].

The intricate interplay and complementary nature between these various action representation formats and their corresponding generation mechanisms profoundly influence the overall capabilities and limitations of VLA models. The judicious selection and implementation of an appropriate action representation invariably involve navigating complex trade-offs among expressiveness, precision, interpretability, and computational efficiency. Contemporary research in this field consistently addresses challenges such as the inherent high-dimensionality of action spaces, the persistent semantic-physical mismatch, pervasive data scarcity, and the critical need for robust generalization across a diverse array of tasks and robotic embodiments. The overarching trajectory of the field is directed towards the development of sophisticated hybrid models that synergistically combine the strengths of disparate representations and decoding strategies, further integrating advanced world models and adaptive reasoning capabilities to foster the emergence of more robust, generalizable, and interpretable embodied AI systems capable of executing complex tasks with human-like proficiency.
### 5.1 Unified Frameworks for Action Generation and High-level Control


Vision-Language-Action (VLA) models fundamentally aim to provide unified frameworks for action generation and high-level control by integrating multimodal inputs (vision and language) to produce actionable guidance [8,11,19,30,35]. This approach enables systems to move beyond mere description to sophisticated task execution, transforming high-level goals from linguistic inputs into concrete action sequences [2,18].

At its core, the VLA unified framework, particularly from an action tokenization perspective, conceptualizes the process as transforming vision and language inputs through a series of VLA modules into a "chain of action tokens" [27,28,32]. These action tokens progressively encode more grounded and actionable information, culminating in executable actions [27,32]. VLA modules are defined as differentiable subnetworks or non-differentiable functional units that facilitate end-to-end gradient flow, while action tokens are their semantically meaningful outputs, analogous to language tokens but encapsulating action-related information [32].

The general architecture for this token-based framework typically involves three main stages: multimodal input acquisition, multimodal fusion, and action token generation, followed by an execution loop [25,28]. Multimodal inputs, such as RGB-D frames, natural language commands, and real-time robot state information, are independently tokenized using specialized encoders (e.g., ViT for vision, LLMs for language, MLP for state) [25,28]. These "Prefix Tokens" capture environmental scenes and linguistic instructions, establishing the contextual basis for understanding goals and layouts [28]. Concurrently, "State Tokens" encode the robot's current configuration, fusing with visual and language tokens to enable reasoning about physical constraints [28]. Through cross-modal attention mechanisms, these tokens are fused into a shared embedding space, allowing for holistic reasoning over object semantics, spatial layout, and physical constraints [25,28]. Finally, an autoregressive decoder, typically a Transformer, generates a series of "Action Tokens" that represent motion control signals, which are then translated into control commands for robot execution, thereby closing the perception-action loop [25,28].

The specific formulation of these action tokens is a primary design choice distinguishing VLA models [27,32]. A comprehensive taxonomy categorizes action tokens into eight types, each serving as a distinct approach to encoding actionable guidance [27,32]:
1.  **Language description**: Natural language expressions that detail intended action sequences, ranging from high-level plans to fine-grained motions.
2.  **Code**: Executable code snippets or pseudocode, often used for robot programming or atomic operations, as seen in 3D-Generalist which outputs "action code" for exposed tools like `add_object` and `retrieve_material` [33].
3.  **Affordance**: Spatially grounded representations of task-specific, interaction-relevant object properties, such as keypoints, bounding boxes, segmentation masks, or affordance maps.
4.  **Trajectory**: A temporally ordered sequence of spatial states, frequently representing end-effector poses or waypoints.
5.  **Goal state**: Predicted future observations, such as images, point clouds, or videos, illustrating the expected outcome of actions, as used by 3D-VLA to "depict imagination about future scenarios to plan actions accordingly" [3].
6.  **Latent representation**: Purposefully pre-trained latent vector sequences that encode action-relevant information. ThinkAct, for instance, compresses "embodied reasoning plans" into "visual plan latents" that condition a downstream action model [12]. DexVLA generates "latent action vectors" that are projected to a diffusion expert [29], and UniVLA employs a "task-centric latent action language" to create universal action representations [21].
7.  **Raw action**: One or more low-level control commands directly executable by a robot, such as joint angle updates, force/torque values, or wheel speeds [28].
8.  **Reasoning**: Natural language expressions explicitly describing the decision-making process that leads to a specific action token, often leveraged in models like DexVLA where "reasoning tokens" guide action generation via FiLM layers [29].

VLA frameworks for action generation and high-level control broadly fall into Monolithic and Hierarchical architectures, reflecting different strategies for unifying vision and language to drive action [26,34].

**Monolithic models** integrate perception, reasoning, and control into a single, tightly coupled pipeline, directly mapping high-level multimodal semantics to low-level actions [26]. These models often aim for end-to-end learning, where the entire process from observation to control command is unified [8]. A notable example is RT-2, which integrates Vision-Language Models (VLMs) directly into the robot's low-level control loop by treating robot-specific "action tokens" as analogous to natural language text tokens [16,20]. This enables VLMs' pre-trained semantic understanding to directly inform physical actions. Other monolithic models like OpenVLA [11] and InstructVLA [5] also exemplify this end-to-end approach, with InstructVLA utilizing a mixture-of-experts adaptation to jointly optimize textual reasoning and precise action generation. Some monolithic designs adopt a dual-system approach, where a VLM backbone interprets the scene and an independent action expert module generates actions based on latent representations. This is seen in DexVLA, where a VLM produces "reasoning tokens" and "action tokens" that condition a billion-parameter diffusion expert [29], and in MemoryVLA, which integrates memory-augmented tokens into a Diffusion Transformer (DiT) for action prediction [31]. AutoVLA further exemplifies this by combining semantic reasoning and action generation within a single autoregressive generation model for autonomous driving, processing raw visual inputs and language instructions to derive trajectories [7].

In contrast, **Hierarchical models** explicitly decouple high-level planning from low-level policy execution, promoting modularity and interpretability [6,26]. The VLA typically outputs high-level instructions, with independent controllers handling low-level motion planning, rather than direct joint torque signals [18]. The high-level planner translates instructions and observations into interpretable intermediate representations—such as subtasks, keypoints, or programs—which a low-level policy then uses to generate action sequences [26]. ThinkAct's "dual-system framework" epitomizes this, generating "embodied reasoning plans" via an LLM and compressing them into "visual plan latents" to condition a downstream action model [12]. Similarly, 3D-Generalist uses VLMs to output "action code" that interfaces with exposed tools to modify 3D environments, facilitating both scene-level and asset-level policies [33]. UAV-VLA extracts task goals and locations from natural language and satellite images to generate executable "path-action sets" for drones [15]. DreamVLA integrates "comprehensive world knowledge forecasting" to establish a perception-prediction-action loop, mapping abstract multimodal reasoning chains to action generation [2].

A notable trend across both monolithic and hierarchical models is the increasing use of **diffusion models** for action generation due to their ability to model complex conditional distributions over future actions. MemoryVLA employs a Diffusion Transformer (DiT) for predicting action sequences [31], DreamVLA utilizes a diffusion-based transformer [2], DexVLA projects action tokens to a diffusion expert [29], and EfficientVLA specifically optimizes the diffusion model for faster action generation [9]. GraspVLA integrates "flow-matching-based action generation" within a Chain-of-Thought process for open-vocabulary grasping [10].

Another critical aspect is the unification of action spaces, transcending embodiment-specific limitations. UMI provides a "unified interface" for robotic arms, enabling skill transfer through shared observation and behavior representations [16]. RDT-1B introduces a "physically interpretable unified action space" to preserve physical meaning across diverse robots [16]. UniVLA proposes a "task-centric latent action language" to learn a universal action representation interpretable by any robot, effectively offering a single "score" for diverse robots to "improvise" [21].

The field also addresses challenges like VLM "hallucinations" in critical applications. OpenDriveVLA mitigates this by integrating BEV perception (Map, Scene, Agent Tokens) with a QFormer to provide grounded context to an LLM, ensuring reliable high-level control in autonomous driving [17]. AutoVLA tackles this by tokenizing actions into a codebook and combining it with rules, ensuring precise and safe action generation [17]. EnerVerse aims for a "full chain" by connecting "future space generation" with "robot action planning" to ensure real-time action prediction [16]. These efforts highlight a debate between purely end-to-end learning and integrating structured representations or rule-based mechanisms to enhance reliability and safety, especially in high-stakes domains. The overarching trend points towards refining these unified frameworks to achieve more robust, generalizable, and interpretable VLA systems capable of complex, long-horizon tasks, bridging the semantic gap between human instructions and robotic execution.
### 5.2 Action Representation Formats and Decoding Strategies
The efficacy of Vision-Language-Action (VLA) models hinges critically on how actions are represented and subsequently decoded from multimodal inputs, bridging the semantic richness of language and vision with the physical realities of embodied agents [18,35]. This section systematically analyzes the diverse landscape of action representation formats and the corresponding decoding strategies employed within VLA research, moving beyond mere description to a critical appraisal of their mechanisms, advantages, disadvantages, and the trade-offs inherent in their application. It seeks to synthesize insights from multiple papers, identifying commonalities, contrasting divergent approaches, highlighting unique contributions, and exploring future developments to address current challenges in action generation.

A spectrum of action representation formats has emerged to imbue VLA models with the capability to understand, plan, and execute tasks. These formats range from high-level, abstract directives to low-level, concrete physical commands. Key categories explored include **Language Descriptions** as action tokens, where natural language serves as an abstract representation for planning and instruction following, leveraging the advanced reasoning capabilities of large language models (LLMs) and vision-language models (VLMs) [27,32]. Similarly, **Code as Action Tokens** offers structured, logical control through executable programs or pseudocode, enabling precise and robust execution for complex, long-horizon tasks [30,32]. **Affordance as Action Tokens** grounds actions spatially, representing potential interactions an object or environment allows, thereby linking visual perception directly to functional interaction [27]. For fine-grained control and direct execution, **Trajectory as Action Tokens** delineates sequences of specific movements or commands [27], while **Goal State as Action Tokens** focuses on desired future configurations to guide planning and enhance robustness against transient errors [27,32]. More abstractly, **Latent Representations as Action Tokens** provide compressed encodings of action semantics, promoting efficiency and generalizability by abstracting away low-level details [30,32]. Finally, **Raw Action as Action Tokens** involves direct mapping from multimodal inputs to physical motor commands for end-to-end learning [26,28], and **Reasoning as Action Tokens** functions as a meta-token or intermediate deliberative process, externalizing thought processes to enhance complex decision-making and planning [26,32].

To generate these varied action tokens, VLA models employ a diverse array of decoding strategies. A common approach is **Autoregressive Transformers**, which sequentially predict action tokens, often by treating motion generation akin to text generation, leveraging the inherent capabilities of LLMs and VLMs [25,26]. Complementing this, **Diffusion Models** and **Diffusion Transformers (DiT)** have gained prominence for their ability to generate continuous and multimodal action distributions, resulting in smoother trajectories and robust control [2,8,9,26,28,29,30]. Simpler **MLP/Token Predictors** are often used for direct regression or classification tasks, particularly in parallel decoding schemes to improve inference speed [26,30]. **Flow Matching** presents an alternative generative method for continuous actions, offering efficiency in trajectory generation [10,30]. Furthermore, explicit **Code Generation** facilitates structured and logical execution by translating natural language into executable programs [33].

Across these formats and strategies, several cross-cutting themes emerge. The field continually navigates the trade-offs between the expressiveness and precision required for fine-grained manipulation versus the interpretability and generalizability necessary for complex, novel tasks. For instance, while discrete tokenization, as seen in RT-2, aligns action generation with VLM text generation for broad semantic transfer [20], it can lead to large errors in fine manipulation, whereas continuous methods, though precise, may suffer from mode collapse or high computational costs [28]. Hierarchical approaches are frequently employed to manage this complexity, where high-level abstract representations (e.g., language plans, goal states) guide lower-level execution models (e.g., trajectories, raw actions). A persistent challenge is the high-dimensionality of action spaces, which drives innovations in techniques like action chunking, compressed tokenization (e.g., FAST for 15x faster inference) [25,30], and caching mechanisms (e.g., EfficientVLA [9]) to enhance computational efficiency and inference speed, crucial for real-time robotic control.

Despite significant advancements, limitations such as data scarcity for real-world robot interactions, the semantic-physical mismatch leading to physically infeasible outputs [18], and challenges in explainability and controllability for safety-critical applications remain. The inherent embodiment specificity of raw actions contrasts with efforts to achieve embodiment-agnostic representations, such as UniVLA's task-centric latent actions or those based on affordances, which promise greater cross-robot generalizability [21]. Future directions lean towards hybrid models that combine the strengths of different representations, unified action spaces, and the increasing integration of world models for predictive capabilities and enhanced reasoning. The continuous pursuit of efficient, robust, and generalizable action representation and decoding strategies is paramount for the advancement of embodied AI, ultimately enabling VLA models to perform a wider array of complex tasks with human-like proficiency.
#### 5.2.1 Language Description as Action Tokens

**Language Descriptions as Action Tokens in VLA Models**

| Aspect                | Description                                                                       | Advantages                                                                      | Limitations                                                                         | Key Examples / Models                                                                                                           |
|-----------------------|-----------------------------------------------------------------------------------|---------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|
| **Role**              | Abstract representation of intended actions for high-level planning & instruction following. | Human interpretability (oversight, intervention), fosters high-level planning & task decomposition. | Ambiguity for fine-grained control, semantic-physical mismatch, requires low-level controller. | NaVILA, PaLM-E (subgoals), SayCan, VoxPoser (semantic sub-actions), Yuanrong Qixing VLA (driving strategies), ThinkAct (embodied reasoning plans), DexVLA (sub-step reasoning as intermediate output), InstructVLA (textual reasoning) |
| **Token Type**        | Natural language commands, plans, sub-goals, symbolic representations.            | Seamless integration with powerful foundation models (LLMs/VLMs), zero-shot planning. | High inference latency, potential for VLM "hallucinations", limited expressiveness for fine physical motion. |                                                                                                                                 |
| **Core Insight**      | Language "plans" are essential for task decomposition; "motions" are less suitable for precise physical control. |                                                                                 |                                                                                     |                                                                                                                                 |



**Language Descriptions as Action Tokens in VLA Models**

| Aspect                | Description                                                                       | Advantages                                                                      | Limitations                                                                         | Key Examples / Models                                                                                                           |
|-----------------------|-----------------------------------------------------------------------------------|---------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|
| **Role**              | Abstract representation of intended actions for high-level planning & instruction following. | Human interpretability (oversight, intervention), fosters high-level planning & task decomposition. | Ambiguity for fine-grained control, semantic-physical mismatch, requires low-level controller. | NaVILA, PaLM-E (subgoals), SayCan, VoxPoser (semantic sub-actions), Yuanrong Qixing VLA (driving strategies), ThinkAct (embodied reasoning plans), DexVLA (sub-step reasoning as intermediate output), InstructVLA (textual reasoning) |
| **Token Type**        | Natural language commands, plans, sub-goals, symbolic representations.            | Seamless integration with powerful foundation models (LLMs/VLMs), zero-shot planning. | High inference latency, potential for VLM "hallucinations", limited expressiveness for fine physical motion. |                                                                                                                                 |
| **Core Insight**      | Language "plans" are essential for task decomposition; "motions" are less suitable for precise physical control. |                                                                                 |                                                                                     |                                                                                                                                 |

The employment of natural language descriptions as action tokens represents a prominent paradigm within Vision-Language-Action (VLA) models, primarily facilitating high-level planning and instruction following. In this approach, language serves as an abstract representation of intended actions, ranging from comprehensive task plans to symbolic representations of sub-goals [27,32]. This methodology leverages the advanced reasoning and generative capabilities of Large Language Models (LLMs) and Vision-Language Models (VLMs) to translate human intent into actionable sequences for embodied agents.

Common architectures and methodologies for integrating language descriptions as action tokens predominantly feature hierarchical structures [26,30,35]. In these frameworks, a high-level VLM planner interprets abstract language commands and decomposes them into a series of intermediate textual instructions or subtasks [26]. For instance, models like NaVILA utilize a high-level VLA to produce mid-level language actions, which are then executed by a low-level locomotion policy [30]. Similarly, PaLM-E generates subgoals or action descriptions for sequential manipulation planning [30]. Models such as SayCan and VoxPoser leverage LLMs to interpret natural language, decomposing complex instructions like "tidy the desk" into semantic sub-actions such as "pick up files -> put into folder" [35]. This allows for logical and contextual control over long-process tasks, which is crucial for complex VLA applications [32,35].

Beyond direct action commands, language descriptions also serve as critical inputs for various functions that influence actions. The Yuanrong Qixing VLA model, for instance, uses "action instruction text encoding" to adjust "驾驶策略" (driving strategies) and interpret "特殊车道的驾驶规则" (special lane rules) based on driver styles and visual cues, guiding vehicle actions through high-level instructions [19]. ThinkAct's multimodal LLM generates "embodied reasoning plans" in a natural language-like format, acting as high-level textual descriptions or symbolic tokens of intended actions or sub-goals [12]. In 3D-Generalist, natural language descriptions are integrated into a Domain Specific Language (DSL) to describe materials (e.g., "smooth light pink plaster wall") and assets (e.g., "plush pink salon chair"), serving as input tokens for retrieval functions that shape the environment [33]. DexVLA employs language descriptions not as direct action tokens, but as "sub-step reasoning" in the form of intermediate language outputs, which implicitly manage task flow and guide subsequent actions, thereby enhancing the model's capacity for complex, long-horizon tasks [29]. InstructVLA further exemplifies this by optimizing "textual reasoning" to translate high-level instructions into precise actions, and its training includes supervising the generation of rule-based language descriptions for interpretability and planning [5,14].

The primary strengths of using language descriptions as action tokens lie in their human interpretability, enabling easier oversight and intervention [14,20,26,30,32,35]. This approach fosters high-level planning and task decomposition, which are essential for tackling complex, long-horizon tasks [32]. Moreover, it benefits from seamless integration with powerful foundation models, leveraging their understanding, generation, reasoning, and planning capabilities, which can enable zero-shot planning and reduce task-specific training through co-training on web-scale data [32].

However, this methodology also presents significant challenges and trade-offs. A major limitation is the inherent ambiguity and imperfect expressiveness of natural language, particularly for fine-grained control in tasks requiring precise spatial or temporal details [18,26,30,32,35]. This can lead to a "semantic-physical mismatch," where linguistic descriptions (e.g., "gently push the balloon") do not translate accurately to the robot's physical actions, potentially resulting in inadequate task grounding [18]. Furthermore, these language-based systems often require a separate low-level control policy for execution, as the textual instructions themselves are not directly executable at the physical level [26]. The generation of high-quality language descriptions often depends on large-scale models, which can incur high inference latency, limiting their applicability in dynamic or real-time scenarios [32]. The potential for VLM "hallucinations" or physically infeasible outputs from open-ended language generation is a significant concern, leading some models like AutoVLA to explicitly move away from outputting actions as text, instead tokenizing them into a codebook for enhanced precision and safety in autonomous driving [17]. While RT-2 aligns action tokens as text tokens during co-fine-tuning, improving semantic grounding, its expressiveness for fine-grained control remains constrained by its 7-token action representation [20].

The observation that "Language motion, limited in expressiveness, is unlikely to become mainstream, while language plans remain essential for task decomposition" [32] encapsulates a critical insight into the evolving role of language in VLA models. This statement highlights a fundamental distinction between high-level language plans and low-level language motions. Language plans, being abstract and semantic, are well-suited for task decomposition and high-level strategy formation, addressing the cognitive aspects of task execution. They provide a human-interpretable roadmap for complex tasks, allowing models like MALMM to generate subtasks for a coder, or HiRobot to decompose open-ended instructions into atomic commands [26].

Conversely, language motions, intended for concrete, fine-grained control (e.g., "move the arm forward," "close gripper"), face inherent limitations due to the ambiguity and insufficient expressiveness of natural language for precise physical actions [32]. The aforementioned semantic-physical mismatch [18], along with the need for low-level execution policies, underscores why direct "language motion" is not the optimal pathway for precise physical interaction. Systems like UniVLA use language for "high-level semantics" to filter task-irrelevant visual changes and inform latent action generation, rather than as low-level action tokens [21]. Similarly, GraspVLA and OpenVLA leverage language for "open-vocabulary generalization" and "strong language grounding abilities" respectively, guiding semantic understanding rather than directly specifying low-level motions [10,11]. This trend suggests that while language is indispensable for defining goals, formulating plans, and providing contextual guidance, its direct application for controlling intricate, physical motions is largely superseded by more precise, non-linguistic action representations or latent action spaces that are often informed by language [15]. Therefore, the enduring utility of language in VLA models resides in its capacity for abstract planning and interpretability, rather than as a direct medium for specifying nuanced physical movements.
#### 5.2.2 Code as Action Tokens

**Code as Action Tokens in VLA Models**

| Aspect             | Description                                                                 | Advantages                                                                 | Limitations                                                                  | Key Examples / Models                                                                                   |
|--------------------|-----------------------------------------------------------------------------|----------------------------------------------------------------------------|------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|
| **Role**           | Executable code snippets or pseudocode for precise, logical control.        | Clear logical structures, transparent & verifiable, leverages third-party libraries. | API dependency (limits novelty), brittleness (generative errors, symbol grounding failures), safety risks. | Code as Policies (Python), ProgPrompt, Instruct2Act, RoboCodeX, 3D-Generalist (DSL), AutoVLA (codebook + rules) |
| **Token Type**     | Robot programs, atomic operations, control structures (conditionals, loops). | Beneficial for complex, long-horizon manipulation tasks.                   | Requires robust parsers/compilers, domain-specific knowledge.                |                                                                                                         |
| **Core Insight**   | Potential unlocked by building comprehensive function libraries that integrate perception and action primitives. |                                                                            |                                                                              |                                                                                                         |



**Code as Action Tokens in VLA Models**

| Aspect             | Description                                                                 | Advantages                                                                 | Limitations                                                                  | Key Examples / Models                                                                                   |
|--------------------|-----------------------------------------------------------------------------|----------------------------------------------------------------------------|------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|
| **Role**           | Executable code snippets or pseudocode for precise, logical control.        | Clear logical structures, transparent & verifiable, leverages third-party libraries. | API dependency (limits novelty), brittleness (generative errors, symbol grounding failures), safety risks. | Code as Policies (Python), ProgPrompt, Instruct2Act, RoboCodeX, 3D-Generalist (DSL), AutoVLA (codebook + rules) |
| **Token Type**     | Robot programs, atomic operations, control structures (conditionals, loops). | Beneficial for complex, long-horizon manipulation tasks.                   | Requires robust parsers/compilers, domain-specific knowledge.                |                                                                                                         |
| **Core Insight**   | Potential unlocked by building comprehensive function libraries that integrate perception and action primitives. |                                                                            |                                                                              |                                                                                                         |

The utilization of programming code as action tokens represents a distinct paradigm within Vision-Language-Action (VLA) models, primarily emphasized for its ability to enable precise, logical, and potentially long-horizon task execution [30,32]. This approach is classified under "Task Planners -> Modular" with a "Code-based" subcategory in VLA taxonomies, signifying its role in structured planning and control rather than direct end-to-end mappings [26,34]. The general interest in AI agents capable of generating and manipulating code extends beyond physical actions, encompassing broader applications for intelligent agents [24].

Models generate and execute code by leveraging large language models (LLMs) to translate natural language instructions into executable code snippets or pseudocode [27,32]. This code can either constitute a complete robot program or specify low-level atomic operations, frequently incorporating control structures such as conditionals and loops [32]. Notable examples include "Code as Policies" which uses LLMs like GPT-3 or Codex to map language instructions to Python code that processes perceptual inputs and parameterizes low-level robot APIs [32]. ProgPrompt extends this by integrating a finite state machine (FSM) framework for orchestrating task execution with programmatic structures in prompts, while Instruct2Act augments coding LLMs with multi-modal foundation models for perception, grounding high-level instructions into policy codes [32]. RoboCodeX further enhances this by fusing information from diverse sources and employing a tree-of-thought framework [32].

In program-based hierarchical planner-only models, the generated code can serve two primary functions: robot-executable programs or auxiliary programs [26]. Robot-executable programs, as seen in Chain-of-Modality, Instruct2Act, and MALMM, directly control robot actions by producing Python code that invokes specific APIs or includes actions and positions [26]. Conversely, auxiliary programs, such as those generated by ROVI and ReLEP, describe potential actions or decompose tasks into basic skills, facilitating task resolution through cost functions or enabling long-horizon performance with a memory bank [26]. Beyond robotics, this concept extends to other domains; for instance, the `3d_generalist` model employs a VLM (GPT-4o) prompted to act as a programmer, outputting Python code in a domain-specific language (DSL) to manipulate `SceneDef` objects and properties for 3D environment creation, thereby providing precise and logical control [33]. Similarly, AutoVLA introduces a "codebook" for tokenizing actions and utilizing explicit rules, which helps mitigate VLM "hallucinations" and ensures precise, physically feasible actions for autonomous driving tasks [17].

The strengths of using code as action tokens are manifold. It inherently provides clear logical structures for planning and control, offering a transparent and verifiable bridge between high-level instructions and low-level robot primitives [32]. This approach enables precise and logical control, particularly beneficial for tasks requiring a structured sequence of operations and hierarchical planning [26,30]. Furthermore, code can leverage rich third-party libraries (e.g., NumPy for spatial reasoning), extending its functional capabilities [32]. This mechanism is especially suited for complex, long-horizon manipulation tasks that demand structured reasoning and adaptability [32].

Despite these advantages, several limitations and challenges exist. A primary concern is API dependency, where the expressiveness of the system is constrained by predefined perception and control API libraries [32]. These APIs may prove inadequate for novel behaviors in dynamic or unobserved environments, thereby limiting adaptability and exploratory capacity [32]. Brittleness is another significant issue, as code generation is susceptible to internal generative errors from LLMs and failures when real-world states violate API preconditions, a problem known as symbol grounding [32]. Such brittleness can lead to failed operations, object damage, or even hardware damage, posing substantial safety risks [32]. Additionally, this approach necessitates robust parsers and compilers for execution and may require domain-specific knowledge for effective code generation [26,30]. Compared to end-to-end learning, direct execution of generated code might be less adaptable to dynamic and unforeseen situations [26].

A key insight highlights that "Code is a powerful alternative whose potential will be unlocked by building comprehensive function libraries that integrate perception and action primitives to solve complex, long-horizon tasks" [32]. The current reliance on predefined APIs inherently restricts the models' ability to handle new or complex scenarios. To unlock this potential, the development of comprehensive function libraries presents both significant opportunities and challenges. Opportunities lie in creating more versatile and robust VLA systems that can dynamically adapt to novel tasks and environments. Such libraries could abstract away low-level robot control, allowing LLMs to generate high-level plans that are then translated into executable actions by a rich set of primitives. Challenges include the immense effort required to design, implement, and maintain such extensive libraries, ensuring they cover a wide range of perception and action capabilities. Furthermore, these libraries must effectively address the symbol grounding problem, ensuring that the generated code accurately reflects the real-world state and desired outcomes, and possess mechanisms to handle unobserved or dynamic environmental conditions to mitigate brittleness and safety risks. Addressing these challenges is crucial for advancing code-based VLA models toward more general and reliable embodied intelligence.
#### 5.2.3 Affordance as Action Tokens

**Affordance as Action Tokens in VLA Models**

| Aspect                | Description                                                                 | Advantages                                                                     | Limitations                                                                          | Key Examples / Models                                                                                                   |
|-----------------------|-----------------------------------------------------------------------------|--------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|
| **Role**              | Spatially grounded "what-to-do" guidance linking visual perception to functional interaction. | Spatial grounding (actionable regions), task-agnostic interaction, cross-platform generalization. | Context-dependency (difficult generalization), inadequate 3D understanding, static properties. | KITE, RoboPoint, CoPa, RAM, ReKep, OmniManip, KUDA (keypoints), DexGraspVLA, A3VLM (bounding boxes), MOO, SoFar, RoboDexVLM, ROCKET-1 (segmentation masks), CLIPort, IGANet, VoxPoser, ManipLLM, ManiFoundation, RoboBrain (affordance maps), A0A0 (contact points) |
| **Token Formats**     | Keypoints, Bounding Boxes, Segmentation Masks, Affordance Maps.             | Robustness (bounding boxes), precision (keypoints, masks), better generalization across object instances. | Vulnerable to visual perturbations (occlusion, blur, noise), effective only with strong visual grounding. |                                                                                                                         |
| **Synergy**           | Often combined with trajectory tokens for complete action specification.    | Defines "what" to interact with (affordance) and "how" (trajectory).           | World models (e.g., GPT-4o for properties) are crucial for richer affordance reasoning. | ManipLVM-R1                                                                                                             |



**Affordance as Action Tokens in VLA Models**

| Aspect                | Description                                                                 | Advantages                                                                     | Limitations                                                                          | Key Examples / Models                                                                                                   |
|-----------------------|-----------------------------------------------------------------------------|--------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|
| **Role**              | Spatially grounded "what-to-do" guidance linking visual perception to functional interaction. | Spatial grounding (actionable regions), task-agnostic interaction, cross-platform generalization. | Context-dependency (difficult generalization), inadequate 3D understanding, static properties. | KITE, RoboPoint, CoPa, RAM, ReKep, OmniManip, KUDA (keypoints), DexGraspVLA, A3VLM (bounding boxes), MOO, SoFar, RoboDexVLM, ROCKET-1 (segmentation masks), CLIPort, IGANet, VoxPoser, ManipLLM, ManiFoundation, RoboBrain (affordance maps), A0A0 (contact points) |
| **Token Formats**     | Keypoints, Bounding Boxes, Segmentation Masks, Affordance Maps.             | Robustness (bounding boxes), precision (keypoints, masks), better generalization across object instances. | Vulnerable to visual perturbations (occlusion, blur, noise), effective only with strong visual grounding. |                                                                                                                         |
| **Synergy**           | Often combined with trajectory tokens for complete action specification.    | Defines "what" to interact with (affordance) and "how" (trajectory).           | World models (e.g., GPT-4o for properties) are crucial for richer affordance reasoning. | ManipLVM-R1                                                                                                             |

Affordances, representing the potential actions an object or environment allows, serve as a crucial interface between perception and action in Vision-Language-Action (VLA) models [27]. By encoding "what-to-do" guidance, affordance tokens enable VLA systems to move beyond mere object recognition to functional interaction, thereby bridging the semantic gap between visual input and executable physical actions [26,32].

Various computational approaches have been developed to detect and represent affordances as action tokens. These representations are typically structured and spatially grounded, focusing on task-specific, interaction-relevant object properties [32]. Key formats include:
1.  **Keypoints**: These are compact and precise interaction targets, often defined as a spatial position $x \in \mathbb{R}^3$ and an interaction direction $d$, i.e., $(x, d)$ [32]. Models like KITE, RoboPoint, CoPa, RAM, ReKep, OmniManip, and KUDA leverage keypoints for exact manipulation targets [32]. RoboPoint, for instance, interprets natural language to generate visual keypoints for precise manipulation [26].
2.  **Bounding Boxes**: Offering a coarser but efficient representation, bounding boxes define instance-level localization, either as 2D $((x_{tl}, y_{tl}), (x_{br}, y_{br}))$ or 3D 8-corner points [32]. Examples include DexGraspVLA, A3VLM, and models by Wake et al. [32].
3.  **Segmentation Masks**: These provide high-resolution spatial representations that capture fine-grained object contours and part-level geometry, typically as binary matrices $M \in \{0,1\}^{H \times W}$ [32]. MOO, SoFar, RoboDexVLM, and ROCKET-1 are systems that utilize segmentation masks [32].
4.  **Affordance Maps**: Dense spatial fields assign a graded suitability score for specific actions to each region, formulated as $A \in \mathbb{R}^{H \times W}$ [32]. CLIPort, IGANet, VoxPoser, ManipLLM, and ManiFoundation are examples employing affordance maps [32]. RoboBrain also uses an A-LoRA module to identify interactable regions, conceptually similar to an affordance map [26].

The underlying mechanisms for generating these affordance tokens often involve vision-language foundation models (VLMs) and vision-foundation models (VFMs), such as OWL-ViT, SAM, Grounding DINO, DINOv2, and Qwen2.5-VL, for perception and grounding. Motion planning modules subsequently convert these affordance representations into raw actions [32]. For instance, A0A0 predicts contact points and post-contact motion as an embodiment-agnostic affordance representation, which an action policy then translates into control [26].

A primary strength of affordance-based action tokens lies in their **spatial grounding**, which directly links visual perception to physical interaction by identifying actionable regions [32]. This approach enables **task-agnostic interaction** and **cross-platform generalization** by abstracting embodiment-specific control, allowing high-level instructions to be executed across diverse robotic systems [26,32]. Affordances inherently focus on the functional properties of objects, leading to better generalization across object instances [26]. Furthermore, bounding boxes offer robustness and computational simplicity due to their coarser representation, while keypoints and segmentation masks provide precision for fine-grained interactions [32].

Despite these advantages, employing affordances as action tokens faces several limitations. A significant challenge is the **context-dependency of affordances**, making generalization difficult [26,32]. Accurate perception and representation of affordances demand strong visual grounding for robust performance [26]. Many current models primarily rely on 2D image representations, leading to **inadequate 3D understanding** which hinders precise control, especially with complex object shapes or occlusions [32]. Moreover, affordances are often encoded as **static properties** (e.g., "graspable handle"), failing to model how affordances evolve during contact-rich tasks, which limits their effectiveness in dynamic scenarios [32]. They are also vulnerable to visual perturbations like occlusion, motion blur, and noise, degrading the accuracy of keypoints and segmentation masks [32].

It is important to note that while some literature directly frames affordances as action tokens [26,27], other works treat affordances as part of a model's internal understanding and reasoning, rather than direct token types for output actions. For instance, some VLA models use affordance information to inform their understanding, such as "apples can be grasped," but do not explicitly describe affordances themselves as tokenized actions [25,34]. Similarly, in the Asset-Level Policy, object affordances are incorporated through "receptacle objects" and "physical plausibility verifiers," where GPT-4o determines object properties, but the affordances themselves are not explicitly tokenized actions but rather attributes informing action grounding [30,33]. DreamVLA integrates "spatial and semantic cues" foundational to affordances for action planning, yet it does not explicitly state that affordances are tokenized [2].

The synergy between affordances and other token types, particularly trajectories, is critical for comprehensive action generation. ManipLVM-R1 exemplifies this by predicting both the affordance area for grasping and the subsequent trajectory of the target object [26]. This integration allows for a more complete action specification, moving from *what* to interact with (affordance) to *how* to interact (trajectory). The conversion of affordance representations into raw actions by motion planning modules [32] inherently involves generating movement trajectories.

World models play a crucial role in supporting the generation and understanding of affordances. By providing comprehensive knowledge about object properties, environmental dynamics, and causal relationships, world models can enrich affordance reasoning. The process of identifying "receptacle objects" and applying "physical plausibility verifiers" in systems like GPT-4o for placing items [33] demonstrates how world knowledge guides affordance-based actions. The desideratum for VLA models to infer complex chains of reasoning, such as a robot seeing a banana peel and understanding its slipperiness, the need to pick it up, and the appropriate tools, underscores the need for deep integration with world knowledge to enable comprehensive, common-sense driven affordance understanding and generalization [18].

Emerging trends in the field highlight the "Chain-of-Affordance" as a paradigm focusing on affordance chain reasoning to address cross-embodiment generalization [28,30,34]. The future roadmap for VLA models anticipates multimodal foundation models encoding rich "affordance data" and top-level language-driven planners being fine-tuned specifically for "affordance reasoning" [28,34]. Pioneering works like "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan)" and "RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation" also emphasize the utility of affordances as intermediate representations in robotic control [30]. These trends collectively suggest a move towards more sophisticated, context-aware, and world-model-informed affordance processing in future VLA systems.
#### 5.2.4 Trajectory as Action Tokens

**Trajectory as Action Tokens in VLA Models**

| Aspect              | Description                                                                    | Advantages                                                                      | Limitations                                                                                             | Key Examples / Models                                                                                                    |
|---------------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------|
| **Role**            | Sequences of specific movements/commands ("how-to-do" paths) for fine-grained control. | Fine-grained control, direct execution, leverages off-domain videos, strong generalization across tasks, inherent interpretability. | High-dimensionality (computational cost, error susceptibility), 2D limitation (lacks 3D info/orientation). | EfficientVLA, DreamVLA, GraspVLA, InstructVLA, GR-3 (generative models), DecisionTransformer, TrajectoryTransformer, Gato (RL Transformers) |
| **Token Formats**   | Point Trajectory, Visual Trajectory, Optical Flow, Continuous Value Trajectories, Discrete Action Tokens. | Various granularities and representations (e.g., BEV waypoints, pixel space paths, motion fields, joint angles, discrete tokens). | Task-dependent effectiveness, less effective for complex interaction logic or partial observation.         | Waymo ODD-E2E (point trajectory), RT-Trajectory, HAMSTER (visual trajectory), AVDC, Im2Flow2Act (optical flow), MemoryVLA (continuous values), RT-2, AutoVLA (discrete tokens) |
| **Generation Methods** | Generative models (diffusion, flow-matching), RL Transformers.                   | Smoother, more noise-resistant action sequences.                                | Requires compression/optimization for efficiency; sensitivity to decoding strategy.                         | Flow-matching (GraspVLA, $\pi_0$-FAST), Diffusion (EfficientVLA, DreamVLA).                                              |
| **Efficiency/Robustness** |                                                                                | Compressed Action Tokenization (FAST), Action Chunking, Caching, trajectory consistency in RL. |                                                                                                         | $\pi_0$-FAST (DCT, 15x accel, 200Hz), EfficientVLA (caching), ThinkAct (RL rewards).                                     |



**Trajectory as Action Tokens in VLA Models**

| Aspect              | Description                                                                    | Advantages                                                                      | Limitations                                                                                             | Key Examples / Models                                                                                                    |
|---------------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------|
| **Role**            | Sequences of specific movements/commands ("how-to-do" paths) for fine-grained control. | Fine-grained control, direct execution, leverages off-domain videos, strong generalization across tasks, inherent interpretability. | High-dimensionality (computational cost, error susceptibility), 2D limitation (lacks 3D info/orientation). | EfficientVLA, DreamVLA, GraspVLA, InstructVLA, GR-3 (generative models), DecisionTransformer, TrajectoryTransformer, Gato (RL Transformers) |
| **Token Formats**   | Point Trajectory, Visual Trajectory, Optical Flow, Continuous Value Trajectories, Discrete Action Tokens. | Various granularities and representations (e.g., BEV waypoints, pixel space paths, motion fields, joint angles, discrete tokens). | Task-dependent effectiveness, less effective for complex interaction logic or partial observation.         | Waymo ODD-E2E (point trajectory), RT-Trajectory, HAMSTER (visual trajectory), AVDC, Im2Flow2Act (optical flow), MemoryVLA (continuous values), RT-2, AutoVLA (discrete tokens) |
| **Generation Methods** | Generative models (diffusion, flow-matching), RL Transformers.                   | Smoother, more noise-resistant action sequences.                                | Requires compression/optimization for efficiency; sensitivity to decoding strategy.                         | Flow-matching (GraspVLA, $\pi_0$-FAST), Diffusion (EfficientVLA, DreamVLA).                                              |
| **Efficiency/Robustness** |                                                                                | Compressed Action Tokenization (FAST), Action Chunking, Caching, trajectory consistency in RL. |                                                                                                         | $\pi_0$-FAST (DCT, 15x accel, 200Hz), EfficientVLA (caching), ThinkAct (RL rewards).                                     |

Trajectory as an action token is a fundamental concept in Vision-Language-Action (VLA) models, representing sequences of specific movements or commands that dictate how an agent, such as a robot or autonomous vehicle, performs a task [27]. These tokens provide precise "how-to-do" paths, crucial for fine-grained control and direct execution in complex environments [26,30,32].

Techniques for generating and learning trajectories in VLA contexts encompass a variety of approaches. Generative models, particularly diffusion models and flow-matching mechanisms, are widely employed to produce continuous and smooth action sequences. For instance, EfficientVLA generates "precise action sequences" using diffusion models, optimizing the computational process through a caching mechanism [9]. DreamVLA leverages a diffusion-based transformer to model the "conditional distribution over future actions," thereby generating action trajectories [2]. Similarly, the integration of diffusion models is noted for optimizing trajectory generation in autonomous driving, enhancing the smoothness and stability of control commands [8]. GraspVLA utilizes "flow-matching-based action generation" to produce continuous actions, typically manifesting as end-effector poses or joint angles for robotic grasping [10]. InstructVLA explicitly employs flow matching to supervise the learning of these action sequences, ensuring precision for robotic control [14]. GR-3 further exemplifies this trend by unifying vision-language understanding with robot trajectory learning using flow-matching [26]. Pioneering efforts like DecisionTransformer (DT) and TrajectoryTransformer (TT) model reinforcement learning problems as autoregressive sequential data, implying that action trajectories are a primary form of action representation and generation for these models [26,34].

Trajectory representations vary across different VLA systems, accommodating diverse application requirements.
*   **Point Trajectory** involves sequences of discrete points, such as $P \in \mathbb{R}^{T \times K \times 2}$, which model the path of $K$ critical points over time $T$. This format is commonly used for predicting future vehicle waypoints in bird's eye view (BEV) space, as seen in the Waymo Open Dataset End to End (WOD-E2E) and NAVSIM benchmarks, which require 5-second and 4-second path point trajectories respectively [17,32].
*   **Visual Trajectory** renders paths directly into pixel space as a new image or video, offering high interpretability by showing actions within a visual context. Examples include RT-Trajectory and HAMSTER [32].
*   **Optical Flow** provides the densest representation, described by a motion field $V \in \mathbb{R}^{H \times W \times 2}$, capturing holistic scene dynamics by detailing pixel motion between frames. This is utilized in models like AVDC and Im2Flow2Act to model complex, multi-object interactions implicitly [32].
*   **Continuous Value Trajectories** are generated by models like MemoryVLA, which outputs continuous values for relative displacement, rotation (Euler angles), and discrete gripper states over 16 future steps, enabling precise robot manipulation [31].
*   **Discrete Action Tokens** derived from trajectories are also prevalent. RT-2, for instance, learns from 130k robot trajectories and condenses their high-dimensionality into 7 discrete "action tokens" designed for fine-grained control [20]. AutoVLA similarly tokenizes continuous trajectories into discrete, feasible actions for integration with language models [7]. This highlights a strategic choice in balancing continuous motion fidelity with the discrete nature often preferred by transformer-based models.

Trajectory tokens are applied across various domains, including robotic manipulation, autonomous driving, and UAV control. In autonomous driving, models like TransDiffuser focus on "End-to-end Trajectory Generation" for critical tasks, with metrics such as L2 distance and Average Displacement Error (ADE) used for evaluation [17]. For robotics, models like MoManipVLA and HAMSTER predict keypoints or waypoints that are then refined into executable trajectories [26]. UAV-VLA systems explicitly use flight paths (sequences of geographical coordinates) as action tokens, generated by path planning algorithms like Dynamic Time Warping (DTW) and K-Nearest Neighbors (KNN) based on visual targets and natural language instructions [15].

The strengths of using trajectory as action tokens are manifold. They offer fine-grained control and direct execution, which is critical for precise tasks [25,26,28,30]. This approach can leverage abundant off-domain videos for training, addressing data scarcity by extracting rich actionable information [32]. Furthermore, trajectory-based models demonstrate strong generalization across tasks due to shared motion patterns, enabling performance even on semantically distinct but kinematically similar tasks [32]. The inherent interpretability of trajectories, which are explicit and understandable by humans, also facilitates training and debugging processes [32].

Despite these advantages, several challenges and limitations persist. A primary concern is the high-dimensionality of trajectories, leading to significant computational costs and susceptibility to errors [25,26,30]. Many generative models for trajectories or videos are computationally expensive for both training and inference [32]. Moreover, most existing works rely on 2D trajectories, lacking explicit 3D information, which can lead to ambiguity and restrict applicability in non-planar tasks, particularly as point trajectories may omit crucial orientation information for dexterous manipulation [32]. While some models incorporate depth data to mitigate this, the limitation remains [32]. The suitability of trajectory tokens is also task-dependent; while excelling at tasks with precise motion paths, they may be less effective in partially observed settings or tasks demanding complex interaction logic, force application, or advanced object affordance understanding [32]. This is exemplified by UAV-VLA, where different path planning algorithms yielded vastly different accuracies, with KNN achieving a significantly lower error (34.22m) compared to sequential (409.54m) or DTW (307.27m) methods [15]. This variability underscores the sensitivity of trajectory generation to the choice of decoding and planning strategies. A notable distinction arises in the literature regarding the level of action generation: some VLA models provide high-level commands, relying on independent controllers for low-level motion planning rather than directly generating fine-grained trajectories themselves [18]. This contrasts with models like MemoryVLA and InstructVLA, which directly output continuous or detailed action sequences.

To make high-dimensional trajectory generation more robust and efficient, researchers have explored several methods. Compression techniques are vital, such as the Discrete Cosine Transform (DCT) used by $\pi_0$-FAST, which encodes action chunks for substantial token compression (up to $13.2 \times$) and smoother action trajectories for high-precision manipulation [27,28]. The "Compressed Action Tokenization (FAST)" method converts continuous action outputs into frequency-domain tokens, enabling 15x inference acceleration for high-frequency control (200Hz) with minimal precision loss [28]. Action chunking, which predicts multiple future actions, further enhances temporal consistency by effectively generating short action trajectories [30]. EfficientVLA's caching mechanism also optimizes the computational process for generating action sequences [9]. Robustness is also bolstered by incorporating "trajectory consistency" in reinforcement learning reward structures, as seen in ThinkAct, which contributes to smooth and effective motion [12].

The relationship between trajectory tokens and affordances is complementary. While trajectory tokens delineate the "how-to-do" paths for executing actions, affordances define "what-can-be-done" with an object or in an environment [32]. Models like ManipLVM-R1 explicitly predict trajectories alongside affordance areas, indicating an integrated approach to understanding both the execution path and the functional possibilities [26]. This synergy enables VLA models to not only plan precise movements but also to understand the context and potential interactions with objects.

The increasing prevalence of trajectory-based learning is evident in major dataset contributions like Open X-Embodiment, which contains "over 1 million real robot trajectories," and the VLA-IT dataset, combining manipulation trajectories with language annotations [14,16]. Models like EnerVerse, which uses an autoregressive diffusion model for robot action planning, and HumanPlus, which learns full-body manipulation from human motion data, underscore the trend towards trajectory-based outputs and generation [16]. The continuous development in trajectory generation techniques, from flow-matching to diffusion models, aims to address the challenges of high-dimensionality and computational efficiency, thereby enhancing the precision and robustness of VLA models in diverse real-world applications.
#### 5.2.5 Goal State as Action Tokens

**Goal State as Action Tokens in VLA Models**

| Aspect                | Description                                                                 | Advantages                                                                    | Limitations                                                                         | Key Examples / Models                                                                                                                             |
|-----------------------|-----------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|
| **Role**              | High-level abstraction of desired outcomes; guide planning & reasoning.     | Robustness to minor errors, task-level specification, enhances long-term planning. | Difficulty in generating high-quality, physically plausible goal states (overspecification, inaccuracies). | SuSIE, CoTDiffusion, 3D-VLA, CoT-VLA (single-frame images), UniPi, AVDC, VLP, Gen2Act, VPP, FLIP, GEVRM (multi-frame videos), 3D-Generalist (text prompt) |
| **Token Formats**     | Single-frame images (2D RGB, 2.5D RGB-D, 3D point clouds), Multi-frame videos. | Data scalability (hindsight goal relabeling), generalization (action-free videos, human-executed goals). | High inference latency (generating future images/videos is computationally intensive). | Waymo ODD-E2E (predicted path points), UAV-VLA (target points from satellite imagery), DreamVLA (world knowledge forecasting), EnerVerse (embodied future space) |
| **Generation/Utilization** | High-level generative model synthesizes goal state, conditions low-level policy. | Task specificity, interpretability (white-box), straightforward evaluation.    | Dependency on subtask segmentation for complex tasks.                               | 3D-VLA (embodied diffusion models predict goal images/point clouds), CoT-VLA, World4Omni (subgoal observations).                                    |
| **Grounding**         | Defines target for trajectory generation and implicitly informs affordances. | Enhances understanding of interaction context.                                |                                                                                     |                                                                                                                                                    |



**Goal State as Action Tokens in VLA Models**

| Aspect                | Description                                                                 | Advantages                                                                    | Limitations                                                                         | Key Examples / Models                                                                                                                             |
|-----------------------|-----------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|
| **Role**              | High-level abstraction of desired outcomes; guide planning & reasoning.     | Robustness to minor errors, task-level specification, enhances long-term planning. | Difficulty in generating high-quality, physically plausible goal states (overspecification, inaccuracies). | SuSIE, CoTDiffusion, 3D-VLA, CoT-VLA (single-frame images), UniPi, AVDC, VLP, Gen2Act, VPP, FLIP, GEVRM (multi-frame videos), 3D-Generalist (text prompt) |
| **Token Formats**     | Single-frame images (2D RGB, 2.5D RGB-D, 3D point clouds), Multi-frame videos. | Data scalability (hindsight goal relabeling), generalization (action-free videos, human-executed goals). | High inference latency (generating future images/videos is computationally intensive). | Waymo ODD-E2E (predicted path points), UAV-VLA (target points from satellite imagery), DreamVLA (world knowledge forecasting), EnerVerse (embodied future space) |
| **Generation/Utilization** | High-level generative model synthesizes goal state, conditions low-level policy. | Task specificity, interpretability (white-box), straightforward evaluation.    | Dependency on subtask segmentation for complex tasks.                               | 3D-VLA (embodied diffusion models predict goal images/point clouds), CoT-VLA, World4Omni (subgoal observations).                                    |
| **Grounding**         | Defines target for trajectory generation and implicitly informs affordances. | Enhances understanding of interaction context.                                |                                                                                     |                                                                                                                                                    |

Goal states, as a distinct type of action token, represent a high-level abstraction of desired outcomes, serving as an intermediate representation to bridge abstract instructions with low-level executable actions within Vision-Language-Action (VLA) models [27,32]. This approach emphasizes defining the ultimate desired future configuration or end-state, thereby guiding the VLA model's planning and reasoning processes towards specific goal completion, as exemplified by the "action-aligned visual rewards" utilized in ThinkAct [12]. This paradigm allows for a task-level specification and contributes to enhanced robustness against transient execution errors, as the system remains focused on the overarching objective rather than meticulously controlling every intermediate step [3,35].

The encoding of goal states manifests in diverse formats. A comprehensive review by [32] categorizes these into:
1.  **Single-frame images**: These include 2D RGB images, 2.5D RGB-D images, or 3D point clouds that depict the intended final scene. Examples of models employing such formats include SuSIE, CoTDiffusion, 3D-VLA, and CoT-VLA [32]. Notably, 3D-VLA leverages "goal images and point clouds" as target representations for action planning within a 3D environment [3,30].
2.  **Multi-frame videos**: These capture short sequences illustrating the evolution of a scene, providing richer temporal context and explicit "how-to-do" cues. Models like UniPi, AVDC, VLP, Gen2Act, VPP, FLIP, and GEVRM utilize this dynamic representation [32].

Beyond these explicit visual representations, goal states can also be implicitly defined. For instance, in 3D-Generalist, the goal is dictated by a text prompt, and the model iteratively refines a 3D environment to achieve a "prompt-aligned" state, measured by CLIP scores [33]. In practical applications like autonomous driving, goal states are often represented as predicted future vehicle states or path points, such as the 5-second path point trajectories in the Waymo Open Dataset End to End (WOD-E2E) [17]. Similarly, for UAV navigation, specific target points identified from satellite imagery, guided by natural language, serve as intermediate goal states for path planning [15]. While UniVLA focuses on "task-centric latent actions" that implicitly encode steps towards a goal by filtering visual changes relevant to the task, this approach aligns with the goal-oriented paradigm [21]. The underlying concept of an "internal, predictive world model" for adaptation also implicitly entails an understanding of goal states [25].

The generation and utilization of goal states typically involve a hierarchical architecture, where a high-level generative model synthesizes the goal state, which then conditions a lower-level policy to produce actions [32]. Common generative models, such as DiT or CVAE, produce goal states from observations and language instructions. Subsequently, these predicted goal states guide lower-level models, including diffusion policies, MLPs, or inverse dynamics models, to derive the necessary actions [26,32]. For example, CoT-VLA employs latent visual goals (predicting future images) to guide subsequent action sequences [26,30], while World4Omni generates subgoal images to inform low-level policies [26]. 3D-VLA utilizes "embodied diffusion models" aligned with 3D-based LLMs to predict goal images and point clouds, thereby defining desired environmental configurations [3]. The broader concept of "World Model Construction" is crucial, as it involves optimizing the simulation of physical laws to predict dynamic changes and future goal states, such as liquid pouring trajectories or stable operation under interference [18]. DreamVLA integrates "comprehensive world knowledge forecasting" to predict future environmental states, which in turn guides its inverse dynamics modeling to derive actions [2]. Similarly, EnerVerse generates "embodied future space" to predict future states for robot manipulation [16]. For planning with multiple candidate goal videos, techniques like beam-search-like algorithms are employed by VLP and FLIP [32].

The integration of goal states as action tokens offers several significant advantages, as outlined by [32] and others:
*   **Robustness and Task-Level Specification**: By focusing on the desired end-state, models become more robust to minor execution errors and allow for abstract, task-level instructions rather than detailed action sequences [3,26,30,35].
*   **Data Scalability and Generalization**: Techniques like hindsight goal relabeling (e.g., LangLfP) enable autonomous generation of large datasets from raw robot trajectories, mitigating the need for extensive action annotation. Leveraging large-scale action-free video data for training goal generators and incorporating human-executed goal states (e.g., Gen2Act) further enhance generalization capabilities across different embodiments [32].
*   **Task Specificity and Interpretability**: Goal states encode precise spatial and visual information, reducing ambiguity for lower-level policies. The "white-box" nature of the training and inference processes often improves interpretability, aiding in human understanding, debugging, and intervention [32].
*   **Straightforward Evaluation**: The quality of generated goal states can be readily assessed using off-the-shelf language-image valuing models that check alignment with textual instructions [32].

Despite these strengths, the use of goal states presents notable challenges and limitations:
*   **Quality and Consistency of Goal State Generation**: A major hurdle is the difficulty in generating high-quality, consistent, and physically plausible goal states [26,30,32]. Overspecification, where unnecessary details are included, can overconstrain flexibility, while inaccuracies or inconsistencies due to insufficient dynamics modeling can lead to misleading guidance and task failure [32]. Defining optimal and feasible goal states remains a complex research problem, and policies may fail when attempting to achieve unfeasible goals [26,30].
*   **High Inference Latency**: Generating future images or videos is computationally intensive, leading to high inference latency. For example, AVDC requires approximately 10 seconds for an 8-frame video, Gen2Act achieves only 3 Hz inference speed, and VPP operates at 7-10 Hz [32]. Such delays pose significant obstacles for real-time robotic control.
*   **Dependency on Subtask Segmentation**: For complex, long-horizon tasks, the effectiveness of goal state planning is often contingent on the quality of subtask segmentation, particularly when relying on LLM-based task decomposition [32].

Goal states contribute significantly to grounding other action token types, notably trajectories. By representing a desired future configuration, goal states inherently define the target for trajectory generation. For instance, in autonomous driving, predicting future path points (trajectories) directly constitutes the goal state, thereby grounding the concept of a desired path within the action token framework [17]. While the digests do not explicitly detail how goal states ground affordances, the visual or 3D representation of an end-state implicitly informs the necessary interactions with objects and environmental elements, guiding the lower-level policy in recognizing and utilizing relevant affordances to achieve the specified goal.
#### 5.2.6 Latent Representation as Action Tokens

**Latent Representations as Action Tokens in VLA Models**

| Aspect                   | Description                                                                    | Advantages                                                                       | Limitations                                                                      | Key Examples / Models                                                                                                        |
|--------------------------|--------------------------------------------------------------------------------|----------------------------------------------------------------------------------|----------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| **Role**                 | Compressed, abstract encodings of action semantics for efficient, generalizable control. | Enhanced scalability, training efficiency, expressive potential (high-level kinematic semantics). | Lack of explainability/controllability (debugging difficult, unsuitable for safety-critical). | InstructVLA (latent action vectors), Diffusion Transformers, DreamVLA (disentangled action representations), ThinkAct (visual plan latent), UniVLA (task-centric latent actions), UniAct (vector-quantized codes), RDT-1B, EnerVerse |
| **Learning Mechanism**   | Multi-stage pipeline: latent space construction (unsupervised, VQ-VAE, FSQ), VLM adapts to predict latent actions, action fine-tuning. | Simplifies pretraining target for VLMs, fosters embodiment-agnostic understanding. | Task-irrelevant visual variations (noise), inadequate granularity/low reconstruction fidelity. |                                                                                                                              |
| **UniVLA's "Musical Score"** | Learns discrete "task-centric latent actions" from videos, filters with language. | Strong cross-body/cross-view generalization, data efficient (4.45% training time vs. OpenVLA). | Incomplete vocabulary of behaviors (policy failures in novel situations).          | UniVLA                                                                                                                       |
| **Memory Integration**   | Latent tokens can integrate memory for contextual action generation.             | Guides action planning via cognitive attention.                                  | Misalignment with human intention.                                               | MemoryVLA (perceptual + cognitive token)                                                                                     |



**Latent Representations as Action Tokens in VLA Models**

| Aspect                   | Description                                                                    | Advantages                                                                       | Limitations                                                                      | Key Examples / Models                                                                                                        |
|--------------------------|--------------------------------------------------------------------------------|----------------------------------------------------------------------------------|----------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| **Role**                 | Compressed, abstract encodings of action semantics for efficient, generalizable control. | Enhanced scalability, training efficiency, expressive potential (high-level kinematic semantics). | Lack of explainability/controllability (debugging difficult, unsuitable for safety-critical). | InstructVLA (latent action vectors), Diffusion Transformers, DreamVLA (disentangled action representations), ThinkAct (visual plan latent), UniVLA (task-centric latent actions), UniAct (vector-quantized codes), RDT-1B, EnerVerse |
| **Learning Mechanism**   | Multi-stage pipeline: latent space construction (unsupervised, VQ-VAE, FSQ), VLM adapts to predict latent actions, action fine-tuning. | Simplifies pretraining target for VLMs, fosters embodiment-agnostic understanding. | Task-irrelevant visual variations (noise), inadequate granularity/low reconstruction fidelity. |                                                                                                                              |
| **UniVLA's "Musical Score"** | Learns discrete "task-centric latent actions" from videos, filters with language. | Strong cross-body/cross-view generalization, data efficient (4.45% training time vs. OpenVLA). | Incomplete vocabulary of behaviors (policy failures in novel situations).          | UniVLA                                                                                                                       |
| **Memory Integration**   | Latent tokens can integrate memory for contextual action generation.             | Guides action planning via cognitive attention.                                  | Misalignment with human intention.                                               | MemoryVLA (perceptual + cognitive token)                                                                                     |

Latent representations, when employed as action tokens, constitute a critical advancement in Vision-Language-Action (VLA) models, enabling the compression and abstract encoding of complex action semantics for more efficient and generalizable robotic control [30,32]. This approach moves beyond direct control signals, leveraging high-level semantic behaviors to bridge the gap between visual-language reasoning and precise physical execution [14].

The mechanisms for learning and utilizing these latent action tokens typically involve a multi-stage pipeline, as detailed in recent surveys [26,32]. Initially, a latent action space is constructed, often in an unsupervised manner, from large datasets. This process may involve models like VQ-VAE to capture visual state transitions or FSQ (Factorized Quantization) on action chunks, providing pseudo-labels for subsequent learning. Following this, a Vision-Language Model (VLM) is adapted during latent pretraining to predict appropriate latent actions given current observations and instructions. Finally, action fine-tuning translates these predicted high-level latent actions into low-level, executable commands for specific robotic embodiments [32].

Specific implementations vary across models. InstructVLA, for instance, generates "latent action vectors" after visual-language reasoning, which are then extracted via $N$ learnable action queries from the VLM's hidden states and fed to an independent action expert module for decoding into executable actions [14]. Similarly, Diffusion Transformer models operate by iteratively denoising a latent space to generate smooth action trajectories, often fusing multimodal features into a shared latent space that an action decoder then translates into executable actions [35]. This shared latent space approach is also prevalent in VLA models to align perceptual information and text instructions, guiding action strategies [25,28]. DreamVLA further exemplifies this by using a diffusion-based transformer that disentangles action representations from shared latent features for action planning [2]. ThinkAct explicitly utilizes a "visual plan latent" to condition downstream action models, bridging high-level reasoning with execution [12]. In contrast, models like RT-2 employ discrete, fixed-length action tokens directly aligned with the VLM's text token space, focusing on semantic transfer rather than learning a separate, continuous latent action space [20]. It is crucial to distinguish these action tokens from latent *perception* representations, such as those used by OpenVLA to reduce visual features for language model input [19] or DINOv2 and I-JEPA for visual encoding [34], which primarily inform the action generation process rather than directly representing the action itself. However, MemoryVLA integrates compressed perceptual tokens with a cognitive token to form working memory, conditioning the action generation model [31].

A significant contribution in this domain is the concept of "unified musical scores" or task-centric latent actions, notably championed by UniVLA [21]. UniVLA's innovation lies in learning compressed, abstract, and discrete "task-centric latent actions" from diverse sources like human and robot videos, utilizing a VQ-VAE to quantify frame-by-frame visual changes into distinct "notes" [21,26]. This mechanism explicitly filters out task-irrelevant visual noise, ensuring the latent actions specifically capture movements essential for the task. This leads to strong "cross-body and cross-view generalization," allowing semantically consistent actions across different embodiments and viewpoints, such as a robotic arm's grasp and a human hand's grasp both mapping to a "lift-place" melody [21]. This approach offers considerable data efficiency; UniVLA achieves comparable performance to OpenVLA with only 4.45% of the training time by simplifying the pretraining target for VLMs [32]. Similarly, UniAct learns a universal action space using vector-quantized codes to represent common atomic behaviors across different robots, facilitating cross-embodiment compatibility [27].

The adoption of latent representations as action tokens offers substantial strengths, including enhanced scalability, training efficiency, and expressive potential. They enable scaling across vast, action-free internet-scale human videos and heterogeneous robot datasets, fostering embodiment-agnostic understanding and efficient downstream fine-tuning [32]. By encoding high-level kinematic semantics into compact sequences, they simplify the pretraining target for VLMs, leading to more efficient training [32]. Furthermore, these compact structures implicitly encode task-relevant semantics and support the integration of non-visual/non-linguistic modalities, offering robust and generalizable representations for complex behaviors [26,32]. For example, RDT-1B and EnerVerse leverage latent representations for long-range and efficient action planning, ensuring transferability across robots [16].

Despite these advantages, the use of latent representations as action tokens is not without challenges and limitations. A primary concern is the **lack of explainability and controllability**, which hinders human intervention or correction of policy failures, making debugging difficult and rendering them unsuitable for safety-critical applications [26,32]. Another limitation is the potential for **task-irrelevant visual variations** (e.g., background clutter or camera shakiness) to be inadvertently captured in the latent space, although some models like UniVLA attempt to mitigate this using techniques like DINOv2 [32]. Moreover, **inadequate granularity and low reconstruction fidelity** can restrict their effectiveness in highly dexterous tasks, while an **incomplete vocabulary of behaviors** within the latent space can lead to policy failures in novel situations [32]. There is also the risk of **misalignment with human intention**, where latent spaces encode information irrelevant to instructions, necessitating robust methods to disentangle task-centric signals from noise [32]. Finally, the stability of training these complex latent spaces remains a significant concern, influencing overall performance and reliability [26,35]. The pursuit of "interpretable intermediate representations" in hierarchical models is a direct response to these limitations, aiming to enhance modularity, generalization, and debuggability [6].

In conclusion, latent representations as action tokens represent a pivotal paradigm in VLA models, driving progress towards abstract, kinematic-agnostic action encoding for enhanced generalization and data efficiency. While solutions like task-centric latent actions ('unified musical scores') demonstrate significant potential in overcoming action symbol incompatibility across diverse robots, critical trade-offs persist between their powerful generalization capabilities and inherent difficulties in training stability, interpretability, and fine-grained control. Addressing these challenges remains a focal point for future research to unlock the full potential of VLA models in complex real-world applications.
#### 5.2.7 Raw Action as Action Tokens

**Raw Action as Action Tokens in VLA Models**

| Aspect                | Description                                                                       | Advantages                                                                      | Limitations                                                                         | Key Examples / Models                                                                                                         |
|-----------------------|-----------------------------------------------------------------------------------|---------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|
| **Role**              | Direct mapping from multimodal inputs to physical actions.                        | Simplifies robot control pipeline, leverages VLM semantic understanding, minimal human expertise needed. | Data scarcity, poor cross-embodiment generalization, catastrophic forgetting.     | LangLfP, BCZ (feature fusion), VIMA, Gato, LEO, JARVIS-VLA (transformer generalists), RT-series (RT-1, RT-2), OpenVLA, OXE, InstructVLA, MemoryVLA, EfficientVLA, DreamVLA, DexVLA |
| **Architectures**     | From feature fusion (MLP/CNN) to Transformer-based generalists and autoregressive VLAs. | Unified VLA model with robust performance, efficient fine-tuning.               | High dimensionality of continuous actions, physically infeasible outputs, semantic-physical mismatch. | Autoregressive Robot VLA (RT-series, OpenVLA), Diffusion-Based Action Chunking (Octo, $\pi_0$, $\pi_0$-FAST, RDT, CogACT, HybridVLA) |
| **Action Outputs**    | Motor commands, joint torques, end-effector poses, discrete tokens.               |                                                                                 | Less practical for long-term control (high context, cost, latency), jerky movements from discrete modes. | Raw continuous values, or discretized tokens (e.g., RT-2's 7-token sequences, AutoVLA's discrete actions).                 |
| **Debate**            | Outputting raw actions vs. higher-level commands with independent controllers.    | Direct physical control for end-to-end learning.                                | 3D-VLA (generative world model for planning), ThinkAct (conditions on visual plan latent), UniVLA (latent action representation) | UAV control (geo-coordinates), 3D generalists (symbolic code) as alternative.   |



**Raw Action as Action Tokens in VLA Models**

| Aspect                | Description                                                                       | Advantages                                                                      | Limitations                                                                         | Key Examples / Models                                                                                                         |
|-----------------------|-----------------------------------------------------------------------------------|---------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|
| **Role**              | Direct mapping from multimodal inputs to physical actions.                        | Simplifies robot control pipeline, leverages VLM semantic understanding, minimal human expertise needed. | Data scarcity, poor cross-embodiment generalization, catastrophic forgetting.     | LangLfP, BCZ (feature fusion), VIMA, Gato, LEO, JARVIS-VLA (transformer generalists), RT-series (RT-1, RT-2), OpenVLA, OXE, InstructVLA, MemoryVLA, EfficientVLA, DreamVLA, DexVLA |
| **Architectures**     | From feature fusion (MLP/CNN) to Transformer-based generalists and autoregressive VLAs. | Unified VLA model with robust performance, efficient fine-tuning.               | High dimensionality of continuous actions, physically infeasible outputs, semantic-physical mismatch. | Autoregressive Robot VLA (RT-series, OpenVLA), Diffusion-Based Action Chunking (Octo, $\pi_0$, $\pi_0$-FAST, RDT, CogACT, HybridVLA) |
| **Action Outputs**    | Motor commands, joint torques, end-effector poses, discrete tokens.               |                                                                                 | Less practical for long-term control (high context, cost, latency), jerky movements from discrete modes. | Raw continuous values, or discretized tokens (e.g., RT-2's 7-token sequences, AutoVLA's discrete actions).                 |
| **Debate**            | Outputting raw actions vs. higher-level commands with independent controllers.    | Direct physical control for end-to-end learning.                                | 3D-VLA (generative world model for planning), ThinkAct (conditions on visual plan latent), UniVLA (latent action representation) | UAV control (geo-coordinates), 3D generalists (symbolic code) as alternative.   |

The paradigm of "Raw Action as Action Tokens" in Vision-Language-Action (VLA) models posits a direct mapping from multimodal inputs—vision and language—to concrete, physical actions executable by a robot, such as motor commands, joint torques, or end-effector poses [26,28,34,35]. This approach views robot control as an extension of language tasks within a unified VLM framework, where the robot's continuous action space is often discretized into a sequence of tokens that the VLM predicts autoregressively or in parallel [25,26,28]. The motivation behind this end-to-end learning strategy stems from the success of foundation models, offering a straightforward alternative when defining appropriate token representations is challenging, requiring minimal human knowledge or structural constraints and leveraging readily available raw action data [27,32].

The progression of raw action token models demonstrates a clear evolution from early feature fusion techniques to sophisticated transformer-based architectures. Initial methodologies, characterized by **Vision-Language Feature Fusion**, employed traditional neural networks like Multilayer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs). For instance, **LangLfP** utilized MLPs and CNNs with a CVAE decoder, integrating 10 million goal-image-conditioned state-action pairs with 10 thousand language samples. Similarly, **BCZ** fused vision and language features using ResNet and a multilingual sentence encoder, followed by a multi-stage FiLM conditioning and MLP to process 26 thousand robot data samples alongside 19 thousand human videos [27]. These early approaches showcased the potential for end-to-end mapping but often relied on smaller, specialized datasets.

A significant architectural shift occurred with the advent of **Transformer-Based Generalists**, drawing inspiration from large language models and vision transformers. Models like **VIMA** integrated Mask R-CNN and ViT for object tokenization, concatenating these with language inputs processed by a pre-trained T5 model for multi-modal prompt generation and subsequent cross-attention-based decoding [27]. **Gato** further exemplified this trend as a 1.2 billion-parameter decoder-only transformer, trained on a diverse set of 596 control tasks and 8 vision-language datasets, establishing itself as a multi-modal, multi-task, multi-embodiment generalist [27]. **LEO** extended Gato by incorporating 3D datasets to enhance 3D reasoning, while **JARVIS-VLA** fine-tuned pre-trained VLMs like Qwen2-VL or LLaVA-NeXT using a three-stage strategy for complex environments like Minecraft [27].

A prominent sub-category within transformer-based models is **Autoregressive Robot VLA**, exemplified by the **RT series**. **RT-1** (Robotic Transformer 1) employed EfficientNet, FiLM conditioning, and a transformer decoder, demonstrating strong performance on seen tasks and generalization to unseen scenarios with 130 thousand demonstrations across over 700 tasks [27]. Building upon this, **RT-2** (Robotic Transformer 2) directly fine-tuned large-scale VLMs such as PaLI-X and PaLM-E into end-to-end VLAs. It discretized raw actions into 7-token sequences for autoregressive inference, co-training on internet-scale vision-language and robot trajectory data. This approach enhanced reasoning capabilities and enabled emergent abilities, even supporting Chain-of-Thought reasoning, effectively allowing the VLM to perform low-level control [16,20,26,27,28]. Other models like **OpenVLA** adopted a similar paradigm, replacing large vision encoders with more efficient ones like SigLIP and DINOv2 and fine-tuning on real-world robot manipulation data [11,26]. The **OXE** project further demonstrated the benefits of unifying over 1 million trajectories from 22 robots for cross-embodiment training [27].

Beyond autoregressive methods, **Diffusion-Based Action Chunking** emerged to address limitations concerning continuous actions and inference frequency. Models like **Octo** utilized a transformer with a diffusion head, employing block-wise attention for flexibility [27]. **$\pi_0$** and **$\pi_0$-FAST** applied flow matching and action chunking, with the latter employing Discrete Cosine Transform (DCT) for action chunk encoding, achieving significant compression and smoother trajectories to mitigate issues of naive binning [27,28]. Other contributions include **RDT** for bimanual manipulation, **CogACT** using a diffusion head for OpenVLA, and **HybridVLA** which integrates both autoregressive and diffusion mechanisms [27]. **DreamVLA** and **DexVLA** also leverage diffusion-based transformers to generate executable raw actions, performing inverse dynamics modeling to achieve manipulation tasks with precise motor commands [2,29].

The core benefit of end-to-end learning for raw actions lies in its ability to simplify the robotic control pipeline. By directly mapping multimodal inputs to motor commands, these models leverage the powerful semantic understanding and generalization capabilities of pre-trained VLMs, often leading to a unified VLA model with robust performance [26,35]. This approach requires minimal human expertise in action definition and reduces the burden of action token annotation, as real-world data is inherently collected in raw action format. Furthermore, it offers similar scaling potential to VLMs and can be fine-tuned efficiently [27,32]. Models like InstructVLA achieve "precise action generation" and "leading manipulation performance" through direct output of concrete control signals, showcasing the effectiveness of this paradigm [5,14]. MemoryVLA and EfficientVLA also demonstrate this direct execution, producing continuous motor commands or precise action sequences respectively [9,31]. The application extends to domains like autonomous driving, where VLA models predict continuous control signals such as steering angle and acceleration, bypassing traditional rule engines for potentially superior global performance [8].

Despite its promise, the "Raw Action as Action Tokens" paradigm faces several significant limitations. A primary concern is **data scarcity**, as collecting massive datasets of raw action trajectories is costly, typically requiring extensive teleoperation and real robot interaction, which limits scalability [27,32]. This is evident in datasets like SynGrasp-1B used by GraspVLA, which relies on synthetic data to achieve billion-frame scale [10]. Furthermore, raw action tokens are inherently specific to the robot platform, leading to **poor cross-embodiment generalization** [27,32]. Fine-tuning large VLMs on robot-specific data can also lead to **catastrophic forgetting** of their rich pre-trained visual-language knowledge [27].

From a computational and control perspective, the high dimensionality of continuous action spaces when discretized can be problematic, and such models can struggle with physically infeasible action outputs and semantic-physical mismatches [26]. Directly generating raw control signals is often less practical for **long-term control tasks** due to high context length, computational cost, and inference latency [27]. Discrete autoregressive tokenization particularly struggles with continuous or multi-modal actions required for dexterous tasks and limits the inference frequency. Even with action chunking, inference latency can result in outdated observations and discontinuities or jerky movements from mode shifts, which are detrimental to real-time robot control [27]. Moreover, end-to-end models often struggle with explicit reasoning or planning over multiple steps [26,32].

The literature presents a debate regarding the direct generation of raw actions. Many papers explicitly define VLA models as directly mapping to raw actions, including motor commands, joint angles, force/torque values, or wheel speeds [25,28,30]. However, some studies argue that VLA models often output higher-level commands, with low-level motion planning being completed by independent controllers rather than directly generating joint torque signals [18]. For instance, **3D-VLA** departs from direct perception-to-action mapping by incorporating a generative world model for planning, implying a move towards planned, goal-conditioned actions over raw outputs [3]. Similarly, **ThinkAct** generates raw actions via a downstream action model but conditions them on an intermediate "visual plan latent" rather than direct end-to-end generation from multimodal inputs [12]. Models like **UniVLA** explicitly move away from raw, embodiment-specific action tokens, aiming to learn a unified, latent action representation abstracting away specific control signals to achieve cross-robot generalizability, highlighting the incompatibility and high annotation cost of raw actions [21]. Furthermore, in specific domains, such as UAV control, generated flight paths are high-level geo-coordinates rather than raw motor commands [15], and some 3D generalist models output symbolic code for manipulating 3D scene elements instead of physical controls [33]. Advanced approaches like **AutoVLA** and **OpenDriveVLA** also introduce intermediate tokenization steps to prevent VLM hallucinations and ensure structured, rule-based action generation, moving beyond purely "raw" outputs [17].

Despite these challenges and ongoing debates, the promise of learning general-purpose robotic policies through the raw action token paradigm remains significant. By integrating diverse datasets and leveraging the scaling properties of foundation models, efforts like **GR00T N1** (using a data pyramid and hierarchical architecture) and **UniAct** (learning vector-quantized codes for a universal action space) aim to overcome embodiment limitations and foster broader applicability [27]. The ability to directly translate complex visual and linguistic instructions into physical robot movements represents a transformative framework for embodied AI [19,35], with ongoing research focusing on refining action representations, improving data efficiency, and enhancing generalization capabilities.
#### 5.2.8 Reasoning as Action Tokens

**Reasoning as Action Tokens in VLA Models**

| Aspect                   | Description                                                                 | Advantages                                                                    | Limitations                                                                         | Key Examples / Models                                                                                                         |
|--------------------------|-----------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|
| **Role**                 | Meta-token or intermediate process to enhance decision-making, planning, and interpretability. | Improves interpretability, facilitates human-in-the-loop, enhances decision-making for complex tasks/long-horizon planning. | Increased inference time, reduced execution speed, potential for hallucinations if not grounded. | Embodied CoT (ECoT), CoT-VLA, GraspVLA (unified CoT), DriveVLM, AlphaDrive (AD CoT), AutoVLA (Adaptive Reasoning), InstructVLA (textual reasoning, MoE), MemoryVLA (cognitive token), DreamVLA (world knowledge forecasting) |
| **Evolution**            | From language-centric CoT to action-token-based reasoning with multimodal feedback. | Leverages VLM intrinsic capabilities, emergent symbolic understanding, visually-grounded reasoning. | Redundant computation in language modules, reliance on manually fixed reasoning steps. | 3D-Generalist (CoT for code generation), RT-2 (VLM backbone intrinsic reasoning), RoboMamba.                                   |
| **Hierarchical Reasoning** | VLMs as high-level task planners decomposing complex instructions into subtasks. | Guides overall task flow, cross-embodiment consistency.                       | Data scarcity for high-quality reasoning datasets.                                  | LoHoVLA (hierarchical control), RationalVLA, Embodied-Reasoner, Reinforced Planning, ViLA, HiRobot, DexVLA (sub-step reasoning), ThinkAct (embodied reasoning plans) |
| **Adaptive Computation & Self-Correction** | Dynamic adjustment of reasoning, failure diagnosis, and correction.          | Autonomous recovery, robustness to errors, efficient resource allocation.     | Challenges in real-time integration while maintaining interpretability.           | InstructVLA (adaptive switching), SC-VLA (CoT for diagnosis), 3D-Generalist (iterative refinement).                         |



**Reasoning as Action Tokens in VLA Models**

| Aspect                   | Description                                                                 | Advantages                                                                    | Limitations                                                                         | Key Examples / Models                                                                                                         |
|--------------------------|-----------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|
| **Role**                 | Meta-token or intermediate process to enhance decision-making, planning, and interpretability. | Improves interpretability, facilitates human-in-the-loop, enhances decision-making for complex tasks/long-horizon planning. | Increased inference time, reduced execution speed, potential for hallucinations if not grounded. | Embodied CoT (ECoT), CoT-VLA, GraspVLA (unified CoT), DriveVLM, AlphaDrive (AD CoT), AutoVLA (Adaptive Reasoning), InstructVLA (textual reasoning, MoE), MemoryVLA (cognitive token), DreamVLA (world knowledge forecasting) |
| **Evolution**            | From language-centric CoT to action-token-based reasoning with multimodal feedback. | Leverages VLM intrinsic capabilities, emergent symbolic understanding, visually-grounded reasoning. | Redundant computation in language modules, reliance on manually fixed reasoning steps. | 3D-Generalist (CoT for code generation), RT-2 (VLM backbone intrinsic reasoning), RoboMamba.                                   |
| **Hierarchical Reasoning** | VLMs as high-level task planners decomposing complex instructions into subtasks. | Guides overall task flow, cross-embodiment consistency.                       | Data scarcity for high-quality reasoning datasets.                                  | LoHoVLA (hierarchical control), RationalVLA, Embodied-Reasoner, Reinforced Planning, ViLA, HiRobot, DexVLA (sub-step reasoning), ThinkAct (embodied reasoning plans) |
| **Adaptive Computation & Self-Correction** | Dynamic adjustment of reasoning, failure diagnosis, and correction.          | Autonomous recovery, robustness to errors, efficient resource allocation.     | Challenges in real-time integration while maintaining interpretability.           | InstructVLA (adaptive switching), SC-VLA (CoT for diagnosis), 3D-Generalist (iterative refinement).                         |

Reasoning stands as a fundamental mechanism within Vision-Language-Action (VLA) models, transcending mere descriptive capabilities to function as a meta-token or an indispensable intermediate process that profoundly enhances the effectiveness of all other action token types [26,30,32,34]. This deliberative thinking process is often externalized, frequently in natural language, to bridge the gap between high-level instructions and low-level executable actions, allowing models to "think step-by-step" [27]. Its integration shifts VLA models from simple reactive control towards more advanced, deliberative decision-making, crucial for complex and long-horizon tasks [26,34].

The evolution of reasoning capabilities in VLA models has progressed from initial language-centric approaches to more sophisticated, action-token-based reasoning with rich multimodal feedback. Initially, reasoning drew heavily from Chain-of-Thought (CoT) prompting in Large Language Models (LLMs), which was subsequently extended to visual and multimodal contexts, sometimes involving the generation of visual intermediates like bounding boxes [27]. Early works, such as Inner Monologue, leveraged LLMs for recursive multi-step language planning [27,30]. Current approaches increasingly employ Vision-Language Models (VLMs) as backbones due to their inherent multimodal knowledge, often fine-tuned or retrained for specific reasoning patterns [27]. For instance, 3D-Generalist explicitly instructs its VLM (GPT-4o) to use a CoT approach to break down problems and generate code, with self-improvement fine-tuning enhancing visually grounded understanding and reducing visual hallucinations, thereby improving reasoning capabilities based on visual feedback [33]. Similarly, RT-2 integrates reasoning by leveraging its VLM backbone's intrinsic capabilities, aligning robot action tokens with natural language text tokens to exhibit emergent symbolic understanding and visually-grounded reasoning, enabling the execution of more complex instructions [20]. RoboMamba also enhances reasoning and manipulation through collaborative training of visual encoders and a Mamba language model, aligning visual data with language embeddings [16].

A prevalent approach is hierarchical reasoning, where VLMs function as high-level task planners that decompose complex instructions into a sequence of manageable subtasks [6,28,34]. These subtasks, or high-level plans, can themselves be considered "action tokens" at a higher level of abstraction, guiding the overall task flow [34]. For example, LoHoVLA employs a "Hierarchical Closed-Loop Control" mechanism to address planning errors, while RationalVLA integrates reasoning to condition controllers or refuse infeasible commands [26]. Models like Embodied-Reasoner, Reinforced Planning, ViLA, HiRobot, and DexVLA generate natural language subtasks as intermediate outputs to guide actions and manage task flow, particularly for long-horizon tasks [26,30]. DexVLA specifically learns to generate these "sub-step reasoning" annotations as intermediate language outputs, allowing its VLM to implicitly act as a high-level policy model for managing complex task flows like laundry folding without an external high-level policy [29]. This generation of "embodied reasoning plans" by multimodal LLMs is fundamental to guiding agents' actions over long horizons, as exemplified by ThinkAct, which provides explicit, visually-grounded reasoning plans optimized through reinforcement learning for self-correction and adaptation [12,30].

Several models have implemented distinct reasoning mechanisms. Embodied CoT (ECoT) generates a reasoning chain combining high-level task planning with visually grounded features, improving performance on long-horizon tasks and showing enhanced generalization to unseen objects [26,27]. CoT-VLA extends this by predicting subgoal observations (future images) before generating action sequences, embedding a sub-goal generation mechanism [26,30]. GraspVLA incorporates a "unified Chain-of-Thought process" to integrate perception and action generation, connecting high-level semantic understanding with low-level action generation for better generalization and sim-to-real transfer [10]. In autonomous driving, DriveVLM applies CoT across scene description, analysis, and hierarchical planning, while AlphaDrive is an RL-trained VLM for reasoning in this domain [27]. AutoVLA uses "Adaptive Reasoning" during fine-tuning and applies action tokenization with rules to mitigate VLM hallucinations, demonstrating how explicit, often rule-based or adaptively learned, reasoning guides action generation and ensures adherence to complex policies like traffic rules [17]. InstructVLA emphasizes textual reasoning at inference time to boost manipulation performance and adaptively switches between reasoning and operation modes, decoupling language output (reasoning) from action generation for efficient caching and reuse across action steps [5,14]. MemoryVLA integrates reasoning through a "cognitive token" and a "cognitive memory stream," where a LLaMA component produces a cognitive token encapsulating high-level understanding and intent, guiding action planning via a cognitive attention layer [31]. DreamVLA, inspired by human abstract multimodal reasoning, integrates "comprehensive world knowledge forecasting" for action planning [2].

Adaptive test-time computation plays a crucial role in developing more robust and intelligent VLA systems. Models like InstructVLA demonstrate adaptive switching between reasoning and operation modes based on context, where language results representing reasoning can be cached and reused across multiple action steps, forming a "thinking-understanding-execution" closed loop [14]. Furthermore, self-correcting frameworks like SC-VLA (2024) use chain-of-thought reasoning to query an internal LLM to diagnose failure modes and generate correction strategies, reducing task failure rates significantly [25,28]. This agentic AI vision includes modules that autonomously generate exploration goals, hypothesize outcomes, and self-correct through simulation or real-world rollback, representing a continuous reasoning and adaptation process [28]. Safety mechanisms also integrate "formal verification" layers to symbolically analyze planner outputs and guarantee safety before neural network controllers execute actions, which is a form of logical reasoning [28].

The strengths of integrating reasoning as action tokens are manifold: it significantly improves interpretability and facilitates human-in-the-loop capabilities by externalizing thought processes, allowing for review, failure tracing, and real-time intervention [26,27]. Reasoning enhances decision-making for complex tasks and long-horizon planning, as it explicitly links instructions to anticipated outcomes and enables structured sequencing of intermediate goals [26,32,34]. It also leads to enhanced generalization to unseen objects and scenes by leveraging VLM prior knowledge [11,27]. Moreover, high-level plans derived from reasoning can maintain consistency across different embodiments, fostering cross-embodiment capabilities [27]. In autonomous driving, reasoning is critical for interpreting complex rules, handling diverse scenarios, and ensuring adherence to safety policies, as seen in models like Yuanrong Qixing VLA and the mechanisms in OpenDriveVLA and AutoVLA that prevent hallucinations and guide compliant actions [17,19]. The "Language" component of VLA models enables robots to "think about the intent behind instructions," such as perceiving a banana peel and inferring "slippery," then planning to "pick up" with "tongs," illustrating an integrated reasoning process guiding action selection [18].

Despite these advantages, several limitations and challenges persist. A primary concern is the increased inference time and reduced execution speed associated with generating lengthy reasoning processes, which can hinder real-time task performance [26,27,32]. While models like ECoT have achieved some speedup through asynchronous execution, further acceleration is needed [27]. The problem of "redundant computation" in the language module responsible for reasoning is also a challenge, with solutions like EfficientVLA aiming to streamline this process by identifying and removing ineffective neural network layers [9]. Another limitation is the potential for hallucinations if reasoning is not well-grounded, requiring robust feedback mechanisms for self-correction [26,33]. Current implementations often rely on manually fixed reasoning steps, which can limit generalization and exploration of potentially superior reasoning pathways, highlighting a defect in action representation flexibility [27]. Furthermore, obtaining high-quality, large-scale reasoning datasets poses a significant data scarcity challenge, as human annotation is often impractical at scale [27]. Integrating complex reasoning into real-time action generation while ensuring its interpretability remains a core challenge, though neuro-symbolic fusion approaches, which combine the interpretability of structured plans with the flexibility of learned policies, offer a promising direction [28].
## 6. Key Applications of VLA Models
Vision-Language-Action (VLA) models are fundamentally transforming how intelligent agents perceive, interpret, and interact with complex environments, moving beyond specialized, pre-programmed functionalities towards more generalist, intuitive, and adaptable operational paradigms [25,35]. This section provides a comprehensive overview of the diverse and impactful application domains where VLA models are demonstrating transformative capabilities, ranging from physical robotic systems to virtual environments and digital interfaces. The core theoretical framework underpinning these applications lies in the seamless integration of multimodal sensory inputs (primarily vision and language) with effective action generation, enabling agents to understand high-level human instructions, reason about their surroundings, and execute precise actions in dynamic and unstructured scenarios [8].

A primary and highly visible application area is **Robotic Manipulation**, where VLA models empower robots to perform intricate tasks with unprecedented dexterity and generalization. By directly interpreting natural language instructions and visual cues, robots can move beyond repetitive, pre-defined movements to engage in complex multi-stage tasks such as "pick-place" operations, laundry folding, or multi-step assembly [25,32]. Pioneering models like RT-2 have demonstrated the efficacy of end-to-end vision-language-action fusion for improved generalization to novel objects and environments [8,20], while OpenVLA and UniVLA highlight the potential of open-source, large-scale trained models to achieve broad applicability and high success rates across numerous tasks and robot embodiments [11,21]. Further advancements from models like DexVLA and TLA illustrate progress in dexterous manipulation and the integration of tactile feedback for contact-rich tasks [29,35], underscoring VLA's capacity to address the nuanced demands of physical interaction.

**Autonomous Driving** represents another critical domain, where VLA models offer a paradigm-shifting approach to enhance safety and efficiency. Unlike conventional modular systems prone to error accumulation, VLA models enable an end-to-end integration of perception, semantic understanding, and real-time action generation. This allows for processing multimodal inputs (visual streams, LiDAR, language instructions) to produce precise control signals, leading to improved generalization and scene adaptability, particularly in complex urban scenarios [8,28]. Models like OpenDriveVLA and AutoVLA exemplify this by facilitating deep understanding of 3D environments and generating interpretable action plans from raw sensor data and language commands [7,17]. The application also extends to aerial robotics with UAV-VLA models, broadening the scope of autonomous navigation and task execution [28].

The overarching concept of **General Embodied AI Tasks** encapsulates the aspiration for VLA models to create intelligent agents capable of operating autonomously and effectively across diverse physical and digital environments, extending beyond traditional robotics to smart appliances, autonomous vehicles, and even digital interfaces [28,34]. These models excel at interpreting complex multimodal instructions, performing long-horizon planning, and adapting to dynamic environments. InstructVLA, for instance, demonstrates superior performance in zero-shot, language-conditioned tasks through advanced instruction tuning [5]. Crucially, models like ThinkAct and MemoryVLA address long-horizon planning and the integration of historical context, enabling more intelligent and adaptive behavior in non-Markovian and sequential tasks [12,31]. An emerging frontier within this domain is the automated generation of 3D content and interactive world models, where VLA paradigms are applied to tasks like generating layouts and assets from text prompts, supporting data generation for other embodied AI applications [33].

Beyond these foundational areas, VLA models exhibit remarkable versatility in **Other Domain-Specific Applications**. In Unmanned Aerial Vehicles (UAVs), systems like UAV-VLA enable natural language-based mission generation, simplifying human-UAV interaction and fostering inter-robot collaboration [15]. The paradigm also translates effectively to **non-physical embodied agents**, revolutionizing human-computer interaction through GUI and general computer control, where models like ShowUI enable VLA agents to navigate and interact with digital interfaces as if they were physical environments [22]. In **Medical and Industrial Robotics**, VLA models enhance precision and safety, exemplified by RoboNurse-VLA for surgical assistance or CogACT for high-precision assembly in industrial settings [28]. Furthermore, **Precision Agriculture** benefits from VLA's ability to interpret visual inputs for tasks like disease detection and selective harvesting, leading to optimized resource management [28]. Finally, VLA models are poised to transform **Augmented Reality (AR) Navigation and immersive virtual environments**, offering context-aware real-time guidance and dynamic environment generation [28,33].

Despite these significant advancements, several challenges remain across all application areas. These include the scarcity of high-quality, diverse, and real-world training data [16], ensuring robustness and generalizability in highly variable and open-ended environments, efficiently handling contact-rich and dexterous tasks, and overcoming issues like "catastrophic forgetting" in continuous learning scenarios [14]. Furthermore, achieving efficient real-time deployment for complex VLA models and establishing reliable benchmarks for evaluating true "omnipotent robot foundation models" are critical for future progress [9,29]. The ongoing development of large-scale pre-training initiatives, open-source models, and advanced simulation environments (e.g., those enabled by 3D-Generalist) are crucial steps towards addressing these challenges and realizing the full potential of VLA models in creating truly intelligent and versatile autonomous agents [33].
### 6.1 Robotic Manipulation

**Key VLA Models and Their Contributions in Robotic Manipulation**

| Model            | Key Contribution / Focus                                                                       | Achievements / Capabilities                                                                                                                                                                                                      | Benchmarks / Environments                                             |
|------------------|------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------|
| **RT-2**         | Pioneer VLA, end-to-end vision-language-action fusion for robot control.                     | 63% improvement on novel objects, stronger semantic understanding, generalizes to new objects/backgrounds/environments, basic reasoning (smallest/closest object).                                                               | General robot tasks (Pick Object, Place Object), SimplerEnv, CALVIN |
| **OpenVLA**      | Open-source, generalist manipulation model, trained on large real-robot data.                  | 81.7% avg success across 29 tasks, 16.5% improvement over RT-2-X (with fewer params), 42% error reduction in multi-object assembly.                                                                                             | OpenX, LIBERO, Meta-World, FrankaKitchen                          |
| **DexVLA**       | Combines large VLM with billion-parameter diffusion action expert for dexterous skills.        | High success on diverse manipulations (drink pouring, bimanual packing), generalizes to novel objects/scenes, outperforms OpenVLA/Octo.                                                                                          | LIBERO, SimplerEnv, Adroit                                            |
| **InstructVLA**  | Instruction-tuned VLA for robust zero-shot language-conditioned tasks.                         | 92% outperformance over fine-tuned OpenVLA (SimplerEnv-Instruct), 27% over Magma (closed-loop), higher success in real-world instruction understanding.                                                                          | SimplerEnv-Instruct                                                   |
| **UniVLA**       | Learns "task-centric latent actions" for cross-embodiment, cross-view generalization.        | 95.2% success on LIBERO (full), 19% improvement over OpenVLA, 68.9% in real-world (Piper arm) with dynamic changes.                                                                                                                | LIBERO, SimplerEnv, Room2Room (navigation)                            |
| **MemoryVLA**    | Dual-memory system for long-horizon temporal tasks.                                            | 83% avg success in long-horizon tasks (real-world), outperforms CogACT/π0, robust to OOD visual conditions.                                                                                                                       | SimplerEnv, LIBERO                                                    |
| **GraspVLA**     | Grasping Foundation Model for open-vocabulary generalization.                                  | Trained on SynGrasp-1B (billion-frame synthetic), mitigates sim-to-real gaps, grasps wide range of objects.                                                                                                                      | Robotic grasping tasks                                                |
| **CogACT**       | Componentized VLA for industrial manipulation, diffusion-based action Transformer.             | 59% higher real task success than OpenVLA in complex industrial tasks (multi-step assembly, screw tightening).                                                                                                                     | Industrial manipulation tasks                                         |
| **HumanPlus / ORION** | Learn full-body/object-centric manipulation from human demonstrations (videos).             | HumanPlus: high success with few demos (40) for humanoid robots. ORION: learns from single human videos, generalizes to new objects.                                                                                             | Humanoid robot tasks, general manipulation                            |
| **RDT-1B / $\pi_0$ Series** | Focus on bimanual manipulation (RDT-1B) or general-purpose VLAs ($\pi_0$).                 | RDT-1B: addresses data scarcity with unified action space. $\pi_0$: strong zero-shot generalization, high control frequency (50Hz).                                                                                             | Bimanual tasks, household operations                                  |
| **TLA**          | Integrates tactile signals for contact-intensive tasks.                                        | Over 85% success in peg-in-hole assembly.                                                                                                                                                                                        | Contact-intensive tasks (peg-in-hole)                                 |



**Key VLA Models and Their Contributions in Robotic Manipulation**

| Model            | Key Contribution / Focus                                                                       | Achievements / Capabilities                                                                                                                                                                                                      | Benchmarks / Environments                                             |
|------------------|------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------|
| **RT-2**         | Pioneer VLA, end-to-end vision-language-action fusion for robot control.                     | 63% improvement on novel objects, stronger semantic understanding, generalizes to new objects/backgrounds/environments, basic reasoning (smallest/closest object).                                                               | General robot tasks (Pick Object, Place Object), SimplerEnv, CALVIN |
| **OpenVLA**      | Open-source, generalist manipulation model, trained on large real-robot data.                  | 81.7% avg success across 29 tasks, 16.5% improvement over RT-2-X (with fewer params), 42% error reduction in multi-object assembly.                                                                                             | OpenX, LIBERO, Meta-World, FrankaKitchen                          |
| **DexVLA**       | Combines large VLM with billion-parameter diffusion action expert for dexterous skills.        | High success on diverse manipulations (drink pouring, bimanual packing), generalizes to novel objects/scenes, outperforms OpenVLA/Octo.                                                                                          | LIBERO, SimplerEnv, Adroit                                            |
| **InstructVLA**  | Instruction-tuned VLA for robust zero-shot language-conditioned tasks.                         | 92% outperformance over fine-tuned OpenVLA (SimplerEnv-Instruct), 27% over Magma (closed-loop), higher success in real-world instruction understanding.                                                                          | SimplerEnv-Instruct                                                   |
| **UniVLA**       | Learns "task-centric latent actions" for cross-embodiment, cross-view generalization.        | 95.2% success on LIBERO (full), 19% improvement over OpenVLA, 68.9% in real-world (Piper arm) with dynamic changes.                                                                                                                | LIBERO, SimplerEnv, Room2Room (navigation)                            |
| **MemoryVLA**    | Dual-memory system for long-horizon temporal tasks.                                            | 83% avg success in long-horizon tasks (real-world), outperforms CogACT/π0, robust to OOD visual conditions.                                                                                                                       | SimplerEnv, LIBERO                                                    |
| **GraspVLA**     | Grasping Foundation Model for open-vocabulary generalization.                                  | Trained on SynGrasp-1B (billion-frame synthetic), mitigates sim-to-real gaps, grasps wide range of objects.                                                                                                                      | Robotic grasping tasks                                                |
| **CogACT**       | Componentized VLA for industrial manipulation, diffusion-based action Transformer.             | 59% higher real task success than OpenVLA in complex industrial tasks (multi-step assembly, screw tightening).                                                                                                                     | Industrial manipulation tasks                                         |
| **HumanPlus / ORION** | Learn full-body/object-centric manipulation from human demonstrations (videos).             | HumanPlus: high success with few demos (40) for humanoid robots. ORION: learns from single human videos, generalizes to new objects.                                                                                             | Humanoid robot tasks, general manipulation                            |
| **RDT-1B / $\pi_0$ Series** | Focus on bimanual manipulation (RDT-1B) or general-purpose VLAs ($\pi_0$).                 | RDT-1B: addresses data scarcity with unified action space. $\pi_0$: strong zero-shot generalization, high control frequency (50Hz).                                                                                             | Bimanual tasks, household operations                                  |
| **TLA**          | Integrates tactile signals for contact-intensive tasks.                                        | Over 85% success in peg-in-hole assembly.                                                                                                                                                                                        | Contact-intensive tasks (peg-in-hole)                                 |

Robotic manipulation stands as a central and transformative application domain for Vision-Language-Action (VLA) models, enabling robots to move beyond pre-programmed routines towards more intuitive, controllable, and adaptable operations in complex, unstructured, and novel environments [6,25,35]. The integration of VLA models allows robots to directly "understand human speech" and "perform actions" based on natural language instructions and visual input, representing a significant paradigm shift in robotic control [8,14]. This capability facilitates a fusion of visual grounding, spatial reasoning, and sequential motion planning, crucial for tackling sophisticated real-world tasks.

VLA models address a wide spectrum of manipulation challenges, including whole-body coordination, long-term reasoning, and complex multi-stage tasks. Examples of such tasks range from precise object handling like "pick-place" [32], "grabbing soda cans" [9], and "screwing bottle caps" [18], to more elaborate sequences like "laundry folding" [27,32], "make a sandwich" [32], "dishwashing" (e.g., with $\pi_{0.5}$) [26], and "opening the fridge → taking out milk → closing the fridge → pouring into a cup" [18]. More complex scenarios include "multi-step assembly, screw tightening, and part sorting" in industrial settings [28] and agricultural tasks like identifying and picking ripened fruit [28]. These tasks often require robots to interpret high-level human instructions, reason about their environment, and execute fine-grained motor control, a capability significantly enhanced by VLA architectures [26].

Several prominent VLA models have made unique contributions to advancing robotic manipulation:
*   **RT-2** (Robotic Transformer 2) is credited with coining the term "VLA" and demonstrated the potential of end-to-end vision-language-action fusion in robot control [8,34]. It directly applies large-scale pre-trained VLMs to robot control, showing improved generalization to novel objects, backgrounds, and environments in tasks like "Pick Object" and "Place Object into Receptacle" [16,20,26]. RT-2 achieved a 63% improvement in performance on novel objects [25].
*   **OpenVLA** stands out as a significant open-source generalist manipulation model. Trained on large datasets like OpenX (over 970k real-world robot demonstrations), it has been evaluated across "29 tasks and multiple robot embodiments," demonstrating broad applicability and achieving an average success rate of 81.7% across these tasks, representing a 16.5% improvement over RT-2-X [11,16,19,34]. OpenVLA also notably reduced the error rate by 42% in multi-object assembly tasks and uses DINOv2 as a vision encoder [19,34]. Despite its efficiency, OpenVLA, with 7B parameters, achieved a 16.5% higher success rate than a 55B-parameter RT-2 variant, demonstrating efficient generalization [25].
*   **DexVLA** couples a large VLM with a billion-parameter diffusion action expert, achieving high success on diverse manipulations and generalizing to novel objects and scenes without task-specific tuning [26,29]. It demonstrated superior performance over OpenVLA, Octo, and Diffusion Policy in tasks requiring whole-body coordination and dexterous skills, such as drink pouring with a dexterous hand (0.90 average score) and bimanual packing (0.90 average score) [29].
*   **InstructVLA** emphasizes instruction tuning, leveraging textual reasoning to boost manipulation performance. It achieved "leading manipulation performance" on a novel 80-task benchmark (SimplerEnv-Instruct), outperforming fine-tuned OpenVLA by 92% and a GPT-4o-aided action expert by 29% in simulation [5,14]. Its real-world experiments showed significantly higher success rates on complex instruction understanding tasks, surpassing OpenVLA by 41.7% and 46.7% in celebrity recognition and tool usage, respectively [14].
*   **UniVLA** demonstrates strong generalization capabilities, achieving high success rates on the LIBERO benchmark (95.2% for the full version) and significant improvements (nearly 19 percentage points over OpenVLA) [21]. In real-world deployments with a Piper 7-DOF mechanical arm, UniVLA achieved an average success rate of 68.9% under challenging conditions (sudden lighting changes, object interference, new objects), significantly outperforming OpenVLA (approximately 20.0%) [21].
*   **RDT-1B** focuses specifically on bimanual manipulation, extending diffusion-based VLA models to complex dual-arm tasks and addressing data scarcity through a unified action space [16,27,30].
*   **GOVLA** exhibited advanced capabilities in full-body coordination and long-range reasoning, offering insights for future intelligent driving applications [8].
*   **ORION** focuses on vision-based manipulation, enabling robots to learn object-centric skills from single human videos and generalize them to new objects and environments [16].
*   **HumanPlus** is a full-stack system designed for humanoid robots, allowing them to learn complex full-body manipulation and movement skills, such as putting on shoes, from human motion data via imitation learning, achieving high success rates with limited demonstrations [16]. Similarly, Figure AI's Helix utilizes fully integrated VLA models for high-frequency full-body operations, adapting to unseen objects and new tasks without retraining [28,30].
*   **GraspVLA** is developed as a "Grasping Foundation Model" for open-vocabulary generalization in grasping, trained on the large SynGrasp-1B dataset, enabling robots to grasp a wide range of objects in real-world scenarios [10].
*   **CogACT**, a componentized VLA framework for industrial manipulation, uses a diffusion-based action Transformer for robust action sequencing, achieving over 59% higher real task success rates than previous models like OpenVLA in complex, high-precision industrial tasks such as multi-step assembly and screw tightening [25,28].
*   **MemoryVLA** significantly outperforms baselines like CogACT and $\pi_0$ in long-horizon temporal tasks, demonstrating enhanced whole-body coordination and long-term reasoning, with an 83% average success rate in real-world scenarios compared to CogACT's 57% (+26%) [31].
*   **$\pi_0$** and its variants like **$\pi_{0.5}$** are general-purpose VLAs that generate continuous action trajectories and demonstrate strong zero-shot generalization and adaptation to new tasks, particularly for multi-stage household operations [16,26,30,32].
*   **TLA** made a unique contribution by integrating tactile signals, achieving over 85% success in contact-intensive tasks like peg-in-hole assembly [35].

These models collectively demonstrate how VLAs overcome the limitations of traditional robotic methods, which often struggle with generalization in unstructured and novel scenarios. By leveraging large-scale vision-language pre-training and diverse robot demonstration datasets (e.g., Open X-Embodiment, which provides over 1 million real robot trajectories across 22 robot instances [16]), VLA models achieve superior performance and generalizability across various manipulation benchmarks, including SimplerEnv, LIBERO, Meta-World, FrankaKitchen, Adroit, and CALVIN [2,5,14,21,31,34]. Challenges in this field still include achieving high dexterity and robustly handling contact-rich tasks [30]. The development of tools like 3D-Generalist, which creates "Simulation-Ready 3D environments," further supports robust training by enabling the generation of high-quality synthetic data for robot policies [33]. The trend towards open-source models (e.g., OpenVLA, Octo) and large-scale pretraining signifies a broader movement towards generalist robot policies capable of executing a wide array of tasks with minimal task-specific tuning, paving the way for truly intelligent robotic systems.
### 6.2 Autonomous Driving

**VLA Models in Autonomous Driving: Approach and Key Contributions**

| Model / Approach | Key Contribution / Focus                                                                    | Capabilities / Benefits                                                                                                                                                                                                                                | Benchmarks / Environments                                      |
|------------------|-------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------|
| **General VLA Approach** | End-to-end integration of perception, semantic understanding, and real-time action generation. | Improved generalization, enhanced scene adaptability, faster/more rational decisions in complex scenarios (intersections, low-light, obstacles), flexible strategy adjustment via language, intuitive human-machine collaboration.                 | nuScenes, Waymo Open Dataset                                   |
| **OpenDriveVLA** | Hierarchical tokenization for 3D environment understanding (Map, Scene, Agent Tokens).      | Breakthroughs in 3D environment understanding, interpretable action plans (steering, accel), robust urban navigation, behavior prediction, 0.33m L2 distance, 0.10 collision rate.                                                               | NAVSIM, WOD-E2E                                                |
| **AutoVLA**      | End-to-end semantic reasoning and trajectory planning from raw inputs & language.         | Unifies reasoning and action generation in single autoregressive model, adaptive reasoning, reinforcement fine-tuning, action tokenization (codebook + rules) to mitigate VLM 'hallucinations', 78.84 Driving Score.                              | nuScenes, Waymo, CARLA                                         |
| **DriveVLM**     | Applies Chain of Thought (CoT) for scene description, analysis, and hierarchical planning. | Generates waypoints from language motions and decision descriptions, aids sophisticated deduction.                                                                                                                                       | (CoT-based AD)                                                 |
| **CoVLA**        | Comprehensive dataset + VLM for verbal instruction comprehension in driving context.      | Over 80 hours real driving videos, synchronized sensors, detailed language annotations. Comprehends "yield to ambulance", "merge into traffic", achieves 0.29m L2 distance.                                                                | Real-world driving data                                        |
| **ORION**        | Integrates QT-Former, LLMs for traffic scene reasoning, generative trajectory planner.    | Retains long-term visual context, high-precision visual question answering, robust trajectory planning in complex scenarios (ambiguous instructions, occluded obstacles).                                                                   | (Complex AD scenarios)                                         |
| **AlphaDrive**   | VLM for reasoning in autonomous driving, trained with SFT and GRPO-based RL.              | Sophisticated deduction, common-sense understanding across diverse objects/scenarios.                                                                                                                                                   | (AD reasoning benchmarks)                                      |
| **Yuanrong Qixing VLA** | Mass-produced end-to-end VLA model in China.                                              | Regional adaptability, style transfer, long-tail scenario processing, interprets special lane rules, significant improvement in traffic scene understanding.                                                                               | Urban intelligent driving (real-world deployment)              |
| **UAV-VLA**      | Extends VLA to aerial robotics, natural language-based mission generation.                | Fuses satellite imagery, language, sensors for autonomous execution of high-level instructions (e.g., "deliver package"), 34.22m spatial accuracy (KNN).                                                                                    | UAV path planning                                              |



**VLA Models in Autonomous Driving: Approach and Key Contributions**

| Model / Approach | Key Contribution / Focus                                                                    | Capabilities / Benefits                                                                                                                                                                                                                                | Benchmarks / Environments                                      |
|------------------|-------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------|
| **General VLA Approach** | End-to-end integration of perception, semantic understanding, and real-time action generation. | Improved generalization, enhanced scene adaptability, faster/more rational decisions in complex scenarios (intersections, low-light, obstacles), flexible strategy adjustment via language, intuitive human-machine collaboration.                 | nuScenes, Waymo Open Dataset                                   |
| **OpenDriveVLA** | Hierarchical tokenization for 3D environment understanding (Map, Scene, Agent Tokens).      | Breakthroughs in 3D environment understanding, interpretable action plans (steering, accel), robust urban navigation, behavior prediction, 0.33m L2 distance, 0.10 collision rate.                                                               | NAVSIM, WOD-E2E                                                |
| **AutoVLA**      | End-to-end semantic reasoning and trajectory planning from raw inputs & language.         | Unifies reasoning and action generation in single autoregressive model, adaptive reasoning, reinforcement fine-tuning, action tokenization (codebook + rules) to mitigate VLM 'hallucinations', 78.84 Driving Score.                              | nuScenes, Waymo, CARLA                                         |
| **DriveVLM**     | Applies Chain of Thought (CoT) for scene description, analysis, and hierarchical planning. | Generates waypoints from language motions and decision descriptions, aids sophisticated deduction.                                                                                                                                       | (CoT-based AD)                                                 |
| **CoVLA**        | Comprehensive dataset + VLM for verbal instruction comprehension in driving context.      | Over 80 hours real driving videos, synchronized sensors, detailed language annotations. Comprehends "yield to ambulance", "merge into traffic", achieves 0.29m L2 distance.                                                                | Real-world driving data                                        |
| **ORION**        | Integrates QT-Former, LLMs for traffic scene reasoning, generative trajectory planner.    | Retains long-term visual context, high-precision visual question answering, robust trajectory planning in complex scenarios (ambiguous instructions, occluded obstacles).                                                                   | (Complex AD scenarios)                                         |
| **AlphaDrive**   | VLM for reasoning in autonomous driving, trained with SFT and GRPO-based RL.              | Sophisticated deduction, common-sense understanding across diverse objects/scenarios.                                                                                                                                                   | (AD reasoning benchmarks)                                      |
| **Yuanrong Qixing VLA** | Mass-produced end-to-end VLA model in China.                                              | Regional adaptability, style transfer, long-tail scenario processing, interprets special lane rules, significant improvement in traffic scene understanding.                                                                               | Urban intelligent driving (real-world deployment)              |
| **UAV-VLA**      | Extends VLA to aerial robotics, natural language-based mission generation.                | Fuses satellite imagery, language, sensors for autonomous execution of high-level instructions (e.g., "deliver package"), 34.22m spatial accuracy (KNN).                                                                                    | UAV path planning                                              |

Autonomous driving represents a critical and highly complex application domain for Vision-Language-Action (VLA) models, demanding tightly coupled perception, semantic understanding, and real-time action generation for safety-critical decision-making [25,28]. The integration of visual perception, language understanding, and action decision-making within a unified framework positions VLA models as a paradigm-shifting approach in this field [8]. Indeed, the significance of VLA models in this area is underscored by the existence of dedicated surveys on "Vision-Language-Action Models for Autonomous Driving" [13].

Conventional modular autonomous driving systems typically suffer from limitations such as error accumulation across different stages and high complexity in managing numerous hand-engineered rules [8]. In contrast, VLA models offer an end-to-end approach, integrating environmental observation directly into control command output [8]. This methodology processes multimodal inputs, including visual streams, LiDAR data, natural language instructions, and internal states, to generate precise control signals within a unified autoregressive model [25,28]. This integrated architecture promises improved generalization and enhanced scene adaptability, enabling VLA models to make faster and more rational decisions in complex scenarios, such as intricate intersections, low-light conditions, or the sudden appearance of obstacles [8]. Moreover, these models can flexibly adjust driving strategies based on natural language instructions, thereby facilitating more intuitive human-machine collaborative driving experiences [8,28].

VLA models are specifically adapted to address the unique demands of autonomous driving by understanding complex 3D environments and predicting trajectories [17]. For instance, VLA models can predict future vehicle waypoints in Bird’s Eye View (BEV) space and generate trajectories and captions, as demonstrated by pipelines like CoVLA [32]. This ability extends beyond pixel-level object recognition, allowing AVs to interpret complex instructions such as "turn right at the second intersection after the gas station" by fusing visual and language signals, interpreting spatial relationships, predicting intentions, and generating context-aware driving actions [25,28]. Decision-making in these models is often framed as token prediction, which aligns human semantics with robot motion, aiming for safer and smarter AV technologies [25,28].

Several notable VLA models have been developed for autonomous driving, each contributing unique architectural innovations and capabilities.
*   **OpenDriveVLA** stands out for its hierarchical tokenization, which facilitates a deeper understanding of 3D environments by processing Map, Scene, and Agent Tokens [17]. Its architecture achieves breakthroughs by hierarchically aligning 2D/3D multi-view visual tokens with natural language inputs, combining egocentric spatial awareness with exocentric scene understanding to build a dynamic agent-environment-self interaction model [25,28]. It generates interpretable action plans (e.g., steering angle, acceleration) and human-understandable trajectory visualizations, demonstrating robustness in urban navigation and behavior prediction [28].
*   **AutoVLA** is a prime example of a VLA model designed for end-to-end autonomous driving, integrating semantic reasoning and trajectory planning directly from raw visual inputs and language instructions [7]. It employs adaptive reasoning and reinforcement fine-tuning, coupled with action tokenization via a codebook, to unify reasoning and action generation within a single autoregressive model [7,17]. A key objective of AutoVLA is to overcome challenges such as physically infeasible actions and lengthy reasoning processes often prevalent in earlier VLA models for this domain [7].
*   **DriveVLM** applies Chain of Thought (CoT) across scene description, scene analysis, and hierarchical planning to generate waypoints from language motions and decision descriptions [27]. It is also highlighted as an application domain benefiting from reasoning as action tokens [32].
*   **CoVLA** provides a comprehensive dataset featuring over 80 hours of real driving videos, synchronized sensor streams (LiDAR, odometry), detailed natural language annotations, and high-resolution driving trajectories [25,28]. It leverages CLIP for visual semantic alignment and LLaMA-2 for instruction embedding, coupled with a trajectory decoder for motion prediction, enabling AVs to comprehend verbal instructions (e.g., "yield to ambulance") and environmental conditions (e.g., "merge into traffic") [25,28].
*   **ORION** incorporates a QT-Former to retain long-term visual context, utilizes large language models for traffic scene reasoning, and integrates a generative trajectory planner for closed-loop AV [25,28]. It unifies the discrete reasoning space of VLMs with the continuous control space of AV, achieving high-precision visual question answering and trajectory planning, particularly in complex scenarios with ambiguous human instructions or occluded obstacles [25,28].
*   **AlphaDrive** is a VLM developed specifically for reasoning in autonomous driving contexts, trained with Supervised Fine-Tuning (SFT) and GRPO-based Reinforcement Learning (RL) exploration [27]. This model underscores the requirement for sophisticated deduction and common-sense understanding across diverse objects and scenarios [27].
*   Other models evaluated in this domain include Senna, UniPlan, HMVLM, DualVAD, GraphAD, VLP-VAD, ReasonNet, TransDiffuser [17], OccLLaMA, and EMMA [30]. UniVLA, while broadly applied in robotics, has demonstrated potential in autonomous navigation with an Oracle success rate of 47.1% in "Room2Room navigation," outperforming OpenVLA by 29.6% [21].
*   Beyond ground vehicles, **UAV-VLA** models extend VLA capabilities to aerial robots, fusing satellite imagery, natural language mission descriptions, and onboard sensors to autonomously execute high-level instructions (e.g., "deliver package to rooftop platform with blue tarp") in logistics, disaster response, and military reconnaissance [25,28].

The industry has recognized the potential of VLA models, with initiatives such as Pony.ai's "end-to-end 2.0" vision and Yuanrong Qixing's VLA model, which is noted as the first end-to-end model in China to achieve mass production and deployment for urban intelligent driving [8,17,19]. These practical deployments highlight the ability of VLA models to adapt driving strategies based on regional driving styles, accurately interpret special lane driving rules, and process long-tail scenarios, such as recognizing temporary traffic signs, thereby significantly improving traffic scene understanding [19]. Furthermore, platforms like the Cosmos World Foundation Model are being developed to assist in building customized world models for autonomous vehicles by predicting and generating physically aware videos of future virtual environment states, accelerating physical AI development for AVs [16]. The continuous advancements across these models and platforms underscore the transformative role VLA models are poised to play in the future of autonomous driving, addressing its inherent complexities and safety demands.
### 6.3 General Embodied AI Tasks

**VLA Models in General Embodied AI Tasks: Capabilities and Examples**

| Capability / Focus                   | Description                                                                                             | Key Examples / Models                                                                                                                                                                                                                                | Domains / Environments                                                                   |
|--------------------------------------|---------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|
| **Multimodal Instruction Interpretation** | Agents understand, reason about, and act based on complex visual and linguistic cues.                     | InstructVLA (zero-shot, language-conditioned tasks, SimplerEnv-Instruct), RT-2 (comprehends complex natural language, generalizes to novel objects).                                                                                                  | Diverse physical & digital environments, HRI                                             |
| **Long-Horizon Planning & Reasoning** | Capability to perform multi-step planning and maintain context for extended tasks.                        | ThinkAct ("embodied reasoning plans," self-correction), MemoryVLA (dual-memory for non-Markovian tasks), DreamVLA ("world knowledge forecasting," perception-prediction-action loop).                                                                | Complex, interactive, dynamic scenarios                                                  |
| **Adaptive Behavior in Dynamic Environments** | Dynamically adjust behavior based on real-time feedback and environmental changes.                      | UniVLA ("task-centric latent actions," cross-embodiment/cross-view generalization for manipulation, Room2Room nav), OpenVLA (robust, generalizable policies).                                                                                     | Open-world physical environments (homes, hospitals, orchards)                            |
| **Generalist Agents & Skill Transfer** | Developing agents capable of wide range of tasks across various embodiments.                             | Gato (multi-modal, multi-task, multi-embodiment generalist), $\pi_{0.5}$ (long-horizon, open-world tasks, cleaning unseen kitchens). DexVLA (diverse tasks across varied environments).                                                            | From Atari games to robotic manipulation                                                 |
| **Automated 3D Content Generation**  | Applying VLA paradigms to create 3D environments, layouts, and assets from text prompts.                  | 3D-Generalist (treats 3D env building as sequential decision-making, VLMs as policies), Cosmos World Foundation Model Platform (customized world models), EnerVerse (generates "embodied future space").                                                | Gaming, AR/VR, synthetic data generation for other embodied AI applications              |
| **Human-Robot Interaction (HRI)**    | Natural communication as primary interface for intuitive and steerable interactions.                      | VLAS, Helix (natural language communication).                                                                                                                                                                                                        | Human-centric environments                                                               |



**VLA Models in General Embodied AI Tasks: Capabilities and Examples**

| Capability / Focus                   | Description                                                                                             | Key Examples / Models                                                                                                                                                                                                                                | Domains / Environments                                                                   |
|--------------------------------------|---------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|
| **Multimodal Instruction Interpretation** | Agents understand, reason about, and act based on complex visual and linguistic cues.                     | InstructVLA (zero-shot, language-conditioned tasks, SimplerEnv-Instruct), RT-2 (comprehends complex natural language, generalizes to novel objects).                                                                                                  | Diverse physical & digital environments, HRI                                             |
| **Long-Horizon Planning & Reasoning** | Capability to perform multi-step planning and maintain context for extended tasks.                        | ThinkAct ("embodied reasoning plans," self-correction), MemoryVLA (dual-memory for non-Markovian tasks), DreamVLA ("world knowledge forecasting," perception-prediction-action loop).                                                                | Complex, interactive, dynamic scenarios                                                  |
| **Adaptive Behavior in Dynamic Environments** | Dynamically adjust behavior based on real-time feedback and environmental changes.                      | UniVLA ("task-centric latent actions," cross-embodiment/cross-view generalization for manipulation, Room2Room nav), OpenVLA (robust, generalizable policies).                                                                                     | Open-world physical environments (homes, hospitals, orchards)                            |
| **Generalist Agents & Skill Transfer** | Developing agents capable of wide range of tasks across various embodiments.                             | Gato (multi-modal, multi-task, multi-embodiment generalist), $\pi_{0.5}$ (long-horizon, open-world tasks, cleaning unseen kitchens). DexVLA (diverse tasks across varied environments).                                                            | From Atari games to robotic manipulation                                                 |
| **Automated 3D Content Generation**  | Applying VLA paradigms to create 3D environments, layouts, and assets from text prompts.                  | 3D-Generalist (treats 3D env building as sequential decision-making, VLMs as policies), Cosmos World Foundation Model Platform (customized world models), EnerVerse (generates "embodied future space").                                                | Gaming, AR/VR, synthetic data generation for other embodied AI applications              |
| **Human-Robot Interaction (HRI)**    | Natural communication as primary interface for intuitive and steerable interactions.                      | VLAS, Helix (natural language communication).                                                                                                                                                                                                        | Human-centric environments                                                               |

Vision-Language-Action (VLA) models represent a foundational paradigm shift towards enabling intelligent agents to operate autonomously and effectively within diverse physical and digital environments, moving beyond specialized functionalities to achieve generalist capabilities [30,35]. This broad application domain, commonly referred to as general embodied AI, encompasses a wide spectrum of physical embodiments, extending beyond traditional robotic arms to include smart appliances, autonomous vehicles, smart glasses, and even digital agents operating within computer interfaces [28,34]. The overarching objective is to develop comprehensive agents that can understand, reason about, and act adaptively in the physical world according to human instructions, ensuring versatility and generalizability in complex, dynamic scenarios [30,34]. VLA models facilitate this by providing a unified, robust, adaptive, and semantically aligned framework that empowers embodied agents to dynamically adjust behavior based on real-time feedback, crucial for dynamic, unstructured environments such as homes, hospitals, or orchards [28].

A core strength of VLA models in general embodied AI tasks lies in their ability to interpret complex multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. For instance, in Human-Robot Interaction (HRI), VLA systems like VLAS and Helix enable natural communication through language (text or speech) as the primary interface, allowing for intuitive and steerable interactions [5,30]. InstructVLA further exemplifies this by enabling robots to operate effectively in the real world via multimodal reasoning and action generation, demonstrating superior performance in zero-shot, language-conditioned tasks across diverse challenges such as verb extension, instruction restructuring, and multilingual expression on the SimplerEnv-Instruct benchmark [5,14]. Similarly, RT-2 showcases emergent capabilities in semantic understanding and basic reasoning, allowing robots to comprehend complex natural language instructions and generalize to novel objects and environments [20]. DexVLA expands on this by enabling robots to perform diverse tasks across varied environments using language commands, mastering intricate manipulation tasks across multiple embodiments [29]. The UAV-VLA system extends this capacity to unmanned aerial vehicles, allowing them to execute autonomous, language-conditioned tasks through natural language input, thereby simplifying human-machine interaction and fostering inter-robot collaboration [15].

Beyond instruction following, VLA models significantly contribute to long-horizon planning and reasoning. Models like ThinkAct, Agentic Robot, CoT-VLA, and LLM-Planner utilize VLA/LLM components for high-level task decomposition and planning, bridging abstract instructions with low-level actions [30]. ThinkAct, in particular, is evaluated on "embodied reasoning" and "complex embodied AI tasks," exhibiting strong capabilities in long-horizon planning and self-correction, which are invaluable for intricate broad embodied AI applications [12]. MemoryVLA addresses the challenge of non-Markovian tasks by integrating historical context, allowing robots to "remember" past events to guide future actions and operate more intelligently in dynamic, interactive environments and complex sequential tasks [31]. DreamVLA further enhances generalization and reasoning in robot manipulation by employing a "perception-prediction-action loop" that integrates comprehensive world knowledge, modeling abstract multimodal reasoning chains prior to execution [2].

The adaptability of VLA models in dynamic environments is crucial for achieving general intelligence. UniVLA contributes to this by learning "task-centric latent actions" that are cross-embodiment and cross-view, enabling enhanced generalization in tasks such as manipulation and Room2Room navigation [21]. OpenVLA focuses on providing robust, generalizable policies for visuomotor control to teach robots new skills, aligning directly with the broader objectives of embodied AI [11]. GraspVLA demonstrates zero-shot generalization through large-scale synthetic data, highlighting a pathway for training embodied agents efficiently [10]. The Procgen benchmark, designed to test reinforcement learning agents on diverse procedural tasks with an emphasis on generalization to new levels, aligns with the scope of general embodied AI, showcasing the importance of adaptability [4]. The pursuit of multi-dimensional generalization—cross-task, cross-domain, and cross-embodiment—is inherent in VLA models, as exemplified by Gato, a multi-modal, multi-task, and multi-embodiment generalist policy capable of performing a wide range of tasks from Atari games to robotic manipulation [26,27]. Similarly, $\pi_{0.5}$ demonstrates remarkable generalization in long-horizon, open-world tasks such as cleaning unseen kitchens [32].

Beyond physical interaction, VLA models are being applied to novel tasks such as the automated generation of 3D content. 3D-Generalist represents a significant step in this direction, treating "3D environment building as a sequential decision-making problem" where VLMs act as policies to generate layouts, materials, lighting, and assets from text prompts [33]. This innovative application is crucial for addressing the labor-intensive nature of traditional 3D graphics design and mitigating the scarcity of 3D-grounded training data, which is vital for scaling up data for various embodied AI tasks that demand rich, interactive 3D environments [33]. The "3D AIGC and world models" theme, encompassing topics like "Interactive 3D world generation," further underscores this trend [24]. Projects like Cosmos World Foundation Model Platform aim to enable developers to "build customized world models for physical AI systems" by simulating and generating virtual worlds with accurate physical interactions using multimodal inputs [16]. Similarly, EnerVerse focuses on generating "embodied future space" to guide robots in complex tasks, indicating broader applications in embodied planning and prediction [16]. 3D-VLA, as an embodied foundation model, also improves reasoning, multimodal generation, and planning capabilities within embodied environments, benefiting from 3D perception and generative world models [3].

While VLA models demonstrate remarkable progress, challenges remain in achieving true "omnipotent robot foundation models" and ubiquitous real-world deployment [9,29]. The issue of "catastrophic forgetting" in VLA models has historically limited generalization to diverse embodied scenarios, which InstructVLA explicitly targets to overcome [14]. Furthermore, ensuring high-performance VLA models operate efficiently enough for practical, real-world deployment is a critical area of ongoing research, as addressed by works like EfficientVLA [9]. Despite these challenges, the rapid development of VLA models continues to drive progress towards open-ended tasks and environments, 3D human-object-environment reconstruction and understanding, and the creation of robotic world models and neural simulators, marking a promising trajectory for general embodied AI [24].
### 6.4 Other Domain-Specific Applications

**Diverse Domain-Specific Applications of VLA Models**

| Domain / Application               | Key Contribution / Capabilities                                                                                                                                                                                                                                   | Key Examples / Models / Technologies                                                                                                                                                                  |
|------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Unmanned Aerial Vehicles (UAVs)** | Natural language-based mission generation, autonomous flight path planning, robot-to-robot collaboration.                                                                                                                                                   | UAV-VLA (translates high-level language into flight paths, integrates satellite imagery), CognitiveDrone (VLA model & evaluation benchmark for cognitive tasks).                                        |
| **GUI & General Computer Control** | Revolutionizing human-computer interaction by enabling VLA agents to navigate and interact with digital interfaces.                                                                                                                                         | ShowUI (end-to-end VLA for GUI/computer use), Cradle (General Computer Control - GCC), Microsoft Research (Automated GUI Agents for OS-Level Intelligence), JARVIS-VLA (Minecraft).                     |
| **Medical Robotics**               | Enhances precision, safety, and adaptability in medical procedures.                                                                                                                                                                                       | RoboNurse-VLA (surgical instrument delivery), TLA (integrates tactile signals for contact-intensive tasks like peg-in-hole), MemoryVLA (patient care, quality checks).                                  |
| **Industrial Robotics**            | Facilitates high-precision assembly, inspection, and collaborative manufacturing.                                                                                                                                                                         | CogACT (high-precision assembly, screw tightening), MemoryVLA (industrial automation, sequential tasks).                                                                                              |
| **Precision Agriculture**          | Monitors plant growth, detects diseases, identifies deficiencies, enables autonomous ground robots/drones for tasks like selective harvesting.                                                                                                           | VLA models for autonomous robots/drones (interprets visual inputs from RGB-D/multispectral sensors/UAVs, executes context-aware actions, e.g., 30% water reduction).                               |
| **Augmented Reality (AR) Navigation & Immersive Virtual Environments** | Enhances interactive AR navigation with context-aware, real-time guidance; enables dynamic environment generation.                                                                                                   | AR intelligent agents (process visual streams from AR devices, natural language queries, generate dynamic prompts), 3D-Generalist (generates 3D environments for gaming, AR, VR).                     |



**Diverse Domain-Specific Applications of VLA Models**

| Domain / Application               | Key Contribution / Capabilities                                                                                                                                                                                                                                   | Key Examples / Models / Technologies                                                                                                                                                                  |
|------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Unmanned Aerial Vehicles (UAVs)** | Natural language-based mission generation, autonomous flight path planning, robot-to-robot collaboration.                                                                                                                                                   | UAV-VLA (translates high-level language into flight paths, integrates satellite imagery), CognitiveDrone (VLA model & evaluation benchmark for cognitive tasks).                                        |
| **GUI & General Computer Control** | Revolutionizing human-computer interaction by enabling VLA agents to navigate and interact with digital interfaces.                                                                                                                                         | ShowUI (end-to-end VLA for GUI/computer use), Cradle (General Computer Control - GCC), Microsoft Research (Automated GUI Agents for OS-Level Intelligence), JARVIS-VLA (Minecraft).                     |
| **Medical Robotics**               | Enhances precision, safety, and adaptability in medical procedures.                                                                                                                                                                                       | RoboNurse-VLA (surgical instrument delivery), TLA (integrates tactile signals for contact-intensive tasks like peg-in-hole), MemoryVLA (patient care, quality checks).                                  |
| **Industrial Robotics**            | Facilitates high-precision assembly, inspection, and collaborative manufacturing.                                                                                                                                                                         | CogACT (high-precision assembly, screw tightening), MemoryVLA (industrial automation, sequential tasks).                                                                                              |
| **Precision Agriculture**          | Monitors plant growth, detects diseases, identifies deficiencies, enables autonomous ground robots/drones for tasks like selective harvesting.                                                                                                           | VLA models for autonomous robots/drones (interprets visual inputs from RGB-D/multispectral sensors/UAVs, executes context-aware actions, e.g., 30% water reduction).                               |
| **Augmented Reality (AR) Navigation & Immersive Virtual Environments** | Enhances interactive AR navigation with context-aware, real-time guidance; enables dynamic environment generation.                                                                                                   | AR intelligent agents (process visual streams from AR devices, natural language queries, generate dynamic prompts), 3D-Generalist (generates 3D environments for gaming, AR, VR).                     |

Beyond their foundational applications in general robotic manipulation, Vision-Language-Action (VLA) models are demonstrating profound versatility and broad applicability across numerous specialized domains. This expansion underscores VLA's capacity to streamline complex human-machine interaction through natural language and enable autonomous agents to perform context-aware actions based on visual inputs, thereby enhancing precision, efficiency, and adaptability across various sectors.

A prominent area of innovation lies in **Unmanned Aerial Vehicles (UAVs)**. The UAV-VLA system exemplifies this, enabling natural language-based mission generation for drones [15]. This system translates high-level natural language commands, such as "fly over all buildings," into actionable flight paths by integrating satellite imagery [15]. This capability significantly simplifies human-UAV interaction, moving beyond manual programming to direct linguistic input. Furthermore, it lays critical groundwork for more sophisticated robot-to-robot automated task generation and collaboration, anticipating future autonomous systems [15]. Another contribution in this space is CognitiveDrone, which serves as both a VLA model and an evaluation benchmark for real-time cognitive task solving and reasoning in UAV operations [30]. This highlights a trend towards integrating VLA for cognitive and reasoning tasks in aerial robotics.

The transferability of VLA concepts and techniques extends significantly to **non-physical embodied agents**, revolutionizing human-computer interaction and broad automation tasks [22]. This is particularly evident in **Graphical User Interface (GUI) and General Computer Control**. Models like ShowUI are developed as open-source, end-to-end VLA systems specifically for GUI agent and computer use, demonstrating direct application in digital environments [22]. Similarly, Cradle focuses on "General Computer Control (GCC)," aiming for comprehensive applicability across diverse computer tasks [22]. This trend is reinforced by research initiatives, such as those identified by Microsoft Research, which call for "Automated GUI Agents for OS-Level Intelligence." These agents are designed to navigate and interact with various desktop applications and operating system interfaces, necessitating multimodal data collection (GUI actions, visual context, application states, OS events) and Multimodal Large Language Model (MLLM) training for understanding and executing complex computer tasks [24]. This effectively positions the computer interface as an "embodied" environment for VLA agents, mirroring physical robotic interactions. The application of VLA in virtual environments, such as JARVIS-VLA in **Minecraft**, further underscores this transferability, fine-tuning pretrained VLM models for action within a simulated world [27].

In **Medical and Industrial Robotics**, VLA models address demands for precision, safety, and adaptability. For instance, in medical robotics, VLA models enhance minimally invasive surgery by fusing laparoscopic video streams, anatomical maps, and voice commands. RoboNurse-VLA, for example, assists with precise surgical instrument delivery, demonstrating robustness across diverse tools, complex lighting, and noisy operating environments [25,28]. The integration of tactile signals in models like TLA for contact-intensive tasks such as surgery and precision assembly has achieved high success rates, exceeding 85% for peg-in-hole assembly, indicating a critical advancement in delicate operations [35]. Specialized datasets such as Kaiwu, which incorporate diverse multimodal signals including electromyography (EMG) and haptic feedback, are crucial for training VLA models for such intricate and delicate operations, significantly enhancing their precision and reliability [35]. Furthermore, VLA-enabled patient-assistive robots can perform tasks like medication retrieval or guiding mobility aids [25]. In industrial settings, VLA models like CogACT facilitate high-precision assembly, inspection, and collaborative manufacturing by processing visual inputs, natural language instructions, and robot states to infer context and execute control commands [28]. MemoryVLA also projects future applications in industrial automation and medical assistance, leveraging enhanced memory capabilities for sequential task control, quality checks, and personalized patient care by remembering histories and plans [31].

**Precision Agriculture** also benefits significantly from VLA models. These systems are employed for tasks such as monitoring plant growth, detecting diseases, and identifying nutrient deficiencies through visual inputs from RGB-D cameras, multispectral sensors, or UAVs [28]. They empower autonomous ground robots and drones to interpret field scenes and execute context-aware actions, such as selective harvesting or adaptive irrigation, dynamically adjusting to occlusions, terrain changes, or specific crop types. This leads to tangible benefits, including potential reductions in water usage by up to 30% [25].

Lastly, VLA models are poised to transform **Augmented Reality (AR) Navigation and immersive virtual environments**. They enhance interactive AR navigation by providing intelligent, context-aware, real-time guidance. AR intelligent agents process continuous visual streams from AR devices and natural language queries (e.g., "show the quietest route to the meeting room"), generating dynamic navigation prompts overlaid on the user's field of view [25,28]. These systems can adapt routes based on real-time crowd flow and obstacles, offering dynamic path highlighting, landmark identification, and obstacle avoidance [25,28]. Furthermore, the capability to generate 3D environments, as discussed in relation to "3D Generalist," holds promise for gaming, augmented reality, and virtual reality applications, extending the operational scope of VLA beyond physical robotics into digital simulation and interaction spaces [33].

Across these diverse domains, the overarching benefit of VLA models is their ability to enable more intuitive, efficient, and intelligent autonomous systems. While challenges remain in data collection, robustness in highly variable environments, and ethical considerations, the consistent demonstration of VLA's capacity to translate complex instructions and visual information into actionable outcomes highlights its transformative potential across both physical and non-physical embodied agents.
## 7. Evaluation Methodologies and Benchmarks
The robust development and real-world deployment of Vision-Language-Action (VLA) models necessitate a sophisticated and multi-faceted approach to evaluation. This section provides a comprehensive overview of the methodologies and benchmarks employed to rigorously assess VLA model performance, encompassing the metrics used, the datasets and environments leveraged, the paradigms of evaluation, and the crucial aspects of generalization and emergent capabilities. The goal is to move beyond mere task completion to ensure models exhibit accuracy, safety, efficiency, and broad applicability across diverse, dynamic, and often novel scenarios [9,18,26].

Initially, evaluation focused on core performance metrics such as trajectory accuracy (e.g., L2 distance, Average Displacement Error) for navigation tasks and simple success rates for robotic manipulation [17,26]. However, the field has evolved to incorporate more comprehensive measures that address critical real-world factors, including safety (e.g., Collision Rate, Rater Feedback Score, Driving Score, Predictive Driver Model Score), computational efficiency (e.g., inference frequency, speed-up factors), and the model's ability to generalize to unseen conditions [9,10,17].

These performance metrics are intrinsically linked to the datasets and simulation environments utilized for both training and evaluation. A vast array of resources has emerged, ranging from large-scale open-loop datasets like nuScenes and Waymo Open Dataset for autonomous driving, to expansive real-robot datasets such as Open X-Embodiment (OXE) for general robotics [16,17]. Internet-scale vision-language datasets (e.g., LAION-5B) provide foundational pre-training, while high-fidelity simulators (e.g., CARLA, Isaac Gym, LIBERO, SimplerEnv) offer controlled and scalable testing grounds [17,25,28,35]. Despite these advancements, challenges persist regarding data quality, dataset bias, and the persistent "reality gap" between synthetic and real-world data, underscoring the need for continuous innovation in data generation and collection [8,25,26,28].

The mode of evaluation—whether open-loop or closed-loop—significantly impacts the insights gained. Open-loop evaluations, though cost-effective and scalable, often fall short in assessing a VLA model's adaptive capabilities in dynamic, interactive environments due to the lack of real-time feedback [17]. Conversely, closed-loop evaluations, which involve real-time feedback and allow models to influence their environment, are crucial for assessing dynamic behaviors, self-correction, and robust interaction, but they face high computational costs and the "domain-gap" between simulation and reality [17,18]. Consequently, hybrid evaluation paradigms, exemplified by platforms like NAVSIM, are gaining prominence, balancing the data richness of open-loop approaches with the dynamic assessment capabilities of closed-loop systems to foster more comprehensive and reliable VLA development [17].

Ultimately, the goal of these evaluation frameworks is to thoroughly probe the generalization and emergent capabilities of VLA models. This includes zero-shot generalization to unseen tasks and objects, out-of-distribution (OOD) performance in novel environments, and cross-embodiment transfer across different robotic platforms [10,18,21]. Advanced VLA models are demonstrating emergent behaviors such as enhanced semantic understanding, practical reasoning, self-correction, and long-horizon planning [12,29,34]. However, significant challenges remain, including abstraction limitations, semantic-physical mismatches, and the difficulty in generalizing to entirely new behaviors, highlighting an ongoing "generalization gap" that current research actively seeks to address [18,25,28].

In summary, this section delineates the multifaceted landscape of VLA evaluation, from the granular details of performance metrics to the broad scope of generalization. It underscores the continuous evolution towards more holistic, ecologically valid, and challenging benchmarks that can truly validate the potential of VLA models for complex real-world applications [17,26].
### 7.1 Performance Metrics

**Key Performance Metrics for Evaluating VLA Models**

| Category               | Metric(s)                                                  | Description                                                                     | Examples / Contexts                                                                                             |
|------------------------|------------------------------------------------------------|---------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| **Accuracy / Trajectory** | L2 Distance, Average Displacement Error (ADE), Path Deviation. | Quantify deviation between predicted and ground truth trajectories.             | Autonomous Driving (nuScenes, WOD-E2E), UAV Navigation (UAV-VLA).                                               |
| **Safety / Comfort**   | Collision Rate, Rater Feedback Score (RFS), Driving Score (DS), Predictive Driver Model Score (PDMS). | Assess risk, human-like judgment, adherence to rules, smoothness of actions.    | Autonomous Driving (DriveVLM, OpenDriveVLA, CARLA, NAVSIM), Robotic Manipulation (Groot N1, SafeVLA).         |
| **Task Success**       | Success Rate, Average Length (steps to complete).          | Proportion of successful task completions, efficiency in task execution.        | Robotic Manipulation (OpenVLA, UniVLA, InstructVLA, MemoryVLA, RT-1, TLA, ConRFT), Embodied AI (DreamVLA).      |
| **Computational Efficiency** | Inference Frequency (Hz), Speed-up Factor, Resource Reduction. | Measure real-time applicability, deployment cost, and resource consumption.     | UniVLA, Pi-0, GR00T N1, Pi-0 Fast, ECoT, EfficientVLA, UAV-VLA, FedVLA, OpenVLA (quantization).                    |
| **Generalization**     | Zero-Shot Generalization, Few-Shot Adaptability, Out-of-Distribution (OOD) Performance. | Model's ability to perform on unseen tasks/objects/environments without retraining. | RT-2, ReVLA, GraspVLA, InstructVLA (SimplerEnv-Instruct), MemoryVLA, UniVLA.                                    |
| **Specialized Metrics** | Semantic & Prompt Alignment (CLIP scores), Layout Coherence, Overall Preference. | Evaluate quality of generated content or alignment with instructions.           | 3D Environment Generation (3D-Generalist), Visual Grounding (Object HalBench, AMBER-Generative), Action-Aligned Visual Rewards (ThinkAct). |



**Key Performance Metrics for Evaluating VLA Models**

| Category               | Metric(s)                                                  | Description                                                                     | Examples / Contexts                                                                                             |
|------------------------|------------------------------------------------------------|---------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| **Accuracy / Trajectory** | L2 Distance, Average Displacement Error (ADE), Path Deviation. | Quantify deviation between predicted and ground truth trajectories.             | Autonomous Driving (nuScenes, WOD-E2E), UAV Navigation (UAV-VLA).                                               |
| **Safety / Comfort**   | Collision Rate, Rater Feedback Score (RFS), Driving Score (DS), Predictive Driver Model Score (PDMS). | Assess risk, human-like judgment, adherence to rules, smoothness of actions.    | Autonomous Driving (DriveVLM, OpenDriveVLA, CARLA, NAVSIM), Robotic Manipulation (Groot N1, SafeVLA).         |
| **Task Success**       | Success Rate, Average Length (steps to complete).          | Proportion of successful task completions, efficiency in task execution.        | Robotic Manipulation (OpenVLA, UniVLA, InstructVLA, MemoryVLA, RT-1, TLA, ConRFT), Embodied AI (DreamVLA).      |
| **Computational Efficiency** | Inference Frequency (Hz), Speed-up Factor, Resource Reduction. | Measure real-time applicability, deployment cost, and resource consumption.     | UniVLA, Pi-0, GR00T N1, Pi-0 Fast, ECoT, EfficientVLA, UAV-VLA, FedVLA, OpenVLA (quantization).                    |
| **Generalization**     | Zero-Shot Generalization, Few-Shot Adaptability, Out-of-Distribution (OOD) Performance. | Model's ability to perform on unseen tasks/objects/environments without retraining. | RT-2, ReVLA, GraspVLA, InstructVLA (SimplerEnv-Instruct), MemoryVLA, UniVLA.                                    |
| **Specialized Metrics** | Semantic & Prompt Alignment (CLIP scores), Layout Coherence, Overall Preference. | Evaluate quality of generated content or alignment with instructions.           | 3D Environment Generation (3D-Generalist), Visual Grounding (Object HalBench, AMBER-Generative), Action-Aligned Visual Rewards (ThinkAct). |

The evaluation of Vision-Language-Action (VLA) models necessitates a comprehensive suite of performance metrics that extend beyond simple task completion to encompass accuracy, safety, efficiency, and generalization. These metrics not only quantify model capabilities but also guide research towards practical real-world applicability [9,26].

A primary category of metrics, particularly crucial for autonomous driving and navigation tasks, focuses on the accuracy of predicted trajectories. The **L2 distance** measures the Euclidean deviation between predicted and ground truth trajectories, typically averaged over a specified duration, such as 3 seconds [17]. For instance, models like Senna and Waymo's EMMA achieved an L2 distance of 0.29m, with OpenDriveVLA reporting 0.33m [17]. Similarly, **Average Displacement Error (ADE)** serves as another average judgment standard for trajectory accuracy, notably utilized in the Waymo Open Dataset End to End (WOD-E2E) [17]. However, the interpretation of ADE requires careful consideration, as an MLP based solely on motion self-state can achieve the best ADE on `nuScenes`, indicating potential limitations of the dataset for evaluating complex planning capabilities [17]. For UAV navigation, trajectory length and error assessment are critical. The UAV-VLA system's trajectory length was 21.6% longer than human ground truth, although its K-Nearest Neighbors (KNN) method achieved a spatial accuracy of 34.22 meters, superior to sequential and Dynamic Time Warping (DTW) methods for error assessment [15].

Beyond pure trajectory accuracy, the evolution of VLA evaluation emphasizes metrics related to **safety and comfort**, particularly vital for systems interacting with dynamic environments or humans. The **Collision Rate** quantifies the probability of predicted trajectories intersecting with other objects, with DriveVLM and OpenDriveVLA achieving a low collision rate of 0.10 [17]. More holistic safety assessments include the **Rater Feedback Score (RFS)**, a primary metric for WOD-E2E that incorporates human-like judgment for long-tail driving scenarios, where UniPlan and HMVLM achieved scores around 7.7 [17]. In CARLA simulations, the **Driving Score (DS)** integrates multiple factors: route completion, violation score, safety, comfort (smoothness of actions), and efficiency [17]. Models such as ReasonNet (79.95) and AutoVLA (78.84) demonstrate competitive DS values [17]. Another comprehensive metric, **Predictive Driver Model Score (PDMS)** used in NAVSIM, aggregates weighted sub-scores for `EgoProgress` (EP), `Time to Collision` (TTC), and Comfort, with penalties for collisions or road boundary deviations [17]. These safety-focused metrics are further substantiated by models like SafeVLA, which reduced unsafe behaviors by over 80% using constrained Markov decision processes [25,28], and Groot N1, which reduced collision failure rates by 28% [28]. The Yuanrong Qixing VLA model for autonomous driving also implicitly targets safety, comfort, and adherence to regulations through its ability to adjust strategies and interpret rules [19].

A core set of metrics reflects a VLA model's ability to successfully complete tasks and make progress. **Success rates** are ubiquitously employed across robotic manipulation and various embodied tasks [2,26]. OpenVLA, for example, achieved an absolute task success rate 16.5% higher than RT-2-X [11,16] and an average success rate of 81.7% across 29 tasks, with a 42% reduction in error rate for multi-object assembly [19]. UniVLA demonstrated remarkable success, with an average success rate of 92.5% on the LIBERO benchmark (Bridge version), improving by 19% over OpenVLA, and 68.9% in real-world robotic arm experiments [21]. InstructVLA showed significant performance gains, including a 92% outperformance over fine-tuned OpenVLA on SimplerEnv-Instruct and a 27% improvement over Magma in closed-loop operations [5,14]. MemoryVLA also consistently outperformed baselines like CogACT and $\pi_0$ across simulated and real-world environments, achieving an 85% success rate in general tasks and 83% in long-horizon temporal tasks, indicating the importance of historical information [31]. Other models reporting high success rates include RT-1 (97%) [28], TLA (over 85% in peg-in-hole assembly) [35], and ConRFT (96.3% in high-contact tasks) [25,28]. Progress scores, such as `EgoProgress` (EP) in NAVSIM, measure the model's advancement along a route, normalized between 0-1, indicating goal-oriented progression [17]. DreamVLA uses `Average Length` on CALVIN ABC-D benchmarks, likely referring to the number of steps to complete a task, alongside success rates (76.7% on real robot tasks) [2].

**Computational efficiency** is another critical dimension, affecting real-time applicability and deployment cost. Metrics include speed-up factors, inference/control frequency, and resource reduction. UniVLA can output a 12-step sequence at 10 Hz [21]. Pi-0 achieves a control frequency of up to 50 Hz, a significant improvement over RT-2’s 5 Hz, while GR00T N1's low-level diffusion transformer operates at 120 Hz [27]. Pi-0 Fast further achieves 15x inference acceleration, supporting 200 Hz high-frequency control [28]. ECoT's asynchronous execution speeds up inference by approximately 40% [27]. EfficientVLA demonstrated a 1.93 times speed increase with only a 0.6% drop in overall task success rate, reducing computation by 71.1% [9]. The UAV-VLA system processed images 6.5 times faster than manual generation [15]. FedVLA evaluates the computational efficiency of its Dual Gating Mixture-of-Experts (DGMoE) [23], and OpenVLA's reduced deployment cost (1/8 of original) highlights efficiency [19]. DexVLA runs at 60 Hz on a single GPU [29].

**Generalization capabilities** are increasingly important for VLA models to operate in novel and diverse environments. This is often quantified through zero-shot generalization [4,10], few-shot adaptability [10], and out-of-distribution (OOD) performance. RT-2 demonstrated about a 2 times improvement over baselines when facing new objects, backgrounds, and environments [20]. ReVLA improved OOD grasp success rates by up to 77% [25,28].

Specialized metrics are also emerging for specific VLA applications. For 3D environment generation, **Semantic and Prompt Alignment** is measured using CLIP scores between rendered scenes and text prompts, where higher scores indicate better alignment [33]. Human studies evaluate "Prompt Alignment," "Layout Coherence," and "Overall Preference" [33]. Metrics for **Visual Grounding and Hallucination Reduction** are assessed using benchmarks like Object HalBench and AMBER-Generative [33]. ThinkAct also considers "action-aligned visual rewards based on goal completion and trajectory consistency" [12].

Critically assessing these evaluation methodologies reveals a trend towards more comprehensive and realistic metrics. Early benchmarks often focused on simple success rates for short-horizon tasks [26]. However, the field is moving towards richer metrics that provide deeper insights into practical challenges, such as subtask success, time efficiency, and robustness to disturbances [26]. While models like AutoVLA highlight adaptive reasoning through qualitative results [7], the importance of detailed quantitative metrics for generalization performance in frameworks like Procgen is acknowledged, even if not explicitly detailed in abstracts [4]. Similarly, GraspVLA emphasizes open-vocabulary generalization, a qualitative measure of breadth of capability, alongside implied success rates [10]. The necessity for metrics to truly represent real-world applicability is paramount [9], prompting the development of complex scoring systems like DS and PDMS that integrate multiple facets of performance, including safety and comfort, into a unified evaluation framework [17]. This evolution signifies a maturing field striving for more robust, safe, and efficient VLA systems.
### 7.2 Datasets and Simulation Environments

**Key Datasets and Simulation Environments for VLA Models**

| Category                  | Type             | Description                                                                                             | Key Examples / Models

The efficacy and generalization capabilities of Vision-Language-Action (VLA) models are fundamentally contingent upon the quality, quantity, and diversity of the datasets and simulation environments employed for their training and evaluation. The development of end-to-end VLA models necessitates "extremely high data quality and quantity" and "high-fidelity simulation environments" that comprehensively cover a wide array of scenarios [8]. This section synthesizes the current landscape of such resources, highlighting commonalities, contrasting differences, and critically appraising their strengths and limitations.

For autonomous driving applications, VLA models rely on a combination of open-loop datasets for realistic data exposure and closed-loop simulators for interactive evaluation. Among the commonly used open-loop datasets are **nuScenes** and **Waymo Open Dataset End to End (WOD-E2E)** [17]. While nuScenes is primarily designed for perception tasks, a significant portion (approximately 75%) of its scenes involve simple straight driving, which limits its utility for evaluating complex planning and action generation in VLA models [17]. In contrast, WOD-E2E offers a collection of 4021 20-second driving segments, specifically curated to focus on challenging long-tail scenarios, such as navigating around marathons or interacting with emergency vehicles, thereby providing a more rigorous testbed for advanced VLA capabilities [17]. Another notable open-loop dataset is CoVLA, which provides over 80 hours of real driving videos, synchronized sensor streams (LiDAR, odometry), detailed natural language annotations, and high-resolution driving trajectories, facilitating comprehensive understanding of driving contexts [25,28]. AutoVLA, for instance, has been evaluated using real-world datasets like nuScenes and Waymo, demonstrating their continued relevance for benchmarking [7].

Closed-loop simulation environments address the limitations of open-loop data by providing interactive platforms where VLA models can execute actions and receive feedback. **CARLA** is a prominent example, enabling VLA models to be evaluated based on a Driving Score (DS) that encompasses route completion, violation adherence, safety, comfort, and efficiency, allowing for the real-time execution of planned commands [17]. **nuPlan** extends this with evaluation metrics including open-loop, non-reactive closed-loop (CLS-NR), and reactive closed-loop (CLS-R) scores, assessing parameters such as average/final displacement and heading errors, collision time, progress, speed limit adherence, and comfort [17]. Bench2Drive, an extension of CARLA developed by Shanghai Jiao Tong University, further enriches the simulation landscape [17]. Beyond autonomous driving, synthetic data generation in simulators like UniSim, which produces photorealistic scenes with occlusions and dynamic lighting, enhances model robustness in complex environments and aids "sim2real fine-tuning with domain randomization and real-world calibration" [25,28]. The Cosmos World Foundation Model Platform offers "World Foundation Models (WFMs)" capable of predicting and generating physically aware videos of future states in virtual environments, directly addressing the "difficult acquisition of physical AI training data" through physics-based simulation and synthetic data generation [16]. Similarly, EnerVerse utilizes a "data engine pipeline" combining generative models with 4D Gaussian Splatting to iteratively improve data quality and diversity, effectively narrowing the sim-to-real gap [16].

To bridge the gap between the richness of open-loop data and the interactivity of closed-loop evaluations, **hybrid evaluation solutions** have emerged. **NAVSIM**, developed by a consortium including TU Wien, Shanghai AI Lab, NVIDIA, and Stanford, serves as a data-driven, non-reactive autonomous vehicle simulator and benchmarking platform [17]. It uniquely combines the data depth of open-loop datasets like nuScenes with the comprehensive metric analysis of closed-loop platforms such as nuPlan. Based on the OpenScenes dataset (a nuPlan subset) with 120 hours of driving data, NAVSIM includes raw images from 8 cameras and point clouds from 5 LiDAR sensors, calculating safety, comfort, and progress metrics using an LQR controller and kinematic bicycle model for trajectory simulation [17].

In the broader context of VLA models for general robotics and embodied AI, the current landscape of datasets is characterized by a monumental effort toward large-scale, diverse, and real-world data collection. The **Open X-Embodiment (OXE) dataset** stands out as the "largest open-source real robot dataset to date" [16]. It comprises "over 1 million real robot trajectories" amassed from "22 robot instances" across "60 existing robot datasets," covering an extensive range of "527 skills (160,266 tasks)" [16,30,35]. OXE's primary objective is to foster the learning of general robot policies and facilitate cross-platform skill transfer, making it a pivotal resource for training VLAs at scale [11,16]. Several prominent VLA models, including MemoryVLA, OpenVLA (trained on 970k real robot operation data), and Octo (trained on 800k demonstrations), leverage OXE or its related datasets, underscoring its foundational role [11,19,25,28,31]. UniVLA also incorporates `multimodal OpenX` data in its training [21], and RT-2-X specifically utilizes OXE to enhance cross-robot skill transfer [26].

Beyond OXE, numerous other large-scale real robot datasets contribute to VLA model development:
*   **DROID** features 76,000 demonstrations across 564 scenes, integrating internet-scale visual data with robot manipulation videos and human-annotated language instructions for complex scenarios [30,35].
*   **CALVIN** focuses on language-conditioned long-process manipulation, offering over 5,000 multi-step demonstrations with fine-grained language instructions and action trajectories [30,35]. DreamVLA, for instance, is evaluated using the CALVIN ABC-D benchmarks [2].
*   **RT-1** and **RT-2** leverage large robot datasets, with RT-1 collecting 130k robot demonstrations for various manipulation tasks, which RT-2 co-finetunes with internet-scale web data [20,25,28].
*   **InstructVLA** introduces a "curated 650K-sample VLA-IT dataset" combining manipulation trajectories with rich language annotations, scene captions, Q&A pairs, and instruction rewriting, allowing for instruction-tuning VLA models [5,14].
*   **SynGrasp-1B** is a billion-frame synthetic robotic grasping dataset generated in simulation with photorealistic rendering and extensive domain randomization, designed to facilitate sim-to-real transfer for grasping tasks [10].
*   Specialized datasets also include **UAV-VLPA-nano-30**, a benchmark for rapid evaluation of vision-language-action systems' global task-solving capabilities in UAV path planning using satellite images from diverse environments [15].

The foundational pre-training of VLA models often begins with internet-scale vision-language datasets such as COCO, LAION-400M, HowTo100M, WebVid, VQA, GQA, and LAION-5B. These vast collections of image-text pairs and instruction-following videos are crucial for pre-training vision and language encoders, aligning multimodal representations, and instilling general knowledge and semantic understanding in VLA models [25,26,28]. For example, the WIT dataset, with 400 million image-text pairs, is utilized for training CLIP [34]. Furthermore, **human interaction data** plays a significant role, with approaches like UMI using "in-the-wild human demonstrations" [16], ORION learning from "single human videos" [16], and HumanPlus capturing "human embodied data" to train models [16].

Simulation environments are indispensable for training and evaluating VLA models in robotics, offering controlled, scalable, and safe testing grounds. The **Procgen benchmark**, with its diverse procedural tasks and vast number of unique levels, is ideal for testing out-of-distribution generalization [4]. **SimplerEnv** serves as a standard robotic manipulation simulation benchmark with Bridge and Fractal subsets, used for quantitative evaluation by models like MemoryVLA, InstructVLA, and EfficientVLA [5,14,31]. InstructVLA further introduces the "SimplerEnv-Instruct" benchmark for evaluating zero-shot generalization [5,14]. Another critical benchmark is **LIBERO**, which comprises five distinct task suites for assessing various aspects of robot operation, used by MemoryVLA, UniVLA, and EnerVerse [16,21,31].

High-fidelity simulators such as **NVIDIA Isaac Gym / Orbit / OmniGibson** provide GPU-accelerated, physically realistic environments, supporting over 1,000 parallel simulations with features like soft body deformation and joint friction. These are vital for training large-scale action samples and boosting training efficiency, significantly reducing the cost compared to real-world data collection [30,35]. **Habitat Sim** is tailored for embodied AI, particularly navigation, offering flexible, high-performance 3D simulations in photorealistic indoor environments with scene customization capabilities [30,35]. Other significant simulation platforms include RoboSuite for robot manipulation [35], AI2-THOR for human-centric indoor environments [35], and iGibson, which builds virtual environments from real building scan data to support physical interaction and reduce the "simulation-to-real domain gap" [35]. Furthermore, popular physics engines and simulators like MuJoCo, PyBullet, SAPIEN, Gazebo, and Webots continue to be widely used [30]. The necessity for "Robotic world models and neural simulators" and "Synthetic data generation for RFM training" further underscores the ongoing research and development in this domain [24]. For example, 3D-Generalist generates its own large-scale synthetic data by rendering millions of images from generated scenes using Omniverse, creating "Simulation-Ready 3D environments" [33].

Despite these advancements, significant challenges persist. **Dataset bias** is identified as a critical issue, where network-crawled datasets can contain inherent biases (e.g., 17% stereotype association), leading to semantic inconsistencies or inappropriate responses in diverse environments [28]. This highlights the imperative for balanced and comprehensive datasets [28]. Moreover, the "reality gap" remains a concern for simulated datasets, as synthetic scenes often lack the visual complexity and physical nuances of real-world environments [26]. While some models like VQ-VLA demonstrate promising linear performance gains with more simulated data and a reduced sim-to-real gap, ongoing research is dedicated to improving sim-to-real transfer through techniques like domain randomization and real-world calibration [25,26,28]. The future direction for VLA datasets involves combining large-scale real-world data acquisition with comprehensive task suites that capture practical challenges, moving beyond existing benchmarks that frequently focus on short-horizon pick-and-place tasks [26].
### 7.3 Open-Loop vs. Closed-Loop Evaluation Paradigms
The robust evaluation of Vision-Language-Action (VLA) models necessitates a clear distinction between open-loop and closed-loop paradigms, each presenting unique advantages and limitations in assessing model performance, particularly in dynamic, interactive environments. A systematic comparison reveals that while open-loop methods offer foundational insights, closed-loop and hybrid approaches are increasingly crucial for evaluating the true capabilities of VLA models in real-world scenarios [17].

**Open-loop evaluation** typically involves assessing a model's predictions against a pre-recorded dataset without allowing its actions to influence subsequent observations. In this paradigm, the VLA model generates a complete plan or sequence of actions, which is then evaluated for quality based on predefined metrics, detached from real-time execution feedback. For instance, in some autonomous navigation tasks, evaluation might involve comparing system-generated flight paths against human-expert paths based on metrics such as path deviation or efficiency [15]. Similarly, the evaluation of generated data from 3D generalist models for downstream tasks like ImageNet classification also often operates in an open-loop fashion [33]. A primary strength of open-loop evaluation lies in its cost-effectiveness and scalability, enabling extensive testing on large, pre-collected datasets like nuScenes [17]. These datasets are often utilized for training policies through imitation learning [30]. However, a significant limitation is its inability to provide real-time feedback on executed commands, which severely restricts the assessment of a model's true performance in dynamic settings [17]. Metrics like Average Displacement Error (ADE) and Final Displacement Error (FDE), commonly used in open-loop evaluations, are often deemed insufficient for evaluating critical aspects such as safety, comfort, and comprehensive task completion in interactive environments. A major "defect" of this paradigm is its failure to account for how a model's actions influence future environmental states, thereby providing an incomplete picture of its adaptive capabilities [17].

In contrast, **closed-loop evaluation** inherently involves real-time feedback and dynamic interaction, where the VLA model's actions directly influence the environment, and subsequent observations feed back into the model for continuous decision-making [2,12,19,25,31]. This paradigm allows for a comprehensive assessment of VLA models designed for "adaptive control, using real-time feedback from sensors to adjust behavior on the fly" [25]. Examples include Yuanrong Qixing's VLA model for autonomous driving [19], RT-2 for direct robotic manipulation [20], MemoryVLA tested on Franka and WidowX robots and in simulators like SimplerEnv and LIBERO [31], and DreamVLA's "perception-prediction-action loop" in both real and simulated environments, measuring success rates and average task lengths [2]. Models like InstructVLA explicitly leverage benchmarks such as SimplerEnv-Instruct, which demand "closed-loop control" for task evaluation, demonstrating significant improvements, such as a 27% gain over Magma in closed-loop operations [5,14]. Furthermore, UniVLA's evaluations for Room2Room navigation and real-world robotic arm experiments are inherently closed-loop, handling dynamic environmental changes and requiring real-time feedback [21]. The ability to exhibit "self-correction behaviors" and "dynamically adapt to perturbations, object shifts, or occlusions" is a hallmark of models assessed through closed-loop evaluations, as seen in SC-VLA for failure recoverability and ORION for autonomous driving [25]. Key strengths of closed-loop evaluation include its capacity to comprehensively evaluate dynamic driving behaviors, safety, comfort, and adherence to rules in interactive environments, and its necessity for assessing "precise motor control" in "unstructured, novel environments" [6,17]. Despite its critical importance, closed-loop evaluation faces challenges, including the "domain-gap" between synthetic simulation data and real-world data, and high computational costs that limit its scalability for extensive testing [17,35]. This "Sim2Real gap" represents a significant hurdle in transferring policies learned in simulation to real-world deployment [18,35].

Given the limitations of both paradigms, a compelling trend towards **hybrid evaluation platforms** has emerged to bridge the gap. The NAVSIM platform exemplifies this by offering the data richness of open-loop datasets alongside detailed closed-loop metrics. By operating as "non-reactive" (only considering initial trajectory output per scene) and employing a data-driven approach, NAVSIM mitigates the domain-gap and computational intensity associated with fully reactive closed-loop systems, providing a more balanced and scalable solution [17]. Furthermore, models like AutoVLA are comprehensively evaluated in both open-loop and closed-loop settings to ascertain their adaptability and robustness [7]. GraspVLA also employs evaluation across "real-world and simulation benchmarks" to assess zero-shot generalization and proactively mitigate "sim-to-real gaps" [10]. This reflects an understanding that while pre-training in virtual simulations (often more open-loop) can reduce training costs, fine-tuning in real environments (inherently closed-loop) is indispensable for robust performance [18].

In conclusion, while open-loop evaluations remain useful for initial assessments and large-scale data processing, they are insufficient for truly evaluating VLA models destined for interactive, real-world tasks. The literature consistently highlights the critical necessity of closed-loop evaluations to assess dynamic adaptation, self-correction, and generalizability in complex environments. The ongoing development of hybrid evaluation platforms and comprehensive evaluation strategies that combine the strengths of both paradigms is a vital step towards fostering robust and reliable VLA development, addressing the inherent trade-offs between realism, cost, and scalability [17]. This evolution underscores a broader trend in the VLA research community towards more rigorous and ecologically valid evaluation methodologies.
### 7.4 Generalization and Emergent Capabilities
Generalization, defined as the capacity to infer from a specific instance and apply that understanding to a broader set of circumstances, represents a cornerstone for the successful real-world deployment of Vision-Language-Action (VLA) models in embodied intelligent systems [4,18]. Unlike conventional robotic policies confined to narrowly defined tasks and controlled environments, VLA models aspire to achieve superior versatility and dexterity across complex, unstructured, and novel settings [6,34]. This pursuit necessitates robust generalization across various dimensions, including zero-shot, out-of-distribution (OOD), cross-task, and cross-robot scenarios, while simultaneously fostering emergent cognitive abilities. The systematic evaluation of these generalization capabilities, especially in OOD environments, is identified as a critical yet underexplored area for VLA models [4].

Significant advancements have been made in enhancing the generalization capabilities of VLA models. **Zero-shot generalization** stands out as a key strength, allowing models to adapt to unseen tasks without extensive retraining [6]. For instance, UniVLA demonstrates substantial zero-shot generalization by executing tasks "cross-embodiment, cross-view, and without action annotation," showcasing semantic consistency of learned latent actions across different physical forms, such as a robotic arm and a human hand [21]. Similarly, GraspVLA achieves "advanced zero-shot generalizability" and "open-vocabulary generalization in grasping" through training on large-scale synthetic and Internet semantics data [10]. RT-2 is particularly noted for its significant zero-shot generalization to new objects and tasks, a feat largely unattainable with traditional control systems [20,25,28,30]. OpenVLA also exhibits strong generalization results in multi-task environments, often with fewer parameters than other models [11,28]. InstructVLA, designed for robust zero-shot performance, significantly outperforms baselines like fine-tuned OpenVLA and GPT-4o assisted action experts on instruction diversity and contextual reasoning benchmarks, indicating its strong capacity for OOD performance and skill transfer [5,14]. Other models like $\pi_0$, DexVLA, and various goal-state methods also report strong zero-shot capabilities [6,16,27,32].

Generalization to **Out-of-Distribution (OOD) environments** and **cross-task scenarios** has also seen considerable progress. UniVLA maintains an average success rate of 68.9% in real-world robotic manipulation despite "sudden lighting changes, prop interference, and new objects," demonstrating resilience to unseen visual variations [21]. RT-2 shows "about 2 times improvement" in generalizing to new objects, backgrounds, and environments [20] and can generalize to novel instructions and perform basic reasoning, such as selecting the smallest or closest object [6,16]. MemoryVLA, through robust memory integration, shows strong OOD generalization by maintaining performance across variations in background, lighting, distractors, occlusion, and object properties in real-world settings [31]. Models like $\pi_{0.5}$ have demonstrated out-of-the-box deployment in new home environments, executing multi-stage tasks with over 90% OOD success rates [6]. In autonomous driving, models like the Yuanrong Qixing VLA demonstrate "regional adaptability," "style transfer," and "long-tail scenario processing," showcasing emergent semantic understanding for handling diverse and unseen conditions [19]. The UAV-VLA system extends this by generating paths and actions from natural language requests in a "global scope" and across "diverse environments" [15].

**Cross-embodiment generalization** represents a complex challenge, with VLA models increasingly demonstrating the ability to transfer skills across different robot platforms. UniVLA's "latent actions" are semantically consistent across varying physical forms [21]. RT-2 successfully unlocked cross-robot zero-shot capabilities by fine-tuning on both internet vision-language and robot data [35]. Octo, for instance, exhibits outstanding generalization by supporting 22 distinct robot platforms through training on over 4 million trajectories [35]. Open X-Embodiment aims to enhance generalization and cross-platform learning, with RT-X models showing "positive transfer" of experience across robots [16]. DexVLA achieves adaptability to challenging tasks across multiple embodiments with limited demonstrations, even on novel embodiments not present in its pre-training data [29]. Hierarchical VLA architectures, by decoupling high-level planning from low-level control, enable leveraging diverse data sources, including simulations and hand-drawn sketches, to generalize across varying robot morphologies and dynamics [6]. The use of "domain randomization" in simulation environments further aids VLA models in learning "invariant features," thereby enhancing generalization to unseen objects and environments [35].

Beyond mere task execution, VLA models also exhibit significant **emergent capabilities**.
*   **Semantic Understanding and Reasoning**: VLAs are designed to foster emergent semantic understanding and practical reasoning in embodied contexts, interpreting language instructions and making context-aware decisions [8,34]. RT-2 shows dramatically stronger semantic understanding and symbolic reasoning, even interpreting math problems despite lacking explicit mathematical training in the case of ChatVLA-2 [6,20]. The 3D-Generalist attributes its high fidelity and effective scene transfer to the "commonsense and visual spatial reasoning of VLMs" [33].
*   **Self-Correction and Adaptability**: Self-improvement fine-tuning, as seen in 3D-Generalist, enables models to self-correct and iteratively refine outputs, such as 3D environments to be more prompt-aligned, while also reducing visual hallucinations [33]. ThinkAct develops "self-correction behaviors" through a reinforced planning mechanism, allowing recovery from errors and adaptation to unforeseen circumstances [12]. MemoryVLA's robustness to diverse visual shifts also highlights its adaptive capabilities [31]. Self-correcting frameworks like SC-VLA employ chain-of-thought reasoning to diagnose failures and generate corrective strategies, indicating an emergent capacity for autonomous recovery [25,28].
*   **Long-Horizon Planning**: Models like DexVLA and ThinkAct demonstrate the capacity for completing complex, long-horizon tasks, with DexVLA leveraging its VLM backbone as an "implicit high-level policy model" to guide action generation through learned sub-step reasoning [6,12,29]. MemoryVLA also shows improved performance in long-horizon tasks [31].

Despite these notable advancements, VLA models still encounter significant **struggles and limitations in achieving robust generalization**, contributing to a discernible generalization gap [18,28]. A primary limitation is the struggle with abstraction, adaptation to dynamic environmental changes, and seamless coordination of modular components, all critical for strong generalization [18]. Specific examples include a robot failing to adapt from screwing a red plastic bottle cap to a blue metal cap or getting "confused" by reflective marble tabletops after training on wooden surfaces [18]. This indicates a broader issue with applying learned knowledge to instances that deviate significantly from the training distribution.

A critical conflict or inconsistency in reported generalization capabilities arises with models like RT-2. While RT-2 demonstrates remarkable progress in generalizing to *new objects and environments* for learned behaviors [20], it simultaneously exhibits limitations in generalizing to entirely *new behaviors* [20]. This suggests that while VLA models can robustly apply known skills to novel contexts, creating or adapting to fundamentally new action sequences remains a challenge. OpenVLA, despite its strong performance, is noted for ignoring approximately 23% of object references in new scenarios, severely impacting accurate instruction understanding and execution [28]. ObjectVLA, for example, could generalize to only 64% of novel objects [25].

Systematically, several factors contribute to this generalization gap. **Semantic-physical mismatches** occur when a model struggles to correctly ground linguistic instructions or visual cues to their physical manifestations in novel settings, as exemplified by OpenVLA's object reference failures [28] (#SemanticPhysicalMismatch). **Data dependency** remains a factor; while models leverage large-scale datasets, the inherent biases or limited diversity within these datasets can hinder generalization to truly unforeseen conditions. Insufficient **abstraction** capabilities prevent models from extracting invariant features that apply broadly across variations, leading to brittle performance when conditions change slightly. Furthermore, inadequate **adaptability** to dynamic environmental changes and insufficient **coordination capabilities** among different modules within a VLA architecture can impede robust generalization, especially in complex, multi-stage tasks [18]. Efforts like ReVLA's model fusion improve OOD grasp success but often introduce computational complexity [28], highlighting the trade-offs involved in mitigating these limitations. The observed redundancy in some VLA models, which EfficientVLA addresses by maintaining or improving performance with significant computational reduction, also suggests that inefficient or distracting computations could contribute to generalization difficulties [9]. Ongoing research, exemplified by calls for "Weak-to-Strong Generalization" and "Self-Improving Architectures," underscores the continued need for addressing these challenges to realize truly generalized and emergent AI systems [24].
## 8. Critical Analysis: Limitations, Challenges, and Debates
Despite rapid advancements, Vision-Language-Action (VLA) models confront a multifaceted array of limitations, challenges, and ongoing debates that impede their robustness, generalizability, and reliable deployment in real-world environments. These critical issues span fundamental conceptual mismatches, inherent restrictions in data and model architectures, and significant hurdles in bridging the gap between simulated and physical realities.

A primary set of challenges stems from the inherent **semantic-physical mismatches and pervasive generalization gaps**. VLA models grapple with aligning the abstract, rich semantics of natural language and visual perception with the concrete, continuous, and physically constrained actions required for robotic manipulation [35]. This discord manifests as a "VLM-Robot Alignment" problem, where superior semantic understanding from web data does not inherently translate into the capacity for genuinely novel physical behaviors [20,27]. A significant modality gap arises from reliance on static 2D visual inputs, severely limiting models' ability to reason about complex 3D geometry, affordances, and dynamic environments, which is crucial for nuanced spatial understanding and functional task execution [26,30,33]. Furthermore, the inherent ambiguity of natural language makes specifying fine-grained control challenging, often leading to "semantic inconsistencies or contextually inappropriate responses" and "physical infeasibility," where generated actions are semantically plausible but unexecutable or unsafe [7,17,28]. This points to a fundamental lack of "physical common sense" in underlying Large Language Models (LLMs) [35]. The **generalization gap** further limits models from robustly applying learned behaviors to diverse, unseen situations, encompassing the pervasive "sim-to-real problem"—where performance degrades significantly from simulation to physical systems [18,26,35]—and struggles with novel objects, environments, tasks, and cross-embodiment transfer [4,21,26,27,28,30]. These issues are often rooted in data scarcity, dataset bias, and "catastrophic forgetting" during fine-tuning [5,10,14]. Moreover, VLA models face considerable difficulties in handling very long-horizon tasks due to sparse rewards and insufficient visual cues, and contact-rich scenarios that demand fine motor skills and precise force control, exacerbated by inadequate 3D understanding and vulnerability to visual perturbations [28,30,31,32,34]. Ultimately, these generalization limitations are encapsulated by "three insufficiencies": lack of abstraction, adaptation, and coordination abilities across vision, language, and action modules [18].

The limitations extend profoundly into **data, training, and architectural design**. Data scarcity is a universally acknowledged barrier, with robot datasets orders of magnitude smaller than those used for pre-training Vision-Language Models, severely limiting learning and generalization capabilities [20,27,28,30,34]. The collection of high-quality, diverse robot demonstration data is prohibitively expensive and labor-intensive, leading to a "scale-quality-diversity contradiction" [18,35]. Compounding this, datasets often suffer from noise, inherent biases, and a lack of crucial modalities like tactile feedback, along with temporal alignment issues due to differing sampling frequencies between modalities [28,31,35]. Architecturally, VLA models exhibit an **imbalance** because their foundation on Large Vision-Language Models was not inherently designed for embodied action, creating an "integration gap" between perception, language understanding, and physical execution [20,28,30,34]. The challenge of cross-modal semantic alignment, integrating high-dimensional visual features with low-dimensional action spaces, often results in a loss of semantic consistency [28]. A significant debate revolves around **end-to-end (monolithic) versus hierarchical/modular architectures**; while end-to-end models aim for unification, they frequently struggle with explicit reasoning, multi-step planning, and exhibit a lack of interpretability, potentially producing physically infeasible actions [12,26,27,28,30,35]. Finally, **training inefficiencies and computational demands** present substantial hurdles, with large VLA models requiring extensive resources and exhibiting slow inference speeds (e.g., 1-3Hz for 55B variants), insufficient for the rapid control needed in dynamic real-world environments (100+Hz) [20,26,27,28,30]. This leads to a critical trade-off between computational efficiency and performance for real-time deployment [24,35], compounded by redundant computation and the problem of "catastrophic forgetting" where fine-tuning degrades pre-trained VLM capabilities [5,14,27].

These fundamental limitations culminate in profound **real-world deployment and sim-to-real challenges**. The **Sim-to-Real Gap** remains a predominant obstacle, characterized by significant performance degradation when models transition from simulated to physical systems [18,30,35]. This gap is driven by trade-offs between physics engine accuracy and efficiency, the pervasive lack of realism in simulated scenes and objects (e.g., visual perception accuracy dropping from 95% to 70% in real-world tests), and the difficulty of automatically associating language with actions in simulations, which limits data scaling [10,26,33,35]. Despite advancements in generative models for improved realism [16], achieving robust generalization to unseen environments, novel objects, and task variations in real-world scenarios remains a persistent problem, as models tend to overfit to narrow training distributions [4,18,28,30]. The **computational demands and hardware limitations** of advanced VLA models result in inference speeds far below what is required for real-time, responsive control in dynamic physical environments, leading to "computational inefficiency" and potential "physical infeasibility" [9,17,20,26,28,32]. Furthermore, achieving **robustness and reliability** in the face of environmental variations, noisy inputs, and the demands of very long-horizon or contact-rich tasks remains challenging [26,28,30]. Crucially, **safety and ethical considerations** are paramount, particularly for critical applications like autonomous driving. The "black box" nature of VLA models hinders interpretability and "safety verification," while "execution brittleness" and potential latency in safety mechanisms pose "substantial safety risks" [8,28,32]. These issues underscore a core contradiction: while VLA models combine abstract intelligence with physical actions, the supporting capabilities in data, hardware, and theory are not yet fully compatible to realize truly general and robust embodied AI [35].
### 8.1 Semantic-Physical Mismatches and Generalization Gaps
Current Vision-Language-Action (VLA) models, despite their advancements, continue to grapple with fundamental challenges rooted in semantic-physical mismatches and pervasive generalization gaps. These issues collectively hinder their ability to reliably translate high-level semantic instructions into precise, robust, and adaptive physical actions within diverse and unstructured real-world environments [26,35].

A core limitation arises from the **semantic-physical mismatch**, defined by the inherent difficulty in aligning the abstract, rich semantics of natural language and visual perception with the concrete, often continuous, and physically constrained actions required for robotic manipulation [35]. This discord manifests in several ways. Firstly, models trained on vast web data, while gaining superior semantic understanding, often fail to translate this knowledge into the capacity to infer or execute genuinely novel physical behaviors . As exemplified by RT-2, web data enhances visual-semantic information but does not inherently teach new physical actions not present in robot datasets [20]. This indicates a fundamental "VLM-Robot Alignment" problem, where VLM pretraining does not inherently yield representations perfectly suited for robotic tasks [27].

A significant contributor to semantic-physical mismatches is the **modality gap**, particularly concerning 3D spatial understanding. Existing VLA models predominantly rely on static 2D visual inputs, which severely limits their ability to reason about complex 3D geometry, affordances, object movement, and dynamic environments [26,30]. This deficiency means that while models might recognize objects, they struggle with nuanced spatial reasoning, often leading to suboptimal layouts or an inability to consider the functional and contextual use of objects in a space, thereby failing to generate functional layouts [33]. The reliance on 2D inputs also limits the comprehension of implicit 3D information, which is crucial for complex dexterous manipulation and navigation in dynamic scenes [3,17].

Furthermore, the inherent ambiguity and insufficient expressiveness of natural language pose a considerable challenge, especially for specifying fine-grained control or contact-rich interactions [32]. This can lead to "semantic inconsistencies or contextually inappropriate responses" [28], such as a linguistic instruction like "gently push the balloon" resulting in the robot applying excessive force [18], or OpenVLA ignoring approximately 23% of object references in new scenarios [28]. Another issue arises from "physical infeasibility," where actions generated by VLA models might be semantically plausible but unexecutable or unsafe in the real world, exemplified by planned trajectories passing through obstacles or incorrect goal state generation due to insufficient dynamics modeling [7,17]. The underlying Large Language Models (LLMs) often lack fundamental "physical common sense" [35], making robust action planning difficult. For instance, the system's path planning in UAV control can be 21.6% longer than human-expert paths, indicating a semantic-physical mismatch in optimal execution [15].

The **generalization gap** is another paramount challenge, referring to the models' limited ability to learn from one instance and apply it robustly to diverse, unseen situations [18]. A central aspect of this gap is the **sim-to-real problem**, where models performing well in simulated environments experience significant performance degradation (e.g., a drop from 90% success in simulation to 60% in real-world scenarios) due to the lack of visual complexity and dynamic realism in synthetic scenes [26,35].

Generalization to **novel objects, environments, and tasks** remains a persistent bottleneck. Models trained on static datasets struggle with lifelong learning, handling unfamiliar objects, or adapting to novel interaction modes in open-world scenarios [26,30]. For example, a robot trained on a specific red plastic bottle cap might fail to recognize a blue metal cap, or a model accustomed to a wooden table might become "confused" by a reflective marble surface [18]. ObjectVLA, for instance, achieves only 64% generalization on novel objects [25,28], underscoring the limited open-world robustness. Furthermore, VLA models exhibit performance degradation (up to 40%) when encountering entirely new tasks or environmental changes not seen during training [28], largely due to overfitting to narrow training distributions and insufficient diverse task representations. The limited systematic evaluation of zero-shot generalization capabilities in out-of-distribution (OOD) environments further highlights this challenge [4].

**Cross-embodiment transfer** also presents a significant generalization hurdle. Raw actions often cannot generalize directly across different robot forms or observation perspectives, leading to "incompatible action symbols" and requiring extensive re-training for new platforms [21,27]. DexVLA, despite its design for cross-embodiment learning, did not observe significant transfer of learning from other embodiments to a specific target [29].

The root causes of these generalization limitations include **data scarcity and bias**. The reliance on costly and labor-intensive real-world data collection [10] combined with the limited diversity of existing robot datasets—which often feature templated, simple instructions—leads to "catastrophic forgetting" of pre-trained vision-language capabilities during fine-tuning for embodied tasks [5,14]. This compromises the rich semantic understanding initially gained from large VLMs.

MemoryVLA addresses the generalization limitation for non-Markovian tasks by incorporating a memory system, demonstrating substantial performance gains in long-horizon scenarios and improved robustness against Out-of-Distribution visual conditions like background, lighting, and object variations, indicating progress in perceptual generalization [31]. OpenVLA, an open-source model, aims to bridge some generalization gaps by providing efficient fine-tuning methods and demonstrating strong generalization across 29 tasks and multiple robot embodiments with significantly fewer parameters than RT-2-X [11]. However, the trade-off between specialization and generalization remains, as models fine-tuned for specific tasks may exhibit reduced cross-task generalization [29].

Compounding these issues, VLA models face considerable difficulties in handling **very long-horizon or contact-rich scenarios**. Long-horizon tasks suffer from the "sparse reward issue" common in reinforcement learning, making multi-step action sequences challenging to learn and execute [28,34]. MemoryVLA's success in non-Markovian tasks highlights the prior struggle with insufficient visual cues for inferring task state over time [31]. Contact-rich tasks, demanding fine motor skills, precise force control, and reasoning about evolving object states, are particularly challenging due to inadequate 3D understanding, static affordance encoding, and vulnerability to visual perturbations like occlusion [30,32]. The need for multimodal sensory intelligence, including tactile sensors, implicitly points to current gaps in handling such complex physical interactions [24].

In summary, the challenges confronting VLA models boil down to what one meta-survey identifies as "three insufficiencies": **lack of abstraction ability** to extract universal rules from specific cases, **lack of adaptation ability** to cope with environmental changes and contingencies, and **lack of coordination ability** among the vision, language, and action modules to operate seamlessly [18]. These insufficiencies manifest as reasoning limitations, where models struggle with explicit, multi-step planning and abstract common-sense reasoning [12,30]. The critical appraisal of VLA models reveals that despite advancements, the inherent complexity of mapping high-level language semantics to precise physical actions in diverse, real-world conditions persists as a significant and multi-faceted problem.
### 8.2 Data, Training, and Architectural Limitations
Vision-Language-Action (VLA) models, despite their transformative potential in robotics, confront pervasive limitations across data, training, and architectural design that impede their robustness, generalizability, and real-world deployability. A critical analysis reveals commonalities in these challenges while also highlighting unique contributions and debates within the literature.

**Data Scarcity, Quality, and Diversity**
The most frequently cited limitation is the pervasive **data scarcity** for robot control and embodied AI [20,23,27,28,30,34]. Unlike the vast internet-scale datasets available for pre-training Vision-Language Models (VLMs) (e.g., 1 billion image-text pairs), robot datasets are orders of magnitude smaller (e.g., RT-2's 130k trajectories) [20]. This fundamental disparity severely limits VLA models' ability to learn and generalize to novel physical behaviors and complex scenarios [14,18,20].

Collecting high-quality, diverse, and large-scale robot demonstration data is prohibitively expensive and labor-intensive, often requiring expert teleoperation and real robot interaction [10,26,27,29,30,35]. For instance, acquiring comprehensive datasets for a single complex setting like a kitchen, with thousands of tasks, can take months and incur significant costs (e.g., over 10,000 RMB/hour in industrial settings) [35]. This leads to a **scale-quality-diversity contradiction**: achieving scale often compromises quality, and obtaining diversity across tasks and environments is challenging [18,35].

Furthermore, dataset quality and inherent biases pose significant problems. Human demonstration data can contain noise (e.g., hand tremors), and sensor errors (e.g., depth map inaccuracies) can lead models to learn incorrect action patterns [35]. Network-crawled datasets used for VLM pre-training inherit biases, potentially leading to unfair or discriminatory behaviors in VLA models [28]. The majority of evaluations remain confined to simplified laboratory settings, focusing predominantly on gripper-based manipulation, which limits their applicability to general-purpose embodied agents in everyday environments [27]. Efforts to mitigate this include generating synthetic data, as proposed by GraspVLA and Cosmos, or developing innovative data collection methods like UMI [10,16]. The Open X-Embodiment initiative, by contributing over 1 million trajectories, directly tackles the data scarcity challenge by providing the "largest open-source real robot dataset to date" [16].

Another critical challenge is **modality imbalance and temporal alignment**. Most datasets prioritize vision and language, lacking crucial modalities like tactile feedback or force control signals, which are vital for precision in contact-intensive tasks [31,35]. Differences in sampling frequencies between modalities (e.g., vision at 30Hz, action at 100Hz) make accurate temporal alignment difficult, risking "instruction-action mismatch" and hindering the learning of causal relationships [35]. Finally, the **high cost of annotation**, particularly for fine-grained details like 6D object poses or semantic linking of language instructions to actions, further exacerbates data limitations, as automated tools often lack accuracy in complex scenarios [35].

**Architectural Imbalance and Complexity**
The architectural design of VLA models presents a distinct set of challenges, primarily stemming from their foundation on Large Vision-Language Models (VLMs) that were not originally designed for embodied action. This creates an **architectural imbalance** where the VLM's focus on perception and language understanding often disconnects from the nuances and physics of embodied action [20,28,30,34]. This "integration gap" means VLMs, while powerful, cannot inherently generate or execute actions [28].

A significant architectural hurdle is **cross-modal semantic alignment**. Integrating high-dimensional visual encoders (e.g., ViT) with low-dimensional action decoders often leads to inconsistent feature spaces, making effective fusion challenging and potentially resulting in a loss of semantic consistency between perception and action [28]. DreamVLA addresses a specific architectural limitation of "interference among the dynamic, spatial and semantic information during training" by proposing a block-wise structured attention mechanism to prevent information leakage [2].

A major debate revolves around **end-to-end (monolithic) versus hierarchical/modular architectures**. End-to-end models, while aiming for unification, notoriously struggle with explicit reasoning, multi-step planning, or adaptation to complex task variations [12,26,27,28,30,35]. They often map inputs directly to actions without interpretable reasoning, leading to an **interpretability lack** and potentially producing physically infeasible action outputs [27,28,30,35]. For instance, VLMs adapted for autonomous driving can suffer from "hallucinations" where outputs are semantically correct but physically incorrect or unsafe due to a lack of inherent 3D spatial and temporal understanding [17]. ThinkAct explicitly critiques these end-to-end systems and proposes a dual-system framework with reinforced planning to introduce explicit reasoning [12]. Conversely, hierarchical designs offer better interpretability and generalization but face challenges in inter-module coordination and semantic grounding of ambiguous feedback [28,34]. The aspiration for an end-to-end model that unifies action generation, planning, and decision-making for tasks like UAV control also implies current multi-module systems might have architectural inefficiencies [15].

**Training Inefficiencies and Computational Demands**
The training and deployment of VLA models are significantly hampered by **computational inefficiency** and high resource demands. Large VLA models, often relying on Transformer architectures with billions of parameters (e.g., RT-2-X with 55B parameters), require substantial GPU memory and computational resources [20,25,28]. This leads to slow inference speeds (e.g., RT-2 operating at 1-3Hz for its 55B variant) that are often insufficient for the rapid, responsive control needed in dynamic real-world environments, particularly for high-frequency control loops [20,26,27,30]. The computational cost of diffusion policies, while offering richer action distributions, can be three times higher than traditional Transformer decoders, further challenging real-time deployment [28].

The **trade-off between computational efficiency and performance** for real-time deployment is a central concern [24,35]. Deploying these models on edge devices (e.g., embedded CPUs with significantly less power than GPUs) often requires reducing accuracy in lightweight models or accepting real-time inference delays [28,35]. Current VLA models suffer from redundant computation across their vision, language, and action modules, leading to overly complex structures and inefficient or unnecessarily long reasoning processes, which is particularly problematic for real-time applications [7,9,26]. Solutions like OpenVLA's 7B-parameter architecture demonstrate improved efficiency over larger models like RT-2-X, achieving superior performance with significantly fewer parameters [11,19]. Similarly, RoboMamba aims for efficiency with 7x faster inference, and UniVLA boasts improved inference speeds for sequences [16,21]. AutoVLA and EfficientVLA aim to optimize reasoning length and model complexity by unifying reasoning and action generation or reducing redundant computation, respectively [7,9].

Finally, **catastrophic forgetting** during fine-tuning is a significant training limitation. Naive fine-tuning with action supervision can degrade the general semantic understanding derived from VLM pre-training [5,14,27]. This points to a need for more robust training strategies that enable joint optimization and knowledge retention, a challenge InstructVLA addresses through its unified framework and Mixture-of-Experts adaptation [5,14].
### 8.3 Real-World Deployment and Sim-to-Real Challenges
Deploying Vision-Language-Action (VLA) models in real-world environments presents a complex array of challenges, primarily stemming from the inherent discrepancies between simulated and physical realities, computational demands, and the stringent requirements for robust, safe, and generalizable performance [32,34,35]. While several VLA models demonstrate promising capabilities in real-world settings, such as DreamVLA's 76.7% success rate on real robot tasks [2] and FedVLA's validation through real-world robotic experiments [23], the underlying challenges of robust deployment are widely acknowledged across the literature.

A predominant obstacle is the **Sim-to-Real Gap**, which describes the significant performance degradation observed when models trained in simulation are transferred to physical systems [18,30,35]. This gap is multifaceted. Firstly, **physics engine accuracy versus efficiency** poses a critical trade-off. High-fidelity engines like MuJoCo offer detailed simulations, including soft body deformation and friction, but are computationally expensive, often requiring over 32GB of GPU memory for just 1000 parallel environments and 10ms per environment step [35]. Conversely, more efficient engines, while supporting massive parallelism (e.g., 10,000+ environments), simplify physical models, neglecting crucial details like surface texture's effect on friction [35]. This leads to VLA models learning "action patterns" in simulation that do not transfer effectively to real robots, resulting in performance drops from 90% simulated success to 60% in real-world tests, thereby negating the cost benefits of simulation and necessitating expensive real-world fine-tuning [35]. The call for optimizing physical law simulations in world models further underscores the need for accurately mimicking real-world dynamics for effective transfer [18].

Secondly, the **lack of realism in simulated scenes and objects** significantly contributes to the Sim-to-Real gap [26,35]. Virtual environments frequently oversimplify material textures, lighting variations, and object properties, causing visual perception models within VLAs to struggle with recognizing familiar objects in real scenes. This disparity can drastically reduce visual perception accuracy from over 95% in simulation to approximately 70% in real-world tests [35]. Efforts to mitigate this include generating high-quality synthetic 3D data with improved realism and prompt-alignment, as seen with 3D-Generalist [33], and employing photorealistic rendering and extensive domain randomization, such as in GraspVLA's SynGrasp-1B, which aims to explicitly "mitigate sim-to-real gaps" [10]. The development of platforms like Cosmos World Foundation Model Platform, which provides tools for physics-based simulation and synthetic data generation, is vital for enhancing data quality and diversity for real-world application, aiming to "accurately simulate spatial relationships and physical interactions" [16]. Similarly, EnerVerse explicitly works to "effectively narrow the sim-to-real gap" using generative models and 4D Gaussian Splatting [16].

A third critical aspect of the Sim-to-Real gap is the **difficulty of automatic language-action association in simulations** [35]. While simulations can automatically generate visual and action data, they typically fail to automatically associate these with natural language instructions. For instance, a robot performing "grasp apple" in a virtual environment requires manual input of corresponding instructions (e.g., "pick up the apple"), limiting the automatic generation of diverse language descriptions and preventing the full leverage of simulation's scale advantage for language data [35]. This scarcity of automatically annotated language data remains a challenge.

Beyond these fundamental simulation fidelity issues, a significant practical hurdle is the **difficulty of achieving strong generalization to unseen environments, novel objects, and variations in tasks** [18,30]. VLA models tend to overfit to narrow training distributions, failing to adapt to novel scenarios or "out-of-distribution (OOD) environments" [4,28]. This limits their applicability beyond simplified laboratory settings, which predominantly feature gripper-based manipulation and are far from the requirements of general-purpose embodied agents in everyday environments [32]. While models like DexVLA demonstrate strengths in tasks with crumpled shirts or novel objects, they still face limitations in extreme scenarios [29]. InstructVLA's focus on zero-shot generalization through benchmarks like SimplerEnv-Instruct directly addresses challenges in unseen, varied real-world environments [14]. MemoryVLA's memory-augmented architecture also aids in robustness to OOD visual conditions, yet its primary reliance on offline training may still pose challenges for online adaptation in entirely novel or rapidly changing real-world scenarios [31]. UniVLA's real-world experiments on a Piper 7-DOF mechanical arm demonstrate robustness against sudden lighting changes and prop interference, but generalization across a wider range of complexities and robot types remains an ongoing challenge [21].

**Computational demands and current hardware limitations** constitute another major impediment to real-world deployment [26,32]. Many advanced VLA models are computationally intensive, leading to excessively slow inference speeds that are incompatible with the real-time responsiveness required for dynamic robotic environments [9,20]. Mainstream VLA models often rely on autoregressive decoding, limiting inference speed to typically 3–5 Hz, far below the 100+ Hz necessary for fine, fluid real-time control in robotics [27,28]. For example, RT-2's largest 55B parameter model operates at only 1-3 Hz, which is often too slow for high-speed or safety-critical tasks [20]. Even parallel decoding, while improving speed, can compromise trajectory smoothness, leading to jerky movements undesirable in high-precision tasks [28]. This 'computational inefficiency' limits deployment on resource-constrained hardware and poses a "physical infeasibility issue" [17,26]. Efforts to overcome this include EfficientVLA, which enables high-performance VLA models to run on "resource-limited real hardware" [9], and OpenVLA, which facilitates efficient fine-tuning on "consumer GPUs" using LoRA and provides efficient serving via "quantization" without significant performance degradation [11].

Furthermore, **robustness and reliability** are constant concerns in dynamic, unpredictable real-world settings. VLA systems must contend with environmental variations such as lighting fluctuations, weather changes, or partial occlusions, which can severely impact stability and accuracy [28]. For instance, OpenDriveVLA's visual module accuracy can drop by 20–30% in low-contrast or shadowed scenes [28], and RoboMamba struggles with object occlusion in cluttered environments, leading to misjudgments [28]. The challenges extend to handling noisy inputs, where models like CoVLA experience reduced language understanding in semantically ambiguous speech environments [28]. Moreover, VLA models face limitations in very long-horizon or contact-rich scenarios, where accurate action recovery and recognizing the correct object state are difficult [26,30]. Current VLA models often exhibit short-sighted behavior due to frame-by-frame reasoning and limited memory mechanisms, impacting their ability to handle complex, long-horizon tasks [26]. The emphasis on "real-time control of embodied AI powered by large models" and "Accelerating the pipeline of data collection, training, and real-world testing" by Microsoft Research highlights the ongoing struggle to achieve responsive and low-latency control in physical systems, and to shorten development cycles for efficient real-world deployment [24].

Finally, **safety and ethical considerations** are paramount, especially for applications like autonomous driving. The "black box" nature and "lack of interpretability" of VLA models make it challenging to diagnose decision errors in edge cases, complicating "safety verification" [8]. The "execution brittleness" of code-based actions, where real-world states can violate API preconditions, translates into "substantial safety risks" [32]. Latency in safety mechanisms, typically 200–500ms for comprehensive checks, can still cause severe hazards in high-speed or critical operations [28]. Defining comprehensive and non-performance-limiting safety rules for diverse real-world tasks remains an open problem [28,30]. The core contradiction underlying these challenges is the mismatch between the generalization needs of "general intelligence" and the physical constraints of "embodied actions," where VLA models combine abstract intelligence with robot actions, but supporting capabilities in data, hardware, and theory are not yet fully compatible [35]. Addressing these challenges requires continued innovation in simulation fidelity, model efficiency, generalization capabilities, and robust safety frameworks.
## 9. Promising Research Directions and Executable Solutions
The advancement of Vision-Language-Action (VLA) models towards truly generalist, embodied agents capable of robust and adaptive operation in complex real-world environments necessitates a concerted and multifaceted research effort. This section synthesizes the critical technical frontiers and paramount non-functional requirements, outlining promising research directions and executable solutions essential for addressing current limitations and ensuring the responsible deployment of VLA technologies.

From a technical perspective, future VLA research must prioritize the development of sophisticated reasoning capabilities. This involves moving towards hybrid hierarchical architectures that dynamically integrate high-level reasoning tokens to guide raw action generation, and cultivating adaptive reasoning mechanisms that adjust complexity based on task demands [2,28]. A significant focus lies in learning universal, embodiment-agnostic action representations to facilitate zero-shot adaptation and cross-embodiment knowledge transfer, enabling VLA models to operate across diverse robot morphologies [20,21]. Furthermore, enhancing VLA models requires robust generative world models with uncertainty quantification, capable of predicting future states and physical interactions, leveraging a 'physical common sense' knowledge base [2,3]. Concurrently, continual meta-learning frameworks are crucial for achieving generalization across task families and mitigating catastrophic forgetting, fostering lifelong learning capabilities [18,25]. Addressing data scarcity remains paramount, necessitating active curiosity-driven sim-to-real data augmentation frameworks and leveraging large-scale synthetic data generation [10,35]. Finally, bridging the modality gap through adaptive multi-modal fusion architectures—integrating diverse sensory inputs like vision, language, touch, and force—and designing lightweight, hardware-aware VLA models for efficient edge deployment, are vital for practical real-world applicability [30,35].

Beyond technical prowess, the successful and responsible deployment of VLA models, particularly in human-centric applications, mandates a paramount focus on safety, reliability, and ethical considerations [24,28]. Robust safety protocols are critical, especially in high-stakes domains like autonomous driving, where evaluation benchmarks rigorously incorporate safety-critical metrics and impose strict penalties for unsafe actions [17,27]. Solutions involve hybrid architectures that integrate VLA models with traditional control methods, employing formal verification techniques such as SafeVLA to achieve risk-aware decision-making and reduce unsafe behaviors [28,30], and developing real-time risk evaluators and human-perception strategies [25]. Ethical alignment further demands addressing bias in training data through auditing and debiasing, ensuring privacy protection via techniques like on-device processing, homomorphic encryption, differential privacy, and privacy-preserving distributed model training as exemplified by FedVLA [23]. Moreover, careful consideration of the socio-economic impact is necessary to promote job augmentation over displacement, guided by transparent impact assessments and regulatory frameworks [25,28]. The inherent limitations of current VLA models, such as generalization failures, the potential for physically infeasible actions, and a lack of explainability, directly impede safe and ethical deployment [18,32]. This underscores the imperative for Human-Aligned VLA Design, incorporating human-in-the-loop (HITL) approaches and formal verification throughout the system lifecycle to build trustworthy and controllable agents.

By concurrently pursuing these advanced technical frontiers and rigorously embedding ethical and safety frameworks, the VLA research community can foster the development of reliable, safe, and intelligent robotic systems capable of sophisticated physical interaction and informed decision-making in diverse real-world scenarios.
### 9.1 Promising Research Avenues
The advancement of Vision-Language-Action (VLA) models necessitates addressing their current limitations through a concerted research effort across multiple interconnected domains. These promising avenues aim to cultivate truly generalist, embodied VLA agents capable of robust and adaptive operation in complex real-world environments.

**Towards Reasoning-Guided Raw Action Generation:**
A critical direction involves developing hybrid hierarchical architectures that dynamically integrate reasoning tokens to guide raw action generation. This approach envisions a high-level reasoning module that processes abstract instructions and environment states to generate semantic sub-goals or modulated reasoning tokens, which then serve as dynamic priors for a fast, low-level raw action generator. Pioneering research suggests that human-like abstract multimodal reasoning chains should precede action execution, emphasizing the need for robust reasoning capabilities within VLA models [2]. Frameworks like ThinkAct demonstrate the potential of "reinforced visual latent planning," where multimodal Large Language Models (LLMs) generate high-level "embodied reasoning plans" that are subsequently compressed into "visual plan latents" to condition low-level action execution [12]. Furthermore, the concept of "Neuro-Symbolic Planning" advocates for hierarchical control architectures where top-level language-driven planners (e.g., LLM variants for affordance reasoning) decompose complex instructions into subtask sequences, which are then converted into parameterized motion plans and executed by low-level diffusion or Transformer controllers [28]. This fusion ensures both the interpretability of structured plans and the flexibility of learned policies [28]. Efforts to mitigate computational and latency issues in long-term control tasks for raw action models, including accelerating reasoning processes and utilizing techniques like inference-time inpainting and soft masking for diffusion action chunking, are also vital for real-time responsiveness [32].

**Dynamic and Adaptive Reasoning Horizon:**
Developing adaptive reasoning mechanisms that dynamically adjust the depth, complexity, and type of reasoning based on task demands, environmental dynamics, and computational budgets is another key research area. This can be achieved through meta-learning or reinforcement learning (RL) frameworks that train the VLA model to decide *when* and *how much* to reason, exploring 'fast-thinking' versus 'slow-thinking' paradigms [32]. For instance, AutoVLA introduces dual thinking modes ("fast thinking" and "slow thinking") coupled with reinforcement fine-tuning (GRPO) to dynamically manage computational complexity and reasoning depth [7]. Similarly, Tsinghua University's DeeR-VLA framework employs a multi-exit architecture and Bayesian optimization to dynamically adjust computational depth, significantly reducing costs for simpler tasks while maintaining performance [18,28]. InstructVLA's use of Mixture-of-Experts (MoE) to flexibly switch between reasoning and operation further exemplifies this approach, highlighting the potential for dynamic allocation of computational resources between reasoning and action modules for optimized efficiency [14]. These adaptive strategies are crucial for improving efficiency and generalizability in varying task complexities and environmental conditions.

**Cross-Embodiment and Task-Agnostic Knowledge Transfer for Action Tokens:**
Future research must prioritize learning universal, embodiment-agnostic action representations that capture fundamental motor primitives and their compositions, effectively forming a 'grammar of actions' [32]. This would enable zero-shot or few-shot adaptation to new robot morphologies or environments, leveraging diverse datasets of human demonstrations or meta-learning of a shared action latent space [32]. UniVLA, for example, offers a promising solution by learning "task-centric latent actions" that serve as a universal "musical score" for diverse robots, establishing a foundation for embodied AI [21]. Models like OpenVLA [11] and RT-2 [20] have demonstrated strong performance across multiple robot embodiments by effectively transferring Vision-Language Model (VLM) knowledge. However, the observed limitations in transfer for models like DexVLA indicate a continued need for truly embodiment-agnostic policy representations or more efficient meta-learning techniques [29]. The long-term vision includes encoding actions in abstract, kinematic-agnostic spaces, combined with meta-learning, to allow new robots to inherit skills with minimal calibration data [28].

**Real-time Continuity and Safety Guarantees for Generative Control:**
Prioritizing the development of robust real-time inference techniques for generative action models (e.g., diffusion models) is essential, especially those that proactively anticipate and mitigate issues like mode shifts or physical infeasibility. This involves enhancing the generative process with uncertainty estimation and incorporating safety filters or control barrier functions that validate generated actions against physical constraints and safety criteria [32]. AutoVLA's innovation of tokenizing actions into a "codebook" and employing rules to eliminate VLM hallucinations directly contributes to generating reliable and physically feasible actions, which is critical in safety-sensitive domains like autonomous driving [17]. Incorporating "physical rationality verification" modules into action decoders and integrating physical constraints and world models are crucial for mitigating physical infeasibility and enhancing robustness [20,35]. Comprehensive approaches advocate for real-time risk assessment modules, reinforcement learning with constrained optimization (e.g., SafeVLA), and formal verification layers for symbolic analysis of planner outputs to ensure provable safety [25].

**Bridging the High-level Instruction to Low-level Action Gap (Refined):**
Further research is needed in developing Action Grounding Networks that explicitly learn to translate high-level symbolic instructions into nuanced, physically constrained, and context-aware low-level continuous control parameters. This must incorporate real-time tactile/force feedback for dynamic refinement [18]. Such a system could involve an RL agent trained on the *deviation* between desired and actual physical outcomes, directly optimizing for precision and physical realism. ThinkAct's framework, which uses multimodal LLMs to generate explicit "embodied reasoning plans" and compress them into "visual plan latents" to condition low-level action execution, directly addresses this gap by ensuring that high-level intent is effectively translated into actionable robot movements [12]. Instruction-Oriented Scene-Parsing techniques further contribute by decomposing and enhancing object-level features based on task instructions, improving contextual understanding for robotic tasks [23].

**Enhanced World Models for Robust Adaptability:**
A significant future direction is research into Generative World Models with Uncertainty Quantification. These models should be capable of predicting future states and modeling the uncertainty of physical interactions (e.g., liquid pouring, object deformation) in unseen environments [18]. They should leverage a 'physical common sense' knowledge base, built from extensive, diverse interaction data, to enable VLA agents to explicitly reason about and adapt to unexpected physical properties [18]. The necessity of world models stems from the understanding that humans are endowed with internal models for imagination and planning [3]. DreamVLA, for example, integrates comprehensive world knowledge forecasting (dynamic, spatial, and semantic information) to improve generalization and reasoning in robot manipulation [2]. World model construction should focus on optimizing the simulation of physical laws to help robots understand complex scene dynamics, thereby enhancing action planning and predictive reasoning [18,26].

**Continual Meta-Learning for Abstraction and Catastrophic Forgetting:**
To achieve robust and adaptable VLA systems, exploration into Meta-Learning for Generalization through Task-Family Abstraction is crucial. This involves developing algorithms that learn *how to learn* new tasks within specific families (e.g., 'pouring tasks', 'grasping tasks'), creating generalized representations for task types and adaptation strategies to rapidly fine-tune. A key challenge to address is catastrophic forgetting, proactively prevented by maintaining a 'knowledge graph' of learned task families and their variations [18]. Incremental learning frameworks, such as those explored by Edinburgh University, directly aim to solve catastrophic forgetting, allowing robots to continuously adapt to new tasks without losing previously acquired skills [18]. Lifelong learning approaches, incorporating memory structures that grow over time and enabling agents to self-organize experiences into reusable abstractions, are vital for fostering long-term competence in open-world settings [26]. Meta-learning frameworks, combined with continual learning algorithms using replay buffers and regularization, are proposed for rapid adaptation while preserving old knowledge [25,28].

**Active and Self-Supervised Data Generation for Generalization:**
Addressing data scarcity and inconsistency is paramount for VLA model generalization. Research should advocate for Active Curiosity-Driven Sim-to-Real Data Augmentation Frameworks, where VLA agents actively query for diverse data in simulation (e.g., novel object arrangements, challenging physical interactions), generate labels via self-supervision, and use this targeted data to address generalization weaknesses, reducing reliance on manual annotation [18,35]. This 'curiosity' could be guided by measures of prediction uncertainty or discrepancy between predicted and observed outcomes. The feasibility of training VLAs entirely with large-scale synthetic action data has been demonstrated, suggesting that improving photorealistic rendering and extensive domain randomization in simulation is a promising avenue [10]. Hybrid simulation-real data integration, synergistically combining large-scale synthetic data with high-quality, diverse real-world data, aims to mitigate the "scale-quality-diversity" contradiction and bridge the sim-to-real gap [35]. Furthermore, expanding behavioral data sources by extracting information from human videos offers a rich, diverse source of physical actions to overcome data scarcity and enhance generalization [20].

**Bridging the Modality Gap for Physical Interaction with Adaptive Integration:**
Proposing the development of adaptive multi-modal fusion architectures is crucial to seamlessly integrate diverse sensory inputs (vision, language, touch, force, proprioception) with varying data rates and noise levels. This includes research into 'adaptive expert networks' where specialized encoders for each modality are dynamically weighted or selected based on task context to achieve robust and precise physical interaction in contact-rich tasks beyond general manipulation [35]. The incorporation of tactile signals, force control, and proprioception is vital for improving precision in contact-intensive tasks like assembly and grasping [35]. Microsoft's Magma model, which integrates "Sets of Marks (SoM)" and "Tracks of Marks (ToM)," exemplifies efforts to enhance spatial reasoning and action planning through multimodal fusion [18]. Moreover, the TLA (Tactile Language-Action) models have shown significant success by integrating tactile streams with language commands, highlighting the importance of expanding multimodal sensory intelligence [28]. Memory systems also need to evolve to support multimodal memory fusion, integrating depth, tactile feedback, and auditory cues for a richer environmental understanding [31].

**Physics-Informed Generative Action Models for Real-World Reliability:**
To enhance the reliability and safety of VLA deployments in unstructured environments, integrating differentiable physics engines directly within VLA models' action generation processes is essential. This involves exploring 'constrained diffusion models' that inherently learn to generate physically plausible trajectories respecting kinematic and dynamic limits, potentially guided by real-time force/torque sensor feedback [35]. Approaches like AutoVLA's use of rules to eliminate VLM hallucinations for reliable and physically feasible actions in autonomous driving are steps in this direction [17]. The integration of physical constraints and world models is also critical to mitigate physical infeasibility issues and enhance robustness [20]. Reinforcement learning with constrained optimization, such as SafeVLA, and formal verification layers can further ensure provable safety and address real-time risk assessment in complex physical interactions [25].

**Lightweight, Hardware-Aware VLA Design for Edge Deployment:**
For VLA models to transition from theoretical prototypes to practical, real-world systems, a strong focus on designing lightweight, hardware-aware architectures is required. This involves quantization-aware training and Network Architecture Search (NAS) specifically optimized for robot-embedded NPUs or specialized low-power processors [35]. Techniques like model compression (e.g., LoRA, knowledge distillation), quantization (mixed-precision operations), and adaptive inference architectures (e.g., early-exit branches like DeeR-VLA) are crucial for reducing memory footprint and inference time [18,25,28]. Projects like TinyVLA and RoboMamba demonstrate the effectiveness of designing computationally efficient VLA models for real-time deployment [16,30]. Investigating 'tiered reasoning architectures,' where high-level, computationally intensive planning occurs offline or in the cloud, while low-level, real-time control is handled by highly optimized, lightweight modules on the robot, offers a practical solution for balancing complex intelligence with instantaneous physical response [35]. The increasing accessibility of parameter-efficient fine-tuning on consumer GPUs and efficient serving via quantization further highlights the trend towards more deployable and resource-conscious VLA systems [11,19].

By pursuing these specific and actionable research directions, which draw from interdisciplinary perspectives and summarize successful methodologies, the VLA research community can effectively address core challenges such as generalization, computational inefficiency, physical infeasibility, and data scarcity. These solutions will foster the development of truly generalist, embodied VLA agents, moving beyond theoretical models to practical, reliable, and safe real-world robotic systems capable of sophisticated physical interaction and intelligent decision-making [18,35].
### 9.2 Ethical Considerations and Safety
The deployment of Vision-Language-Action (VLA) models in real-world environments, particularly those involving human interaction, necessitates a paramount focus on safety, reliability, and ethical considerations [24]. These non-functional requirements are not merely afterthoughts but critically influence model design, evaluation, and responsible deployment [25,28]. The meta-survey insights reveal a significant trend towards recognizing these aspects as central to VLA model development, with several papers explicitly identifying safety as a promising future opportunity for the field [30,34].

The imperative for robust safety protocols is particularly evident in high-stakes applications such as autonomous driving, where VLA models face a "paramount need for enhanced safety" [27]. Evaluation benchmarks in this domain, like NAVSIM, rigorously incorporate safety-critical metrics, including Collision Rate, Rater Feedback Score (RFS), and safety components within Driving Score (DS) and Predictive Driver Model Score (PDMS), imposing strict penalties for unsafe actions [17]. This strong emphasis highlights that VLA models must be engineered with inherent safety guarantees to prevent physically infeasible or dangerous behaviors. A significant challenge in this regard stems from the "black box" nature and interpretability issues of VLA models, which complicate safety verification [8]. Proposed solutions include hybrid architectures that integrate VLA models with traditional rule engines or Model Predictive Control (MPC) to enhance safety redundancy and system robustness [8].

Beyond general acknowledgment, the literature proposes concrete strategies for bolstering VLA model safety. Formal verification, exemplified by SafeVLA, is presented as a method to achieve risk-aware decision-making, demonstrating reductions in unsafe behaviors by over 80% through constrained Markov decision processes (CMDP) [28,30]. However, defining comprehensive safety rules for the vast diversity of real-world tasks remains an open problem [28]. Future VLA systems are envisioned to integrate real-time risk evaluators capable of assessing potential harm to humans or property before executing high-risk actions, seeking natural language consent in ambiguous scenarios [25,28]. Adaptive calibration modules are also proposed to dynamically adjust perception thresholds and control gains, mitigating issues like drift and sensor degradation during prolonged operation [28]. For human-robot interaction, human-perception strategies incorporating proximity sensors, real-time monitoring, and language confirmation for critical actions are crucial [28]. Hierarchical frameworks, by providing human-understandable intermediate outputs, offer enhanced modular flexibility and transparency, which are critical for both complex multi-stage tasks and safety-critical deployments [26].

Ethical alignment constitutes another critical dimension of responsible VLA development. This encompasses addressing "ethical deployment risks" and ensuring socially aligned and trustworthy AI [25]. Key areas of focus include bias detection and correction, where bias auditing tools are necessary to identify demographic or semantic distribution biases in training datasets. These biases can then be ameliorated through adversarial debiasing and counterfactual augmentation [25,28]. Privacy protection is equally vital, especially when VLA models handle sensitive user data in applications like healthcare and smart homes. Privacy-preserving inference techniques, such as on-device processing, homomorphic encryption of sensitive data streams, and differential privacy during training, are essential for safeguarding data security [25,28]. FedVLA is a notable contribution in this area, proposing a solution for privacy-preserving distributed model training [23]. Furthermore, the socio-economic impact of VLA models demands careful consideration; applications should be designed to complement human labor through skill enhancement programs rather than large-scale displacement, necessitating transparent impact assessments and stakeholder involvement [28]. Establishing robust regulatory frameworks and industry standards for VLA safety and accountability is consistently highlighted as a foundational step for fostering responsible innovation that balances technological capabilities with societal values [25,28].

A critical analysis reveals that current VLA model limitations, such as generalization failures, the potential for physically infeasible actions, and the inherent brittleness of code-based actions in unforeseen circumstances, directly impede safe and ethical deployment [18,32]. The lack of explainability and controllability in latent representations, for instance, renders them unsuitable for scenarios demanding "strict safety or reliability guarantees" [32]. These limitations necessitate a concerted focus on explainability, trustworthiness, and the integration of human-in-the-loop (HITL) approaches for complex or safety-critical tasks [18,32]. Research into 'Human-Aligned VLA Design' is therefore crucial, emphasizing transparency, controllability, and adherence to societal values, particularly in human-robot interaction and sensitive applications [18]. This includes integrating formal verification throughout the code lifecycle, from API libraries to dynamically generated code, using logical reasoning to guide safe code generation, and employing static analysis or model checking to prove safety before deployment [32]. Runtime monitoring systems can ensure API preconditions are met, triggering safe shutdowns or recovery procedures in case of anomalies [32]. Furthermore, leveraging the interpretability of code within HITL systems can facilitate effective human-robot collaboration, enabling interactive debugging and collaborative refinement to develop trustworthy and controllable robotic agents [32]. While some surveys briefly mention the importance of safety and reliability, or do not explicitly detail these aspects in their abstracts [6,26,31], the overarching consensus emphasizes their critical role for the future viability and acceptance of VLA technologies.
## 10. Concluding Remarks
Vision-Language-Action (VLA) models represent a profound paradigm shift, extending the cognitive capabilities of large vision and language foundation models into the physical world to enable embodied intelligence [19,25,27,32,34]. Their transformative impact on embodied AI and robotic manipulation is evidenced by their ability to seamlessly integrate visual perception, natural language understanding, and physical action generation [30,34,35]. This integration empowers robots to interpret high-level instructions, generalize to previously unseen scenarios, and execute complex manipulation tasks with unprecedented versatility and robustness [26,35]. Such advancements promise to overcome the limitations of traditional rule-based methods, paving the way for truly general-purpose robotic agents capable of operating effectively in open-world physical environments [6,32].

The architectural landscape of VLA models is diverse, yet unified by core methodological innovations. A central theme is the strategic formulation of "action tokens," which can range from language descriptions and code to affordances, trajectories, goal states, latent representations, raw actions, and reasoning primitives [27,32]. Models like AutoVLA discretize continuous trajectories into feasible actions for autonomous driving [7], while UniVLA distills millions of diverse human and robot videos into "task-centric latent actions" to create a universal, embodiment-agnostic action representation [21]. This diversity in action tokenization critically shapes model design and performance, driving advancements in capabilities like multitask learning and long-horizon task completion [32].

Architectural paradigms generally fall into monolithic end-to-end systems or hierarchical/modular designs [26,35]. While end-to-end models like InstructVLA integrate multimodal reasoning and action generation into a unified framework, addressing challenges such as catastrophic forgetting and reasoning disconnects with a novel VLA-IT training paradigm and large dataset [5,14], others adopt hierarchical approaches. ThinkAct, for instance, employs a dual-system framework that combines a multimodal Large Language Model (LLM) for high-level reasoning with a downstream action model conditioned by compressed visual plan latents, enabling few-shot adaptation and self-correction in complex, multi-step tasks [12]. Further innovations include the integration of memory systems, as seen in MemoryVLA's dual-memory architecture, which significantly improves performance on non-Markovian and long-horizon tasks by leveraging historical context [31]. The emerging role of 3D understanding is highlighted by 3D-VLA, which integrates 3D perception, reasoning, and action through a generative world model and 3D-based LLM, moving beyond the limitations of 2D-centric inputs [3]. Diffusion-based strategies are also becoming prevalent for smoother action generation and enhanced modularity [35], exemplified by DexVLA's use of a billion-parameter diffusion expert for robust visuomotor policies [29] and DreamVLA's diffusion-based transformer for action generation [2].

Several models have showcased remarkable performance and introduced unique contributions. OpenVLA stands out as the first open-source, 7B-parameter VLA model trained on a diverse collection of 970k real-world robot demonstrations [11]. It demonstrates superior performance, outperforming larger closed models like RT-2-X (55B) by 16.5% absolute task success rate with significantly fewer parameters, thereby democratizing access to advanced robotic policies [11,19]. DexVLA further pushes the boundaries of generalist manipulation, achieving superior performance through a three-stage embodied curriculum learning strategy and rapid adaptation to new embodiments [29]. The UAV-VLA system introduces novel language-based path planning for drones, demonstrating flexibility and accuracy in global UAV task generation [15]. GraspVLA showcases the efficacy of training an "Embodied foundation model" on "billion-scale synthetic action data" to mitigate sim-to-real gaps and achieve open-vocabulary generalization, presenting a scalable solution to data scarcity [10]. Similarly, 3D-Generalist leverages text-driven 3D world creation to generate large-scale synthetic environments, providing robust training data and mitigating labor-intensive content creation [33]. Efficiency remains a critical focus; EfficientVLA, for instance, dramatically improves operational speed and computational efficiency through redundant layer removal and visual token filtering, achieving a 1.93x speedup with minimal impact on task success [9]. These developments collectively underscore the significant progress in VLA model capabilities and efficiency.

Despite rapid advancements, VLA models face several critical challenges and limitations. Data scarcity remains a pervasive issue across the field [27,30,32,35], though synthetic data generation and internet-scale knowledge transfer (e.g., RT-2 [20]) offer promising mitigations. Computational inefficiency also poses a significant hurdle, particularly for real-world deployment on resource-limited hardware [27,30,35], necessitating continued exploration of techniques like LoRA and quantization [11,25]. Generalization gaps and poor cross-embodiment transferability persist, with many evaluations still confined to simplified lab settings rather than complex real-world requirements [18,32,34]. Furthermore, issues such as semantic-physical mismatches, the modality gap, and the potential for "hallucinations" in action generation continue to challenge robust deployment, particularly in safety-critical applications like autonomous driving [8,17,27]. The problem of "catastrophic forgetting," where VLM capabilities degrade during VLA training, also demands careful architectural and training solutions [5,14]. Finally, addressing interpretability, safety verification, bias, and ethical alignment is paramount for the responsible development and widespread adoption of VLA systems [25,30].

Looking ahead, the continued importance of VLA research for achieving artificial general intelligence is undeniable, prompting sustained investment and focus from institutions like Microsoft Research [24]. Promising research avenues include strategic token synthesis, enhanced 3D understanding, improved temporal dynamics, and developing proactive VLA agents with advanced cognitive architectures and strong safety alignment [32]. Further efforts should concentrate on advanced knowledge preservation, real-time performance optimization, adaptive action representations, and scalable data strategies to move towards truly general-purpose embodied intelligence [27]. This includes developing more robust foundation models, enhancing world models, and ensuring improved safety measures [34]. Key priorities also encompass adaptation across embodiments, scalable real-world deployment, tighter coupling between high-level reasoning and low-level execution, and leveraging human demonstrations at internet scale [26]. The fusion of VLMs, VLA architectures, and agentic AI systems is poised to propel robotics towards Artificial General Intelligence, providing a unified understanding and a structured path forward for developing intelligent, embodied, and human-aligned agents [25]. The field stands at the cusp of significant breakthroughs, requiring multidisciplinary advancements in algorithms and hardware to fully integrate intelligent robots into daily life [18]. This continuous evolution underscores the dynamic and critical role VLA models play in shaping the future of AI.

## References

[1] InstructVLA：融合理解与操控的视觉-语言-动作指令微调 [http://www.paperreading.club/page?id=325650](http://www.paperreading.club/page?id=325650) 

[2] DreamVLA: 融合世界知识的通用机器人操作模型 [https://ui.adsabs.harvard.edu/abs/arXiv:2507.04447](https://ui.adsabs.harvard.edu/abs/arXiv:2507.04447) 

[3] 3D-VLA: Generative World Model for Embodied VLA [http://www.paperreading.club/page?id=215007](http://www.paperreading.club/page?id=215007) 

[4] Procgen环境下的VLA模型零样本泛化性能基准测试 [http://www.paperreading.club/page?id=304507](http://www.paperreading.club/page?id=304507) 

[5] InstructVLA: 统一视觉、语言与动作的指令调优新范式 [https://arxiv.org/abs/2507.17520](https://arxiv.org/abs/2507.17520) 

[6] 大型VLM驱动的机器人操控：综述 [http://www.paperreading.club/page?id=332123](http://www.paperreading.club/page?id=332123) 

[7] AutoVLA: 端到端自动驾驶的自适应推理与强化微调VLA模型 [https://ui.adsabs.harvard.edu/abs/arXiv:2506.13757](https://ui.adsabs.harvard.edu/abs/arXiv:2506.13757) 

[8] 自动驾驶新范式：VLA模型详解 [https://mp.m.ofweek.com/ai/a756714955517](https://mp.m.ofweek.com/ai/a756714955517) 

[9] 上海交大：EfficientVLA框架让机器人处理速度大幅提升 [https://baijiahao.baidu.com/s?id=1835821907955142122&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1835821907955142122&wfr=spider&for=pc) 

[10] GraspVLA：基于十亿级合成动作数据预训练的抓取基础模型 [http://www.paperreading.club/page?id=303819](http://www.paperreading.club/page?id=303819) 

[11] OpenVLA：首个开源的机器人视觉-语言-动作模型 [https://zhuanzhi.ai/paper/5beb2b8f137597d3fc6904e2a866b228](https://zhuanzhi.ai/paper/5beb2b8f137597d3fc6904e2a866b228) 

[12] ThinkAct: 强化视觉潜在规划的视觉-语言-行动推理 [http://www.paperreading.club/page?id=325295](http://www.paperreading.club/page?id=325295) 

[13] 自动驾驶VLA模型综述 [https://blog.csdn.net/qinsd19/article/details/150340813](https://blog.csdn.net/qinsd19/article/details/150340813) 

[14] InstructVLA：65万数据训练，端到端VLA模型性能超越GPT-4o [https://mp.weixin.qq.com/s?__biz=MzU2NjU3OTc5NA==&mid=2247601854&idx=1&sn=0e916b20989eb6a1fd10cec0f6a15641&chksm=fd54b91f315da356496ce9282fc909a19c67de25488ea58d9ff314252f0e35503743f49ab30e&scene=27](https://mp.weixin.qq.com/s?__biz=MzU2NjU3OTc5NA==&mid=2247601854&idx=1&sn=0e916b20989eb6a1fd10cec0f6a15641&chksm=fd54b91f315da356496ce9282fc909a19c67de25488ea58d9ff314252f0e35503743f49ab30e&scene=27) 

[15] 从语言到路径：UAV-VLA实现自然语言与无人机协同作业 [https://www.bilibili.com/read/cv40408472](https://www.bilibili.com/read/cv40408472) 

[16] 具身智能年度影响力论文盘点 [https://www.bilibili.com/opus/1036379710266802182](https://www.bilibili.com/opus/1036379710266802182) 

[17] 端到端VLA模型性能评测：OpenDriveVLA与AutoVLA谁更胜一筹？ [https://baijiahao.baidu.com/s?id=1841502520183595752&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1841502520183595752&wfr=spider&for=pc) 

[18] VLA模型：具身智能泛化之路的挑战与探索 [https://baijiahao.baidu.com/s?id=1834590217982735404&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1834590217982735404&wfr=spider&for=pc) 

[19] 视觉语言动作模型：赋能自动驾驶的新型AI框架 [https://baike.baidu.com/item/视觉语言动作模型/65508304](https://baike.baidu.com/item/视觉语言动作模型/65508304) 

[20] 谷歌 DeepMind RT-2：大模型赋能机器人控制新突破 [https://www.zhihu.com/question/614768538/answer/3457674449](https://www.zhihu.com/question/614768538/answer/3457674449) 

[21] UniVLA：机器人动作的“统一乐谱”实现跨模态AI新突破 [https://baijiahao.baidu.com/s?id=1832811211083663547&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1832811211083663547&wfr=spider&for=pc) 

[22] Vision-Language Models 概览 [https://github.com/topics/vision-language-model](https://github.com/topics/vision-language-model) 

[23] FedVLA: 隐私保护的机器人操作视觉-语言-动作联邦学习 [https://ui.adsabs.harvard.edu/abs/arXiv:2508.02190](https://ui.adsabs.harvard.edu/abs/arXiv:2508.02190) 

[24] 微软研究院联合研究项目征集 [https://news.microsoft.com/ko-kr/2025/01/14/research-notice-2025/](https://news.microsoft.com/ko-kr/2025/01/14/research-notice-2025/) 

[25] Vision-Language-Action Models: Concepts, Progress, Applications, and Challenges [https://arxiv.org/html/2505.04769](https://arxiv.org/html/2505.04769) 

[26] 大型VLM驱动的视觉-语言-动作模型在机器人操控领域的应用综述 [https://arxiv.org/html/2508.13073v2](https://arxiv.org/html/2508.13073v2) 

[27] VLA模型：行动标记化视角下的全面综述 [https://blog.csdn.net/weixin_38252409/article/details/149194157](https://blog.csdn.net/weixin_38252409/article/details/149194157) 

[28] VLA模型：概念、进展、应用与挑战综述 [https://www.cnblogs.com/emergence/p/19043338](https://www.cnblogs.com/emergence/p/19043338) 

[29] DexVLA: 增强机器人通用控制的视觉-语言-动作模型 [https://arxiv.org/html/2502.05855v1](https://arxiv.org/html/2502.05855v1) 

[30] Awesome VLA for Robotics: 资源汇总 [https://github.com/Jiaaqiliu/Awesome-VLA-Robotics](https://github.com/Jiaaqiliu/Awesome-VLA-Robotics) 

[31] MemoryVLA：机器人“记忆”赋能的视觉-语言-动作模型 [https://blog.csdn.net/jiaquan3011/article/details/150983421](https://blog.csdn.net/jiaquan3011/article/details/150983421) 

[32] VLA模型：动作标记化视角下的综述 [https://blog.csdn.net/weixin_38252409/article/details/149168629](https://blog.csdn.net/weixin_38252409/article/details/149168629) 

[33] 3D-Generalist: 驱动多模态模型创建3D世界 [https://arxiv.org/html/2507.06484v1](https://arxiv.org/html/2507.06484v1) 

[34] 具身AI的视觉-语言-行动模型综述 [https://www.renrendoc.com/paper/439029355.html](https://www.renrendoc.com/paper/439029355.html) 

[35] VLA模型：机器人操控的变革性框架 [https://blog.csdn.net/weixin_39699362/article/details/149418295](https://blog.csdn.net/weixin_39699362/article/details/149418295) 

