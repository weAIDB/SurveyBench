# A Survey on Graph Neural Networks

# 0. A Survey on Graph Neural Networks

## 1. Introduction
The increasing prevalence of graph-structured data across diverse domains underscores a growing challenge for traditional deep learning methods, which are inherently designed for Euclidean data such as images and text [1,8,10,19,28]. Real-world data, including social networks, citation networks, molecular chemistry, electric grids, knowledge graphs, and e-commerce platforms, often exhibit complex, irregular, and non-Euclidean structures, characterized by varying numbers of nodes, diverse connections, and inherent interdependencies that traditional models like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) struggle to process efficiently [1,6,8,11,19,26,28]. This limitation arises because operations foundational to traditional deep learning, such as convolution, are difficult to apply directly to graphs due to their unordered and variable-sized nature [6].

In response to these limitations, Graph Neural Networks (GNNs) have emerged as a powerful paradigm for analyzing graph-structured data, demonstrating superior performance in various tasks by effectively handling the complexity and relational information inherent in non-Euclidean spaces [7,9,13,23,30,35]. The evolution of GNNs is rooted in generalizing concepts from conventional deep learning models; for instance, the message passing framework draws parallels with the local connection and shared weights principles of CNNs, extending them to the graph domain to extract multi-scale localized spatial features and compose expressive representations [1,20,37]. Key advantages of GNNs include their ability to capture complex dependencies between graph nodes through iterative message passing and aggregation, learn low-dimensional representations for nodes and graphs, and overcome the inflexibility and costliness of manual feature engineering prevalent in traditional machine learning methods [1,4,7,8,16,20]. This inherent capability allows GNNs to model complex relationships and dependencies, making them suitable for diverse real-world problems [18,35].

GNNs have achieved significant breakthroughs across a wide array of fields, including but not limited to social network analysis, recommender systems, molecular fingerprinting, protein interface prediction, disease classification, traffic prediction, and even complex combinatorial optimization problems like Max-k-Cut [2,3,4,13,16,17,18,21,22,40]. They are also increasingly applied in AI for Science, interpreting structures from non-structural data like text and images, and addressing challenges in fraud detection and knowledge graph reasoning [12,13,35]. Despite their widespread success, the field faces ongoing challenges such as ensuring interpretability, addressing fairness concerns (especially at the node level) [5,25,32], improving theoretical understanding of their representational power [24], managing scalability to large instances, mitigating issues like overfitting and over-smoothing, and developing robust methods for dynamic graphs or handling unlabeled data [12,15,20].

This survey aims to provide a comprehensive and structured overview of Graph Neural Networks, integrating recent advancements and critical insights from foundational and cutting-edge research [1,3,13,30]. We delineate the field by systematically categorizing GNN models, analyzing their core ideas, design components, and examining their diverse applications. Furthermore, this survey highlights current limitations, open problems, and promising future research directions, offering valuable insights for students, researchers, and practitioners in this rapidly evolving domain.

The remainder of this survey is organized as follows: Section 2 provides a foundational understanding of graph theory and the necessity for graph-specific deep learning models. Section 3 details the evolution and core architectures of GNNs, including recurrent, convolutional, autoencoder-based, and spatial-temporal variants. Section 4 presents a comprehensive review of GNN applications across various domains, categorizing them into structural, non-structural, and other specialized scenarios. Section 5 discusses key challenges in GNN research, such as interpretability, fairness, scalability, and theoretical expressiveness. Finally, Section 6 concludes the survey by summarizing key findings and outlining promising avenues for future research and development in the field of Graph Neural Networks.
## 2. Background and Fundamentals
Graph Neural Networks (GNNs) represent a rapidly evolving paradigm for machine learning on graph-structured data. This section lays the fundamental groundwork for understanding GNNs by first establishing the basic graph notations and representations essential for modeling interconnected data [7,15,18,21,30,35]. It details how diverse attributes—node-level, edge-level, and graph-level—are incorporated to enrich these representations, providing a comprehensive view of complex real-world systems such as social networks, knowledge graphs, and molecular structures [18,35].

Central to the operation of GNNs is the **message passing** mechanism, which facilitates information exchange and aggregation among nodes to iteratively update their representations [9,20,35,37,40]. This iterative process, conceptually similar to convolution in traditional neural networks, deconstructs into distinct phases: message construction, message aggregation, and node update [9,18,20]. The choice of aggregation functions, such as sum, mean, or max, plays a critical role in shaping a GNN's performance and its ability to learn from neighborhood information [18,24,35,40]. The section also explores the expressive power of GNNs, particularly in relation to their ability to distinguish graph structures, and addresses the ongoing challenges in enhancing their interpretability, which is crucial for practical applications [11,24,25,32].

Understanding GNNs further necessitates a grasp of their underlying mathematical foundations, encompassing concepts from linear algebra, calculus, and probability. The evolution of GNNs, from early recurrent models to sophisticated convolutional GNNs, graph autoencoders, and spatial-temporal variants, is briefly traced, distinguishing them from related graph analysis techniques like network embedding and graph kernel methods [1,4]. This comprehensive overview serves as a foundational reference for readers to delve deeper into the methodologies and applications of Graph Neural Networks.
### 2.1 Graph Representation
Graph data fundamentally comprises nodes (or vertices) and edges, which represent entities and their relationships, respectively [19,35]. This structure is crucial for modeling diverse real-world networks such as social networks, knowledge graphs, and molecular structures [35]. A graph \(G\) is formally defined as a pair \((V, E)\), where \(V\) is the set of \(N\) nodes and \(E\) is the set of \(N^e\) edges [7,21,22,30]. Edges can be directed, indicating information flow from a source node \(v_{src}\) to a destination node \(v_{dst}\), or undirected, allowing bidirectional information flow. An undirected edge between two nodes is equivalent to two directed edges, one in each direction [18,21,30].

The primary method for representing graph connectivity is the **Adjacency Matrix (A)**, an \(N \times N\) matrix where \(A_{ij} = 1\) if an edge exists between node \(i\) and node \(j\), and \(A_{ij} = 0\) otherwise [1,15,18]. While conceptually straightforward and amenable to matrix operations, the Adjacency Matrix can be highly inefficient in terms of memory usage for sparse graphs, which are common in real-world applications. It requires \(O(N^2)\) memory, regardless of the number of edges, making it prohibitive for graphs with millions of nodes [6]. For computational efficiency, operations on sparse graphs are typically optimized using specialized sparse matrix formats rather than dense adjacency matrices. The **Degree Matrix (D)**, a diagonal matrix where diagonal elements correspond to node degrees, is often used in conjunction with the adjacency matrix to form graph Laplacians, which are fundamental in many graph algorithms and spectral methods [1].

Beyond connectivity, effective graph representation involves incorporating various types of attributes to enrich the data. According to `a_gentle_introduction_to_graph_neural_networks`, attributes, typically in the form of scalars or embeddings, can be associated with nodes, edges, or implicitly, the entire graph.

* **Node Attributes**: Node features are represented by a **Feature Matrix (X)**, an \(N \times d\) matrix where each row corresponds to a \(d\)-dimensional feature vector for a specific node [1,15]. For instance, in traffic networks, nodes representing intersections might have attributes such as intersection types [40]. These features are crucial for graph neural networks (GNNs) as they provide initial information for message passing and aggregation processes.

* **Edge Attributes**: Similar to node attributes, edge features can be captured in an **Edge Feature Matrix (\(X^e\))**, an \(M \times c\) matrix where \(M\) is the number of edges and each row represents a \(c\)-dimensional feature vector for an edge [1]. Edge weights, such as \(\mathbf{W}_{ij} \in \mathbb{R}\) for edge \((i,j)\), are a common form of edge attribute [22]. In traffic networks, road segments (edges) can be enriched with attributes like length or speed limits [40]. These attributes are critical for GNNs to modulate the information flow between connected nodes.

* **Graph-level Attributes**: While not explicitly defined as direct input attributes in `a_gentle_introduction_to_graph_neural_networks`, the concept of dynamic or spatial-temporal graphs implies graph-level properties. A spatial-temporal graph is defined as \(G(t) = (V, E, X(t))\), where \(X(t) \in \mathbb{R}^{n \times d}\) indicates node attributes that change over time [1,30]. This representation allows GNNs to capture evolving relationships and features across the entire graph over time, essential for tasks like time series prediction [21]. Graph-level representations for tasks like graph classification are typically derived by aggregating information from all nodes and edges into a single embedding.

Advanced representations, such as those employing TensorFlow-Keras RaggedTensors, offer flexibility in handling the irregular nature of graphs, where different nodes possess varying numbers of neighbors [6,36]. The `kgcnn` package utilizes this approach, which provides a transparent tensor structure that can enhance code readability and debugging for complex graph structures [36]. This flexibility contributes to more memory-efficient storage and computation for sparse and heterogeneous graph data compared to dense matrix representations. The effective representation of nodes, edges, and their associated attributes is paramount for GNNs to accurately learn from and understand the complex relationships embedded within graph data [19,35].
### 2.2 Message Passing and Aggregation

![Message Passing Paradigm in GNNs](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/EEpKXy1Yy7aKdUZNKgsjz_/home/surveygo/data/requests/13543/survey/imgs/Message%20Passing%20Paradigm%20in%20GNNs.png)

Message Passing (MP) serves as the foundational mechanism underpinning the operations of Graph Neural Networks (GNNs), enabling nodes to iteratively exchange and integrate information with their neighbors [7,13,35,37]. This iterative process allows GNNs to effectively capture the complex dependencies within graph structures and propagate information throughout the network, ultimately updating node representations and enhancing performance in various tasks [1,4,10,11,30,35,40]. The operation of message passing is conceptually analogous to convolution in traditional neural networks, where information from an element's local neighborhood is aggregated and processed to update its value [18,28].

The message passing paradigm can be systematically deconstructed into three primary components: message construction, message aggregation, and node update [9,18].

1.  **Message Construction**: In this initial phase, each node generates messages to be sent to its neighbors. These messages are typically constructed based on the node's own features, its current hidden state, and potentially the features of its connected edges and neighbors. For instance, within the Message Passing Neural Network (MPNN) framework, message functions ($M_t$) are employed to formulate messages ($m_{tv}$) from neighbors [7].
2.  **Message Aggregation**: Following message construction, each node collects the messages received from its neighbors. These messages are then combined into a fixed-size vector through an aggregation function. A critical requirement for this aggregation step is permutation invariance, ensuring that the order of neighbors does not affect the aggregated result [4,18]. This phase can involve irregular memory access patterns due to the sparse nature of graphs [2].
3.  **Node Update**: The final stage involves integrating the aggregated message with the node's current representation or previous hidden state to generate a new, updated representation for that node. This is often achieved through a combination function or an update function. The MPNN framework utilizes update functions ($U_t$) to update hidden states ($h_{tv}$) [7]. Similarly, in many GNN architectures, an aggregation kernel combines attributes from neighboring nodes, while a separate combination kernel processes feature vectors [19].

Collectively, these steps form an iterative process. For a node $v$ at the $k$-th layer or iteration, its representation $h_v^{(k)}$ is typically updated as follows [24,28]:
$$
h_v^{(k)} = \text{COMBINE}\Bigl(h_v^{(k-1)},\ \text{AGGREGATE}\Bigl(\{ h_u^{(k-1)} : u \in N(v) \}\Bigr)\Bigr)
$$
where $h_v^{(k-1)}$ represents the feature vector of node $v$ at the $(k-1)$-th iteration (with $h_v^{(0)} = X_v$ being the initial node features), and $N(v)$ denotes the set of neighbors of node $v$ [24]. $\text{AGGREGATE}$ signifies the aggregation of neighbor representations, and $\text{COMBINE}$ integrates the aggregated information with the node's own previous representation to generate the new state [24]. After $k$ such aggregation iterations, a node's representation effectively captures features from its $k$-hop neighborhood [24]. More generally, the updated node representation $h_v$ can be viewed as a function $f$ of its own features ($x_v$), connected edge features ($x_{co}$), and the representations ($h_{ne}$) and features ($x_{ne}$) of its neighbors [8,20]:
$$
h_v = f(x_v,\ x_{co},\ h_{ne},\ x_{ne})
$$

The choice of the aggregation function significantly influences the performance and expressive power of GNNs [24]. Common aggregation functions include sum, mean (or average), and max [18,35]. Each function offers distinct characteristics for combining information:

*   **Sum Aggregation**: This function sums the feature vectors of all neighbors. It effectively captures the total magnitude of features across the neighborhood. However, it can be sensitive to the degree of a node, as nodes with more neighbors might have larger aggregated feature magnitudes, potentially leading to issues if not appropriately normalized.
*   **Mean/Average Aggregation**: This function computes the average of the neighbor feature vectors. By averaging, it intrinsically normalizes for the number of neighbors, making the aggregated representation more robust to variations in node degrees. It effectively captures the "average" characteristic of a node's neighborhood [19].
*   **Max Aggregation**: This function takes the element-wise maximum across the feature vectors of all neighbors. It acts as a feature selector, highlighting the most salient or prominent feature dimension present among the neighbors. This can be particularly useful for identifying key patterns or attributes within the neighborhood.

These aggregation functions, while simple, are crucial for propagating information through the graph and for determining the type of local structural information a GNN can learn. The selection of an appropriate aggregation function depends on the specific task and the nature of the graph data.
### 2.3 Expressive Power and Interpretability
The ability of Graph Neural Networks (GNNs) to effectively distinguish between different graph structures—commonly referred to as their expressive power—is a critical aspect determining their applicability and performance. A theoretical framework connects the representational capacity of GNNs to the Weisfeiler-Lehman (WL) test for graph isomorphism, a widely recognized benchmark for graph distinguishing power [3,24]. Specifically, GNNs can achieve a discriminatory power analogous to the WL test if their aggregation functions are sufficiently injective, meaning they can uniquely map distinct multisets of neighboring features to distinct aggregated representations [24]. The formalization of a node's neighborhood as a multiset of feature vectors is central to analyzing how different aggregation functions influence the ability to differentiate between these multisets [24]. It has been posited that GNN architectures employing more injective aggregation functions inherently possess stronger representational capabilities [24]. However, established GNN variants, such as Graph Convolutional Networks (GCN) and GraphSAGE, have been identified to struggle with distinguishing certain graph structures, thereby limiting their expressive power [24]. Furthermore, limitations of early GNN models—such as the original GNN—include their unsuitability for node representation learning due to the fixed-point iteration, which can lead to overly smooth and less informative representations for distinguishing individual nodes [20].

Despite their convincing performance, the interpretability of GNNs presents a significant challenge. While GNNs have been lauded for their high interpretability in certain contexts, contributing to their widespread application as a graph analysis tool [11], they are frequently regarded as “black boxes” [25]. This opacity necessitates the development of sophisticated explanation methods to foster trust and facilitate their adoption, particularly in sensitive domains [25]. The lack of transparent decision-making processes in GNNs motivates ongoing research into explainable AI (XAI) within graph-based learning [32]. For instance, the development of models like XGNN directly addresses this concern by aiming to provide model-level explanations, thereby increasing the transparency of GNN predictions and enhancing understanding of how these complex models operate [25,32]. This endeavor is crucial for bridging the gap between computational efficacy and human comprehensibility in graph learning.
## 3. Graph Neural Network Architectures

![Main Categories of GNN Architectures](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/TzHl7YHaeiD5GlkFK4mt7_/home/surveygo/data/requests/13543/survey/imgs/Main%20Categories%20of%20GNN%20Architectures.png)

This section delves into the diverse landscape of Graph Neural Network (GNN) architectures, exploring their fundamental principles, key innovations, and evolutionary trajectory. GNN architectures can be broadly categorized based on their underlying convolutional mechanisms, such as spectral-based versus spatial-based approaches [7,28], or by their core design paradigms, including recurrent, attention-based, autoencoder-driven, and spatio-temporal models [1,6,9,30,35]. It is important to note that these categories are not mutually exclusive, with models often incorporating elements from multiple paradigms to enhance performance and address specific challenges [9].

Many of these diverse architectures are unified under the Message Passing Neural Network (MPNN) framework, which provides a generalized paradigm for processing graph-structured data through iterative information exchange among nodes [18,37]. This framework abstracts the core operations of message aggregation and node state updating, allowing for flexible instantiation of various GNN models. Each subsequent sub-section will systematically present the key innovations, underlying mathematical formulations, and computational complexities of these GNN categories, adhering to an organization based on architectural similarity or chronological development [7,9]. Furthermore, a comparative analysis will highlight their respective strengths and weaknesses, delineating the specific types of graph data and tasks for which each architecture is optimally suited.

Recent advancements also focus on deepening GNN models, such as the Network In Graph Neural Network (NGNN), which inserts non-linear feedforward neural network layers within each GNN layer to enhance model capacity and maintain stability against perturbations, without directly increasing the number of GNN layers or widening hidden dimensions [12]. This comprehensive overview aims to provide researchers with a clear understanding of the architectural choices in GNNs, their theoretical underpinnings, practical implications, and the ongoing efforts to address challenges such as scalability, computational efficiency, and expressive power across various real-world applications [2].
### 3.1 Spectral-based GNNs
Spectral-based Graph Neural Networks (GNNs) draw their theoretical foundations from graph signal processing, interpreting graph convolution as an operation to denoise graph signals within the spectral domain [1,6,30]. This approach defines convolution using the eigenvalues and eigenvectors of the graph Laplacian matrix [4,9,28,35].

The core of spectral convolution relies on the graph Fourier transform. For an undirected graph, the normalized graph Laplacian matrix is commonly used, defined as 
$$
L = I_N - D^{-1/2} A D^{-1/2},
$$
where $A$ is the adjacency matrix, $D$ is the diagonal degree matrix, and $I_N$ is the identity matrix [6,28,30]. The Laplacian matrix $L$ can be spectrally decomposed as 
$$
L = U \Lambda U^T,
$$
where $U$ is the matrix of eigenvectors and $\Lambda$ is a diagonal matrix containing the corresponding eigenvalues [6,8,26]. The graph Fourier transform of a signal $x$ is defined as 
$$
\mathcal{F}(x) = U^T x,
$$
and its inverse transform is 
$$
\mathcal{F}^{-1}(\hat{x}) = U\hat{x}
$$
[1,6,8,26,30].

Based on the convolution theorem, the convolution operation $g \star x$ between a filter $g$ and a graph signal $x$ is defined in the spectral domain as the element-wise product of their graph Fourier transforms, followed by an inverse transform [1,7,8,26,30]:
$$
g \star x = \mathcal{F}^{-1}(\mathcal{F}(g) \odot \mathcal{F}(x)) = U((U^T g) \odot (U^T x)).
$$
Here, $\odot$ denotes the element-wise (Hadamard) product [1,6,30]. By parameterizing the filter $U^T g$ as a learnable diagonal matrix $g_{\theta}(\Lambda)$, the convolution simplifies to 
$$
U g_{\theta}(\Lambda) U^T x
$$
[8,20,26].

To derive the Graph Convolutional Network (GCN) layer propagation rule, approximations are introduced to reduce computational complexity. A common approach involves expressing $g_{\theta}$ as a polynomial of the Laplacian eigenvalues, such as a truncated Chebyshev polynomial expansion [8,26,28]. Specifically, the filter can be defined as 
$$
g_{\theta}(\Lambda) = \sum_{k=0}^{K-1} \theta_k T_k(\tilde{\Lambda}),
$$
where $T_k(\cdot)$ are Chebyshev polynomials and 
$$
\tilde{\Lambda} = \frac{2\Lambda}{\lambda_{max}} - I_N
$$
normalizes the eigenvalues to the interval $$ [8,26]. This yields the convolution 
$$
g_{\theta} \star x = \sum_{k=0}^{K} \theta_k T_k(L) x,
$$
where $T_k(L)$ can be computed recursively and represents localized operations on the graph [8,26].

Further simplification, particularly in GCNs, involves setting the polynomial order $K=1$ and approximating $\lambda_{max} \approx 2$ and parameters $\theta_0 = -\theta_1 = \theta$. This leads to the approximate convolution 
$$
g_{\theta} \star x \approx \theta\Bigl(I_N + D^{-1/2}AD^{-1/2}\Bigr)x
$$
[8]. To address numerical stability and improve performance, a renormalization trick is applied: 
$$
I_N + D^{-1/2}AD^{-1/2} \rightarrow \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2},
$$
where $\tilde{A} = A + I_N$ (adding self-loops) and $\tilde{D}$ is the degree matrix of $\tilde{A}$ [8]. This results in the GCN layer propagation rule, which can be generalized for node features $H^t$:
$$
H^{t+1} = \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^t W,
$$
where $W$ represents learnable weights. This can be interpreted as an aggregation process akin to self-attention in natural language processing, with $\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}$ acting as a sophisticated weight matrix for feature propagation [8].



**Spectral-based vs. Spatial-based GNNs**

| Feature                | Spectral-based GNNs                          | Spatial-based GNNs                                  |
| :--------------------- | :------------------------------------------- | :-------------------------------------------------- |
| **Domain of Operation**| Spectral domain (Graph Fourier Transform)    | Vertex domain (Directly on graph structure)         |
| **Core Operation**     | Interprets convolution as signal denoising   | Defines convolution by aggregating neighbor info     |
| **Foundation**         | Eigenvalues/eigenvectors of Graph Laplacian  | Message Passing, CNN-like aggregation               |
| **Computational Basis**| Requires eigendecomposition of Laplacian     | Operates directly on local neighborhoods            |
| **Computational Cost** | High: O(N^2) per convolution, O(N^3) initial eigendecomposition; Impractical for large N | Lower: More efficient, especially for large graphs  |
| **Generalization**     | Poor: Filters tied to specific graph's eigenbasis, struggles with unseen or dynamic graphs | Better: More robust to topology changes, applicable to unseen graphs |
| **Memory Requirement** | Requires loading entire graph into memory    | Can use sampling, more memory efficient for sparse graphs |
| **Key Advantage**      | Strong theoretical foundation                | Scalability, flexibility, robustness                |
| **Examples**           | ChebNet, GCN (spectral), CayleyNet           | GCN (spatial), GraphSAGE, GAT, MPNN                 |

Despite their theoretical elegance, spectral methods face significant challenges. A primary limitation is computational complexity [14,28]. The reliance on the Laplacian matrix's eigenvectors necessitates $O(N^2)$ time complexity for each convolution operation and $O(N^3)$ for the initial eigendecomposition, making them computationally intensive and impractical for large-scale graphs with many nodes ($N$) [28]. Furthermore, spectral methods inherently struggle with generalization across different graph structures [14]. The learned filters are tied to the specific eigenbasis of the graph Laplacian, meaning a model trained on one graph structure cannot be directly applied to a graph with a different structure [7,20]. This dependence limits their applicability primarily to transductive tasks and poses challenges for inductive learning scenarios where the graph structure might change or new nodes are introduced [7]. Another practical drawback is the need to load the entire graph into memory, which becomes inefficient for very large graphs [6].

Various spectral-based GNN models have been proposed to address these limitations or explore different filter designs. Early models like Spectral CNN treated the filter as a set of learnable parameters $\Theta_{i,j}^{(k)}$ [1,30]. ChebNet (Chebyshev Spectral CNN) and GCN simplified the convolution by using Chebyshev polynomials to approximate the spectral filter, as discussed above [8,26,28,30]. ChebNet employs a $K$-order polynomial approximation, while GCN simplifies it further to a first-order approximation with specific parameter constraints [8,28]. Other notable spectral models include CayleyNet, which approximates the filter using Cayley polynomials to obtain localized filters and avoid dense matrix operations [28,30]. Beyond these, models like Adaptive Graph Convolutional Network (AGCN) learn residual graph Laplacians to capture underlying node relationships, while Deep Graph Convolutional Network (DGCN) considers both local and global consistencies [26]. Graph Wavelet Neural Networks (GWNN) utilize graph wavelet transforms instead of Fourier transforms for convolution [7,26]. Multi-kernel spectral clustering (MKSC) and Multi-View Spectral Embedding (MVSE) are examples where spectral techniques are applied for clustering by integrating kernel features or merging similarity graphs [27]. These advancements demonstrate efforts to mitigate the challenges of spectral methods, primarily focusing on computational efficiency and filter design, while still operating within the spectral domain.
### 3.2 Spatial-based GNNs
Spatial-based Graph Neural Networks (GNNs) operate directly on the graph structure, defining convolutions in the vertex domain rather than relying on spectral properties [8,9,26]. This approach draws inspiration from traditional Convolutional Neural Networks (CNNs) by convolving a central node's representation with those of its neighbors [1,6,28,30]. A primary challenge in non-spectral methods lies in defining convolution operations that can accommodate varying neighborhood sizes while maintaining the local invariance characteristic of CNNs [7,20,26].

The message passing framework is central to spatial-based GNNs, where information is iteratively aggregated from a node's local neighborhood to update its representation [1,6,28,30]. An early example, Neural Network for Graphs (NN4G), summarizes neighborhood information to update node states, where the node state in the next layer is given by:
$$
h_v^{(t)} = f\Bigl(x_v, \sum_{u \in N(v)} x_{v,u}^e, h_u^{(t-1)}\Bigr)
$$
where \( f \) is an activation function, \( h_v^{(0)} \) is initialized to a zero vector, \( x_v \) and \( x_u \) are feature vectors of nodes \( v \) and \( u \), and \( x_{v,u}^e \) is the feature vector of the edge between \( v \) and \( u \) [1]. The Message Passing Neural Network (MPNN) framework generalizes this process, defining the message passing function as:
$$
h_v^{(k)} = U_k\Bigl(h_v^{(k-1)}, \sum_{u \in N(v)} M_k\Bigl(h_v^{(k-1)}, h_u^{(k-1)}, x_{vu}^e\Bigr)\Bigr)
$$
where \( U_k \) and \( M_k \) are learnable functions [30]. A more simplified initial approach involves summing neighbor representations:
$$
x = h_v + \sum_{i=1}^{\mathcal{N}_v} h_i
$$
$$
h'_v = \sigma\Bigl(xW_L^{\mathcal{N}_v}\Bigr)
$$
where \( W \) varies based on the number of neighbors [8]. Graph Convolutional Networks (GCNs), when interpreted spatially, perform aggregation and transformation using a normalized adjacency matrix:
$$
H^{l+1} = \sigma\Bigl(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}H^lW^l\Bigr)
$$
Here, \( \sigma \) is a non-linear activation, \( \tilde{A} = A + I \) includes self-loops, and \( \tilde{D} \) is the degree matrix, normalizing the sum over neighbors [28]. Spatial-based GCNs can be broadly categorized into recurrent-based methods, which use the same graph convolution layer iteratively, and composition-based methods, which stack different graph convolution layers [6].

Different aggregation functions and message passing schemes are employed across various spatial-based GNNs to address the complexities of graph structures. GraphSAGE provides a versatile framework for aggregation, where the `AGGREGATE` function can be mean, max-pooling, sum, or LSTM [8]. For instance, max-pooling with ReLU is a common aggregation function:
$$
\text{AGGREGATE} = \max\Bigl(\{ \text{ReLU}(W \cdot h_u) : u \in N(v) \}\Bigr)
$$
where \( W \) is a learnable parameter matrix [24]. GraphSAGE further addresses varying neighborhood sizes by uniformly sampling a fixed-size set of neighbors for aggregation [26]. In contrast, Neural FPs employ different weight matrices, \( W_{deg(v)} \), based on a node's degree to handle varying connectivity [26]. Diffusion Convolutional Neural Networks (DCNNs) utilize transition matrices (\( P, P^2, \dots, P^K \)) to define neighborhoods and diffusion representations. For node classification, their diffusion representations can be expressed as
$$
Z = \operatorname{concat}(X, KP^*)
$$
where \( P^* \) contains the power series of the degree-normalized transition matrix \( P \) [26]. Key examples of spatial-based ConvGNNs include NN4G, Contextual Graph Markov Model (CGMM), Diffusion Convolutional Neural Network (DCNN), Diffusion Graph Convolution (DGC), PGC-DGCNN, Partition Graph Convolution (PGC), MPNN, Graph Isomorphism Network (GIN), GraphSAGE, and Graph Attention Network (GAT) [7,30].

Spatial-based GNNs offer significant advantages over their spectral counterparts, particularly in terms of computational efficiency and generalization capabilities. By performing convolution operations directly in the vertex domain and aggregating information from a node's immediate neighbors, spatial methods avoid the computationally intensive eigendecomposition required by spectral methods, making them more efficient for processing large-scale graphs [8,9]. Furthermore, this direct operation on local graph structures enhances their generalization ability. Spatial-based GNNs are inherently more robust to changes in graph topology and can be applied to unseen or dynamic graphs without requiring re-computation of the spectral basis, which is a common limitation for spectral methods that rely on a fixed graph structure [8]. This flexibility allows spatial-based models to adapt to diverse real-world graph data where structures can vary widely and evolve over time.
### 3.3 Graph Attention Networks (GATs)
Graph Attention Networks (GATs) represent a significant advancement in Graph Neural Networks (GNNs) by incorporating the attention mechanism to selectively aggregate information from neighboring nodes [4,20,28]. Unlike traditional Graph Convolutional Networks (GCNs) that assign uniform weights to all neighbors, GATs dynamically compute the importance of each neighbor, allowing the model to focus on more relevant information [19,28]. This mechanism has precedents in sequence-based tasks like machine translation and reading comprehension, and its integration into GNNs marks a notable improvement in graph representation learning [20].

The core of GATs lies in the calculation and application of attention coefficients. For a given node i and its neighbor j, the unnormalized attention coefficient e₍ij₎ is computed as a measure of their interaction or relevance. This calculation typically involves a learnable weight matrix W applied to the hidden states of nodes i and j before being passed through a single-layer feedforward neural network a [20,26]. Specifically, the unnormalized attention score can be expressed as:
$$
e_{ij} = a\Bigl(W\overrightarrow{h_i},\, W\overrightarrow{h_j}\Bigr)
$$
where \( \overrightarrow{h_i} \) and \( \overrightarrow{h_j} \) are the hidden states of node i and node j, respectively [20]. A common variant for computing \( e_{ij} \) involves concatenating the transformed features and applying a learnable vector \( a^T \) followed by a non-linear activation function such as LeakyReLU [8]:
$$
e_{ij} = \operatorname{LeakyReLU}(a^T)
$$
To make attention coefficients comparable across different neighbors of a node, a softmax function is applied, normalizing the scores over all neighbors \( N_i \) of node i. This yields the normalized attention coefficients \( \alpha_{ij} \):
$$
\alpha_{ij} = \operatorname{softmax}_j(e_{ij}) = \frac{\exp(e_{ij})}{\sum_{k \in N_i} \exp(e_{ik})}
$$
These normalized attention coefficients \( \alpha_{ij} \) quantify the importance of neighbor j to node i [20]. They are then used to weight the aggregation of neighbor features. The aggregated feature for node i is obtained by summing the linearly transformed features of its neighbors, weighted by their respective attention coefficients. The final output hidden state \( h_i' \) for node i is typically computed as:
$$
h_i' = \sigma\Biggl( \sum_{j \in N_i} \alpha_{ij}\,W h_j \Biggr)
$$
where \( \sigma \) is an activation function [8,26,30]. The attention mechanism is also utilized in some Spatio-Temporal Graph Neural Network (STGNN) models for aggregating node features, with some models purely relying on attention and others mixing it with convolutional approaches [21].

To enhance the stability and expressive power of the learning process, GATs often employ multi-head attention, similar to its application in Transformers [18,20]. This involves applying \( K \) independent attention mechanisms (heads) that compute their own sets of attention coefficients and aggregated features. The outputs from these multiple heads are then either concatenated or averaged to form the final representation of the node [8,20,26].

Concatenation:
$$
h_i' = \underset{k=1}{\overset{K}{\parallel}}\, \sigma\left( \sum_{j \in N_i} \alpha_{ij}^k\, W^k h_j \right)
$$

Averaging:
$$
h_i' = \sigma\left( \frac{1}{K} \sum_{k=1}^{K} \sum_{j \in N_i} \alpha_{ij}^k\, W^k h_j \right)
$$
where \( W^k \) and \( \alpha_{ij}^k \) correspond to the k-th attention head [26]. This multi-head approach allows GATs to capture diverse aspects of relationships and varying importances among neighboring nodes, thereby enriching the node representations [17].

The advantages of using attention mechanisms in GATs are manifold. They lead to improved interpretability by explicitly highlighting which neighbors contribute most to a node's representation, thereby enabling the neural network to focus on more relevant nodes and edges [9,17]. This selective aggregation increases the expressiveness of the model, as it can dynamically adjust the influence of neighbors based on their features, rather than relying on a fixed graph structure or pre-defined weights. This dynamic weighting also provides robustness by implicitly handling noisy or irrelevant neighbors, as their attention weights can be learned to be negligible [9]. Furthermore, attention mechanisms can be guided by causal-based supervision to focus on aspects that directly influence the final prediction, leading to more powerful and relevant feature learning [33]. Graph Kernels with Attention (GKATs) are also explored, demonstrating that attention layers can scale linearly with the number of nodes even for dense graphs, potentially reducing computational demands for certain GAT variants [38].

When comparing GATs with GCNs, GATs offer enhanced expressive power due to their ability to assign differential weights to neighbors, unlike GCNs which typically employ fixed, structure-based weights (e.g., inverse degree normalization) [19,28]. This allows GATs to model more complex relationships and adapt to diverse graph structures without requiring prior knowledge of graph topology. While GATs are recognized as a prominent GNN variant [13], the provided digests do not offer a detailed comparison regarding the computational complexity between GATs and GCNs. However, it's generally understood that the attention calculation in GATs introduces additional computational overhead compared to the simpler aggregation of GCNs, particularly due to the pairwise attention score computation and normalization steps. Nevertheless, the benefits in terms of flexibility and performance often outweigh this increased complexity for many applications. It is also notable that Transformers can be conceptualized as GNNs where attention mechanisms define edge embeddings in a fully connected graph, contrasting with GNNs' typical assumption of sparse connectivity [18].
### 3.4 Recurrent Graph Neural Networks (RecGNNs)
Recurrent Graph Neural Networks (RecGNNs), also recognized as Graph Recurrent Networks (GRNs) [9,13], constitute a foundational category within Graph Neural Networks. The core principle of RecGNNs revolves around their recursive neural structures, designed to learn node representations by assuming nodes iteratively exchange information or "messages" with their neighbors until a stable equilibrium is achieved [1,35]. This iterative message-passing mechanism enables the network to extract high-level feature representations for all nodes while utilizing the same set of parameters [30]. In this process, the hidden state of a node v at time step t is updated based on its own features, edge features, and the hidden states of its neighbors from the preceding time step. A general representation of this update function is given by:
$$
h_v^{(t)} = f\Bigl(x_v, \sum_{u \in N(v)} x_{v,u}^e, h_u^{(t-1)}\Bigr)
$$
where $h_v^{(0)}$ denotes the initial state vector of node v, $x_v$ and $x_u$ are the feature vectors of nodes v and u respectively, $x_{v,u}^e$ represents the feature vector of the edge between v and u, and $h_u^{(t-1)}$ is the state vector of neighbor u at the previous time step [1,30]. The concept of "stable equilibrium" is crucial, implying that the node representations converge after a sufficient number of iterations, thereby facilitating the propagation of information across arbitrary path lengths within the graph. Early GRN models frequently employed architectures such as Bidirectional RNNs (BiRNN) and Long Short-Term Memory Networks (LSTM) to manage this recursive evolution effectively [9].

A primary advantage of RecGNNs stems from their inherent capacity to model long-range dependencies within a graph. Through repeated information exchange, signals from distant nodes can ultimately influence the representation of a given node, allowing for a comprehensive capture of global graph structure and intricate dependencies. However, this iterative nature is also the source of their main drawback: computational complexity. The sequential and often time-consuming iterative propagation, coupled with the potential for gradient explosion or vanishing problems during training (particularly when employing backpropagation through time), can significantly limit their scalability and efficiency, especially when applied to large-scale graphs [14].

To mitigate some limitations of earlier GNN models, such as challenges in ensuring convergence or improving training stability, more sophisticated RecGNN variants have been introduced. A notable example is the **Gated Graph Neural Network (GGNN)**. GGNNs integrate gated recurrent units (GRUs) directly into their propagation steps, which negates the need for explicit coordination parameters often required to ensure convergence in predecessor models [1,20]. This incorporation of GRU-like update functions allows GGNNs to more effectively assimilate information from neighboring nodes and previous time steps into each node’s hidden state, while simultaneously addressing issues such as vanishing gradients [20]. The update mechanism for a node v in GGNNs, utilizing a GRU-based approach, is formulated through a series of equations:
$$
a_v^t = \sum_{u \in \mathcal{N}(v)} A_{vu} h_u^{t-1} + b
$$
$$
z_v^t = \sigma\Bigl(W^z a_v^t + U^z h_v^{t-1}\Bigr)
$$
$$
r_v^t = \sigma\Bigl(W^r a_v^t + U^r h_v^{t-1}\Bigr)
$$
$$
\tilde{h_v^t} = \tanh\Bigl(W a_v^t + U\bigl(r_v^t \odot h_v^{t-1}\bigr)\Bigr)
$$
$$
h_v^t = \bigl(1 - z_v^t\bigr) \odot h_v^{t-1} + z_v^t \odot \tilde{h_v^t}
$$
Here, $a_v^t$ serves to aggregate neighborhood information, while $z_v^t$ and $r_v^t$ are the update and reset gates, respectively, regulating the flow of information [8,20]. GGNNs typically unroll the recurrence for a predetermined number of steps $T$ and apply backpropagation through time (BPTT) for learning model parameters [1,20].

Another significant model is **Stochastic Steady-State Embedding (SSE)**, which aims to enhance the scalability of RecGNNs for larger graphs. SSE addresses the scalability challenge by proposing a learning algorithm that samples batches of nodes for state updates and gradient calculations, rather than processing the entire graph iteratively [1]. To maintain stability during this sampling process, SSE defines its recursive function as a weighted average of the historical state and the newly computed state:
$$
h_v^{(t)} = \bigl(1 - \alpha\bigr) h_v^{(t-1)} + \alpha\, f\Bigl(x_v, \sum_{u \in N(v)} x_{v,u}^e, h_u^{(t-1)}\Bigr)
$$
where $\alpha$ is a hyperparameter and $h_v^{(0)}$ is randomly initialized [1]. This approach effectively mitigates the computational burden associated with full-graph iterations on extensive datasets.

Recurrent GNNs are particularly adept at processing sequential data and dynamic graphs, finding extensive application within Spatio-Temporal Graph Neural Networks (STGNNs). In such contexts, recurrent structures, predominantly GRUs, are leveraged to capture the temporal dependencies intrinsic to sequential graph data [21]. Many RNN-based STGNN methods integrate graph convolutions to capture spatio-temporal dependencies. Specifically, graph convolution layers ($Gconv(\cdot)$) are employed to filter inputs and hidden states before they are fed into the recurrent units. A common update formulation for the hidden state $h_t$ is:
$$
h_t = f\bigl(h_{t-1},\, Gconv(x_t)\bigr)
$$
or
$$
h_t = Gconv\bigl(h_{t-1},\, x_t\bigr)
$$
[14]. This hybrid approach facilitates the simultaneous modeling of spatial relationships through graph convolutions and temporal dynamics through recurrent units. Illustrative models include the Graph Convolutional Recurrent Network (GCRN), which combines LSTM networks with ChebNet; the Diffusion Convolutional Recurrent Neural Network (DCRNN), integrating a diffusion graph convolution layer into a GRU network within an encoder-decoder framework; and Structural-RNN, a recursive framework that utilizes both node-RNNs and edge-RNNs to process temporal information and incorporates spatial information, often by grouping nodes and edges into semantic clusters to reduce model complexity [14].

When comparing Recurrent GNNs with Convolutional GNNs (CGNNs) on tasks involving dynamic graphs, it is essential to note their complementary strengths. While CGNNs excel at capturing static, local structural patterns, their direct application to dynamic graphs, which evolve over time, typically requires specialized adaptations. Recurrent GNNs, particularly when integrated with graph convolutions, offer a more intuitive and direct mechanism for modeling temporal dynamics and long-range dependencies in such evolving structures. By incorporating recurrent units, RecGNNs can explicitly track the evolution of node states and feature changes across sequential time steps, which is critical for tasks like time series prediction and classification on graphs [21]. Unlike static CGNNs that process a single graph snapshot, RecGNNs are capable of maintaining and updating internal states over time, thereby leveraging historical information to inform future predictions or classifications. Consequently, for dynamic graph tasks, RecGNNs, often hybridized with convolutional operations, provide a robust framework that seamlessly integrates both spatial and temporal dependencies, a capability that standard CGNNs inherently lack without additional temporal modeling components.
### 3.5 Graph Autoencoders (GAEs)
Graph Autoencoders (GAEs) represent a class of Graph Neural Networks (GNNs) that leverage the principles of autoencoders for graph representation learning [1,9]. As unsupervised learning frameworks, GAEs are designed to encode graph data—including nodes and entire graphs—into a compressed latent vector space and subsequently reconstruct the original graph structure or attributes from this learned encoding [30,35]. This core mechanism allows GAEs to serve two primary purposes: learning effective network embedding representations and generating new graphs [1].

The architecture of a GAE typically comprises an encoder and a decoder component [9]. The encoder maps input graph information (nodes, edges, features) into a lower-dimensional latent space, while the decoder aims to reconstruct the input from this latent representation. For instance, in Graph Auto-Encoder (GAE*), a prominent variant, Graph Convolutional Networks (GCNs) are utilized as the encoder to simultaneously capture node structure and feature information [1]. Specifically, the encoder may consist of multiple graph convolutional layers, represented as:
$$
Z = Gconv(ReLU(Gconv(A, X, W_1)), W_2)
$$
where \(A\) is the adjacency matrix, \(X\) denotes the node feature matrix, \(W_1\) and \(W_2\) are learnable weight matrices, \(ReLU\) is the activation function, \(Gconv\) signifies the graph convolution operation, and \(Z\) is the resulting network embedding matrix [1]. The decoder then reconstructs the graph's adjacency matrix from the node embeddings, typically using a simple inner product or logistic sigmoid function, defined as:
$$
\hat{A}_{ij} = \sigma(z_i^T z_j)
$$
where \(\hat{A}_{ij}\) is the reconstructed adjacency matrix element, \(\sigma\) is the sigmoid function, and \(z_i\) and \(z_j\) are the latent representations of nodes \(i\) and \(j\) respectively [1,20]. The learning process minimizes the discrepancy between the original and reconstructed adjacency matrices.

GAEs find extensive applications across various graph-related tasks. For network embedding, GAEs learn low-dimensional node representations that preserve graph topology information, such as adjacency matrices or Positive Pointwise Mutual Information (PPMI) matrices [1,30]. These learned embeddings are highly valuable for downstream tasks like node clustering, where nodes with similar embeddings tend to belong to the same cluster, and link prediction, where the likelihood of a connection between two nodes can be inferred from their latent representations. Examples of GAE models used for network embedding include DNGR, SDNE, GAE*, VGAE, ARVGA, and DRNE [30]. In addition to embedding, GAEs are effectively employed for graph generation problems [1,28]. Some methods generate graphs incrementally, adding nodes and edges step by step, while others generate the entire graph directly. MolGAN is one such model discussed for its application in graph generation [30]. GAEs have demonstrated utility in diverse domains, including social network analysis, bioinformatics, and recommendation systems [17,35].

Different GAE variants offer distinct advantages and disadvantages. DNGR (Deep Neural Network for Graph Representations), for instance, leverages a random walk model to capture structural information and employs a stacked denoising autoencoder for learning low-dimensional node representations, which enhances robustness by randomly zeroing parts of the input [9]. However, a notable disadvantage of DNGR is its sensitivity to minor changes in graph structure due to its neglect of node attribute information [9]. To address this, MGAE (Marginalized Graph Autoencoder) integrates both node attribute features and graph structure information by utilizing spectral-based graph convolution network layers within its autoencoder architecture [9]. Another advanced variant, Siamese Attribute-missing Graph Auto-encoder (SAGA), is specifically designed to handle graph representation learning on attribute-missing graphs [39]. SAGA's advantages include entangling attribute and structure embeddings, using K-Nearest Neighbors (KNN) and structural constraints for enhanced learning, and employing a masking strategy to recover the true adjacency matrix [39]. The general advantage of GAEs lies in their unsupervised nature, allowing them to learn meaningful representations without requiring explicit labels, and their ability to model complex, higher-order social relations through stacked layers [9,17].

While not always explicitly termed as "regularization," various techniques are implicitly employed within GAE frameworks to improve performance, enhance robustness, and prevent overfitting. For example, DNGR's use of random zeroing of input features functions as a denoising strategy, which can be interpreted as a form of regularization to make the learned representations more robust to noise and minor structural perturbations [9]. Similarly, SAGA's masking strategy to recover the true adjacency matrix can be seen as a regularization technique that encourages the model to learn more robust and complete representations even when attributes are missing [39]. These techniques help in stabilizing the learning process and ensuring that the generated embeddings are generalizable and capture the underlying graph structure effectively.
### 3.6 Spatial-Temporal Graph Neural Networks (STGNNs)
Spatial-Temporal Graph Neural Networks (STGNNs) are a specialized class of GNNs designed to model dynamic relationships and learn hidden patterns from complex spatio-temporal graphs [1,14,30,35]. These models are crucial for applications where both the spatial interdependencies between connected nodes and the temporal dynamics of node features evolve over time [1,14]. STGNNs capture these intricate dependencies to predict future node values or labels, as well as spatio-temporal graph labels [1]. They are widely applied in domains such as traffic flow and speed prediction, human action recognition, driver manipulation perception, and general time series forecasting [28,30,35].

STGNN architectures generally fall into two primary categories based on their approach to capturing temporal dependencies: Recurrent Neural Network (RNN)-based STGNNs and Convolutional Neural Network (CNN)-based STGNNs [1,14,28,30]. Both categories integrate graph convolutional layers to aggregate node-level spatial information, but they differ significantly in their handling of temporal sequences.

RNN-based STGNNs integrate graph convolutions with recurrent units, where graph convolution layers filter inputs and hidden states before feeding them into recurrent components. This allows for the simultaneous learning of spatial dependencies and temporal dynamics, expressed generically as 
$$
h_{(t)} = f\bigl(Gconv\bigl(X_{(t)}\bigr),\; h_{(t-1)}\bigr)
$$
[1]. Prominent examples like the Graph Convolutional Recurrent Network (GCRN) and Diffusion Convolutional Recurrent Neural Network (DCRNN) illustrate this paradigm, employing specific graph convolutions (e.g., ChebNet or Diffusion) alongside recurrent units (e.g., LSTM or GRU) to model spatio-temporal correlations.

In contrast, CNN-based STGNNs process spatio-temporal graphs in a non-recursive manner, primarily leveraging 1D-Convolutional Neural Network (CNN) layers for temporal pattern recognition and graph convolution layers for spatial correlation capture [1,14,30]. These models often interweave or alternate between 1D-CNN and graph convolution layers to learn dependencies modularly [1,14,30]. Examples include CGCN and ST-GCN, which demonstrate the flexibility of disentangling spatial and temporal learning through strategic application of CNN and graph convolution layers.

A key distinction between these two categories lies in their computational characteristics and handling of sequence data. CNN-based STGNNs offer significant advantages in terms of computational efficiency and gradient stability due to their non-recursive nature, enabling parallel computation, reduced training time, and processing of larger datasets [1,14,30]. They are less susceptible to issues like vanishing or exploding gradients common in deep RNNs, leading to more stable optimization and often lower memory requirements [1,14,30]. While RNN-based models are adept at capturing complex, long-range temporal dependencies sequentially, CNN-based models provide a more parallelizable and often more robust alternative for time-series forecasting on graphs. Different models also adopt varied strategies for integrating spatial and temporal information, either by processing them in separate modules, integrating them into a single module, or extending static graph structures with temporal connections to apply traditional GNNs [20,21]. The choice between RNN-based and CNN-based architectures often depends on the specific application's requirements for sequential dependency modeling versus computational efficiency and stability.
#### 3.6.1 RNN-based STGNNs
Recurrent Neural Network (RNN)-based Spatio-Temporal Graph Neural Networks (STGNNs) are designed to capture complex spatio-temporal correlations by integrating graph convolutions with recurrent units [1,14]. The fundamental principle involves using graph convolutions to filter both the inputs and the hidden states that are subsequently fed into the recurrent units, thereby allowing the models to simultaneously learn spatial dependencies and temporal dynamics [1,14,30].

The generic structure of an RNN can be expressed as:
$$h_{(t)} = f(X_{(t)}, h_{(t-1)})$$
where $$X_{(t)}$$ represents the node features at time $$t$$, and $$h_{(t-1)}$$ is the hidden state from the previous time step [1]. By incorporating a graph convolution layer, $$Gconv(\cdot)$$, this equation is adapted to explicitly account for graph structure:
$$h_{(t)} = f(Gconv(X_{(t)}), h_{(t-1)})$$
Here, $$Gconv(X_{(t)})$$ applies graph convolution to the current node features, enabling the capture of spatial relationships before the temporal information is processed by the recurrent function $$f(\cdot)$$ [1].

Several prominent models exemplify this architectural approach, including the Graph Convolutional Recurrent Network (GCRN) and the Diffusion Convolutional Recurrent Neural Network (DCRNN) [14,30]. The GCRN specifically combines Long Short-Term Memory (LSTM) networks with ChebNet, a Chebyshev polynomial-based graph convolutional network [1,14]. This combination leverages LSTM's ability to model long-range temporal dependencies and ChebNet's spectral graph convolution capabilities to capture localized spatial features efficiently.

In contrast, the DCRNN integrates a diffusion graph convolution layer into a Gated Recurrent Unit (GRU) network [14]. While GCRN employs a ChebNet for spatial filtering and LSTM for temporal modeling, DCRNN utilizes diffusion convolutions, which are based on random walks on the graph, to capture spatial dependencies. Furthermore, DCRNN adopts an encoder-decoder framework, a distinct architectural choice that facilitates the prediction of future node values by encoding the input sequence and decoding the future states [14]. This architectural differentiation in the choice of recurrent unit (LSTM in GCRN versus GRU in DCRNN), the specific type of graph convolution (ChebNet in GCRN versus Diffusion in DCRNN), and DCRNN's incorporation of an encoder-decoder structure, highlight varied methodological approaches to modeling spatio-temporal dynamics and predicting future states within the field of STGNNs.
#### 3.6.2 CNN-based STGNNs
CNN-based Spatio-Temporal Graph Neural Networks (STGNNs) represent a distinct paradigm for modeling complex spatio-temporal dependencies in graph data by processing spatio-temporal graphs in a non-recursive manner [1,14,30]. This approach primarily leverages 1D-Convolutional Neural Network (CNN) layers for capturing temporal patterns and graph convolution layers for discerning spatial correlations [1,14]. Specifically, these models interweave or alternate between 1D-CNN layers and graph convolution layers to learn temporal and spatial dependencies, respectively, in a modular fashion [1,14,30].

A significant advantage of CNN-based STGNNs over their Recurrent Neural Network (RNN)-based counterparts lies in their computational efficiency and gradient stability [1,14,30]. The non-recursive nature of CNNs facilitates parallel computation, thereby significantly reducing training time and enabling the processing of larger datasets [1,14,30]. Furthermore, CNNs are inherently less susceptible to issues like vanishing or exploding gradients, which are common challenges in training deep RNNs, thus leading to more stable gradient propagation during optimization [1,14,30]. Additionally, CNN-based architectures often exhibit lower memory requirements compared to RNN-based models, making them more resource-efficient [1,14].

Illustrative examples of CNN-based STGNN models include CGCN and ST-GCN, both of which exemplify the alternating architectural design [14,30]. CGCN (Graph Convolutional Gated Recurrent Unit) integrates 1D convolutional layers with either ChebNet or GCN layers to capture spatio-temporal features [14]. This combination allows CGCN to model dynamic patterns over time through 1D convolutions while simultaneously leveraging ChebNet’s spectral graph convolutions or GCN’s first-order approximations to aggregate information across graph nodes. In contrast, ST-GCN (Spatio-Temporal Graph Convolutional Network) employs 1D convolutional layers for temporal modeling and utilizes Graph Convolutional (PGC) layers for spatial feature extraction [14]. The PGC layers in ST-GCN are designed to handle irregular graph structures effectively, enabling the model to learn spatial features directly from the graph topology, while the 1D CNNs manage the sequential dependencies in the temporal dimension. Both CGCN and ST-GCN underscore the flexibility and effectiveness of disentangling spatial and temporal learning through the strategic application of CNN and graph convolution layers.
### 3.7 Other GNN Variants / Message Passing Neural Networks (MPNNs)
Message Passing Neural Networks (MPNNs) represent a foundational and generalized framework that encapsulates a broad spectrum of Graph Neural Network (GNN) architectures [7,18,20,37]. This framework processes graph data through an iterative message passing mechanism, allowing deep learning to be applied effectively on graph structures [37].

At its core, the MPNN framework consists of two principal phases: a message passing phase and a readout phase [7,8,26]. In the message passing phase, node states are iteratively updated. This involves a message function (Mₜ) that aggregates information from a node’s neighbors and an update function (Uₜ) that combines the aggregated message with the node’s previous state to form a new hidden state [7,26]. Formally, the message mᵥ^(t+1) aggregated for node v at step t+1 and its updated hidden state hᵥ^(t+1) are defined as:

$$m_v^{t+1} = \sum_{u \in N(v)} M_t(h_v^t, h_u^t, e_{vu})$$
$$h_v^{t+1} = U_t(h_v^t, m_v^{t+1})$$

Here, N(v) denotes the set of neighbors of node v, and e₍vu₎ represents the features of the edge between nodes v and u [8,26]. The GNNs generally adopt a “graph-in, graph-out” architecture, progressively transforming embeddings without altering the input graph’s connectivity, with message passing occurring between nodes or edges [18].

Following the message passing phase, a readout phase computes a feature vector for the entire graph. This is achieved using a readout function R, which aggregates the final hidden states of all nodes in the graph to produce a graph-level representation, ˆy [7,8,26]:

$$\hat{y} = R(\{h_v^T \mid v \in G\})$$

Here, T indicates the total number of message passing steps [26]. The significant flexibility of the MPNN framework stems from its ability to instantiate various GNN models by simply varying the definitions of the message, update, and readout functions [7]. For instance, Gated Graph Neural Networks (GGNNs) can be expressed within this framework where

$$M_t(h_v^t, h_w^t, e_{vw}) = A_{e_{vw}}h_w^t,$$

and Uₜ is a Gated Recurrent Unit (GRU) [8]. Similarly, Non-Local Neural Networks (NLNNs) can be conceptualized as capturing non-local information through functions that compute relationships between distant nodes [8].

Another extension, Network in Graph Neural Network (NGNN), deepens MPNNs by inserting an additional neural network into the message passing function, allowing the “message” to be further processed before aggregation [12]. Graph Isomorphism Network (GIN) is another GNN variant that modifies the aggregation and combine steps, often using mean-pooling and adjusting the weight of center nodes with a parameter ε^(k) [24,30].

Beyond the MPNN umbrella, other novel GNN variants continue to emerge. One such variant is the Graph Transformer, which enhances its structural expressive power by incorporating structural encoding methods like shortest path encoding [33]. Another mentioned variant is the “Adaptive Kernel Graph Neural Network” [39], although detailed architectural specifics or a comprehensive discussion of its advantages and limitations are not provided in the digests. Furthermore, approaches like XGNN function as meta-models that operate atop existing GNN architectures, rather than being specific GNN variants themselves, showcasing a diverse ecosystem of graph machine learning models [32]. The concept of learnable graph sketches, or “graphots,” which compactly encode topological properties and node features, also highlights ongoing research into novel graph representations [38].

Graph Generative Networks (GGNs) represent a distinct class of GNNs focused on generating new graphs with specific attributes [9]. GGNs are designed to address the inherent challenges of modeling complex distributions on graphs, including the non-uniqueness of graph representations, their high dimensionality, and the intricate non-local dependencies within graph data, by recombining nodes and edges according to predefined rules [9].

While the general discussion of GNNs often includes various pooling methods crucial for obtaining graph-level representations, specific methods like SortPooling or DiffPool and their detailed impact on performance were not extensively covered in the provided digests. However, the basic principle of pooling for prediction is noted to utilize graph connectivity, particularly in simpler GNN architectures where initial embeddings might not be fully expressive. In models like GCNs, pooling often involves simple aggregation functions such as mean-pooling to combine neighbor information [24]. Future research in GNN variants and pooling mechanisms will likely continue to explore methods that balance expressive power, computational efficiency, and scalability, especially for large and complex real-world graph datasets.
## 4. Graph Construction and Structure Learning

**Graph Construction Methodologies**

| Methodology               | Description                                    | Key Advantages                                   | Key Disadvantages                                | Computational Cost          |
| :------------------------ | :--------------------------------------------- | :----------------------------------------------- | :----------------------------------------------- | :-------------------------- |
| **Predefined Graphs**     | Uses explicit domain knowledge and static relationships (e.g., fixed connectivity in transport) | High interpretability, straightforward, stable | Rigid, struggles with implicit/evolving relationships, lacks adaptability to diverse structures | Low (given)                 |
| **Learned Graphs**        | Infers/refines graph topology directly from data (e.g., adaptive adjacency matrices) | Uncovers latent relationships, task-specific, enhances robustness/generalization | Higher complexity, increased computational overhead, can reduce interpretability | Medium to High              |
| **Dynamic Graphs**        | Incorporates temporal information; relationships and node states evolve over time (e.g., real-time traffic) | Captures transient interactions, adapts to evolving dependencies, high accuracy for time-sensitive data | Highest computational expense, often O(n^2) for spatial dependency weights, scalability challenges | High                        |

The efficacy of Graph Neural Networks (GNNs) is profoundly influenced by the quality and appropriateness of their underlying graph structures. In many real-world applications, the availability, explicit nature, and static properties of graph structures vary significantly, posing a fundamental challenge in applying GNNs [21,26]. This section systematically categorizes graph construction methodologies into three primary approaches: predefined, learned, and dynamic [14]. Each approach offers distinct advantages and disadvantages, necessitating a careful consideration of trade-offs between interpretability, computational complexity, and the ability to accurately represent complex, evolving relationships.

Predefined graphs leverage explicit domain knowledge and observable static relationships to establish connections between entities. This method is straightforward and can provide high interpretability when the underlying relationships are well-understood and relatively stable, such as fixed connectivity in transportation networks or explicit social links [21]. The inherent challenge, however, lies in their rigidity; predefined graphs often struggle to capture implicit, latent, or time-varying relationships, and they may not adapt well to diverse structural properties, such as varying sparsity or density [14,18]. This limitation can lead to suboptimal performance when the true underlying graph structure is not fully known or is subject to change.

To overcome the constraints of fixed structures, particularly when explicit graph information is incomplete, noisy, or unavailable, significant research has focused on learned graphs, also known as graph structure learning [14,21]. This paradigm aims to automatically infer the graph topology or refine existing structures directly from data, thereby uncovering hidden dependencies and enhancing the GNN's robustness and generalization capabilities [29]. Techniques range from adaptively learning adjacency matrices, as seen in Graph WaveNet where an adaptive matrix 
$$
\mathbf{A}_{\text{adp}} = \text{softmax}\big(\text{ReLU}(\mathbf{E}_1\mathbf{E}_2^\top)\big)
$$ 
captures dependency weights, to learning residual graph Laplacians with AGCN, and employing graph generators or masked connection strategies like SAGA [14,26,30,32,39]. While learned graphs offer superior flexibility and can adapt to task-specific requirements, they typically incur higher computational costs due to the additional complexity of the learning process, which can be a limiting factor for large-scale datasets [14].

Dynamic graph construction addresses systems where relationships and node states evolve over time, such as real-time traffic networks where connections and their strengths are highly variable [40]. This approach involves incorporating temporal information into the graph structure and employing mechanisms, often attention-based, to learn and adapt to these evolving dependencies [14,30]. For instance, models like GaAN and ASTGCN utilize attention mechanisms to dynamically update edge weights based on instantaneous node features or to capture both spatial and temporal dependencies [14]. The primary advantage of dynamic graphs is their ability to capture transient interactions, leading to improved accuracy in time-sensitive applications. However, this adaptivity comes at the highest computational expense, frequently involving an 
$$
O(n^2)
$$ 
complexity for computing spatial dependency weights among $n$ nodes, posing scalability challenges for very large graphs [14].

The overarching trade-off among these approaches revolves around the balance between model fidelity, interpretability, and computational efficiency. Predefined graphs offer simplicity and interpretability but lack the adaptability for complex or evolving data. Learned graphs provide enhanced accuracy by uncovering latent structures but introduce computational overhead and can sometimes reduce interpretability. Dynamic graphs offer the most sophisticated modeling of time-varying systems, yielding high fidelity for complex scenarios, yet they demand the greatest computational resources. Future research directions in graph construction and structure learning include developing more efficient and scalable algorithms for learned and dynamic graphs, improving the interpretability of autonomously derived structures, and exploring hybrid approaches that judiciously combine domain knowledge with data-driven learning to leverage the strengths of each method while mitigating their respective weaknesses [27,29].
### 4.1 Predefined Graphs
The construction of graph structures in Graph Neural Networks (GNNs) often leverages domain knowledge to define explicit relationships between entities. This approach is prevalent in many applications, as noted by research emphasizing that the majority of selected papers in certain domains, such as Spatio-Temporal Graph Neural Networks, focus on predefined graph structures when available [21]. The primary rationale behind utilizing these predefined structures, which are often derived from expert insights or observed static relationships, is to extract maximum information and enhance model interpretability [21]. By encoding known relationships, such as connectivity in transportation networks or dependencies in social graphs, GNNs can directly incorporate valuable structural information, thereby facilitating more effective feature propagation and aggregation.

However, relying solely on predefined graphs introduces significant limitations, particularly when attempting to capture the inherent complexity and dynamic nature of real-world relationships. The structure of real-world graphs exhibits considerable variability across different types of data; for instance, some graphs might be sparse with many nodes but few connections, while others are dense with extensive interconnections [18]. A static, predefined graph struggles to adapt to these diverse structural properties, especially when relationships are implicit, evolving, or subject to contextual changes. Such fixed structures may fail to accurately represent complex interactions that are not immediately evident from domain knowledge or that change over time. Consequently, predefined graphs can hinder a GNN's ability to learn from dynamic or latent relationships, potentially limiting its performance in scenarios where the true underlying graph structure is fluid or difficult to explicitly define.
### 4.2 Learned Graphs / Graph Structure Learning
The increasing interest in Graph Neural Networks (GNNs) has highlighted a critical challenge: the performance of GNNs heavily relies on the quality of the input graph structure. In many real-world applications, an explicit or accurate graph structure may be unavailable, incomplete, or noisy. This has spurred significant research into techniques for learning graph structures directly from data [14,21]. These learned graphs can potentially uncover latent relationships, adapt to specific tasks, and overcome the limitations of predefined or fixed graph structures, which often suffer from cross-graph alignment issues and poor generalization to unseen data [29].

Various techniques have been proposed to learn graph structures, primarily focusing on generating or refining adjacency matrices or underlying graph topologies. One prominent approach involves adaptively learning the adjacency matrix to capture dependencies. For instance, Graph WaveNet proposes an adaptive adjacency matrix to perform graph convolution, effectively learning a potential static graph structure from data [14]. This adaptive matrix, denoted as 
$$
A_{adj} = \operatorname{SoftMax}(\operatorname{ReLU}(E_1E_2^T))
$$
where \(E_1\) represents source node embeddings and \(E_2\) represents target node embeddings. The multiplication of \(E_1\) and \(E_2\) captures the dependency weights between source and target nodes, and the SoftMax function is applied along the row dimension [14]. Graph WaveNet demonstrates strong performance even without a predefined adjacency matrix, utilizing a complex Convolutional Neural Network (CNN)-based spatio-temporal architecture to learn these dependencies [14].

Another method, AGCN, learns a "residual" graph Laplacian that is subsequently added to the original Laplacian matrix [26]. This allows the model to refine or adjust an existing graph structure or learn structural perturbations. The SAGA approach offers a distinct method for learning graph structures by manually masking connections on multiple adjacent matrices [39]. By forcing a structural information embedding sub-network to recover the true adjacency matrix, SAGA compels the network to selectively exploit higher-order discriminative features from the data [39]. Beyond direct adjacency matrix learning, other techniques focus on generating graphs. For example, the XGNN approach falls under the category of learning graph structures by training a graph generator to create graphs that highlight the decision-making process of a GNN model, which is particularly useful for model interpretability [32].

The adoption of learned graphs presents several advantages over relying solely on predefined graphs. Learned structures can uncover hidden relationships in data, make GNNs more robust to missing or noisy structural information, and enhance generalizability across different instances or datasets. This ability to adapt is crucial when explicit graph structures are not readily available or are suboptimal for a given task [21]. However, these benefits often come with trade-offs, particularly concerning computational complexity and accuracy. Learning a graph structure, especially for large-scale datasets, can introduce significant computational overhead during training. Models like Graph WaveNet, while performing well, rely on complex underlying architectures (e.g., CNN-based spatio-temporal neural networks), which contribute to their computational demands [14]. Researchers are actively exploring methods to mitigate this complexity, such as developing efficient mini-batch training methods at the sub-graph level to enable parallel training and satisfy memory constraints for large-scale networks [29]. Despite the increased complexity, the enhanced accuracy achieved by capturing intrinsic data dependencies often justifies the additional computational cost, making learned graph structures a promising direction for advancing GNN capabilities.
### 4.3 Dynamic Graph Construction
Dynamic graphs are crucial for modeling systems where relationships and states evolve over time, offering a significant advantage in capturing time-varying relationships and adapting to changing data patterns [40].

They achieve this by incorporating timestamps on nodes and edges, which represent their states during specific time periods [40].

Learning potential dynamic spatial dependencies within these graphs has been shown to improve model accuracy, particularly in applications like traffic networks where, for instance, the arrival time between two roads is contingent upon current traffic conditions [14].

Attention mechanisms play a pivotal role in constructing such dynamic graphs by enabling models to learn and adapt to these evolving dependencies [14].

For instance, the Graph Attention Aggregated Network (GaAN) leverages an attention mechanism to learn dynamic spatial dependencies using an RNN-based approach [14].

In GaAN, an attention function dynamically updates edge weights between connected nodes based on the current input from those nodes, effectively allowing the graph structure to adapt to instantaneous changes in node features [14].

Another notable example is Attention Spatial-Temporal Graph Convolutional Networks (ASTGCN), which employs both a spatial attention function and a temporal attention function within a CNN-based framework to capture dynamic spatial and temporal dependencies [14].

This dual attention mechanism allows ASTGCN to discern not only which spatial connections are most relevant at a given moment but also how these connections evolve across time [14].

Despite their effectiveness in modeling dynamic interactions, a common challenge associated with learning potential spatial dependencies using attention mechanisms is the computational cost. This often requires $O(n^2)$ complexity to compute spatial dependency weights between each pair of nodes in a graph with $n$ nodes, posing scalability issues for very large graphs [14].

Nevertheless, the ability of attention mechanisms to dynamically construct and refine graph structures based on transient information underscores their importance in advancing the capabilities of graph neural networks for complex time-series data.
## 5. Training and Optimization Techniques
The effective training and optimization of Graph Neural Networks (GNNs) are critical for their successful application across diverse tasks and scales. This section provides a comprehensive overview of the methodologies employed to guide GNN learning, enhance model performance, prevent overfitting, and address the computational challenges inherent in processing large-scale graph data.

The initial step in training any GNN involves defining an appropriate **loss function**, which serves as the objective for the learning process [7,26]. The choice of loss function is highly contingent on the specific GNN task, whether it is node classification, link prediction, graph classification, or regression. For instance, node classification typically employs Cross-entropy loss for labeled nodes, while regression tasks often utilize Mean Squared Error (MSE) [40]. More complex tasks, such as graph embedding and representation learning, necessitate specialized loss formulations to preserve intricate structural properties, often involving composite losses that capture both first-order and second-order proximities, or contrastive learning objectives that encourage separation between positive and negative samples. Furthermore, GNNs applied to discrete optimization problems, like Max-k-Cut, can directly incorporate the problem's objective as a relaxed continuous loss, or even leverage reward functions within reinforcement learning frameworks for generative tasks like graph explanation [22,32].

Once a loss function is established, **optimization algorithms** are employed to iteratively adjust the GNN's parameters to minimize this loss, a process akin to backpropagation in other deep learning models [4,35]. Common gradient-based optimizers like Adam and Stochastic Gradient Descent (SGD) are widely used for GNN training, alongside adaptive learning rate methods [24,28]. In specialized applications, such as training graph generators for model explanations, reinforcement learning approaches, including policy gradient methods, are utilized to optimize the generation policy based on a desired reward [25,32]. The selection of an optimizer affects training efficiency, convergence properties, and the ability to find optimal solutions.

To enhance the generalization capabilities and prevent overfitting, various **regularization techniques** are applied to GNNs [29]. These methods constrain model complexity or introduce robustness during training, encompassing techniques such as dropout, weight decay, and architectural innovations like skip or residual connections (e.g., in Highway networks) [4,8,24,28,40]. Beyond conventional dropout, graph-specific methods like "DropGNN" introduce random node dropping to enhance expressiveness and prevent over-reliance on specific graph structures [29]. Additionally, regularization can manifest as enforcing structural constraints, as seen in methods that ensure the generation of valid graphs [25,32].

Finally, a significant challenge in GNN research is **training on large-scale graphs**, which often exceed available memory and present computational bottlenecks due to their inherent structure and irregular data access patterns [26,29]. Original GCN methods, for instance, face limitations due to their reliance on the full graph Laplacian and the exponential growth of receptive fields, leading to high computational complexity and difficulties with inductive learning [8,20]. To overcome these challenges, various strategies have been developed, predominantly involving sampling techniques. Approaches like GraphSAGE, FastGCN, StoGCN, and Cluster-GCN utilize neighbor sampling or graph clustering to process subsets of the graph, thereby reducing memory and computational load [30,39]. While these methods significantly improve efficiency, they introduce an approximation that can impact accuracy. Complementary to sampling, system-level advancements including data management, distributed training strategies, and hardware-aware optimizations such as data tiering and adaptive graph partitioning, are crucial for minimizing CPU-GPU communication overhead and efficiently utilizing hardware resources [2,29,39]. These combined efforts aim to enable the scalable and efficient deployment of GNNs for real-world large-scale applications.
### 5.1 Loss Functions
The choice of an appropriate loss function is paramount in training Graph Neural Networks (GNNs), as it directly dictates the objective of the learning process and influences the suitability of the model for specific graph‐related tasks. Different GNN applications, ranging from node classification to graph clustering, necessitate distinct loss formulations to effectively capture the desired properties of the graph data and guide model optimization.

For **node-level classification tasks**, particularly in semi-supervised settings, the Cross-entropy loss is a widely adopted standard. It is applied to the labeled nodes within the training set, enabling the GNN to learn a probabilistic distribution over classes for each node [7,26]. This loss function is particularly effective for multi-class classification problems, promoting high confidence in the correct class while penalizing incorrect predictions.

In scenarios involving **regression or prediction tasks** where the output is continuous, such as time series forecasting or traffic flow prediction, the Mean Squared Error (MSE) is a commonly employed loss function [21,40]. MSE quantifies the average squared difference between the predicted and actual continuous values, driving the model to minimize prediction errors and achieve higher accuracy in numerical estimations.

For **graph embedding and representation learning**, which serve as foundational steps for various downstream tasks like link prediction and clustering, more specialized loss functions are developed to capture the intricate structural properties of graphs. Models like SDNE utilize a composite loss function to preserve both first-order and second-order proximities. The first-order loss is defined as
  L₁ₛₜ = Σ₍v,u₎∈E Aᵥ,ᵤ ‖enc(𝚋𝚕𝚘𝚗𝚍𝚛𝚊𝚝 𝑥ᵥ) − enc(𝚋𝚕𝚘𝚗𝚍𝚛𝚊𝚝 𝑥ᵤ)‖²,
which aims to ensure that directly connected nodes have similar embeddings. Concurrently, the second-order loss is expressed as
  L₂ₙd = Σᵥ∈V ‖(dec(enc(𝚋𝚕𝚘𝚗𝚍𝚛𝚊𝚝 𝑥ᵥ)) − 𝚋𝚕𝚘𝚗𝚍𝚛𝚊𝚝 𝑥ᵥ) ⊙ 𝚋𝚕𝚘𝚗𝚍𝚛𝚊𝚝 𝒃ᵥ‖²,
where 𝒃ᵥ is a weighting vector, focusing on reconstructing the original node features from their learned embeddings, thereby preserving the structural context around each node [30].

Another widely used paradigm in representation learning, particularly for **link prediction** and unsupervised learning, involves contrastive learning principles. GraphSage, an inductive graph embedding framework, employs a negative sampling-based loss function:
  L(𝒛ᵥ) = -log(dec(𝒛ᵥ, 𝒛ᵤ)) - 𝔼₍ᵥₙ∼Pₙ(v)₎ .
This formulation encourages the embedding of a node 𝒛ᵥ to be close to its true neighbor 𝒛ᵤ while being pushed away from negatively sampled non-neighbors 𝒛ᵥₙ [30]. This approach effectively learns embeddings that capture the likelihood of connections between nodes.

Beyond proximity preservation, **reconstruction errors** are also fundamental in learning effective graph representations. DRNE, for instance, defines its reconstruction error as
  L = Σᵥ∈V ‖𝒛ᵥ − LSTM({𝒛ᵤ ∣ u ∈ N(v)})‖²,
where the goal is to reconstruct a node's embedding from a sequence of its neighbors' embeddings processed by an LSTM network [30]. Similarly, NetRA utilizes a general reconstruction loss,
  L = -𝔼₍𝒛∼P₍data₎(𝒛)₎ ,
which seeks to minimize a distance metric between the original node embedding and its reconstructed version [30]. These reconstruction-based losses are crucial for autoencoder-like GNN architectures.

For **graph clustering or partitioning problems**, such as the Max-k-Cut problem, the problem objective itself can be directly formulated as the loss function. The discrete Max-k-Cut problem originally stated as
  min₍𝐗 ∈ 𝒳₎ f(𝐗; 𝐖) ≔ Tr(𝐗𝐖𝐗ᵀ)
is often relaxed into a continuous optimization problem
  min₍𝐗 ∈ Δₖᴺ₎ f(𝐗; 𝐖)
for practical optimization using GNNs [22]. In this context, the trace function serves as the direct optimization target, guiding the GNN toward a partition that minimizes the cut value.

Furthermore, in advanced applications like **graph generation or explanation**, GNNs may be trained using **reward functions** within a reinforcement learning framework, rather than traditional gradient-based loss minimization. For example, the XGNN approach leverages a reward signal derived from the GNN's predictions to train a graph generator. The objective is to synthesize graphs that maximize a specific GNN prediction, thereby providing interpretable model-level explanations by identifying critical graph structures [32]. This paradigm shifts from minimizing prediction error to maximizing a desired outcome, offering flexibility for complex, goal-oriented graph tasks.

In summary, the selection of a loss function in GNNs is highly task-dependent. While standard losses like Cross-entropy and MSE suffice for classification and regression, respectively, more sophisticated tasks such as graph embedding, link prediction, and clustering necessitate specialized loss functions tailored to preserve graph structural properties or solve discrete optimization problems. Moreover, emerging applications like graph generation increasingly leverage reward-based learning objectives.
### 5.2 Optimization Algorithms
The training of Graph Neural Networks (GNNs) and their auxiliary components often necessitates the application of sophisticated optimization algorithms to ensure effective model convergence and performance. While a wide array of optimization strategies exists across machine learning, specific methods are sometimes adapted for the unique structural and learning challenges posed by graph-structured data. For instance, in the context of generating graph explanations for GNNs, a reinforcement learning approach has been leveraged. Specifically, a policy gradient method, a common technique within reinforcement learning, is employed to train the graph generator [32]. This method facilitates the iterative refinement of policies that dictate graph generation, aiming to produce explanatory graphs. A comprehensive comparative analysis of the convergence properties and computational efficiency of various optimization algorithms, such as stochastic gradient descent variants, adaptive learning rate methods, or reinforcement learning approaches like policy gradients, across diverse GNN architectures and tasks, is crucial for understanding their practical implications. Such an analysis would typically involve evaluating metrics like training time, stability, and the rate at which algorithms reach an optimal or near-optimal solution. However, the detailed exploration of these comparative aspects, particularly concerning the convergence and efficiency of policy gradient methods versus other optimization paradigms in GNN contexts, warrants further dedicated investigation.
### 5.3 Regularization Methods
Regularization methods are crucial for enhancing the generalization performance of Graph Neural Networks (GNNs) by mitigating overfitting and improving model robustness, thereby enabling them to perform effectively on unseen data [29]. These techniques aim to constrain the model's complexity or introduce noise during training, preventing the network from memorizing training data and encouraging it to learn more transferable features.

One prevalent regularization technique involves the use of skip or residual connections. These connections, such as those found in Highway networks, are employed to address challenges associated with increasing network depth in GNNs [4,8]. Specifically, they help mitigate the issue of exponential growth in the receptive field, ensuring effective information propagation across many layers and preventing vanishing or exploding gradients [8]. An illustrative example of a Highway network's update rule is given by:
$$
T(h^t) = \sigma(W^t h^t + b^t)
$$
$$
h^{t+1} = h^{t+1} \odot T(h^t) + h^t \odot \bigl(1 - T(h^t)\bigr)
$$
where $h^t$ represents the activation at layer $t$, and $T(h^t)$ is a transform gate that controls the flow of information [8]. By allowing information to bypass intermediate layers, skip connections enable the training of deeper GNNs without degradation, thus fostering the learning of more complex and generalizable representations.

Another widely adopted regularization strategy is dropout, which is particularly effective in reducing overfitting in GNN models by randomly deactivating neurons during training [40]. Standard applications involve applying a dropout ratio, for instance, of 0.5 after dense layers, as seen in some GNN architectures [24]. Beyond conventional dropout, specialized GNN adaptations have emerged, such as DropGNNs. This method involves executing multiple GNN runs, each with random and independent node dropping on the input graph [29]. This approach is designed to enhance the expressiveness of GNNs and overcome limitations inherent in standard GNN frameworks, thereby improving their ability to generalize by preventing over-reliance on specific nodes or substructures [29].

Furthermore, certain regularization forms are tailored to the graph structure itself. For instance, the XGNN approach incorporates specific graph rules to encourage the generation of valid graphs [25,32]. This mechanism acts as a form of regularization by constraining the output space to plausible or meaningful graph structures, which can be critical for tasks like model-level explanations where structural validity is paramount [32]. By enforcing these structural constraints, the model is guided towards learning more robust and contextually appropriate representations, thereby improving its generalization capability to new, structurally similar data.

In summary, regularization methods in GNNs encompass a diverse set of techniques. From architectural enhancements like skip connections that facilitate deeper architectures and better information flow, to stochastic methods like dropout that prevent overfitting and promote robust feature learning, and even domain-specific structural constraints, these strategies empower GNNs to learn more generalizable and transferable representations, ultimately leading to improved performance on unseen data and increased model robustness.
### 5.4 Training on Large Graphs
Training Graph Neural Networks (GNNs) on large-scale graphs presents significant computational challenges, primarily due to memory constraints and the inherent structure of graph data. A graph is often considered large-scale when its adjacency matrix or graph Laplacian cannot be efficiently stored and processed by available hardware, particularly limiting GPU memory capacity [26,29]. These limitations, coupled with irregular data access patterns common in graph-based data structures, and high CPU-GPU communication overhead, constitute major computational bottlenecks [29]. Addressing these issues necessitates methods that balance computational efficiency with model accuracy.

One prominent solution involves the adoption of various sampling techniques, which enable GNNs to operate on subsets of the graph rather than the entire structure, thereby reducing memory consumption and computational load [7,18]. GraphSAGE is a foundational approach in this category, enhancing efficiency by not involving the entire graph in each node's computation, instead sampling only a subset of neighbors for aggregation [8]. This method replaces the full graph Laplacian with learnable aggregation functions and employs neighbor sampling to mitigate the receptive field expansion issue, which can lead to exponentially increasing memory requirements with deeper GNN layers [20]. GraphSAGE further contributes to memory reduction through a batch processing algorithm for ConvGNNs, where it recursively expands the neighbors of a root node, sampling a fixed number of samples at each step, and aggregates node representations from bottom to top within each sampling tree [30]. Some GraphSAGE variants assign importance weights to sampled neighbors, often proportional to the inverse of their degree, as shown by the formula: 

$$q(v) \propto \frac{1}{|\mathcal{N}_v|} \sum_{u \in \mathcal{N}_v} \frac{1}{|\mathcal{N}_u|}$$

[8].

Building upon the concept of sampling, other methods offer distinct strategies. FastGCN, for instance, samples a fixed number of nodes for each graph convolution layer, reinterpreting graph convolution as an integral transformation of the node embedding function under probabilistic measurement [30]. StoGCN addresses the receptive field challenge by using historical node representations as control variables, allowing for the reduction of the graph convolution’s receptive field to an arbitrarily small scale [30]. Cluster-GCN employs a graph clustering algorithm to partition the large graph into subgraphs, performing graph convolution on the nodes within these sampled subgraphs [30]. While these sampling-based methods significantly improve computational efficiency and memory footprint, they inherently introduce an approximation by not utilizing the entire graph structure, which can potentially impact the final model accuracy depending on the sampling strategy and graph properties.

Beyond sampling, advancements in data management and hardware-aware optimization provide complementary solutions. The "Graph Neural Network Training with Data Tiering" method directly tackles GPU memory limitations and irregular data access patterns by statistically analyzing and identifying frequently accessed data before GNN training [29]. This approach leverages the input graph's structure and insights from the GNN training process to optimize data placement and access, effectively minimizing CPU-GPU communication overhead and even supporting multi-GPU training. Crucially, this method aims to achieve higher prediction results, suggesting an improvement in efficiency without sacrificing accuracy [29]. Similarly, the Adaptive Graph Partition (AGP) strategy proposes hardware-aware techniques such as edge leveling, interval interleaving task scheduling, and source node caching to alleviate memory bottlenecks and eliminate overhead from graph repartitioning between GNN layers [2]. These system-level optimizations focus on streamlining data flow and computation, complementing algorithmic advancements in large-scale GNN training.
## 6. Advanced Topics in GNN Research
The field of Graph Neural Networks (GNNs) has rapidly expanded beyond fundamental architectures and training methodologies, with recent advances focusing on specialized topics that address key challenges and unlock broader applications. These emerging trends are crucial for enhancing GNN capabilities, ensuring their responsible deployment, and expanding their utility in complex real-world scenarios, marking a transition from foundational concepts to the frontiers of GNN research [3,33].

A significant area of focus is **GNN interpretability and trustworthiness**. Addressing the "black box" nature of GNNs, research in **Explainability of GNNs** aims to provide insights into how models arrive at specific decisions, thereby building user trust, facilitating debugging, and ensuring accountability in critical applications [21]. This involves developing techniques for both instance-level (explaining individual predictions) and model-level explanations (understanding general decision mechanisms) [25]. Complementing this is the critical endeavor of **Fairness in GNNs**, which seeks to mitigate biases and ensure equitable outcomes, especially in sensitive domains. This research distinguishes between group and individual fairness, exploring methods to jointly optimize model utility and fairness while navigating the complexities of bias propagation within graph structures [5].

Another major trend involves methods that enhance data efficiency and enable the creation of novel graph structures. **Self-Supervised Graph Representation Learning** is rapidly gaining prominence by generating supervisory signals directly from data, significantly reducing the reliance on costly human annotations and leading to more robust graph embeddings [3,15]. This paradigm often leverages innovative approaches to data augmentation and, in some cases, avoids the explicit need for negative examples, contributing to more efficient learning [33]. Concurrently, the domain of **Graph Generation and Transformation** is vital for synthesizing new graph structures or modifying existing ones, with applications ranging from drug discovery to providing model-level explanations by identifying influential graph patterns [18,30,32,39].

Furthermore, GNN research is extending to handle increasingly complex and dynamic graph data. **Dynamic Graph Neural Networks (DGNNs)** are specifically designed to model evolving relationships and characteristics in graphs over time, tackling challenges such as non-stationary data and long-term dependencies through architectures like Recurrent Neural Networks (RNNs) and Temporal Convolutional Networks (TCNs) [3,33]. The fundamental problem of **Graph Matching**, which involves finding structural correspondences between graphs, continues to be a crucial area, with modern GNN-based approaches offering scalable solutions for approximate matching in diverse fields like computer vision and social network analysis [3]. Lastly, **Multi-View GNNs** integrate information from distinct perspectives or "views" of graph data, aiming to produce more comprehensive and robust representations by synthesizing complementary information and mitigating the inherent limitations of relying solely on a single, potentially incomplete, data view [27].

Together, these advanced topics represent the cutting edge of GNN research, pushing the boundaries of what GNNs can achieve and paving the way for their successful deployment in an ever-widening array of sophisticated applications, addressing limitations and expanding the utility of GNN variants [4].
### 6.1 Explainability of GNNs
The inherent complexity of Graph Neural Networks (GNNs), often characterized as "black box" models, necessitates robust explainability methods, particularly in critical applications where trust, transparency, and accountability are paramount [3,21]. Explainability in GNNs aims to provide insights into how a model arrives at a specific prediction or decision, thereby enhancing user comprehension and facilitating model debugging and improvement. Explanation techniques for GNNs can be broadly categorized based on their scope: instance-level (or node/subgraph-level) explanations and model-level explanations.

Instance-level Explanations focus on elucidating why a specific prediction was made for a given input instance, typically by highlighting the most influential nodes, edges, or subgraphs. A prominent example is GNNExplainer, which frames the problem as identifying the most relevant subgraph crucial for a particular task [11]. It leverages attribution techniques to assign importance values to different components of the input graph, thereby revealing the parts most relevant to the GNN's output. The strength of GNNExplainer lies in its ability to provide localized, highly interpretable explanations by presenting concrete subgraphs directly linked to a prediction. Another significant approach is DEGREE, a decomposition-based explanation method for GNN predictions [33]. DEGREE offers reliable explanations by meticulously decomposing the information generation and collection mechanisms within GNNs. This allows for precise tracking of how the input graph contributes to the final prediction, and its subgraph-level explanation algorithm can uncover previously overlooked features, enhancing the depth and trustworthiness of the instance-level insights [33].

In contrast, Model-level Explanations aim to provide a more holistic understanding of the GNN's overall decision-making logic and common patterns that govern its predictions, rather than focusing on a single instance. XGNN exemplifies this category by generating graph patterns that reveal the general mechanisms by which the GNN makes predictions [25]. XGNN achieves this by training a graph generator that produces graph patterns specifically designed to maximize certain model predictions [25]. This approach offers valuable insights into the model's inherent biases, learned heuristics, and the significant structural motifs it identifies across various inputs, providing a broader perspective on the GNN's behavior.

When comparing these techniques in terms of accuracy, efficiency, interpretability, and applicability:
• Interpretability: Instance-level methods like GNNExplainer and DEGREE offer high interpretability for specific predictions by pinpointing influential graph components, which is crucial for debugging and building trust in individual outcomes. XGNN, as a model-level explainer, provides interpretability at a higher abstraction, revealing general decision rules or critical structural patterns across the entire model's operation.
• Accuracy/Reliability: DEGREE explicitly emphasizes providing "reliable explanations" through its decomposition [33]. All methods strive for fidelity in their explanations, meaning the explanation should accurately reflect the GNN's reasoning process. XGNN's generative approach aims to accurately represent the patterns the GNN has learned.
• Applicability: Instance-level explainers are highly applicable for post-hoc analysis of specific GNN predictions, useful in scenarios requiring justifications for individual decisions (e.g., in medical diagnosis or fraud detection). Model-level explainers, such as XGNN, are more suitable for understanding the broader behavior of the GNN, identifying potential systematic flaws, or gaining insights into the features the model prioritizes generally, making them valuable for model development and global validation.
• Efficiency: The efficiency of these methods can vary significantly. Post-hoc attribution methods like GNNExplainer might be computationally less intensive during inference compared to training a generative model like XGNN, which requires an additional learning phase. DEGREE's decomposition process, while providing deep insights, may also incur computational costs depending on the complexity of the GNN and the graph size.

Despite considerable progress, explainability in GNNs remains an active and critical area of research [21]. Continuous efforts are needed to develop more accurate, efficient, and universally applicable explanation techniques that can effectively unveil the inner workings of increasingly complex GNN architectures.
### 6.2 Fairness in GNNs
Achieving fairness in Graph Neural Networks (GNNs) is a critical concern, particularly given their increasing deployment in sensitive applications. A fundamental distinction in this domain lies between group fairness and individual fairness [5]. Group fairness primarily focuses on ensuring equitable outcomes across different subgroups, often defined by protected attributes such as gender, age, or race [5]. This approach aims to prevent discriminatory impacts on entire demographic categories.

However, existing works have predominantly concentrated on group fairness, which may not fully address the intricacies of bias propagation within graph structures [5]. A significant challenge in GNN fairness is the necessity of ensuring fairness at the node level, moving beyond aggregate group statistics. This individual fairness perspective posits that similar individuals should receive similar predictions, irrespective of their group affiliations [5]. The inherent structure of graphs, where information propagates through connections, can exacerbate bias, making it crucial to consider fairness at the granular node level.

To promote individual fairness in GNNs, researchers have begun exploring specialized techniques. One such approach refines the notion of individual fairness from a ranking perspective, formulating it as a ranking-based individual fairness promotion problem [5]. An exemplary framework is REDRESS, which is designed to jointly maximize GNN model utility while simultaneously promoting this ranking-based individual fairness [5]. This framework facilitates end-to-end training, indicating an integrated approach to fairness optimization [5].

The pursuit of fairness often introduces a trade-off with model utility, where improving one aspect might compromise the other. Techniques like REDRESS explicitly address this by aiming for a joint optimization [5]. By enabling the simultaneous maximization of utility and fairness, such methods seek to balance these competing objectives, offering practical solutions for deploying fair and effective GNN models [5].
### 6.3 Self-Supervised Graph Representation Learning
Self-supervised learning (SSL) has emerged as a prominent paradigm for learning robust graph representations by generating supervisory signals directly from the data, thereby mitigating the reliance on extensive human-annotated labels [3]. Within this domain, various methodologies have been proposed, differing in their approach to generating self-supervisory tasks and handling implicit negative examples.

One notable method is Deep Graph Bootstrapping (DGB), which is inspired by the Bootstrapping Your Own Latent (BYOL) approach from computer vision [15]. DGB distinguishes itself by learning node embeddings effectively without leveraging negative examples, a common requirement in many contrastive learning frameworks [15]. This is achieved through an innovative architecture comprising two neural networks: an online network and a target network. The online network is tasked with predicting the output representation generated by the target network. Crucially, the target network's parameters are updated through a slow-moving average of the online network's parameters, rather than direct gradient descent [15]. This mechanism allows the networks to learn from each other in a stable manner, circumventing the need for explicit negative samples to prevent representational collapse—a challenge often addressed in contrastive learning.

In contrast to DGB's non-contrastive bootstrapping approach, other methods like ConGraT employ a self-supervised contrastive pretraining framework [33]. ConGraT focuses on learning joint graph and text embeddings by training distinct encoders to align their respective representations. Its training objective is extended to incorporate considerations of node similarities and predictions of subsequent states or guesses, which are characteristic elements of contrastive learning paradigms where positive and negative pairs are implicitly or explicitly constructed [33]. The fundamental difference lies in DGB's ability to learn through a predictive bootstrapping mechanism, avoiding the explicit construction or sampling of negative examples, which can be computationally intensive or challenging in graph contexts.

Data augmentation plays a critical role in enhancing the performance and robustness of self-supervised graph representation learning methods like DGB [15]. DGB leverages a diverse set of augmentation strategies to generate different views of the input graph, which are then used for the bootstrapping objective. These strategies encompass node augmentation techniques, such as node feature dropout and node dropout, which selectively mask or remove node attributes or entire nodes [15]. Additionally, adjacent matrix augmentation is employed, utilizing graph-specific operations like personalized PageRank (PPR) and heat kernel transformations to perturb the graph structure while preserving its underlying connectivity patterns [15]. The combination of these augmentation methods, either individually or in concert, has been shown to significantly improve the learned representations by encouraging the model to extract invariant features across different perturbed views of the same graph, thereby contributing to the effectiveness of the self-supervision signal [15].
### 6.4 Graph Generation and Transformation
The ability to generate and transform graphs is a crucial area within graph neural networks (GNNs), with applications spanning various domains, including drug discovery where novel molecular structures with desired properties are sought [18]. This field encompasses diverse methodologies for synthesizing new graph structures and modifying existing ones.

Approaches for graph generation primarily include generative adversarial networks (GANs) and variational autoencoders (VAEs), as indicated by the broader survey context [39]. A prominent method within this domain is the Graph Variational Autoencoder (GraphVAE), which aims to learn a latent representation of graphs and generate new ones by sampling from this space. GraphVAE optimizes the variational lower bound
$$
L(\phi, \theta; G) = \mathbb{E}_{q_\phi(z \mid G)}\bigl - \operatorname{KL}\Bigl(q_\phi(z \mid G)\,\Big\|\,p(z)\Bigr)
$$
In this formulation, the prior $p(z)$ is typically assumed to be Gaussian, and $\phi$ and $\theta$ represent the learnable parameters of the encoder and decoder, respectively [30]. The first term maximizes the reconstruction likelihood, while the second term regularizes the latent space distribution.

Beyond VAEs, reinforcement learning (RL) has emerged as a powerful paradigm for graph generation. In such frameworks, the graph generation process is formulated as a sequential decision-making problem, where a generator iteratively predicts how to add edges or nodes to the current graph structure [25]. An notable application of this approach is the XGNN method, which employs an RL-trained graph generator to produce graph structures that maximize the prediction of a pre-trained GNN. This specific application highlights how graph generation can be leveraged not just for novel graph synthesis but also for providing model-level explanations of GNN behavior by identifying influential graph patterns [32].

Complementary to graph generation, graph transformation focuses on modifying existing graphs. While the broader field covers techniques such as graph edit operations and graph-to-graph translation models, the provided digest content primarily indicates the general relevance of graph transformation in comprehensive GNN tutorials [3]. Specific detailed methodologies for graph transformation, beyond the conceptual mention, were not elaborated in the analyzed digests. This suggests an area that may warrant deeper exploration in future surveys.
### 6.5 Dynamic Graph Neural Networks
Dynamic Graph Neural Networks (DGNNs) are essential for modeling evolving relationships and characteristics within graph structures, offering robust solutions for scenarios where graph topology and features change over time [3]. These networks are designed to capture temporal dependencies alongside structural information. Common approaches for processing dynamic graphs often leverage architectures capable of handling sequential data, such as Recurrent Neural Networks (RNNs) and Temporal Convolutional Networks (TCNs). RNN-based models, including LSTMs and GRUs, are particularly adept at maintaining hidden states that evolve with the graph, thus capturing long-term temporal correlations. TCNs, on the other hand, utilize dilated convolutions to aggregate information across various temporal scales, offering advantages in parallelization and capturing dependencies over long sequences without the vanishing gradient issues sometimes associated with deep RNNs.

Training GNNs on dynamic graphs introduces several distinct challenges. A primary concern is dealing with non-stationary data, where the underlying data distribution and graph structure can change significantly over time. This non-stationarity requires models to continuously adapt and learn from new data, rather than relying on static assumptions. Furthermore, capturing long-term dependencies within evolving graph streams presents a considerable hurdle. As graphs change, the influence of past states or events on current predictions might diminish or become highly complex to model, necessitating mechanisms that can effectively retain and integrate historical information without being overwhelmed by its volume or noise.

Addressing these challenges, research has focused on developing sophisticated frameworks. For instance, a semi-supervised anomaly detection (SAD) framework has been proposed for dynamic graphs [33]. This framework is specifically designed to detect anomalies in evolving graph streams by effectively utilizing large-scale unlabeled samples. The ability of such a framework to process evolving graph streams and learn from abundant unlabeled data directly mitigates the difficulties posed by non-stationary data and the often-limited availability of labeled data in real-world dynamic environments. The successful application of SAD underscores the progress in developing GNN architectures that can robustly adapt to the continuous changes inherent in dynamic graph data, thereby capturing critical patterns and anomalies even when faced with significant data evolution.
### 6.6 Graph Matching
Graph matching, a foundational problem in various fields, seeks to find a correspondence between the nodes of two graphs that preserves their structural relationships. This essential topic is covered in the tutorial [3], highlighting its significance in the realm of graph neural networks.

Different algorithmic approaches exist for tackling the graph matching problem, varying in their exactness, computational complexity, and applicability. Among classic methods, the **Hungarian algorithm** stands out as an exact polynomial-time algorithm primarily designed for solving the assignment problem on bipartite graphs. It efficiently finds a perfect matching with the maximum weight in a bipartite graph, which can be adapted for graph isomorphism or subgraph isomorphism under certain conditions. While providing optimal solutions for its specific domain, its applicability to general, large-scale, or non-bipartite graph matching problems is limited due to computational constraints and its precise structural requirements.

In contrast to traditional, exact algorithms, modern approaches leverage deep learning paradigms, such as the **Graph Matching Network (GMN)**. GMNs represent a prominent example of neural network-based methods that learn to embed graphs and subsequently compute similarity or correspondence scores. These networks are typically designed to approximate solutions for general graph matching, often in scenarios where exact solutions are computationally prohibitive or unnecessary. GMNs learn robust features and a similarity metric directly from data, enabling them to handle graphs with varying sizes, noise, and structural complexities. Unlike the combinatorial optimization inherent in the Hungarian algorithm, GMNs operate within a differentiable framework, allowing for end-to-end learning and greater scalability for approximate matching tasks.

The applications of graph matching are extensive and crucial across multiple domains. In **computer vision**, graph matching is instrumental for tasks such as object recognition, image retrieval, and 3D shape correspondence. Here, objects, scenes, or their constituent features (e.g., keypoints, patches) are modeled as graphs, and graph matching techniques are employed to establish correspondences between these visual structures for classification, retrieval, or pose estimation. For instance, recognizing an object regardless of its orientation or scale can involve matching its graphical representation to a library of known object graphs. Similarly, in **social network analysis**, graph matching finds utility in identifying user profiles across different platforms, detecting bot networks, or discovering communities. By representing social interactions or user attributes as graphs, matching algorithms can uncover latent connections, identify malicious activities through structural anomalies, or merge incomplete profiles to build more comprehensive user representations. The ability to find structural similarities between graphs makes graph matching a powerful tool for analyzing complex, relational data in these diverse fields.
### 6.7 Multi-View GNNs
Multi-view Graph Neural Networks (GNNs) are designed to leverage information from multiple distinct perspectives or 'views' of graph data, aiming to enrich learning and improve model robustness. Within the broader domain of multi-view learning, approaches that utilize neural networks commonly focus on generating joint representations by integrating diverse data streams. This strategy has been shown to significantly enhance performance in various downstream tasks, including clustering [27]. For instance, a comprehensive survey on multi-view clustering highlights methods that employ neural networks for view fusion. One notable example, DeConFCluster, specifically utilizes Deep Convolutional Transform Learning (CTL) to effectively mitigate overfitting, thereby improving the robustness and accuracy of the learned representations in multi-view datasets [27]. While the precise integration strategies for combining multiple views within the GNN framework can vary (e.g., early, late, or hybrid fusion of different graph structures, semantic information, or attributes), the fundamental benefit remains consistent: by synthesizing complementary information from disparate views, multi-view GNNs can derive more comprehensive, resilient, and informative node or graph embeddings. This holistic integration facilitates a deeper understanding of complex graph relationships, addressing the inherent limitations that arise from relying solely on a single, potentially incomplete, data view. The core principle involves learning a unified representation that effectively captures shared underlying information across views while also preserving view-specific discriminative characteristics, ultimately leading to superior analytical outcomes and enhanced generalization capabilities.
## 7. Applications of Graph Neural Networks
Graph Neural Networks (GNNs) have emerged as a transformative paradigm for analyzing and extracting insights from complex, interconnected data, demonstrating pervasive applicability across a multitude of scientific and engineering domains [1,3,7,9,10,16,21,28,30,35]. Their intrinsic ability to model relational structures allows them to address diverse tasks, including node-level predictions (e.g., node classification, aggregation, representation learning), edge-level tasks (e.g., link prediction, edge classification, clustering), and graph-level analysis (e.g., graph classification, generation, similarity analysis) [1,9,18,28]. 

**Key Applications of Graph Neural Networks**

| Domain                    | Key Tasks Addressed by GNNs                               | GNN Advantage/Impact                                    | Challenges/Opportunities (GNN-Specific)                                                |
| :------------------------ | :-------------------------------------------------------- | :------------------------------------------------------ | :------------------------------------------------------------------------------------- |
| **Social Network Analysis** | User behavior/interest prediction, social influence, friend recommendation, social circle detection | Models complex relational data, captures nuanced social influence, higher-order relationships | Sophisticated node feature engineering, handling heterogeneity/multi-view data, sentiment/fake news detection |
| **Recommendation Systems**| User-item interaction modeling, link prediction, cold-start scenarios, personalized recommendations | Models intricate user-item interactions, captures high-order patterns, leverages social relations | Integrating diverse user data sources (multi-modal, social connections)                  |
| **Time Series Forecasting**| Traffic flow prediction, urban planning, spatio-temporal event forecasting                  | Captures variable interdependencies & temporal dynamics, embeds regional info          | Need for more comparative analyses, detailed discussions on challenges                  |
| **Computer Vision**       | Scene graph generation, point cloud classification, action recognition in videos          | Semantic understanding, models complex relationships between visual entities, leverages structural context | Leveraging structural context beyond pixel-level processing                            |
| **Natural Language Processing** | Text classification, syntactic/semantic relationship modeling, knowledge-enhanced pre-training | Captures non-linear dependencies, utilizes explicit graph structures (e.g., dependency trees) | Optimizing large-scale models, ensuring robust performance with limited data          |
| **Bioinformatics**        | Molecular property prediction, drug discovery, protein function/interaction prediction, multi-omics integration | Leverages inherent graph nature of molecules/proteins, computational advantages for large-scale analysis | Complexity/size of biological graphs, integrating diverse/noisy multi-modal data        |
| **Combinatorial Optimization**| Max-k-Cut, graph coloring, path optimization (traffic/logistics) | Learns sophisticated heuristics directly from graph structures, improved solution quality/efficiency | Adapting learned decision-making across diverse graph structures                         |
| **Knowledge Graph Reasoning**| Entity classification, relation prediction, enhancing knowledge graph embeddings         | Enables sophisticated analysis of entities & relationships, enriches representational quality | Reducing semantic distances between related entities, improving reasoning capabilities   |

This section provides a comprehensive overview of GNN applications, organized by domain, highlighting their advantages over traditional methodologies, the key tasks they address, and the persistent challenges and future opportunities within each field.

In **Social Network Analysis**, GNNs are particularly adept at modeling complex relational data to predict user behavior or interests and capture nuanced social influence by representing users and their interactions as graphs [10,16,17]. Key tasks include social circle detection, user behavior prediction, and friend recommendation. GNNs offer a significant advantage by moving beyond simple statistical models to capture higher-order relationships. However, challenges remain in sophisticated node feature engineering and handling the heterogeneous and multi-view nature of social network data [24,27]. Future research can explore integrating richer auxiliary information and developing GNN-specific solutions for complex tasks such as sentiment and fake news detection, where general deep learning techniques are currently more prevalent [31].

**Recommendation Systems** greatly benefit from GNNs' capacity to model intricate user-item interactions, often formalized as link prediction on bipartite graphs [1,10,30]. By representing users and items as nodes, GNNs can capture high-order interaction patterns and leverage social relations, leading to enhanced accuracy and personalization, especially in cold-start scenarios [17]. This contrasts with traditional matrix factorization methods by integrating richer relational context. The primary opportunities lie in further integrating diverse user data sources, such as multi-modal information and social connections, to provide even more personalized and robust recommendations [27].

For **Time Series Forecasting and Classification**, Spatio-temporal Graph Neural Networks (STGNNs) are uniquely positioned to capture both variable interdependencies and temporal dynamics, making them ideal for complex systems like traffic flow prediction [21]. GNNs, particularly Graph Convolutional Networks (GCNs), embed regional information directly into the graph structure, which aids in tasks like alleviating traffic congestion [28,40]. While STGNNs demonstrate promising capabilities, there is a need for more comparative analyses of different STGNN models and detailed discussions on their overarching challenges and future research directions in this domain.

In **Computer Vision**, GNNs enable a more semantic understanding of visual data by modeling complex relationships between entities within images and videos [3]. They facilitate edge-level inference and excel in tasks like scene graph generation, where objects are nodes and their relationships are edges, providing a structured semantic summary [1,18]. Other applications include point cloud classification and action recognition in videos, where GNNs effectively extract relational information from diverse visual modalities [1,30]. The advantage of GNNs lies in their ability to leverage structural context, moving beyond pixel-level processing.

**Natural Language Processing (NLP)** leverages GNNs to model intricate, non-linear dependencies within text, extending beyond traditional sequential processing [39]. GNNs are adept at capturing syntactic and semantic relationships, utilizing explicit graph structures like dependency trees, which significantly enhances tasks such as text classification [1,30]. The integration of knowledge-enhanced pre-training strategies further empowers GNNs to improve understanding and performance, especially in scenarios with scarce training data [34]. Challenges involve optimizing large-scale models and ensuring robust performance with limited data.

**Bioinformatics** applications of GNNs exploit the inherent graph-structured nature of molecules, proteins, and biological networks [3]. GNNs are highly effective for molecular property prediction (e.g., toxicity, odor) and drug discovery, by representing atoms as nodes and bonds as edges [1,10,18]. They are also crucial for protein function prediction, interaction analysis, and integrating multi-omics data for disease subtype identification [1,3,27]. GNNs offer computational advantages for large-scale analysis, such as predicting material properties like lattice phonon vibrational frequencies, which are traditionally demanding [29]. Key challenges include the complexity, size, and variability of biological graphs, as well as the integration of diverse and noisy multi-modal data.

In **Program Analysis**, GNNs are applied for tasks such as program verification and reasoning, representing code logic, data flow, or control flow as graphs [1,3]. The primary advantage lies in automating complex analysis tasks and identifying program errors or security vulnerabilities. However, the field requires deeper exploration into comparative performance across different program representations (e.g., Control Flow Graphs, Abstract Syntax Trees) and specific models' precision in pinpointing distinct classes of software defects. Scaling GNN models to handle large, complex codebases while maintaining interpretability remains a significant challenge.

For **Combinatorial Optimization**, GNNs provide a promising approach for NP-hard problems by learning sophisticated heuristics directly from graph structures, bypassing the limitations of hand-crafted heuristics [22]. Applications include the Max-k-Cut problem, graph coloring, and various path optimization problems in areas like traffic and logistics [22,39,40]. GNNs offer the potential for improved solution quality and computational efficiency compared to traditional exact or heuristic algorithms, particularly for large instances, by adapting learned decision-making across diverse graph structures.

**Knowledge Graph Reasoning** utilizes GNNs to enable sophisticated analysis of entities and relationships, supporting core tasks such as entity classification and relation prediction [16]. GNNs enhance the expressiveness and utility of knowledge graph embeddings (KGEs), with techniques like contrastive learning (e.g., KGE-CL) reducing semantic distances between related entities and relations, thereby enriching representational quality [39].

Beyond these established areas, GNNs are finding success in **Other Emerging Applications**, including spatio-temporal prediction in urban contexts (e.g., cellular traffic, urban event forecasting) [3,21], security and anomaly detection (e.g., encryption traffic classification, adversarial attack prevention) [1,3,21], and biomedical/healthcare informatics (e.g., brain networks, personalized medicine through multi-view clustering) [1,27]. These diverse applications highlight GNNs' versatility in modeling complex relationships across various data types and domains, underscoring their capacity to extract meaningful insights from interconnected data and drive advancements in fields requiring deep structural understanding and predictive capabilities [1].

Across these domains, common advantages of GNNs include their ability to intrinsically capture relational inductive biases, leverage structural information, and generalize across graph instances. However, persistent challenges include developing more robust and interpretable models, ensuring scalability for very large and dynamic graphs, improving generalization to out-of-distribution graphs, and addressing the complexities of integrating multi-modal and heterogeneous data effectively. Future research will likely focus on developing advanced GNN architectures that can handle greater data complexity, enhance model explainability, and seamlessly integrate with other AI paradigms to unlock even more sophisticated applications.
### 7.1 Social Network Analysis
Graph Neural Networks (GNNs) have emerged as a powerful paradigm for addressing a diverse range of tasks within social network analysis due to their inherent ability to model complex relational data. GNNs can be effectively applied to predict user behavior or interests [16], and capture nuanced social influence within social networks by modeling intricate user-user social relations [17]. Specific applications include social circle detection, user behavior prediction, and friend recommendation, alongside fundamental tasks such as node classification [10]. A classic instance of a node-level prediction problem in social networks, illustrating the utility of GNNs, is the classification of members in Zach’s karate club based on their loyalties [18].

In terms of performance, specific GNN architectures have demonstrated superior capabilities in social network analysis tasks. For example, GKATs have shown notable performance gains when evaluated on social network classification tasks, outperforming other GNN models [38]. This indicates a progressive development in GNN design, leading to more effective solutions for relational inference in social graphs.

Despite the significant advancements, several challenges and opportunities persist for future research in the application of GNNs to social network analysis. One critical aspect revolves around node feature engineering. While some approaches involve creating node features based on node degrees to generate one-hot encodings or setting uniform feature vectors for datasets like REDDIT [24], there is a need for more sophisticated methods to learn and represent node features dynamically and semantically within social contexts. This could involve integrating richer auxiliary information or developing adaptive feature learning mechanisms.

Another significant opportunity lies in handling the heterogeneous and multi-view nature of social network data. Social networks often encompass diverse data sources, including social interactions, user behavior logs, and content analysis data. Multi-view clustering techniques already leverage these varied perspectives to identify communities, influencers, or trends more accurately [27]. Future research can explore how GNNs can be specifically designed to integrate and process these multiple views, thereby capturing a more comprehensive understanding of social dynamics. Although deep learning techniques generally contribute to areas like sentiment analysis, opinion analysis, and fake news detection in social networks [31], there remains a lack of detailed GNN-specific applications and theoretical frameworks for these complex tasks. This gap presents an opportunity to develop tailored GNN models that explicitly leverage the graph structure for enhanced performance in such nuanced analytical domains, moving beyond general deep learning applications to GNN-native solutions.
### 7.2 Recommendation Systems
Graph Neural Networks (GNNs) have emerged as a pivotal technology for enhancing the accuracy and personalization of recommendation systems, primarily due to their intrinsic ability to model complex relationships within graph-structured data [10,30]. In this context, users and items are typically represented as nodes, and their interactions or other relevant relationships (e.g., content similarity, social connections) are depicted as edges within a graph [1,30]. This graph-based representation allows GNNs to effectively capture intricate user-item dependencies, often formalized as a bipartite graph, thereby significantly improving recommendation accuracy [10]. The fundamental task in many GNN-based recommendation systems is to score the importance of an item to a user, which is conceptually transformed into a link prediction problem within the graph [1]. This approach facilitates the generation of personalized recommendations by leveraging the rich network of relationships between users, items, and content information [1,16]. The scalability of GNNs further enables their application in real-world scenarios involving the recommendation of billions of items to millions of users [18].

While the provided digests do not offer a direct comparative analysis of the performance of various GNN models (e.g., GCN, GraphSAGE, GAT) on specific recommendation tasks, they consistently highlight the general improvements in accuracy and personalization that GNNs bring to the field [10]. GNNs inherently enhance recommendation performance by moving beyond traditional matrix factorization or content-based methods, allowing for the incorporation of high-order interaction patterns and multi-modal information.

A significant advantage of GNNs in recommendation systems is their capacity to leverage social relations to predict user preferences, particularly in social recommender systems [17]. These systems capitalize on the premise that users are often influenced by their social connections. GNNs effectively model these social graphs, integrating social influence into the recommendation process [17]. For instance, the GATE-SR model exemplifies this approach by employing a multi-head graph attention autoencoder to predict user-item interactions [17]. This model explicitly aims to improve recommendation accuracy, especially in challenging cold-start scenarios where interaction data for new users or items is scarce. It achieves this by accentuating the influence of relevant neighbors and meticulously modeling higher-order social connections, thereby providing more nuanced and effective personalized recommendations [17]. Furthermore, the integration of diverse user data sources, such as behavior, preferences, and demographics, which can be seen as different “views” of a user, can also contribute to highly personalized recommendations, a process that GNNs can facilitate by operating on multi-view graph structures [27].
### 7.3 Time Series Forecasting and Classification
Spatial-temporal Graph Neural Networks (STGNNs) offer significant advantages for time series analysis due to their inherent ability to capture both variable interdependencies and temporal dependencies, making them particularly well-suited for complex dynamic systems [21]. This capability allows STGNNs to model intricate relationships across different entities within a system, as well as their evolution over time.

A primary application demonstrating the utility of STGNNs in time series analysis is traffic flow prediction, which is a critical component of intelligent transportation systems [40]. In this domain, road networks naturally form a graph structure, where intersections or road segments can be represented as nodes and connections as edges [28]. Traffic flow itself is a highly time-dependent phenomenon, exhibiting significant variations by hour and day [28]. GNNs, particularly Graph Convolutional Networks (GCNs), have been effectively applied to this task, aiding in the alleviation of traffic congestion and the enhancement of road usage efficiency [40]. The ability of STGNNs to embed regional information directly into the graph structure further strengthens their performance in such spatio-temporal forecasting tasks [28].

While STGNNs show promising capabilities in time series forecasting and classification, the provided digests do not offer specific comparisons of different STGNN models or their performance on various benchmark datasets. Furthermore, discussions regarding the overarching challenges and future research directions in applying GNNs to time series forecasting and classification are not detailed within the scope of the provided materials. Addressing these aspects would require a broader survey of the literature, including experimental results and methodological critiques beyond the current digest information.
### 7.4 Computer Vision
Graph Neural Networks (GNNs) have emerged as a powerful paradigm in computer vision, primarily due to their intrinsic ability to model and capture complex relationships between entities within images and videos [3]. This capability allows GNNs to move beyond pixel-level or grid-based processing, enabling a more semantic understanding of visual data. For instance, in image scene understanding, GNNs facilitate what is known as edge-level inference, where the models are designed to predict the nuanced relationships between various objects identified within an image [18]. By representing visual elements as nodes and their interactions or spatial relationships as edges, GNNs can effectively leverage the structural context inherent in visual scenes.

A prominent application where GNNs excel in capturing these relationships is **scene graph generation**. In this task, GNN models parse raw image data and transform it into structured semantic graphs. These graphs comprise nodes representing detected objects and edges signifying the semantic relationships that exist between them, such as "person *riding* bicycle" or "cat *on* mat" [1]. This structured representation provides a rich, interpretable summary of the visual content, which is valuable for higher-level reasoning tasks.

Beyond scene graph generation, GNNs have demonstrated significant utility in other specialized computer vision domains. They are extensively applied in **point cloud classification**, where 3D points are often treated as nodes in a graph, and connections are established based on proximity or geometric features, enabling robust classification of intricate 3D structures. Another critical application is **action recognition** in video sequences, where GNNs can model spatio-temporal relationships between human body parts, objects, and environmental cues over time to identify complex actions and activities [1,30]. These applications underscore the versatility of GNNs in extracting and analyzing relational information from diverse visual data modalities.
### 7.5 Natural Language Processing
Graph Neural Networks (GNNs) offer a robust framework for modeling intricate relationships within natural language data, extending beyond traditional sequential processing to capture non-linear dependencies [39]. GNNs are adept at representing the interrelationships between words, sentences, and entire documents, which is crucial for deep linguistic understanding [1,30].

A key strength of GNNs in Natural Language Processing (NLP) lies in their ability to capture both syntactic and semantic relationships within text. For instance, GNNs can effectively leverage explicit graph structures inherent in natural language, such as syntactic dependency trees [1]. Syntactic Graph Convolutional Networks (GCNs), in particular, are employed to aggregate hidden word representations based on these dependency tree structures, thereby incorporating grammatical relationships directly into the learning process [30]. The importance of integrating diverse information, including syntactic and semantic features, is further highlighted in related applications like multi-view clustering for document clustering and sentiment analysis, where it significantly enhances result quality and relevance [27].

Advancements in GNN-based NLP also include the development of knowledge-enhanced pre-training strategies. These approaches aim to imbue models with rich domain knowledge, improving their understanding and performance on various tasks. For example, toolkits like EasyNLP facilitate the construction of Pre-trained Model (PTM)-based NLP applications by supporting knowledge-enhanced pre-training [34]. Such frameworks also incorporate functionalities like knowledge distillation and prompt-based few-shot learning, which are vital for optimizing the performance of large-scale PTMs, particularly when training data is scarce, and for distilling models into more compact versions suitable for real-world deployment [34].

Among the most common applications of GNNs in NLP is text classification [1]. In this domain, GNNs harness the internal relationships between documents or words to infer document labels, demonstrating superior performance by considering the global structural context of the text data rather than isolated features [30]. While the general scope of GNNs in NLP also extends to areas like machine translation, the provided digests primarily emphasize their utility and advancements in capturing structural dependencies and facilitating knowledge-enhanced learning for tasks such as text classification [1].
### 7.6 Bioinformatics
The application of Graph Neural Networks (GNNs) to biological data presents significant opportunities, leveraging the inherent graph-structured nature of molecules, proteins, and biological networks [3]. In bioinformatics, GNNs are particularly adept at modeling entities where relationships and interactions are critical, such as atoms and chemical bonds in molecules [1,30]. These models treat atoms as nodes and chemical bonds as edges, enabling the representation and analysis of molecular/compound graph structures [1,30].

A primary opportunity for GNNs in this domain lies in **molecular property prediction and drug discovery**. GNNs can learn molecular fingerprints and predict various properties, including toxicity or odor, directly from atomic information and connectivity [1,10,18]. For instance, the Leffingwell Odor Dataset facilitates graph-level prediction tasks, classifying molecular graphs based on properties like scent [18]. Beyond simple property prediction, GNNs are crucial for more complex tasks such as synthesizing compounds and are explicitly used in drug development efforts [1,3]. Furthermore, knowledge graph embeddings, often derived using GNNs, are employed to predict adverse biological effects of chemicals, contributing to chemical safety and toxicology assessments [39].

Another critical area is **protein function prediction and interaction analysis**. GNNs are utilized for inferring protein structures and predicting their functions and interactions, which are fundamental problems in structural and functional genomics [1,3]. The mining of biomedical knowledge graphs, often enriched with GNN-based methods, further enhances the understanding of complex biological systems and disease mechanisms [3].

Despite these opportunities, challenges persist. Biological data can be incredibly complex, with nodes often possessing categorical input features [24]. The sheer size and structural variability of biological graphs, such as crystal structures where calculating properties like vibration frequencies via density functional theory (DFT) methods is computationally demanding, present significant hurdles for large-scale materials screening [29]. GNNs offer a computational advantage, as demonstrated by their use in predicting lattice phonon vibrational frequencies, employing techniques like zero-padding to handle the variable dimensions of the frequency spectrum [29]. Moreover, integrating multi-omics data, while highly beneficial for uncovering biologically meaningful correlations and insights into disease mechanisms, also poses challenges in identifying consistent and differential patterns across diverse data types [27]. GNNs, particularly those capable of handling multi-view or multi-modal inputs, represent an avenue to address these integration complexities for tasks like identifying disease subtypes and supporting personalized treatment [27].

Regarding specific GNN architectures and methodologies, various approaches have been developed. For molecular representation learning, methods like ASBA (atomic and subgraph-aware bilateral aggregation) have been proposed. ASBA considers both atomic and subgraph-level information to enhance the accuracy and generalization ability of molecular property prediction, illustrating an architectural advancement in handling hierarchical features [33]. The applicability of GNNs in bioinformatics challenges has been further demonstrated by experimental results involving Graph Kernel Attention Networks (GKATs), highlighting the flexibility of GNN designs for this domain [38]. These specialized GNN architectures aim to effectively capture the intricate relationships and properties within diverse biological graphs, from small molecules to complex proteins and crystal structures.
### 7.7 Program Analysis
Graph Neural Networks (GNNs) have emerged as a prominent tool within the domain of program analysis and software mining [3]. While the general application of GNNs in this field is acknowledged, a comprehensive comparison of different GNN‐based approaches for program analysis—specifically concerning program representations such as control flow graphs (CFGs) or abstract syntax trees (ASTs)—is crucial for understanding their efficacy. Similarly, a detailed analysis of the effectiveness of GNNs in identifying diverse types of program errors and security vulnerabilities is essential for practical deployment. However, the provided digest for [3] primarily indicates the inclusion of GNN applications in program analysis without furnishing specific technical details on different graph representations, comparative methodologies, or empirical results pertaining to error and vulnerability detection. Therefore, a deeper exploration into these specific aspects—including the comparative performance of GNNs utilizing various program representations and their precision in pinpointing distinct classes of software defects—necessitates further detailed analysis from a broader range of literature.
### 7.8 Combinatorial Optimization
Graph Neural Networks (GNNs) have emerged as a promising approach for addressing complex combinatorial optimization problems, which are often characterized by their NP-hard nature and the need for efficient heuristic solutions [22]. These problems involve finding an optimal object from a finite set of objects, such as optimal paths, partitions, or assignments on a graph.

One prominent application is in solving the Max‑k‑Cut problem, a fundamental combinatorial optimization challenge that involves partitioning the vertices of a graph into $k$ disjoint subsets to maximize the total weight of edges connecting vertices in different subsets [22]. For this problem, a GNN‑based Relax‑Optimize‑and‑Sample (ROS) framework has been proposed to efficiently tackle instances with arbitrary edge weights [22]. The essence of applying GNNs in such scenarios lies in their ability to learn sophisticated heuristics directly from graph structures, bypassing the limitations of hand‑crafted heuristics often employed in traditional optimization algorithms.

Beyond Max‑k‑Cut, GNNs demonstrate versatility in other combinatorial optimization contexts. For instance, in graph coloring, GNNs can be integrated to enhance column generation techniques through machine‑learning‑based pricing heuristics [39]. This highlights how GNNs can learn to make intelligent decisions within iterative optimization frameworks, guiding the search process more effectively. Furthermore, GNNs are applicable to various path optimization problems, extending the scope of classic shortest path algorithms, particularly in domains like traffic and logistics where complex graph structures and dynamic conditions are prevalent [40].

When comparing GNN‑based methods with traditional optimization algorithms, GNNs offer a distinct advantage in learning complex, data‑driven heuristics. Traditional exact algorithms, such as branch‑and‑bound or cutting plane methods, guarantee optimality but often face computational intractability for large‑scale NP‑hard problems. Heuristic and meta‑heuristic traditional algorithms, while faster, rely on pre‑defined rules or iterative improvement strategies that may struggle to adapt to diverse graph structures or converge to sub‑optimal solutions. GNNs, conversely, can learn to extract relevant structural features and predict high‑quality solutions or guide search processes by learning effective strategies, potentially leading to improved solution quality and computational efficiency, especially for large instances. By leveraging the intrinsic relational inductive biases of graph data, GNNs can generalize heuristics across different problem instances, offering a powerful paradigm shift from fixed, human‑engineered heuristics towards adaptive, learned decision‑making in combinatorial optimization.
### 7.9 Knowledge Graph Reasoning
Graph Neural Networks (GNNs) play a pivotal role in knowledge graph reasoning by enabling sophisticated analysis of relationships and entities within these structured data representations. A primary application of GNNs in this domain is facilitating core tasks such as entity classification and relation prediction [16]. Beyond these fundamental inference capabilities, GNNs contribute to enhancing the expressiveness and utility of knowledge graph embeddings (KGEs). One notable training strategy identified for improving KGEs is KGE-CL, which leverages contrastive learning principles [39]. This approach specifically aims to reduce the semantic distance between related entities and entity‐relation pairs across different triples, thereby enriching the representational quality of knowledge graph embeddings [39].
### 7.10 Other Emerging Applications
Beyond their foundational applications in areas such as social networks and recommender systems, Graph Neural Networks (GNNs) have demonstrated considerable utility across a diverse array of emerging domains, addressing unique challenges and presenting novel opportunities [1].

One prominent area of application is **spatio-temporal prediction and analysis**, particularly in urban contexts. GNNs, including Spatio-Temporal GNNs, are increasingly employed for tasks such as cellular traffic prediction, urban spatio-temporal event forecasting, and general urban intelligence initiatives [3,21]. The unique challenge in these domains lies in effectively modeling the complex, dynamic interdependencies between geographical locations over time, which GNNs address by naturally capturing both spatial proximity and temporal sequence. This capability offers significant opportunities for improved urban planning, resource allocation, and real-time decision-making.

In the realm of **security and anomaly detection**, GNNs provide robust frameworks for identifying unusual patterns and preventing malicious activities. Applications include anomaly detection, encryption traffic classification, and adversarial attack prevention [1,3,21]. The inherent challenge here involves discerning subtle deviations from normal behavior or sophisticated attack vectors within intricate network structures, where traditional methods might fail to capture higher-order dependencies. GNNs leverage their ability to learn rich representations of graph-structured data, offering enhanced precision in detecting rare and critical events.

Furthermore, GNNs are finding applications in **computational analysis of programs and systems**. This includes program verification and program reasoning, where the graph structure can represent code logic, data flow, or control flow [1]. The opportunity lies in automating complex analysis tasks that are traditionally manual and prone to error. Challenges involve representing abstract program constructs as graphs effectively and scaling GNN models to handle large, complex codebases while maintaining interpretability.

In the field of **biomedical and healthcare informatics**, GNNs are being applied to analyze complex biological networks, such as brain networks, offering insights into neurological conditions [1]. A notable application is in personalized medicine, where multi-view clustering leveraging graph-based approaches can identify disease subtypes by integrating heterogeneous medical data, such as imaging and genetic information [27]. For instance, GNN-based multi-view learning has been developed to identify imaging-related subtypes in mild cognitive impairment (MCI) [27]. The primary challenge in this domain is effectively integrating diverse, often high-dimensional, and noisy multi-modal data to uncover hidden patterns relevant to diagnosis and prognosis. The opportunity is to move towards more precise and tailored medical interventions.

Additional emerging applications span across social impact prediction, general event detection, and combinatorial optimization, showcasing the versatility of GNNs in modeling complex relationships across various data types and domains [1]. These diverse applications highlight GNNs' capacity to extract meaningful insights from interconnected data, presenting significant opportunities for advancements in fields requiring deep structural understanding and predictive capabilities.
## 8. Challenges and Future Directions
Despite the significant advancements and burgeoning success of Graph Neural Networks (GNNs) across diverse domains, their widespread adoption and full potential are currently constrained by a myriad of open challenges and limitations. Addressing these fundamental issues will be critical for advancing GNN research and enabling their deployment in complex, real-world scenarios. 

![Major Challenges and Future Directions in GNNs](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/oAAoyHph4GCgQfiDUPRz0_/home/surveygo/data/requests/13543/survey/imgs/Major%20Challenges%20and%20Future%20Directions%20in%20GNNs.png)

This section synthesizes the primary challenges facing GNNs, encompassing computational, algorithmic, theoretical, and ethical dimensions, and delineates promising future research trajectories.

A paramount challenge revolves around **scalability and efficiency**, particularly when applying GNNs to large-scale graphs prevalent in real-world applications [1,9,21,27,30,40]. The inherent computational cost, memory requirements, and hardware limitations, especially for dynamic or very dense graph structures, pose significant bottlenecks [2,21,29]. Current research aims to mitigate these issues through graph simplification techniques, such as sampling or clustering, though these often entail trade-offs with model accuracy [1,30]. Consequently, future work is crucial in developing more scalable and efficient GNN algorithms, potentially leveraging architectural innovations or optimized data management strategies like Adaptive Graph Partitioning [2].

Closely related to efficiency is the burgeoning field of **hardware acceleration**, which seeks to overcome the computational intensity and irregular memory access patterns typical of GNN workloads [2]. Specialized architectures, including Field-Programmable Gate Arrays (FPGAs) and Near-Memory Computing (NMC) solutions like NEM-GNN, offer enhanced performance and energy efficiency by bringing computation closer to data or allowing for highly customized GNN operations, distinguishing them from general-purpose GPUs which may struggle with graph sparsity [2,19,29]. Continued innovation in this area is vital for pushing the boundaries of GNN applicability.

Another critical area is enhancing **robustness**, as GNNs are notably susceptible to adversarial attacks and inherent noise, with even minor perturbations leading to significant prediction shifts [3,14,29]. This fragility is compounded by their typical design assumption of static, homogeneous graphs, which contrasts sharply with the dynamic and complex nature of real-world graph data [9,27]. Future research must focus on designing intrinsically more stable architectures, such as Network-in-Graph Neural Networks (NGNN), and developing robust training methodologies capable of withstanding various forms of noise and adversarial attacks [12].

The "black-box" nature of many GNNs underscores the pressing need for improved **explainability**. Understanding *why* a GNN makes a particular decision is paramount for trust, accountability, and regulatory compliance, especially in sensitive applications [21,25,32]. While methods like model-level explanations via graph generation (e.g., XGNN) offer promising avenues, future work must focus on generating higher-quality, more actionable, and comprehensible explanations, extending interpretability to dynamic and heterogeneous graph structures [32].

Beyond performance, the **fairness and ethical considerations** of GNNs are increasingly vital. GNNs possess the potential to perpetuate or even amplify existing societal biases embedded in training data or graph structures, leading to discriminatory outcomes [5]. Ensuring individual fairness, treating similar individuals similarly, and addressing privacy concerns related to sensitive relational data are crucial for responsible deployment [5]. Future directions necessitate comprehensive strategies, including data auditing, fairness-aware model design, robust evaluation metrics, privacy-preserving techniques, and continuous ethical oversight [5].

A significant limitation of existing GNNs lies in their struggle with **dynamic graphs**, which involves incorporating temporal information and adapting to evolving graph topologies [1,7,9,30]. Traditional GNNs are primarily designed for static structures, making it challenging to capture long-term dependencies within evolving graph sequences. Research is actively exploring the integration of memory networks and attention mechanisms to enable GNNs to maintain historical information and dynamically weigh relevant patterns as graphs change.

From a foundational perspective, understanding the **theoretical limitations** of GNNs is paramount. Research into their expressive power, generalization ability, and convergence properties provides crucial insights into their capabilities and inherent boundaries [3,22,24]. Future work needs to further delineate the scope of GNN applicability and explore novel architectures beyond traditional message passing to enhance their representational capacities and optimization prospects [24].

Finally, the **integration with other machine learning techniques** represents a powerful future direction for expanding GNN capabilities. Synergistic combinations with reinforcement learning can facilitate tasks requiring sequential decision-making in graph environments, such as generating explanatory graph structures [32]. Moreover, GNNs hold significant promise as generative models for synthetic graph creation, leveraging their ability to learn complex graph distributions [9]. The development of multi-network fusion frameworks, combining various GNN algorithms and even AutoML for GNNs, will be crucial for addressing the increasing complexity and heterogeneity of real-world graph data [3,9].

In summary, the future of GNN research lies in overcoming these multifaceted challenges. Progress in scalability, robustness, interpretability, fairness, dynamic graph modeling, theoretical understanding, and strategic integration with other AI paradigms will collectively pave the way for more powerful, reliable, and ethically responsible GNN systems, unlocking their full potential across a broad spectrum of real-world applications.
### 8.1 Scalability
The computational bottlenecks that limit the scalability of Graph Neural Networks (GNNs) are multifaceted, posing significant challenges for their application to large-scale real-world graphs. A primary concern is the high computational cost, particularly due to the substantial memory requirements for calculating adjacency matrices and node embeddings, especially prevalent in dynamic graph structures [21]. Furthermore, the intrinsic properties of GNNs, such as increasing network depth, can lead to an exponential increase in aggregated neighbor features, resulting in higher computational complexity and a greater risk of overfitting, especially when dealing with large datasets [9]. Hardware-level limitations also contribute to scalability issues, including the exponential area requirement that arises as weight precision increases, primarily driven by the need for data replication across different memory banks [19]. Additionally, communication overheads, such as the frequent CPU-GPU interactions, can significantly impede the training speed of GNNs on extensive graphs [29].

To address these scalability challenges, various techniques have been proposed, each with inherent trade-offs between efficiency and model accuracy. One widely adopted approach involves simplifying the graph structure through methods like sampling or clustering [1,30]. While these techniques effectively reduce the computational load, they often compromise the integrity of the graph, as important graph information may be lost, thereby potentially affecting the model's performance and accuracy [1,30]. For instance, anchor-based methods offer a scalable solution for large datasets by selecting a smaller set of representative data points (anchors) to approximate the original dataset, which can be viewed as a form of selective sampling or clustering [27].

Beyond graph simplification, other methods focus on architectural innovations and optimized data management. For example, some GNN architectures incorporate GKAT attention layers, which are designed to scale linearly with the number of nodes, even in dense graph scenarios, directly tackling computational complexity at the layer level [38]. From a data and memory management perspective, strategies such as the Adaptive Graph Partition (AGP) have been introduced to mitigate memory bottlenecks and reduce graph repartition overhead between GNN layers [2]. Similarly, implementing data tiering methods can significantly reduce CPU-GPU communication, leading to improved training speeds on large graphs [29].

Integrated frameworks also demonstrate substantial progress in scalability. The Relax-Optimize-Sample (ROS) framework, for instance, has showcased its ability to scale effectively to large instances comprising up to 20,000 nodes within a few seconds, surpassing the performance of state-of-the-art algorithms [22]. While the specific internal mechanisms of such frameworks are diverse, their success underscores the potential for comprehensive solutions.

In summary, improving GNN scalability frequently involves navigating a complex landscape of trade-offs. The inherent dilemma lies in balancing computational efficiency with the preservation of graph information and model accuracy. Balancing network depth with the necessity to limit nodes per layer to prevent information obstruction remains a critical area for ongoing research, highlighting the fundamental tension between expressive power and computational feasibility in GNNs [9].
### 8.2 Robustness
Graph Neural Networks (GNNs), despite their efficacy in learning from complex graph-structured data, are notably susceptible to adversarial attacks and inherent noise [14,29]. This vulnerability is a critical concern, as even minor perturbations to graph structures or node features can lead to significant and often undesirable shifts in model predictions [3]. The discrete and combinatorial nature of graph data, coupled with the local aggregation mechanism central to many GNN architectures, makes them particularly sensitive to such alterations, as changes can propagate through the network.

A significant challenge contributing to GNN fragility stems from the underlying assumptions often made in their design. Most current GNNs are typically formulated for static, homogeneous graphs, presuming fixed graph structures and features drawn from a single source distribution [9]. However, real-world applications, such as social networks and recommendation systems, frequently involve dynamic and complex heterogeneous graphs where topological structures and features evolve over time [9]. The difficulty in effectively adapting GNNs to these dynamic and complex environments inherently introduces a form of instability or implicit noise, thereby impeding their robust deployment [9]. Furthermore, external noise originating from the data generation phase can significantly impact model performance. This issue is not exclusive to GNNs but is a common challenge across various data-driven methodologies, where conventional approaches often overlook the noise arising during data acquisition, struggling to capture high-dimensional information from nonlinear subspaces or to account for the intricate relationships between data components [27].

To mitigate these vulnerabilities, various methods have been explored, broadly categorizable into architectural enhancements and improved generalization strategies. From an architectural perspective, models can be designed to be inherently more stable against perturbations. For instance, the Network-in-Graph Neural Network (NGNN) model has demonstrated notable stability against both node feature perturbations and graph structure changes [12]. This indicates that embedding internal networks within deeper GNN architectures can reduce their sensitivity to noise present in the input graph data, thereby enhancing overall robustness [12]. This approach focuses on building resilience directly into the model's design. In contrast, other methods aim to improve generalization capabilities, which indirectly contributes to robustness by enabling models to better handle unseen or slightly perturbed data. For example, the Relax-Optimize-Sample (ROS) framework exhibits strong generalization capabilities across both in-distribution and out-of-distribution instances [22]. While not exclusively an adversarial defense mechanism, the ability to generalize well to data beyond the training distribution can make a model more resilient to minor shifts or anomalies that might be present in real-world deployments.

Despite these advancements, significant challenges and opportunities for future research persist. A paramount challenge is the development of GNNs that can inherently adapt to dynamic graph scenarios and effectively operate on large-scale heterogeneous graphs, which are ubiquitous in practical applications [9]. Addressing noise that originates during the data generation process also remains a crucial frontier, requiring the development of more robust data pre-processing techniques and noise-aware model designs [27]. Future research directions include exploring novel architectural paradigms, potentially building upon concepts like NGNN, to create GNNs that are intrinsically robust to both adversarial attacks and various forms of noise. Additionally, developing robust training methodologies that can withstand adversarial perturbations and data irregularities, coupled with strategies for explicit out-of-distribution generalization, will be critical for unlocking the full potential of GNNs in safety-critical and dynamic environments.
### 8.3 Explainability
Explainability is a critical consideration for Graph Neural Networks (GNNs), largely due to their inherent complexity and often "black-box" nature [21]. While GNNs have demonstrated remarkable performance across various tasks, their opaque decision-making processes present significant limitations, especially in sensitive domains such as healthcare, finance, or autonomous systems, where trust, accountability, and regulatory compliance are paramount. The ability to understand *why* a GNN makes a particular prediction or classification is essential for debugging, identifying biases, ensuring fairness, and facilitating human oversight and intervention [25]. The lack of transparency fundamentally restricts the broader adoption and deployment of GNNs in real-world critical applications [32].

To address this crucial limitation, research has focused on developing methods to provide insights into GNN operations. One promising avenue involves generating "model-level explanations" [32]. For instance, XGNN is introduced as a novel approach specifically designed to interpret GNNs by providing such model-level insights [25]. This method focuses on understanding the overall behavior of the GNN rather than just individual predictions. Another approach to generate model-level explanations involves leveraging graph generation techniques [32], which aim to synthesize input graphs that elicit specific model behaviors or predictions, thereby revealing the underlying patterns the GNN has learned. While these approaches offer pathways to enhance GNN transparency, the provided literature primarily highlights the necessity of explainability and introduces specific methodologies without a comparative analysis of different techniques.

Despite these advancements, significant challenges persist in GNN explainability. A primary challenge lies in the inherent complexity of graph data, including its irregular structure and the intricate interdependencies between nodes and edges, which complicate the attribution of model decisions. Future research must concentrate on "improving the quality and actionability of the generated explanations" [32]. This includes ensuring that explanations are not only faithful to the model's actual computations but also comprehensible and practically useful for domain experts. Opportunities exist in developing more robust evaluation metrics for explainability, exploring counterfactual explanations, and integrating human-in-the-loop approaches to refine explanatory models. Furthermore, extending explainability to dynamic graphs and heterogeneous graph structures presents additional research avenues.
### 8.4 Fairness and Ethical Considerations
The deployment of Graph Neural Networks (GNNs) in real-world applications, particularly those involving high-stake decision-making scenarios, necessitates a rigorous examination of their ethical implications. A primary concern revolves around the potential for GNNs to perpetuate or even amplify existing societal biases, leading to unfair or discriminatory outcomes [5]. This susceptibility to bias stems from various sources, including biased training data, pre-existing structural biases within the graph topology, or algorithmic design choices that inadvertently favor certain groups or individuals.

Central to these ethical considerations is the concept of fairness, specifically at the individual level [5]. While group fairness metrics aim to ensure similar outcomes across predefined demographic groups, individual fairness strives to treat similar individuals similarly, irrespective of their group affiliations. The challenge in GNNs lies in defining and measuring "similarity" on a graph, where relationships and structural positions play a crucial role in representations and predictions. Ensuring individual fairness in GNNs is paramount to prevent disparate impact on specific users or entities within a graph, especially when decisions directly affect their opportunities or access to resources. For instance, in recommendation systems or credit assessment, a GNN might inadvertently penalize individuals based on their connections or neighborhood characteristics if biases are embedded in the graph data or the learning process [5].

Beyond bias and fairness, privacy concerns also arise in GNN applications. Graphs inherently represent relationships, which can be highly sensitive. The aggregation and propagation of information across graph structures can inadvertently leak private attributes of individuals or reveal sensitive relational patterns, even when direct personal identifiers are removed. This necessitates careful consideration of data anonymization techniques, differential privacy mechanisms, and secure multi-party computation methods when developing GNN systems.

To address these complex ethical challenges, it is imperative to establish comprehensive guidelines and best practices for the responsible development and deployment of GNNs. These should include:
1.  Data Auditing and Preprocessing: Thoroughly examine graph datasets for biases in node features, edge attributes, and graph structure. Employ data preprocessing techniques to mitigate or correct identified biases before model training.
2.  Fairness-Aware Model Design: Incorporate fairness constraints directly into the GNN training objective or design architecture that inherently promotes fairness, such as those explicitly optimizing for individual fairness [5].
3.  Transparency and Interpretability: Develop GNN models that are more transparent and interpretable, allowing stakeholders to understand how decisions are made and identify potential sources of bias.
4.  Robust Evaluation Metrics: Go beyond traditional performance metrics (e.g., accuracy, F1-score) to include a diverse set of fairness metrics (e.g., individual fairness, disparate impact, equalized odds) during model evaluation.
5.  Privacy-Preserving Techniques: Implement robust privacy-preserving methods, such as differential privacy or federated learning, to protect sensitive individual and relational data.
6.  Regular Auditing and Monitoring: Continuously monitor deployed GNN systems for emergent biases or unfair outcomes, adapting models as needed to maintain ethical standards over time.
7.  Ethical Review and Stakeholder Engagement: Establish interdisciplinary ethical review boards for GNN projects, involving ethicists, legal experts, and representatives from potentially affected communities, to ensure a holistic approach to responsible AI development.

Adhering to these principles will facilitate the creation of GNN systems that are not only powerful and efficient but also equitable, transparent, and respectful of individual rights and societal values.
### 8.5 Dynamic Graph Challenges
Existing Graph Neural Networks (GNNs), predominantly designed for static graphs, encounter significant limitations when applied to dynamic graph structures. A primary challenge lies in their inherent inability to robustly capture long-term dependencies within evolving graph sequences and to effectively adapt to perpetually changing graph topologies. Traditional GNN architectures struggle to process graphs where nodes, edges, or their attributes change over time, as their aggregation and update mechanisms are typically defined for fixed connectivity patterns. This necessitates the development of novel approaches to perform graph convolution on dynamic spatial relationships, highlighting a critical need for new graph convolutions specifically adapted to dynamic graphs [30].

To overcome these limitations, advanced techniques are being explored for modeling dynamic graphs. One promising direction involves the incorporation of memory networks. These architectures can maintain and update a latent memory state that accumulates historical information about the graph's evolution, thereby enabling the GNN to capture long-term temporal dependencies and adapt to shifts in graph structure. Another compelling approach is the integration of attention mechanisms. By dynamically weighting the importance of different neighbors or historical graph states, attention mechanisms allow GNNs to selectively focus on relevant information as the graph changes, providing a flexible way to handle varying degrees of connection dynamism and highlight critical temporal patterns. These techniques offer pathways toward GNNs that can more effectively learn from and reason about dynamic and evolving graph data.
### 8.6 Theoretical Limitations
Understanding the theoretical underpinnings of Graph Neural Networks (GNNs) is crucial for delineating their capabilities, predicting their performance, and identifying their inherent boundaries. Key theoretical aspects encompass their expressive power, generalization ability, and convergence properties, providing a deeper understanding of why GNNs work and when they are most effective [3,22].

A fundamental area of theoretical inquiry revolves around the **expressive power** of GNNs, which quantifies their inherent capacity to distinguish between different graph structures and learn complex graph-structured data representations [3]. Research in this domain aims to provide a deeper understanding of the representational capacity inherent in various GNN architectures, thereby shedding light on why certain GNN models perform effectively on specific tasks and what types of structural information they can capture.

Beyond expressive power, the **convergence properties** and the conditions under which GNNs can reliably find optimal or near-optimal solutions are critical for their practical application, particularly in optimization problems. For instance, in the domain of combinatorial optimization, theoretical analyses have integrated geometric landscape analysis with statistical theory to elucidate the behavior of GNN-based approaches [22]. This integration has been instrumental in establishing the consistency of function values between a continuous solution, often obtained through a GNN-based relaxation, and its corresponding mapped discrete counterpart [22]. Such theoretical consistency provides a robust foundation for understanding when GNNs are most effective in solving complex problems like Max-k-Cut. A specific insight, detailed in Theorem 3.2, further reinforces this understanding by stating that for a globally optimal solution $\boldsymbol{X}^{\star}$, every point within its neighborhood $\mathcal{N}(\boldsymbol{X}^{\star})$ shares the same objective value as $\boldsymbol{X}^{\star}$ [22]. This particular finding highlights a condition under which the optimization landscape is smooth and predictable around optimal points, thereby reinforcing the theoretical understanding of GNN effectiveness and reliability in certain problem domains.

While these theoretical advancements elucidate the strengths and operational mechanisms of GNNs, especially in terms of their expressive capabilities and convergence characteristics in specific settings, ongoing research also strives to pinpoint their inherent limitations and understand their generalization abilities across diverse datasets and tasks. The analysis of problem landscapes, as exemplified by the consistency results, implicitly defines the scope where current theoretical guarantees hold, thereby informing about the boundaries of GNN applicability and guiding future improvements.
### 8.7 Integration with Other Machine Learning Techniques
The efficacy and applicability of Graph Neural Networks (GNNs) can be significantly enhanced through their integration with other machine learning paradigms, addressing complex challenges in diverse domains. This integration often leverages the strengths of GNNs in capturing structural relationships while complementing them with capabilities from other techniques.

A prominent area of synergy is the combination of GNNs with reinforcement learning (RL). This integration holds substantial promise for tasks requiring sequential decision-making within graph-structured environments, such as graph exploration and control. For instance, the XGNN approach exemplifies this by integrating reinforcement learning with GNNs to achieve enhanced explainability, where the GNN acts as a policy network to generate explanatory graph structures [32].

Beyond control and exploration, GNNs present significant opportunities as generative models for graphs. This capability is crucial for applications such as the design of novel molecules with desired properties or the synthetic generation of social networks for research purposes. The foundational strength of GNNs in processing node and edge features enables them to learn complex graph distributions. Specifically, for tasks like graph generation, Graph Generative Networks (GGNs) can be incorporated to produce probability distributions from node embeddings pre-processed by Graph Convolutional Networks (GCNs) [9]. This hierarchical approach leverages GCNs for their proficiency in extracting node attribute and topological information, which then informs the generative process of GGNs. The concept of graph generation is also implicitly supported by methods that generate graphs for interpretability, as seen with XGNN's model-level explanations via graph generation [32].

Furthermore, the integration within the broader GNN framework itself is vital for handling the complexity of real-world data. Given the diverse nature of graphs and tasks, multi-network fusion is often necessary, combining various GNN algorithms to collectively extract and integrate data features from different perspectives [9]. This can involve combining GCNs, known for their high performance and adaptability, with other GNN variants to capture complementary information [9]. For instance, GCNs can handle initial node embedding and topology extraction, while Graph Recurrent Networks (GRNs) can be utilized to encompass multi-step information for evolving graph sequences [9]. Such comprehensive integration allows for a robust and adaptable GNN framework capable of addressing a wide range of sophisticated graph-based problems. The development of AutoML for GNNs also signifies an advanced form of integration, streamlining the process of combining and optimizing various GNN components and architectures [3].
### 8.8 Hardware Acceleration
The computationally intensive and data-dependent nature of Graph Neural Networks (GNNs) necessitates specialized hardware architectures to achieve high performance and energy efficiency. Such hardware accelerators offer significant benefits, including enhanced programmability, superior performance, and improved energy consumption compared to general-purpose processors [2]. For instance, some specialized architectures, like SPA-GCN, demonstrate substantial speedups over multi-core CPU and GPU implementations, particularly when dealing with workloads involving numerous small graphs [29].

Different hardware architectures offer distinct advantages and trade-offs for GNN acceleration. Field-Programmable Gate Arrays (FPGAs) stand out due to their inherent programmability, which allows for highly customized GNN operations, leading to optimized performance and energy efficiency [2]. The flexibility of FPGAs enables them to adapt to evolving GNN models and diverse graph structures, a capability that has led many cloud service providers to integrate FPGA clusters into their infrastructure for domain-specific algorithms [2]. An example is the Adaptive GNN Accelerator framework (AGA), which leverages FPGAs' strengths by adopting a unified processing module that concurrently supports both the Aggregation and Combination phases of GNNs [2]. While FPGAs offer high customization and efficiency, their initial development cost and design complexity can be higher than off-the-shelf general-purpose processors.

In contrast, Graphics Processing Units (GPUs) provide massive parallel processing capabilities, making them highly effective for large-scale, dense matrix operations commonly found in traditional deep learning. However, the irregular memory access patterns and sparse computations inherent in many GNN workloads can limit the efficiency of GPUs, especially when compared to specialized GNN accelerators like SPA-GCN, which are optimized for graph-specific challenges such as processing many small graphs [29]. This indicates that while GPUs offer raw computational power, their performance and energy efficiency for GNNs can be suboptimal for certain graph topologies or sizes due to their fixed architecture.

Near-memory computing (NMC) architectures represent another promising direction, aiming to mitigate the memory wall bottleneck by bringing computation closer to data. NEM-GNN exemplifies this approach by introducing a scalable, reconfigurable, graph, and sparsity-aware near-memory accelerator for GNNs [19]. This architecture ingeniously reuses the L1 cache within the CPU core for in-memory computation, coupled with additional near-memory logic, thereby eliminating the need for a separate, dedicated accelerator setup. A key innovation in NEM-GNN is its digital bit-serial Processing-in-Memory (PIM) methodology for the combination phase, which circumvents the need for power-hungry and area-intensive Digital-to-Analog Converter (DAC) and Analog-to-Digital Converter (ADC) components. This design choice not only enhances robustness against process variations but also frees the design's throughput from ADC-related constraints, leading to improved energy efficiency and performance by drastically reducing data movement.

The trade-offs between performance, energy efficiency, and cost are central to selecting the appropriate hardware. GPUs offer high peak performance for suitable workloads but can suffer from lower energy efficiency for GNNs due to memory access patterns and underutilization of compute units for sparse graphs. FPGAs, while potentially incurring higher initial design and development costs, offer superior energy efficiency and highly optimized performance for specific GNN operations through custom hardware designs [2]. Near-memory computing solutions like NEM-GNN promise significant gains in both performance and energy efficiency by fundamentally addressing the memory bottleneck, potentially lowering the effective cost of computation by reducing power consumption and increasing throughput, despite the complexities associated with integrating logic directly into memory subsystems. Ultimately, the optimal choice depends on the specific GNN application's requirements regarding throughput, latency, power budget, and flexibility.
## 9. Conclusion
Graph Neural Networks (GNNs) have emerged as a powerful paradigm for processing graph-structured data, fundamentally transforming how complex relationships are modeled across diverse domains [3,10,11]. This survey has synthesized key advancements, foundational models, architectural innovations, and practical applications, providing a comprehensive overview of the current state of GNN research. GNNs effectively learn representations by normalizing graph structures and leveraging deep neural networks, demonstrating their efficacy through rigorous mathematical proofs and empirical analyses [9]. Their ability to capture intricate dependencies between nodes and edges offers more efficient and accurate solutions compared to traditional methods [10].

The core concepts of GNNs often revolve around a message-passing mechanism, as exemplified by the Message Passing Neural Network (MPNN) framework, which aggregates local node information to derive a global understanding of the graph [37]. This versatile approach facilitates deep learning on graph-structured data. Architecturally, significant progress has been made in enhancing GNN capabilities. For instance, the Network in Graph Neural Network (NGNN) method proposes inserting feedforward neural networks within GNN layers to deepen models, thereby increasing capacity without succumbing to overfitting or over-smoothing [12]. Research has also focused on understanding and pushing the theoretical representational limits of GNNs, leading to the proposal of provably powerful variants based on neighborhood aggregation [24]. Furthermore, self-supervised learning techniques, such as Deep Graph Bootstrapping (DGB), have been developed to learn effective graph representations in an unsupervised manner, eliminating the need for negative examples [15].

The practical utility of GNNs spans a wide array of applications, addressing real-world challenges in critical domains. In scientific fields, GNNs have shown immense potential in AI for Science by accurately modeling particle interactions and adhering to physical laws, and are being applied across physics, chemistry, and biology [23,35]. Beyond scientific discovery, GNNs have proven effective in traffic flow prediction and path optimization within transportation and logistics [40], and in enhancing social recommendation systems by accentuating influential neighbors for more accurate predictions, even in cold-start scenarios [17]. Their prowess extends to solving complex combinatorial optimization problems, such as the Max-k-Cut problem, where a GNN-based framework can generate high-quality solutions for large instances with up to 20,000 nodes in seconds [22].

Despite these remarkable advancements, the GNN landscape continues to evolve rapidly, presenting several promising avenues for future research [18,28]. A key direction involves improving the efficiency and scalability of GNNs, particularly for large-scale graphs. This includes the development of adaptive hardware accelerators, such as FP-GNN, which offers flexible execution and efficient on-chip resource utilization, achieving significant speedups and energy efficiency over conventional CPU/GPU platforms [2]. More advanced near-memory GNN accelerators like NEM-GNN demonstrate substantial performance, throughput, and energy efficiency gains, outperforming predecessors by orders of magnitude [19]. Another critical area is enhancing the interpretability and trustworthiness of GNNs, moving beyond their "black-box" nature. Approaches like XGNN, which train graph generators to produce patterns maximizing specific model predictions, offer valuable insights into GNN decision-making and can guide model improvements [25,32].

Future work also encompasses addressing challenges such as fairness in GNN models, with frameworks like REDRESS introducing ranking-based methods to enhance individual fairness while maintaining model utility [5]. Further research is needed to explore new GNN architectures, improve their generalization capabilities to new environments and unseen graph structures, and facilitate their effective integration with existing domain-specific systems [21,35,40]. The development of user-friendly frameworks, such as kgcnn in TensorFlow-Keras, continues to democratize GNN implementation and experimentation, supporting customization and accelerating research [36].

In conclusion, GNNs have established themselves as an indispensable tool for graph-structured data analysis, offering powerful solutions to complex problems. As the field matures, researchers are encouraged to tackle the identified challenges related to scalability, interpretability, fairness, and architectural innovation, while continuing to explore novel applications in emerging scientific and engineering domains [1,7,13]. The continued success of GNNs will hinge on a collective effort to refine theoretical foundations, push technological boundaries, and expand their practical utility across an ever-growing spectrum of real-world problems.

## References

[1] TNNLS | GNN综述：图神经网络全面研究 [https://zhuanlan.zhihu.com/p/538914712](https://zhuanlan.zhihu.com/p/538914712) 

[2] FP-GNN: Adaptive FPGA Accelerator for Graph Neural Networks [https://www.sciencedirect.com/science/article/abs/pii/S0167739X22002217](https://www.sciencedirect.com/science/article/abs/pii/S0167739X22002217) 

[3] IJCAI 2022 图神经网络教程：基础、前沿与应用 [https://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247603096&idx=1&sn=1948bdd6964fd5e2d98f4eecfc9946ad&chksm=fc87b28bcbf03b9dc7709af4e61c00bfa62e323cd538f4e977fccd1a279b4ecd2101c67c8579&scene=27](https://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247603096&idx=1&sn=1948bdd6964fd5e2d98f4eecfc9946ad&chksm=fc87b28bcbf03b9dc7709af4e61c00bfa62e323cd538f4e977fccd1a279b4ecd2101c67c8579&scene=27) 

[4] 《Graph Neural Networks: A Review of Methods and Applications》阅读笔记 [https://www.cnblogs.com/ydcode/p/11050417.html](https://www.cnblogs.com/ydcode/p/11050417.html) 

[5] Individual Fairness in Graph Neural Networks via Ranking [https://dl.acm.org/doi/10.1145/3447548.3467266](https://dl.acm.org/doi/10.1145/3447548.3467266) 

[6] 图神经网络（GNN）综述 [https://zhuanlan.zhihu.com/p/75307407](https://zhuanlan.zhihu.com/p/75307407) 

[7] 图神经网络方法与应用综述 [https://zhuanlan.zhihu.com/p/413648055](https://zhuanlan.zhihu.com/p/413648055) 

[8] Graph Neural Networks (GNN) 综述简介 [https://zhuanlan.zhihu.com/p/68015756](https://zhuanlan.zhihu.com/p/68015756) 

[9] 图神经网络（GNN）最新综述论文解读 [https://zhuanlan.zhihu.com/p/465443674](https://zhuanlan.zhihu.com/p/465443674) 

[10] 图神经网络 (GNNs): 图结构数据处理的未来 [https://cloud.tencent.com/developer/article/2457967](https://cloud.tencent.com/developer/article/2457967) 

[11] Introduction to Graph Neural Networks [https://nlp.csai.tsinghua.edu.cn/~lzy/books/gnn_2020.html](https://nlp.csai.tsinghua.edu.cn/~lzy/books/gnn_2020.html) 

[12] Network In Graph Neural Network (NGNN): Deepening GNNs with Internal Networks [https://ui.adsabs.harvard.edu/abs/arXiv:2111.11638](https://ui.adsabs.harvard.edu/abs/arXiv:2111.11638) 

[13] Graph Neural Networks: Methods, Applications, and Future Directions [https://www.sciencedirect.com/science/article/pii/S2666651021000012](https://www.sciencedirect.com/science/article/pii/S2666651021000012) 

[14] 时空图神经网络（STGNNs）综述 [https://zhuanlan.zhihu.com/p/12514019111](https://zhuanlan.zhihu.com/p/12514019111) 

[15] Self-Supervised Graph Representation Learning via Deep Graph Bootstrapping [https://www.sciencedirect.com/science/article/abs/pii/S0925231221005154](https://www.sciencedirect.com/science/article/abs/pii/S0925231221005154) 

[16] 学科前沿：IEEE数据库图神经网络（GNN）技术最新研究 [https://mp.weixin.qq.com/s?__biz=MzA5MTM5NzY1MQ==&mid=2650579822&idx=1&sn=560cc26af888ae7e5e0ae093c275e4de&chksm=88750812bf0281049b081e3e532563eb20175dbe65f6d51dc83d3e5e2122b629b536182fd5cf&scene=27](https://mp.weixin.qq.com/s?__biz=MzA5MTM5NzY1MQ==&mid=2650579822&idx=1&sn=560cc26af888ae7e5e0ae093c275e4de&chksm=88750812bf0281049b081e3e532563eb20175dbe65f6d51dc83d3e5e2122b629b536182fd5cf&scene=27) 

[17] GATE-SR: Multi-Head Graph Attention Autoencoder for Higher-Order Social Recommendation [https://www.sciencedirect.com/science/article/pii/S0306437924001327](https://www.sciencedirect.com/science/article/pii/S0306437924001327) 

[18] A Gentle Introduction to Graph Neural Networks [https://distill.pub/2021/gnn-intro](https://distill.pub/2021/gnn-intro) 

[19] NEM-GNN: DAC/ADC-less Near-Memory GNN Accelerator with Graph and Sparsity Awareness [https://dl.acm.org/doi/10.1145/3652607](https://dl.acm.org/doi/10.1145/3652607) 

[20] GNN方法与应用综述：论文翻译与解读 [https://developer.aliyun.com/article/988847](https://developer.aliyun.com/article/988847) 

[21] 时空图神经网络在时间序列预测与分类中的应用综述 [https://mp.weixin.qq.com/s?__biz=MjM5ODIwNjEzNQ==&mid=2649899108&idx=3&sn=95ee5999d518a766ebfbf3e438cc1df7&chksm=bfa92097e3dd8801330d76cdc4a0583f9165bbe07b3c404215aa3d6c7ae5effc4345f3d07ae7&scene=27](https://mp.weixin.qq.com/s?__biz=MjM5ODIwNjEzNQ==&mid=2649899108&idx=3&sn=95ee5999d518a766ebfbf3e438cc1df7&chksm=bfa92097e3dd8801330d76cdc4a0583f9165bbe07b3c404215aa3d6c7ae5effc4345f3d07ae7&scene=27) 

[22] GNN-based Relax-Optimize-Sample Framework for Max-k-Cut Problems [https://arxiv.org/html/2412.05146v2](https://arxiv.org/html/2412.05146v2) 

[23] 图神经网络(GNN)必读论文综述 [https://blog.csdn.net/yanqianglifei/article/details/116591392](https://blog.csdn.net/yanqianglifei/article/details/116591392) 

[24] GIN：图神经网络表征能力的最强解读 [https://www.cnblogs.com/akaman98/p/17355702.html](https://www.cnblogs.com/akaman98/p/17355702.html) 

[25] XGNN: Model-Level Explanations for Graph Neural Networks [https://dl.acm.org/citation.cfm?id=3403085](https://dl.acm.org/citation.cfm?id=3403085) 

[26] 图神经网络方法与应用综述：翻译与解读 [https://blog.csdn.net/qq_41185868/article/details/103886369](https://blog.csdn.net/qq_41185868/article/details/103886369) 

[27] Multi-View Clustering: A Comprehensive Survey [https://link.springer.com/article/10.1007/s10462-025-11240-8](https://link.springer.com/article/10.1007/s10462-025-11240-8) 

[28] 一文轻松掌握图神经网络 (GNN) [https://m.blog.csdn.net/hecongqing/article/details/104351885](https://m.blog.csdn.net/hecongqing/article/details/104351885) 

[29] 机器学习学术速递[11.12]: 图学习、Transformer、GAN等 [https://cloud.tencent.com/developer/article/1902295](https://cloud.tencent.com/developer/article/1902295) 

[30] GNN 综述：图神经网络的综合调查 [https://blog.csdn.net/sinat_34072381/article/details/110847993](https://blog.csdn.net/sinat_34072381/article/details/110847993) 

[31] Deep Learning for Social Network Analysis: Applications and Schemes [https://link.springer.com/article/10.1007/s13278-021-00799-z](https://link.springer.com/article/10.1007/s13278-021-00799-z) 

[32] XGNN: Model-Level Explanations for Graph Neural Networks via Graph Generation [https://dl.acm.org/doi/10.1145/3394486.3403085](https://dl.acm.org/doi/10.1145/3394486.3403085) 

[33] GNN论文周报：北大、中山、港中文、MIT等机构前沿研究 [https://zhuanlan.zhihu.com/p/633082954](https://zhuanlan.zhihu.com/p/633082954) 

[34] EasyNLP: A Toolkit for Building NLP Applications with Pre-Trained Models [https://dl.acm.org/doi/abs/10.1145/3511808.3557510](https://dl.acm.org/doi/abs/10.1145/3511808.3557510) 

[35] 图神经网络概述及AI for Science应用 [https://www.bilibili.com/read/cv26979449/](https://www.bilibili.com/read/cv26979449/) 

[36] kgcnn: Graph Neural Networks in TensorFlow-Keras with RaggedTensors [https://www.sciencedirect.com/science/article/pii/S266596382100035X](https://www.sciencedirect.com/science/article/pii/S266596382100035X) 

[37] MPNN：图神经网络的通用框架——消息传递神经网络 [https://developer.baidu.com/article/detail.html?id=2190425](https://developer.baidu.com/article/detail.html?id=2190425) 

[38] 人工智能学术速递(2021.7.19): 论文标题集锦 [https://cloud.tencent.com/developer/article/1852788](https://cloud.tencent.com/developer/article/1852788) 

[39] 机器学习学术速递[12.10]：图学习、Transformer、GAN与半/弱/无监督学习进展 [https://cloud.tencent.com/developer/article/1917002](https://cloud.tencent.com/developer/article/1917002) 

[40] GNN在交通与物流中的应用：流量预测与路径优化 [https://wenku.csdn.net/column/uqry6yyrkq](https://wenku.csdn.net/column/uqry6yyrkq) 

