# A Survey on Generative Diffusion Models

# 0. A Survey on Generative Diffusion Models

## 1. Introduction
Generative modeling, a fundamental pursuit in artificial intelligence, has evolved significantly in its capacity to synthesize novel data. Early advancements saw the prominence of Generative Adversarial Networks (GANs) as a powerful class of models [32,38]. However, GANs inherently suffer from several limitations, including susceptibility to mode collapse—which leads to a lack of diversity in generated samples—difficulties in learning multi-modal distributions, challenges in achieving stable training due to their adversarial game-theoretic framework, and often extended training times [23,32,38]. The adversarial nature of GAN training also makes debugging and optimization particularly intricate [17].



**Comparison: Generative Adversarial Networks (GANs) vs. Diffusion Models (DMs)**

| Feature             | Generative Adversarial Networks (GANs)              | Generative Diffusion Models (DMs)                       |
| :------------------ | :-------------------------------------------------- | :------------------------------------------------------ |
| **Core Mechanism**  | Adversarial game between Generator and Discriminator | Two-stage process: Forward Diffusion & Reverse Denoising |
| **Training**        | Unstable, challenging, often extended training times | Enhanced stability, likelihood-maximization objective   |
| **Mode Collapse**   | Susceptible (lack of diversity)                     | Superior mode coverage (mitigates mode collapse)        |
| **Multi-modal Dist.** | Difficulties in learning multi-modal distributions  | Robust in modeling complex multi-modal distributions   |
| **Fidelity**        | High-fidelity samples, but can suffer from artifacts | High-fidelity sample generation, state-of-the-art       |
| **Theoretical Basis** | Game-theoretic framework                            | Non-equilibrium thermodynamics, mathematically tractable |
| **Debugging**       | Particularly intricate                              | More streamlined and robust                             |

In recent years, diffusion models (DMs) have emerged as a promising approach to generative modeling, drawing theoretical inspiration from non-equilibrium thermodynamics [32,38]. These models have rapidly achieved state-of-the-art performance across various domains, particularly in image generation, surpassing GANs in several aspects [1,23,26,29,35]. The key strengths of diffusion models include their capability for high-fidelity sample generation, enhanced training stability due to their likelihood-maximization objective, theoretical tractability, and superior mode coverage—which mitigates the mode collapse issue prevalent in GANs [6,7,30,36,38]. This paradigm offers a more streamlined and robust approach to generative modeling, circumventing complex issues like adversarial training and intractable partition functions encountered in other generative frameworks such as VAEs, EBMs, and normalizing flows [23,30].

The core principles of diffusion models revolve around a two-stage process: a forward diffusion process and a reverse diffusion process [8,38]. In the forward process, data is progressively perturbed by the gradual addition of random Gaussian noise, transforming the original data distribution into a simple, tractable prior distribution—typically a standard Gaussian [7,8,9,32]. Conversely, the reverse process involves learning to iteratively remove this noise from the noisy data to recover the original high-fidelity sample [8,38]. This learning is achieved by training a neural network to predict and remove noise at each step of a Markov chain, effectively reversing the diffusion process [17,32]. This noise-based generation paradigm offers higher mathematical controllability and generation stability, directly addressing the limitations of previous generative models by providing a more stable and diverse generation mechanism without reliance on adversarial dynamics [23,38]. Under specific hyperparameter settings, the training process of diffusion models can be equivalent to denoising score matching, and the sampling process to annealed Langevin dynamics, highlighting their deep theoretical connections [6,36].

Despite the growing body of literature on diffusion models, existing surveys often exhibit limitations—such as a lack of comprehensive coverage on specific aspects, including controllable generation techniques or a systematic review of the latest advancements [5,22,31].

This survey aims to provide a comprehensive overview of the latest advancements in generative diffusion models. Our contributions include a systematic review of the field, offering a critical analysis of different algorithmic approaches and their practical implications. We categorize improvements in diffusion models, focusing on enhancing sampling speed, improving likelihood estimation, and strengthening data generalization [5,28,31]. Furthermore, we explore their wide-ranging applications across various domains—including computer vision, natural language processing, and multimodal modeling—and discuss future research directions and open challenges in this rapidly evolving field [25,35].
## 2. Background and Theoretical Foundations

Generative Diffusion Models represent a powerful class of probabilistic generative models, fundamentally designed to learn complex data distributions by modeling a two-stage process: a forward diffusion and a reverse diffusion. This framework allows for the synthesis of high-fidelity data samples by progressively transforming data into noise and subsequently learning to reverse this transformation [1,15,25,26,27,28,30,31,35].

The process commences with a **forward diffusion process**, a meticulously defined mechanism that gradually corrupts a clean data sample \( \mathbf{x}_0 \) by systematically injecting Gaussian noise over a series of discrete timesteps \( T \). This is conceptualized as a Markov chain, where each state \( \mathbf{x}_t \) depends solely on its immediate predecessor \( \mathbf{x}_{t-1} \) [6,14]. Through sequential additions of Gaussian noise, controlled by a predefined variance schedule \( \beta_t \), the original data distribution is transformed into a tractable, typically standard Gaussian, noise distribution by the final timestep \( \mathbf{x}_T \) [6,8,29,36,38]. The ability to directly sample any noisy state \( \mathbf{x}_t \) from the initial data point \( \mathbf{x}_0 \) due to the additive property of Gaussian distributions is crucial for efficient training.

Complementary to this is the **reverse diffusion process**, the generative phase, which learns to systematically transform an unstructured noise vector back into a coherent data sample. This involves learning a denoising process, where noise is gradually removed across the discrete timesteps, effectively reversing the corruption introduced in the forward phase [6,8,14,29]. This learned process aims to approximate the true conditional probabilities, enabling the reconstruction of the original data distribution.

A more generalized and continuous perspective on these processes is provided by **Stochastic Differential Equations (SDEs)** and **Score SDEs**. This framework unifies and extends discrete-time models, such as Denoising Diffusion Probabilistic Models (DDPMs) and Score-Based Generative Models (SGMs), by describing both the forward noise injection and the reverse denoising process as solutions to SDEs [29,30,35]. The key insight here is that the reverse SDE, which enables sample generation, relies critically on the estimation of the score function—the gradient of the log probability density of the perturbed data distribution.

The training of diffusion models heavily relies on the concept of **score matching**. This technique involves estimating the score function, which signifies the direction of increasing data likelihood, and is fundamental for guiding the reverse diffusion process towards meaningful data samples [6,36]. By training a neural network to estimate this score function (often implicitly by predicting the noise added to data, as in denoising score matching), models learn how to effectively denoise samples and generate new data. The primary training objectives, such as maximizing the variational lower bound (VLB) of the data likelihood, are often simplified in practice to minimize the mean squared error between the predicted noise and the actual noise added during the forward process.

Finally, diffusion models can be classified based on their specific formulations and training objectives. Prominent examples include **Denoising Diffusion Probabilistic Models (DDPMs)**, which explicitly model discrete Markov chains for both forward and reverse processes and simplify the training objective to noise prediction, and **Score-Based Generative Models (SGMs)**, which leverage the score function directly for generation via techniques like Langevin dynamics [1,9,17]. The SDE framework provides a unifying lens, demonstrating that DDPMs and SGMs can be viewed as specific discretizations of continuous-time SDEs, highlighting the theoretical coherence and versatility of the diffusion model paradigm.
### 2.1 Forward Diffusion Process
The forward diffusion process lies at the core of generative diffusion models, serving as a meticulously designed mechanism to systematically transform a clean data sample into pure Gaussian noise. This process is conceptualized as a Markov chain, where each successive state \( x_t \) is contingent solely upon its immediately preceding state \( x_{t-1} \) [1,5,6,14,20,21,25,31,35]. The gradual transformation is achieved through the sequential addition of Gaussian noise at each timestep, ultimately leading the data distribution to converge towards a standard Gaussian distribution as the process progresses to a sufficiently large number of steps \( T \) [1,5,7,9,10,23,31,35,38].

The transition probability distribution, or Gaussian transition kernel, from \( x_{t-1} \) to \( x_t \) is formally expressed as:
\
Here, \(\mathbf{I}\) denotes the identity matrix, and \(\beta_t\) represents the variance schedule, a critical hyperparameter that dictates the amount of Gaussian noise injected at each step \( t \) [1,6,14,23,25,26,27,38]. Typically, the variance schedule \(\beta_t\) is a pre-defined sequence, satisfying the condition
\
implying that more noise is progressively added as the timestep \( t \) increases [25,27,38]. This controlled noise injection ensures that \( x_T \) ultimately approximates an isotropic Gaussian distribution [31,38].

A notable advantage of this Markovian structure is the ability to directly sample \( x_t \) from the initial data point \( x_0 \) without requiring iterative computations through intermediate steps. This closed-form expression, derived from the additive property of Gaussian distributions, is pivotal for efficient training and generation processes [14,25]. By defining 
\
the conditional marginal distribution \( q(\mathbf{x}_t \mid \mathbf{x}_0) \) can be obtained as:
\
This formula directly relates any noisy state \( \mathbf{x}_t \) to the original data \( \mathbf{x}_0 \) [6,20,21,26,27,35,38].

To enable efficient computation and gradient-based optimization within this framework, the reparameterization trick is indispensable [6,8,17,23]. This technique allows for the expression of \( \mathbf{x}_t \) as a deterministic function of \( \mathbf{x}_0 \) and a standard Gaussian noise variable \( \boldsymbol{\epsilon} \):
\
This reparameterization bypasses the need to sample directly from a distribution whose parameters depend on the data, thus making the entire process amenable to gradient-based learning algorithms [8,17,26].

The choice of noise schedule, \(\beta_t\), significantly impacts the dynamics of the forward diffusion process and, consequently, the convergence and stability of the training. Common strategies for defining this schedule include linear, quadratic, and cosine schedules [14,21,25]. For instance, Denoising Diffusion Probabilistic Models (DDPMs) typically employ a linear schedule where \(\beta_t\) increases linearly from a small value (e.g., \(10^{-4}\)) to a larger one (e.g., \(0.02\)) over \( T \) timesteps [25]. Other works, such as those exploring cosine schedules for \(\bar{\alpha}_t\), aim to provide smoother noise progression across timesteps [21]. The design of these schedules is critical as they control the rate at which information is destroyed and noise is introduced, directly influencing the learning difficulty at different timesteps and thus affecting the overall training efficiency and the quality of generated samples [20]. While the variance schedule is typically treated as a hyperparameter and not learned during training, its careful selection is paramount for robust model performance [27].
### 2.2 Reverse Diffusion Process
The reverse diffusion process is the generative phase of a diffusion model, meticulously designed to transform an unstructured noise vector, typically sampled from a standard Gaussian distribution, back into a coherent data sample [7,28]. This transformation is achieved through a learned denoising process, where noise is gradually removed across a series of discrete time steps [6,14]. Conceptually, it involves learning a Markov chain that iteratively refines the noisy input, restoring the unperturbed data structure [28,35].

Mathematically, the reverse diffusion process aims to learn the conditional probabilities 
$$p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)$$ 
for each step from $t=T$ down to $t=1$, where $\mathbf{x}_T$ is pure noise and $\mathbf{x}_0$ is the target data distribution [26,27]. The overall joint distribution is learned as:
$$
p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)
$$
where 
$$p(\mathbf{x}_T) = \mathcal{N}(\mathbf{0}, \mathbf{I})$$ 
is the prior distribution [27]. Given that the forward process transitions are Gaussian, the reverse transitions 
$$p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)$$ 
can also be approximated as Gaussian, particularly when the noise schedule parameter $\beta_t$ is small [1,5,31]. This is parameterized by a neural network:
$$
p_{\theta}(\mathbf{x}_{t-1}\vert\mathbf{x}_t) = \mathcal{N}\Bigl(\mathbf{x}_{t-1};\ \boldsymbol{\mu}_{\theta}(\mathbf{x}_t,t),\ \boldsymbol{\Sigma}_{\theta}(\mathbf{x}_t, t)\Bigr)
$$
where the neural network, represented by parameters $\theta$, learns to predict the mean 
$$\boldsymbol{\mu}_{\theta}(\mathbf{x}_t,t)$$ 
and, in some formulations, the covariance 
$$\boldsymbol{\Sigma}_{\theta}(\mathbf{x}_t, t)$$ 
[6,26]. A common practice, especially in DDPMs, is to fix the variance $\Sigma_\theta$ (e.g., to $\beta_t$) and train the neural network to predict the noise $\epsilon$ that was added at step $t$ in the forward process [17,25]. This predicted noise $\epsilon_\theta(\mathbf{x}_t, t)$ then allows for the estimation of the denoised mean $\mu_\theta(x_t, t)$, as exemplified by the expression:
$$
\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right)
$$
This strategy simplifies the objective function and often enhances sample quality [14,25].

The neural network tasked with this denoising prediction is typically a U-Net, known for its effectiveness in image-to-image translation and noise reduction tasks due to its skip connections preserving fine-grained details across different resolutions [8,14,25]. While U-Nets are predominant, other architectures such as Transformers can also be employed to model the denoising process, depending on the data modality and specific requirements.

The training objective for learning the reverse process is fundamentally based on maximizing the variational lower bound (VLB) of the data likelihood, which is equivalent to minimizing the KL divergence between the true and learned posterior distributions [28]. In practice, this often translates to minimizing the difference between the predicted noise $\epsilon_\theta(\mathbf{x}_t, t)$ and the actual noise $\epsilon$ added during the forward process at each time step [6,8]. This weighted prediction of noise across time steps forms the core of the loss function [28].

A critical aspect of the reverse process is its guidance by the score function, which is the gradient of the log probability density function, 
$$\nabla_x \log p_t(x_t)$$ 
[9,20,23]. This score function provides the direction for denoising, steering the generative process towards regions of higher data likelihood and lower noise [9,24]. In many diffusion models, estimating the score function is directly equivalent to predicting the noise or even the original image, simplifying the parameterization and optimization of the reverse steps [20,36].

For generating new data samples from the learned diffusion model, various sampling techniques are employed. The most straightforward is ancestral sampling, where the process begins with pure Gaussian noise 
$$\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$$ 
and iteratively applies the learned denoising steps from $t=T$ down to $t=1$, refining the sample at each step [25]. This is expressed as:
$$
x_{t-1} = \frac{1}{\sqrt{\alpha_t}}\Bigl(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t, t)\Bigr) + \sigma_t z
$$
where $z$ is sampled from $\mathcal{N}(0, I)$ and $\sigma_t$ determines the noise variance for the step [8]. Beyond this standard approach, techniques like Denoising Diffusion Implicit Models (DDIM) offer faster sampling by allowing for non-Markovian generation steps, though not explicitly detailed in the provided digests. Another novel approach is the Momentum Sampler, which reverses the general linear corruption process by using a convex combination of corruptions at different diffusion levels, enhancing sampling efficiency [11]. This systematic denoising, guided by learned parameters, enables diffusion models to generate high-quality, diverse data samples.
### 2.3 Denoising Diffusion Probabilistic Models (DDPMs)
Denoising Diffusion Probabilistic Models (DDPMs) constitute a powerful class of generative models, unified under the broader framework of diffusion models [11]. They represent a significant advancement in generative modeling, particularly noted for their ability to generate high-resolution data [18]. At their core, DDPMs operate through two parameterized Markov chains: a forward process that progressively adds noise to data and a reverse process that learns to denoise and reconstruct the original data [17,23,30]. This two-stage mechanism enables DDPMs to predict the original image distribution by modeling the distribution of noise added to images [9,29].

The **forward diffusion process** (or corruption process) is a fixed Markov chain that gradually transforms an original data sample 
$$\boldsymbol{x}_0 \sim q(\boldsymbol{x}_0)$$ 
into pure Gaussian noise over $T$ discrete time steps [15]. At each step $t$, a small amount of Gaussian noise is added to the data point from the previous step, $\boldsymbol{x}_{t-1}$, to produce $\boldsymbol{x}_t$. This transition is defined by the conditional probability distribution:
$$q(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}) = \mathcal{N}\Bigl(\boldsymbol{x}_t; \sqrt{\alpha_t}\,\boldsymbol{x}_{t-1},\, (1-\alpha_t)I\Bigr).$$
Here, $\alpha_t$ is a parameter directly influencing the noise added at step $t$ [8]. More commonly, this process is controlled by a noise schedule parameter $\beta_t \in (0, 1)$, where $\beta_t$ represents the variance of the Gaussian noise introduced at step $t$. The relationship between these parameters is typically 
$$\alpha_t = 1 - \beta_t,$$
and a common term 
$$\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$$ 
is often used to directly relate $\boldsymbol{x}_t$ to $\boldsymbol{x}_0$ via 
$$q(\boldsymbol{x}_t \mid \boldsymbol{x}_0) = \mathcal{N}\Bigl(\boldsymbol{x}_t; \sqrt{\bar{\alpha}_t}\,\boldsymbol{x}_0,\, (1-\bar{\alpha}_t)I\Bigr)$$
[26]. The schedule of $\beta_t$ values (e.g., linearly increasing) ensures that as $T$ approaches infinity, the data distribution $\boldsymbol{x}_T$ converges to a standard Gaussian distribution, independent of the initial data $\boldsymbol{x}_0$ [15,31].

The **reverse diffusion process** is a learned Markov chain that aims to reverse the noise-adding steps, gradually transforming a sample from pure Gaussian noise (i.e., 
$$\boldsymbol{x}_T \sim \mathcal{N}(0, I)$$) back into a clean data sample $\boldsymbol{x}_0$. This is achieved by iteratively removing noise using a neural network, parameterized by $\theta$, to approximate the conditional probability 
$$p_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t).$$ 
This neural network learns to predict the noise component $\epsilon$ that was added at each step, or equivalently, the mean and covariance of the reverse transition probability [8,14,26]. Specifically, the reverse step is modeled as:
$$p_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t) = \mathcal{N}\Bigl(\boldsymbol{x}_{t-1};\, \boldsymbol{\mu}_\theta(\boldsymbol{x}_t, t),\, \boldsymbol{\Sigma}_\theta(\boldsymbol{x}_t, t)\Bigr).$$
In the foundational DDPM work by Ho et al., the covariance $\boldsymbol{\Sigma}_\theta(\boldsymbol{x}_t, t)$ is often fixed to a constant, and the mean $\boldsymbol{\mu}_\theta(\boldsymbol{x}_t, t)$ is parameterized as a function of the predicted noise [26]. The iterative sampling process begins with $\boldsymbol{x}_T$ (pure noise) and proceeds backwards through $T$ steps, with the U-Net architecture commonly employed to predict the noise $\epsilon_\theta(\boldsymbol{x}_t, t)$ from the noisy input $\boldsymbol{x}_t$ and time step $t$ [14]. This allows for the iterative reconstruction of $\boldsymbol{x}_{t-1}$ until the original data $\boldsymbol{x}_0$ is obtained [14].

The **training objective** of DDPMs is to maximize the likelihood of the training data, which is equivalent to maximizing the variational lower bound (VLB) of the negative log-likelihood [17,28]. This VLB is derived from variational inference and serves as the loss function for learning [1,15]. Conceptually, maximizing the VLB is equivalent to minimizing the Kullback-Leibler (KL) divergence between the true posterior 
$$q(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0)$$ 
and the learned reverse distribution 
$$p_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)$$ 
over all time steps [28].

While the full VLB can be complex, involving multiple terms (reconstruction, prior matching, consistency) [20], a simplified training objective proved highly effective in practice. This simplified objective focuses on training the neural network to predict the noise added at each step. Specifically, the model is trained to minimize the mean squared error between the actual noise $\boldsymbol{\epsilon}$ added during the forward process and the noise $\epsilon_\theta(\boldsymbol{x}_t, t)$ predicted by the neural network [8]:
$$L_{simple} = \mathbb{E}_{t \sim U(1,T),\, \boldsymbol{x}_0 \sim q(\boldsymbol{x}_0),\, \boldsymbol{\epsilon} \sim \mathcal{N}(0, I)} \Bigl.$$
This formulation, known as the simplified objective, effectively measures the distance between the true noise and the noise estimate for a random time step $t$ of the forward process [26]. Other variants of the loss function also exist, such as directly predicting the cleaned sample $\boldsymbol{x}_0$ or minimizing the negative log-likelihood decomposed into multiple terms [6,21]. The optimization objective involves minimizing the negative log-likelihood, which can be expressed in terms of expectations over the data and diffusion process [6]. This robust training framework allows DDPMs to learn complex data distributions and generate high-fidelity samples.
### 2.4 Score-Based Generative Models (SGMs)
Score-Based Generative Models (SGMs) represent a class of generative models that leverage the concept of the score function to model complex data distributions. Fundamentally, the score function of a probability density p(x) is defined as the gradient of its logarithm with respect to the input data x, denoted as 
$$\nabla_x \log p(x).$$ 
This score function essentially points in the direction of increasing data likelihood, guiding samples toward denser regions of the data distribution [9,20,23,24,26,28,29] [20].

SGMs establish a forward diffusion process by constructing a Stochastic Differential Equation (SDE) to smoothly perturb the original data distribution into a known prior distribution, typically a simple Gaussian noise distribution. This SDE is generally expressed as 
$$dx = f(x, t)\,dt + g(t)\,dw,$$ 
where f is the drift coefficient, g is the diffusion coefficient, and w is the standard Wiener process [1,5,15,31,35].
To reverse this process and generate data, a corresponding reverse SDE—which requires knowledge of the score function at each time step—is employed [5,15,31].
Notably, Denoising Diffusion Probabilistic Models (DDPMs) can be viewed as discrete forms of SGMs [5,11,15,26,31].

The estimation of this score function is central to SGMs. To achieve this, SGMs perturb data with a series of Gaussian noise perturbations at different scales [26,28,29,36]. A deep neural network, specifically a Noise-Conditional Score Network (NCSN), is trained to estimate the score function for these noise-perturbed data distributions across various noise levels [18,23,26,28,30]. This training is typically performed using score-matching techniques, which involve optimizing objectives such as 
$$L = \mathbb{E}_t\,\mathbb{E}_{x_t},$$ 
or 
$$L(\theta) = \sum_{t=1}^{T}\lambda(\sigma_t)\,\mathbb{E}_{x\sim p_{\sigma_t}(x)}$$ 
to align the network’s output with the true score [10,11,15,26,31].

Once the score function is estimated, it is used to guide the sampling process, which is typically executed through Langevin dynamics or Langevin Monte Carlo [6,24,26,28,29,30,36]. The Langevin dynamics algorithm is an iterative method where samples are updated by moving in the direction of the estimated score function, coupled with a random Brownian motion term [26,29]. The iterative update can be formulated as 
$$x_{i+1} = x_i + \gamma\,\nabla_x \log p(x_i) + \omega_i,$$ 
where γ is a step size and ω_i is a random noise term [26]. To mitigate issues such as data falling into inaccurate regions or balancing noise addition, techniques such as Annealed Langevin dynamics have been introduced [29]. Furthermore, advancements like Critically-Damped Langevin Diffusion (CLD) have demonstrated superior performance by interpreting the process as a joint diffusion in an extended space, incorporating auxiliary “velocity” variables analogous to Hamiltonian dynamics [37].

A significant advantage of score-based models is their ability to achieve sampling quality comparable to Generative Adversarial Networks (GANs) without requiring adversarial training [30,36]. This bypasses the challenges associated with GAN training, such as mode collapse and training instability. Additionally, SGMs can model data distributions without requiring explicit normalization [24]. This robust framework has positioned SGMs as a powerful paradigm for generative modeling.
### 2.5 Stochastic Differential Equations (SDEs) and Score SDEs
Stochastic Differential Equations (SDEs) provide a powerful and unified continuous-time framework for describing both the forward and reverse processes inherent in generative diffusion models. This continuous formulation, notably influenced by the work of Yang Song et al., offers a generalized perspective that encompasses and extends discrete-time models such as Denoising Diffusion Probabilistic Models (DDPMs) and Score-based Generative Models (SGMs) [11,20,23,24,26,29,30]. This framework is considered a core approach for various generative tasks, including unconditional image generation [18].

In this framework, the forward diffusion process, which gradually transforms a complex data distribution into a tractable prior distribution (typically Gaussian noise), is conceptualized as an SDE that continuously injects noise into the data [23,24]. This process can be formally expressed as:
$$dx = f(x, t)dt + \sigma(t)dw_t$$
where $x$ represents the data at time $t$, $f(x, t)$ is the drift coefficient determining the tendency of the process, $\sigma(t)$ is the diffusion coefficient controlling the magnitude of injected noise, and $dw_t$ represents a standard Wiener process (Brownian motion) [26,29]. The forward processes of DDPMs and SGMs can be precisely represented as specific discretizations of such forward SDEs, demonstrating the unifying power of this continuous perspective [10,23,28].

Conversely, the reverse process, crucial for sample generation, is formulated as solving a reverse-time SDE. This SDE iteratively removes noise, transforming the simple prior distribution back into the original complex data distribution [10,23,24]. A common form for this reverse-time SDE is given by:
$$dx = dt + g(t)d\bar{w}$$
where $\bar{w}$ is the standard Wiener process in reverse time [1]. This formulation highlights that the key to reversing the diffusion process and generating data lies in accurately estimating the score function, $\nabla_x \log p_t(x)$, which represents the gradient of the log-probability density of the perturbed data distribution at time $t$ [1,24]. The reverse-time SDE indicates that by accurately estimating and eliminating the "drift" caused by the forward noise injection, the original data can be recovered from pure noise [26]. Furthermore, this reverse-time SDE has an equivalent form known as the probability flow Ordinary Differential Equation (ODE), which can be solved using standard ODE solvers to efficiently generate samples [28,29]. This ability to leverage ODE solvers offers flexibility and efficiency in sampling strategies.

Score Stochastic Differential Equations (Score SDEs) provide an overarching theoretical framework that unifies various diffusion and denoising processes. They describe these processes as solutions to SDEs, where the SDEs' drift and diffusion functions are specifically crafted. This framework not only covers the forward noise injection but critically defines the reverse process using the score function, thereby enabling the generation of novel samples [18,28]. The elegance of Score SDEs lies in their ability to provide a continuous mathematical foundation for generative diffusion models, offering a flexible and powerful paradigm for synthesizing complex data distributions. Practical implementations further demonstrate the utility of setting up SDEs for training and inference in these models [36].
### 2.6 Score Matching and Training Objectives
Generative diffusion models rely on sophisticated training objectives to effectively learn the underlying data distribution and guide the generative process. A fundamental concept in this context is score matching, which is a technique for estimating the score function, defined as the gradient of the log-probability density of the data distribution with respect to the data itself, i.e., 
$$\nabla_{\mathbf{x}} \log p(\mathbf{x}).$$ 
[36] This score function plays a crucial role in directing the reverse diffusion process, enabling the model to transform noise into meaningful data samples 
[35]. The score matching objective is intimately connected to Fisher divergence, which quantifies the distance between two probability distributions 
[30,36].

Various approaches have been developed to estimate the score function. Denoising score matching is a prominent technique, which involves training a neural network to predict the noise added to data, where this predicted noise implicitly serves as an estimate of the score function 
[10,23,36]. For instance, some models are trained using a weighted sum of denoising score matching objectives 
[36]. Another significant advancement is sliced score matching, introduced to provide a more scalable approach for density and score estimation by matching random projections of the original scores 
[24]. Beyond these, novel methods like Soft Score Matching have been proposed, specifically designed for general linear corruption processes, provably learning scores as long as additive noise is present in the corruption 
[11]. In the context of Score SDEs, a time-varying score model is parameterized to estimate the score function by generalizing the score matching objective 
[28]. Furthermore, specialized score matching objectives have been derived for models like Critically-Damped Langevin Diffusion (CLD), where the model learns the score function of the conditional distribution of velocity given data, simplifying the learning task compared to directly learning data scores 
[37].

The training of diffusion models is often framed within the maximum likelihood estimation paradigm, aiming to maximize the log-likelihood of the model's predicted distribution, which is equivalent to minimizing the negative log-likelihood 
[9,17]. A common approach to achieve this is through the optimization of the variational lower bound (VLB) or Evidence Lower Bound (ELBO) of the data log-likelihood 
[17,25,30]. The VLB can be expressed using Kullback-Leibler (KL) divergence, as shown by the objective 
$$L = \mathbb{E}_q,$$ 
[17].

In practice, a simplified training objective is widely adopted, particularly in models like Denoising Diffusion Probabilistic Models (DDPMs). This simplified objective approximates the prediction of the noise $\epsilon$ added during the forward diffusion process. The goal is to minimize the difference between the predicted noise 
$$\epsilon_\theta(\mathbf{x}_t, t)$$ 
and the actual noise $\epsilon$ added to the input $\mathbf{x}_t$ at time step $t$ 
[8,14]. This is commonly formulated as a Mean Squared Error (MSE) loss, such as:
$$
L_{\text{simple}} = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\right\|^2,
$$
where the expectation is taken over the time step $t$ (often sampled uniformly), the initial data $\mathbf{x}_0$, and the noise $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ 
[6,8,25]. This simplified objective demonstrates a direct connection to denoising score matching 
[6]. Some models, such as FaceTalk, adopt a similar diffusion loss that aims to minimize the squared difference between the input and the model's reconstruction from the noisy input 
[21]:
$$
\mathcal{L}_{\boldsymbol{\theta}} = \mathbb{E}_{\boldsymbol{x}, t}\left\|\boldsymbol{x} - \mathcal{G}_{\theta}(\boldsymbol{x}_t, t, \boldsymbol{c})\right\|_2^2.
$$

Ultimately, different objectives for training diffusion models can be derived by varying the approach to optimize the denoising matching term within the ELBO 
[20]. This results in three equivalent perspectives on training diffusion models: predicting the original input, predicting the noise added to the input, or predicting the score of the data distribution 
[20]. The various loss functions found in the literature, while sometimes appearing different, are often mathematically connected and serve to learn the underlying score function necessary for data generation 
[1,5].
## 3. Model Architectures and Advanced Techniques

**Sampling Acceleration Techniques in Diffusion Models**

| Category           | Core Principle                                          | Key Methods/Examples                                    | Advantages                                     | Trade-offs/Considerations                  |
| :----------------- | :------------------------------------------------------ | :------------------------------------------------------ | :--------------------------------------------- | :----------------------------------------- |
| **Learning-Free**  | Optimizing numerical solutions of SDEs/ODEs           | Optimized Discretization, SDE Solvers (ALD, Euler-Maruyama), ODE Solvers (DDIM, DPM-Solver, gDDIM) | Preserves sample fidelity, no extra training | Computation proportional to steps, can be slow |
| **Learning-Based** | Training models/modifying process for faster generation | Non-Markovian Process (DDIMs), Partial Sampling (Progressive Distillation), Consistency Models (CMs) | Substantial speed improvements, fewer steps  | Minor trade-offs in sample quality, requires additional training/distillation |

The efficacy and versatility of generative diffusion models are profoundly influenced by their underlying architectural designs and the sophisticated techniques developed to enhance their performance and applicability. This section provides a comprehensive overview of the principal advancements in these areas, moving beyond foundational principles to address critical aspects such as model efficiency, controllability, and adaptability to diverse data structures.



**Comparison: U-Net vs. Transformer Architectures in Diffusion Models**

| Feature               | U-Net (e.g., DDPM, LDM)                             | Transformer (e.g., integrated in U-Nets, LDMs)         |
| :-------------------- | :-------------------------------------------------- | :----------------------------------------------------- |
| **Primary Role**      | Noise prediction, fine-grained spatial detail preservation | Capturing global dependencies, handling conditional info |
| **Structure**         | Encoder-Decoder with skip connections               | Attention mechanisms (self-attention, cross-attention) |
| **Strengths**         | Processing local features, computational efficiency for standard resolutions, inductive biases for images | Capturing long-range dependencies, integrating complex conditional information, coherent and semantically rich outputs |
| **Limitations**       | Computationally intensive for very high resolutions without latent space (e.g., LDM) | Quadratic computational/memory complexity with input sequence length, expensive for raw high-res pixels |
| **Typical Use**       | Core denoising network in image DMs                 | Conditioning (text-to-image), global context within U-Net |
| **Adaptations**       | Wide Residual Networks (WRN), Group Normalization (GN), Efficient U-Net, operate in latent space (LDMs) | Positional encodings for time step, multi-head attention, cross-attention for external conditioning (e.g., CLIP) |

A cornerstone of diffusion model design lies in its neural network architecture, predominantly featuring **U-Nets** and increasingly incorporating **Transformers**. U-Nets, with their characteristic encoder-decoder structure and skip connections, are optimally suited for tasks requiring fine-grained spatial detail preservation, making them a standard for noise prediction in image-based diffusion models. Their strengths lie in processing local features and maintaining computational efficiency for standard resolutions. Conversely, Transformers, leveraging their attention mechanisms, excel at capturing global dependencies and integrating complex conditional information, thereby enhancing the model's ability to generate coherent and semantically rich outputs. The synergistic combination of U-Nets for local feature extraction and Transformers for global contextual understanding and conditioning represents a robust architectural paradigm in contemporary generative diffusion models, mitigating the limitations of each individually [2,4,7,17,21].

Building upon these architectural foundations, a significant research thrust has been directed towards achieving **controllable generation**. This core concept is pivotal for expanding the practical utility of diffusion models, allowing users to guide the generative process based on specific conditions or attributes [22]. The section delves into a taxonomy of techniques for achieving this control, including various conditioning methods like concatenation and cross-attention, as well as advanced guidance strategies such as classifier-guided and classifier-free guidance. These methods transform unconditional generators into versatile tools capable of diverse conditional generation tasks, from text-to-image synthesis to multi-modal fusion. The discussion also touches upon related capabilities like image editing, inversion, and customization, which further extend the interactive potential of diffusion models [2,13,22,29].

Despite their powerful generative capabilities, diffusion models traditionally suffer from slow sampling speeds, necessitating substantial iterative steps during the reverse process. Consequently, a major area of advancement involves **sampling acceleration techniques**. These methods are broadly categorized into learning-free approaches, which optimize the numerical solutions of underlying stochastic or ordinary differential equations, and learning-based approaches, which involve training models to achieve faster generation, often with minor trade-offs in sample quality. The section analyzes these techniques, comparing their impact on generation speed, sample quality, and training stability, and discusses the inherent trade-offs involved in achieving efficiency [1,3,15,16,24,28].

Furthermore, the ability of diffusion models to provide accurate density estimations, crucial for applications like image compression and semi-supervised learning, is explored under the theme of **likelihood enhancement and optimization**. This sub-section details various approaches designed to improve the maximum likelihood estimation, primarily by obtaining tighter bounds on the data likelihood. Key strategies include meticulous objectives designing to align loss functions with likelihood maximization, optimizing noise schedules in the forward process, and learning the variance of the reverse process. These advancements are critical for improving both the quantitative performance of likelihood estimation and the qualitative aspects of sample generation [3,15,16,31].

Finally, this section addresses the crucial challenge of **handling special data structures and improving data generalization**. While initial diffusion models were largely confined to continuous, Euclidean data, significant progress has been made in adapting them for diverse modalities, including manifold, invariant, and discrete data. This involves techniques such as feature space unification, which transforms data into a continuous latent space before diffusion, and data-dependent transition kernels, which tailor the diffusion process to the inherent characteristics of specific data types. The discussion highlights how innovations like latent spaces, Soft Diffusion, and specialized models such as FaceTalk tackle the complexities of various data formats, thereby broadening the applicability and robustness of diffusion models across a wide spectrum of real-world scenarios [1,3,4,11,15,16,21].

Collectively, the advancements in model architectures and advanced techniques underscore the dynamic evolution of generative diffusion models, transforming them into more efficient, controllable, and generalizable tools for a wide array of generative tasks.
### 3.1 Network Architectures (U-Nets, Transformers)
The efficacy of generative diffusion models hinges significantly on their underlying neural network architectures, with U-Nets and, increasingly, Transformers, serving as foundational components. These architectures are meticulously adapted and optimized to address the specific challenges of noise prediction and high-quality data generation [17].

The U-Net architecture, initially designed for medical image segmentation, has become a cornerstone in diffusion models due to its inherent suitability for spatially structured data such as images [19,25]. Its characteristic "U" shape comprises a contracting path (encoder) that captures context by progressively downsampling feature maps, and an expansive path (decoder) that recovers spatial resolution through upsampling [17,25]. Crucially, skip connections directly link corresponding feature maps from the encoder to the decoder. This mechanism allows the network to effectively propagate fine-grained details that might otherwise be lost during downsampling, thereby enhancing the model's ability to reconstruct clear, high-fidelity data during the denoising process [25]. In the context of diffusion models, the U-Net typically functions as the noise prediction network in the reverse diffusion process [8]. Specific adaptations for diffusion models include the integration of Wide Residual Networks (WRN) as the core structure for noise prediction in DDPM [17]. Furthermore, Group Normalization (GN) is frequently employed for improved training stability and performance [17]. For computational efficiency and faster convergence, models like Imagen utilize specialized "Efficient U-Net" architectures [2]. Latent Diffusion Models (LDMs) further enhance efficiency for high-resolution image synthesis by applying U-Nets within a learned latent space, enabled by pre-trained autoencoders that reduce data dimensionality before the diffusion process, significantly alleviating computational burden and memory usage [4].

Transformers, known for their prowess in sequence modeling and attention mechanisms, are increasingly integrated into diffusion model architectures, particularly for handling conditional information and global dependencies. Positional encodings, a concept borrowed from Transformers, are applied to encode the diffusion time step $t$ within the U-Net, allowing the model to condition its noise prediction on the current step in the diffusion process [17]. A more direct integration involves attention layers, including self-attention or multi-head self-attention, which are strategically placed between convolutional layers within the U-Net architecture to capture long-range dependencies across the image features [17]. Beyond self-attention, cross-attention mechanisms are vital for conditioning diffusion models on external inputs, such as text or image embeddings. In LDMs, a transformer network encodes conditional information into a latent embedding, which then interacts with the U-Net's intermediate layers via cross-attention [32]. The cross-attention mechanism is mathematically represented as:
$$
Attention(Q, K, V) = \operatorname{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
$$
where $Q$, $K$, and $V$ are query, key, and value matrices, respectively, derived from the input and conditioning features, and $d$ is the dimension of the keys [32]. This allows the U-Net to selectively attend to relevant parts of the conditioning information during denoising. Transformers, with their self-attention mechanisms, are also employed in multi-modal applications, for instance, to process image features for generating audio spectrograms [7]. In some cases, multi-head transformer decoders with FiLM layers are explicitly used to incorporate diffusion timestamps for specific tasks like motion diffusion [21].

In terms of benefits and drawbacks, U-Nets excel in processing local spatial features, making them highly effective for pixel-level denoising and reconstruction in image generation due to their convolutional nature and skip connections. Their inductive biases are well-suited for image data, leading to relatively good computational efficiency for standard resolutions. However, without modifications like latent space processing, U-Nets can become computationally intensive and memory-demanding for very high-resolution images. Transformers, conversely, are powerful for capturing global context and long-range dependencies, and are highly effective for incorporating diverse conditional inputs. Their flexibility and ability to model complex relationships contribute to high generation quality, especially when integrating rich semantic information. The primary drawback of standard Transformers lies in their quadratic computational and memory complexity with respect to the input sequence length, making their direct application to high-resolution images prohibitively expensive. This limitation is precisely why they are often used in conjunction with U-Nets or operate on lower-dimensional representations (e.g., latent embeddings) rather than raw pixels. The synergistic combination of U-Nets for efficient local feature processing and Transformers for global contextual understanding and conditional guidance represents a robust architectural paradigm in modern generative diffusion models.
### 3.2 Conditional Diffusion Models and Controllable Generation
Controllable generation is a pivotal capability for diffusion models, significantly expanding their utility in practical applications by enabling users to guide the generation process based on specific conditions or attributes. The field has seen a rapid evolution, with comprehensive surveys defining and categorizing techniques for controlled generation within diffusion models, addressing gaps in broader overviews and providing detailed taxonomies based on formulation, methodologies, and evaluation metrics [22]. The introduction of conditioning into Denoising Diffusion Probabilistic Models (DDPMs) marked a significant advancement, drawing parallels to the impact of Conditional Generative Adversarial Networks (CGANs) and Deep Convolutional Generative Adversarial Networks (DCGANs) in the GAN landscape, underscoring its importance for conditional generation tasks [19].

Various conditioning techniques allow for guiding the generation process based on diverse input modalities. These methods transform unconditional generators into controllable conditional generators by incorporating external information. Common conditioning inputs include text prompts, images, semantic graphs, or bounding boxes, facilitating tasks such as text-to-image synthesis, layout-to-image synthesis, image-to-audio generation, and degradation-robust multi-modal image fusion [4,7,13,32].

Conditioning mechanisms primarily involve integrating the conditional information into the model's architecture. Key approaches include concatenation, where the conditioning information is directly appended to the input features, and cross-attention mechanisms [4,29]. Cross-attention is particularly crucial for its ability to weight the importance of different parts of the input, enabling the model to relate information from conditional signals, such as text prompts, to the generated visual features [25]. Large language models, like a frozen T5-XXL encoder, are often employed to encode input text into rich embeddings, which then condition the image generation process [2]. Other advanced control mechanisms, such as ControlNet, also facilitate fine-grained control over the generated output [29].

Among the prominent conditioning approaches, classifier-guided diffusion and classifier-free diffusion guidance (CFG) represent distinct strategies for incorporating conditional information during the sampling process. Classifier-guided diffusion achieves gradient guidance in each sampling step by employing a classifier, often integrated with a UNet Encoder architecture, to generate conditional gradients for specific labels or attributes [30]. While effective, this approach typically requires a separately trained classifier.

In contrast, classifier-free diffusion guidance offers notable benefits, including improved sample quality and enhanced flexibility, often outperforming classifier-guided methods [29]. CFG operates by training the diffusion model on both conditional and unconditional data. During training, the conditioning input is randomly replaced with null conditioning for a certain percentage of samples, for instance, 25% probability as seen in FaceTalk [21]. During inference, the generation process combines the outputs from conditional and unconditional predictions. This is typically achieved through a weighted combination, allowing for the amplification of conditional guidance strength. The sample at a given step is computed as:
$$
\big\{\boldsymbol{\theta}_{exp}^{c}\big\}^{1:N}_{t} = w \cdot \big\{\boldsymbol{\theta}_{exp}^{c}\big\}^{1:N}_{t} + (1-w) \cdot \big\{\boldsymbol{\theta}_{exp}^{u}\big\}^{1:N}_{t}
$$
where $\boldsymbol{\theta}_{exp}^{c}$ refers to conditionally generated samples and $\boldsymbol{\theta}_{exp}^{u}$ refers to unconditionally generated samples, with $w$ denoting the guidance strength which can be set greater than 1 to amplify the conditioning effect [21]. This mechanism allows for the use of very large classifier-free guidance weights, further enhancing the controllability and quality of the generated outputs [2]. The primary strength of CFG lies in its ability to achieve strong conditioning without requiring an auxiliary classifier, simplifying the training and inference pipeline and yielding superior results compared to adding a separate classifier [29].

While the preceding discussions primarily focus on the generation of novel content based on conditions, an important extension of controllable generation involves editing existing images and customizing models for personalized image generation [29]. Such techniques often rely on inverting images back into the model's latent space, allowing for manipulations and subsequent regeneration. However, specific methodologies for image inversion and detailed approaches for model customization for personalized outputs are not extensively detailed within the provided digests, indicating a potential area for further exploration.
### 3.3 Sampling Acceleration
The original formulation of generative diffusion models, while demonstrating remarkable capabilities in generating high-quality samples, often necessitates a substantial number of iterative steps in their reverse process, thereby leading to slow sampling speeds [15,25,31]. This computational overhead limits their practical applicability in real-time or resource-constrained scenarios [15,31]. Consequently, the development of efficient sampling mechanisms has emerged as a critical research area within the field of diffusion models [9,12].

To address this challenge, research efforts have largely converged on two primary categories of acceleration methods: learning‐free approaches and learning‐based approaches [3,28]. These methodologies are systematically evaluated and compared based on key performance indicators such as generation speed, sample quality, and training stability [1,3,16,28].

Learning‐free sampling methods primarily focus on optimizing the numerical solutions of the underlying Stochastic Differential Equations (SDEs) or Ordinary Differential Equations (ODEs) that govern the diffusion process [1]. Techniques within this category, such as optimized discretization, aim to significantly reduce the number of sampling steps while preserving or even enhancing the fidelity of the generated samples [1,35]. This often involves refining SDE solvers with correctors or leveraging the deterministic trajectories offered by ODE solvers.

In contrast, learning‐based sampling methods involve training a model or modifying the sampling process to achieve faster generation, often accepting minor trade‐offs in sample quality for substantial speed improvements. Key strategies include the adoption of non‐Markovian processes, exemplified by Denoising Diffusion Implicit Models (DDIMs), which allow for larger step sizes and increased sampling efficiency [1,10]. Another prominent learning‐based technique is partial sampling, which encompasses methods like progressive distillation, where a more efficient “student” model is trained to emulate the output of a more complex “teacher” model in fewer steps, effectively halving the required operations [1]. Furthermore, Consistency Models (CMs) represent a significant advancement, capable of generating high‐quality data in a single or very few steps [24]. These models can be trained either by distilling knowledge from pre‐trained diffusion models or as standalone generative models, with recent improvements in their training techniques addressing stability issues and enhancing performance [24,25].

The subsequent sub‐sections will delve into the specific mechanisms and comparative analyses of these learning‐free and learning‐based sampling acceleration techniques.
#### 3.3.1 Learning-Free Sampling
Learning-free sampling methods in generative diffusion models primarily involve the discretization of reverse-time Stochastic Differential Equations (SDEs) or probability flow Ordinary Differential Equations (ODEs) [28]. A fundamental aspect of these methods is that the computational cost of sampling is directly proportional to the number of discretization steps employed [28]. This inherent proportionality has spurred significant research into “Discretization Optimization” techniques, aiming to reduce the number of discrete steps while rigorously maintaining or enhancing the quality of the generated samples [5,15,31]. Complex SDEs, being analytically intractable, necessitate approximate discrete solutions, making the optimization of these discretization methods crucial for efficiency [5,15].

Various approaches have been developed for SDE solvers within this framework. These include methods like Annealed Langevin Dynamics (ALD) and various numerical SDE solvers, such as the Euler-Maruyama sampler [28,36]. Such solvers are fundamental to architectures like Score-Based Generative Modeling, among others [3]. A general methodology for solving the reverse process involves applying the same discrete method used for the forward SDE to its reverse counterpart [5,15,31]. For instance, given a forward SDE discretized as 
  $x_{t+1} = x_t + f(x_t, t)\Delta t + g(t)\sqrt{\Delta t}\,z_t$, 
the reverse SDE can be analogously discretized as 
  $x_t = x_{t+1} + \Delta t + g(t)\sqrt{\Delta t}\,z_t$ [5,15]. 

To further improve sample quality and distribution accuracy, a corrector can be integrated into the SDE solver. This corrector, typically utilizing Markov Chain Monte Carlo (MCMC) methods, adjusts the distribution of newly generated samples at each step to ensure correctness [5,15,31]. Empirical evidence suggests that incorporating such a corrector is more efficient in achieving desired sample distributions than merely increasing the number of solver steps [15].

In addition to SDE solvers, ODE solvers constitute another significant category of learning-free sampling methods [3]. These are often employed in models such as Denoising Diffusion Implicit Models (DDIM) [3] and include methods like gDDIM, DPM-Solver, Pseudo Numerical Methods for Diffusion Models on Manifolds, Fast Sampling of Diffusion Models with Exponential Integrator, and Poisson flow generative models [3]. A notable characteristic of ODE solvers, particularly probability flow ODEs, is their capacity to yield deterministic trajectories, which can contribute to more stable and potentially faster sampling compared to their SDE counterparts [28].

An example of an implicit sampler, such as DDIM, can be formulated as:
  $$
  d\bar{x}(t) = \eta_{\theta}^{(t)}\Bigl(\frac{\bar{x}(t)}{\sqrt{\delta^2+1}}\Bigr)d\delta(t)
  $$
where the parameter $\delta_t$ is given by $\sqrt{\frac{1-\alpha}{\alpha}}$, and the transformed variable $\bar{x}$ is defined as $x/\sqrt{\alpha}$ [30]. Beyond these primary solver categories, other analytical and differential equation solver samplers also contribute to learning-free generation [30]. The Momentum Sampler, for instance, draws inspiration from the continuous formulation of diffusion models, employing a convex combination of corruptions across different diffusion levels to facilitate generation [11].
#### 3.3.2 Learning-Based Sampling
Learning-based sampling methods aim to significantly accelerate the generation process in diffusion models by optimizing specific learning objectives, often entailing a trade-off where minor compromises in sample quality are accepted for substantial improvements in sampling speed [28]. These methodologies encompass a range of techniques, including optimized discretization, knowledge distillation, and truncated diffusion, alongside innovations like Non-Markovian process methods.

One prominent approach to accelerating sampling involves the use of Non-Markovian Process methods, which transcend the limitations of traditional Markovian processes. In these methods, each step of the inverse diffusion process can leverage a broader history of past samples to make more informed predictions. This enhanced predictive capability allows for the use of larger step sizes during sampling, thereby expediting the overall generation process [5,15]. A key development in this area is Denoising Diffusion Implicit Models (DDIMs), which depart from the assumption of a Markovian forward process in their reverse sampling formulation. The DDIM sampling process can be conceptualized as a discrete neural ordinary differential equation, offering increased efficiency and supporting sample interpolation [15]. The reverse process in DDIMs follows a specific distribution given by:
$$
q(x_{t+1} \mid x_t, x_0) = \mathcal{N}\Bigl(x_{t+1};\; \sqrt{\alpha_{t+1}}\,x_0 + \sqrt{1-\alpha_{t+1}-\sigma_t^2}\,\frac{x_t-\sqrt{\alpha_t}\,x_0}{\sqrt{1-\alpha_t}},\; \sigma_t^2 I\Bigr)
$$
[15]. Further research has revealed that DDIM can be considered a specialized instance of the diffusion model PNDM within a manifold [15].

Another category, Partial Sampling methods, directly reduce sampling time by selectively skipping certain time nodes in the generative process and only utilizing the remaining ones [5,15,31]. Knowledge distillation is a key learning-based sampling technique within this category, which involves transferring "knowledge" from a larger, more complex "teacher" model to a smaller, more efficient "student" model to achieve faster inference [30]. Progressive Distillation exemplifies this, wherein a more efficient diffusion model is distilled from an already trained one [5,15,31]. In this paradigm, a new student model is trained such that a single sampling step in the student model corresponds to two steps in the original, pre-trained teacher model, effectively halving the required sampling operations [5,15,31]. The iterative application of this distillation process can exponentially decrease the number of required sampling steps [15]. The specific algorithm for Progressive Distillation can be summarized as follows:
*   Sample $x_T \sim \mathcal{N}(0, I)$
*   For $t = T,\; T/2,\; T/4,\; \dots,\; 1$:
*   \quad $x_{t-1} = f_{\theta}(x_t, t)$
*   End for [15]

Beyond these, other specific learning-based methods include Optimized Discretization, which focuses on learning optimal step schedules for numerical solvers (e.g., Learning to Efficiently Sample from Diffusion Probabilistic Models, GENIE: Higher-Order Denoising Diffusion Solvers, Learning fast samplers for diffusion models by differentiating through sample quality), and Truncated Diffusion, which accelerates models by prematurely halting the diffusion process (e.g., Accelerating Diffusion Models via Early Stop of the Diffusion Process, Truncated Diffusion Probabilistic Models) [3].

Consistency Models (CMs), developed by Yang Song, represent a notable advancement in single-step generative modeling. These models can be trained using two primary approaches: distillation from pre-trained diffusion models or as standalone generative models [24]. Training via distillation leverages the knowledge embedded in a high-quality, but potentially slow, pre-trained diffusion model to train a faster CM. This approach often benefits from the stable pre-training of the teacher model, leading to robust performance. Conversely, standalone training of CMs involves directly optimizing the consistency objective without reliance on an existing diffusion model. This method offers greater flexibility in model architecture and training procedures, potentially leading to novel and highly efficient generative models, but might also present more challenges in optimization compared to distillation from an already stable teacher [24]. Recent improvements in consistency training, as detailed in an ICLR 2024 paper, have addressed limitations of earlier approaches. These advancements include refinements such as the removal of Exponential Moving Average (EMA) from the teacher consistency model, the adoption of Pseudo-Huber losses for more stable optimization, and the introduction of a lognormal noise schedule, all contributing to enhanced performance and training stability of CMs [24]. These improvements signify a move towards more direct and robust training methodologies for generating high-quality samples efficiently in a single step.
### 3.4 Likelihood Enhancement and Optimization
Maximum Likelihood Estimation (MLE) plays a pivotal role in various applications of generative models, including image compression, semi-supervised learning, and adversarial purification, where accurate density estimation is paramount [15,31]. While traditional diffusion models have historically underperformed other likelihood-based generative models in this aspect, improving their likelihood estimation remains a key area of research focus [5,12,15,31]. The direct computation of log-likelihood for diffusion models is inherently challenging, leading researchers to concentrate on optimizing and analyzing the Variational Lower Bound (VLB) [5,15,31]. The primary goal of these enhancements is to provide tighter bounds on the data likelihood, which subsequently leads to improved sample quality and more accurate density modeling [1,15].

Techniques aimed at enhancing the maximum likelihood estimation of diffusion models are broadly categorized into Objectives Designing, Noise Schedule Optimization, and Learnable Reverse Variance [1,3,15]. Broader solutions also include improved Evidence Lower Bound (ELBO) and variational gap optimization [30].

**Objectives Designing** focuses on formulating the loss function to directly maximize the VLB and, consequently, the log-likelihood. This approach leverages the relationship between the log-likelihood of generated data and the score function matching loss. Song et al. demonstrated that by judiciously designing the weight function of the loss, the likelihood function value of samples generated by the plug-in reverse Stochastic Differential Equation (SDE) can be made less than or equal to the loss function value, effectively turning the loss into an upper bound of the likelihood function [1,15,24]. Specifically, for a score function fitting loss:
$$
L = \mathbb{E}_t\left
$$
By setting the weight function $\lambda(t)$ to the diffusion coefficient $g(t)$, the loss function directly becomes the VLB of the likelihood function, ensuring that 
$$
\log p(x_0) \geq L
$$
[1,15]. This method offers a direct way to align the training objective with the ultimate goal of likelihood maximization.

**Noise Schedule Optimization** involves designing or learning the noise schedule of the forward diffusion process to increase the VLB. In classical diffusion models, the noise schedule is typically handcrafted [28]. However, optimizing this schedule, sometimes jointly with other model parameters, can significantly boost log-likelihood values by maximizing the VLB [7,28]. The Variational Diffusion Model (VDM) proved that as the number of discrete steps approaches infinity, the loss function becomes solely determined by the endpoints of the signal-to-noise ratio (SNR) function, SNR(t) [1,15]. The loss can be approximated as:
$$
L \approx SNR(0) - SNR(T)
$$
This insight allows for VLB optimization by learning the endpoints of the SNR(t) function, while further improvements can be achieved by learning the function values in the intermediate parts of the SNR(t) [15]. This technique offers fine-grained control over the diffusion process's trajectory, which impacts both sample quality and likelihood.

**Learnable Reverse Variance** aims to maximize the VLB by learning the variance of the reverse process, thereby reducing fitting errors. Analytic-DPM demonstrated the existence of optimal expectations and variances in the reverse processes of popular models like Denoising Diffusion Probabilistic Models (DDPM) and Denoising Diffusion Implicit Models (DDIM) [1,15]. Using the trained score function, the optimal VLB can be approximately achieved under a given forward process. For instance, the optimal mean $\mu^*$ can be expressed as:
$$
\mu^* = \left(I - \Sigma_\theta(x_t, t)\right)^{-1} x_t
$$
where $\Sigma_\theta(x_t, t)$ represents a term involving the predicted variance [1,15]. This method refines the generative path, leading to more accurate data density estimation.

Collectively, these techniques address the inherent challenges of direct log-likelihood computation in diffusion models by focusing on optimizing the VLB. By providing tighter bounds on the data likelihood, they not only enhance the quantitative maximum likelihood estimation performance but also contribute to superior sample quality and more precise density modeling. The effectiveness of these methods directly translates into improved performance in downstream applications such as high-fidelity image synthesis, semi-supervised learning tasks, and robust adversarial purification [15,31].
### 3.5 Handling Special Data Structures and Data Generalization
The inherent design of early diffusion models, which assume data resides within Euclidean space and primarily utilize Gaussian noise perturbation, intrinsically limited their initial application to continuous data domains such as images. This fundamental assumption renders them less effective for directly handling discrete data or other data types, thereby constraining their broader applicability [15,28,31]. Consequently, adapting diffusion models to accommodate data with special structures and to generalize across diverse, non-image modalities represents a critical research frontier [3,12].

To overcome these limitations and enhance data generalization, research has broadly categorized extension methods into two primary approaches: Feature Space Unification and Data-Dependent Transition Kernels [1,15]. A complementary perspective on improving generalization involves diversifying distribution handling across discrete, continuous, and structurally constrained spaces [30].

**Feature Space Unification** methods address the challenge by transforming data into a unified, continuous latent space before applying the diffusion process. A notable example is LSGM, which leverages a Variational Autoencoder (VAE) framework to project data into a continuous latent space for subsequent diffusion [1,5,15]. A key challenge within this approach lies in simultaneously training the VAE and the diffusion model. LSGM overcomes this by employing the traditional Evidence Lower Bound (ELBO) from VAEs as the loss function, noting that the score matching loss is no longer directly applicable due to an intractable potential prior [15]. LSGM efficiently learns and optimizes the ELBO by parameterizing the score function of the samples within the diffusion process. The relationship between ELBO and score matching is established as:

$$
ELBO = \mathbb{E}_{q} - \operatorname{KL}\Bigl(q(z|x) \parallel p(z)\Bigr)
$$

This formulation holds when constants are disregarded, enabling effective optimization [15].

In contrast, **Data-Dependent Transition Kernels** methods involve designing the transition kernels within the diffusion process to specifically align with the characteristics of the data type, allowing direct application to diverse modalities [15]. For discrete data, D3PM adapts diffusion models by constructing forward noise processes with bespoke transition kernels, such as lazy random-walks or absorbing states [1,5,15,28]. Similarly, VQ-Diffusion replaces the conventional Gaussian noise with random walks or random masking operations tailored for discrete data spaces [28]. For structured data like 3D molecular graphs, GEODIFF designs a translation-rotation invariant graph neural network. This method demonstrates that an initial distribution and transition kernel possessing invariance can derive an edge distribution that also exhibits invariance. Specifically, if 

$$
T(x) = Rx + t
$$ 

then the generated sample distribution $p(x)$ will maintain this invariance, meaning 

$$
p(T(x)) = p(x)
$$ 

[1,15].

Techniques like dimensionality reduction, particularly within Latent Diffusion Models (LDMs), play a crucial role in enabling the application of diffusion models to high-dimensional data by learning compact latent representations. LDMs exhibit robust generalization capabilities, extending to resolutions larger than those encountered during training, especially in spatially conditioned tasks like semantic image synthesis [4]. Furthermore, they can function as versatile upsamplers, exemplified by LDM-BSR, which upscales samples from class-conditional LDMs to achieve higher resolutions [4].

Beyond these foundational approaches, specific innovations further improve data generalization across various data types. Soft Diffusion, for instance, focuses on refining the corruption process by minimizing the Wasserstein distance between distributions along the diffusion path. This aims to achieve a smoother and more general transition from a fully corrupted state to a clean data distribution [11]. Another specialized application is FaceTalk, which addresses the generation of audio-driven 3D head motion sequences. By operating within the latent space of Neural Parametric Head Models (NPHMs), FaceTalk effectively handles this complex data structure, ensuring the temporal coherence and realism of the generated head movements from audio signals [21]. These advancements collectively illustrate the ongoing efforts to broaden the scope and improve the robustness of diffusion models for real-world applications across diverse data modalities.
## 4. Relationships with Other Generative Models

**Comparison: Generative Diffusion Models vs. Variational Autoencoders (VAEs)**

| Feature             | Generative Diffusion Models                               | Variational Autoencoders (VAEs)                       |
| :------------------ | :-------------------------------------------------------- | :---------------------------------------------------- |
| **Foundation**      | Variational inference, hierarchical Markovian VAE (DDPMs) | Variational inference, learn encoders & decoders       |
| **Training Objective** | Maximize log-likelihood (e.g., VLB)                       | Maximize Evidence Lower Bound (ELBO)                 |
| **Forward Process** | Fixed, non-trainable, progressively adds Gaussian noise   | Trainable, encoder maps data to latent space           |
| **Latent Space**    | Dimension identical to data (DDPMs), full data destruction | Lower-dimensional, compressed, preserves information   |
| **Generator Structure** | Shared neural network across decoder layers (DDPMs)     | Separate encoder and decoder networks                  |
| **Challenges**      | Inference speed (improving), handling discrete data      | Quality issues (random sampling), posterior selection  |
| **Hybrid Potential** | Can leverage VAEs for efficient latent space (LDMs)      | Can provide efficient latent spaces for DMs            |


**Comparison: Generative Diffusion Models vs. Generative Adversarial Networks (GANs)**

| Feature             | Generative Diffusion Models                               | Generative Adversarial Networks (GANs)                |
| :------------------ | :-------------------------------------------------------- | :---------------------------------------------------- |
| **Training Stability** | Generally more stable, likelihood-based objectives         | Often unstable, convergence issues, Nash equilibrium   |
| **Sample Diversity** | Greater sample diversity, superior mode coverage           | Susceptible to mode collapse, limited diversity        |
| **Fidelity**        | High-fidelity sample generation, SOTA                      | Can generate high-fidelity samples, but artifacts possible |
| **Inference Speed** | Traditionally slow (iterative, multi-step), but improving | Generally faster (single-pass generation)             |
| **Training Objective** | Maximize likelihood (e.g., VLB, noise prediction MSE)    | Minimax game, discriminator learning real vs. fake     |
| **Latent Space**    | Often original data dimensionality, Gaussian prior         | Lower-dimensional, compressed latent space           |
| **Hybrid Potential** | Can stabilize GAN training by adding noise                | Can enhance realism and inference speed when combined |

This section provides a systematic comparison of diffusion models with other prominent generative model paradigms, including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Normalizing Flows, Autoregressive Models, and Energy-Based Models [3,5,9,19,31]. Each comparison will highlight the unique strengths and inherent weaknesses of diffusion models relative to these established frameworks, while also analyzing the advantages and limitations of the other generative models themselves [15,31].

Fundamentally, diffusion models often present advantages in terms of training stability and diversity of generated samples compared to methods like GANs, which commonly face issues such as unstable training and mode collapse [24,25,26,36,38]. While VAEs and diffusion models share a common variational inference foundation and objective functions based on maximizing data likelihood, their approaches to latent space management and the generative process diverge significantly [17,20,26]. Diffusion models tend to be more flexible in architecture and training objectives compared to VAEs, although VAEs offer explicit learning of compressed latent representations [23,25]. Normalizing Flows, while offering explicit likelihood computation, are constrained by the requirement for invertible functions and tractable Jacobian determinants—limitations that diffusion models can often circumvent [15,31]. Autoregressive Models, despite their conceptual clarity, struggle with computational inefficiency and unidirectional bias due to their sequential generation process [9,26]. Energy-Based Models, which directly define an energy function, typically face challenges in learning and sampling stability, areas where diffusion models offer robust solutions through their score-matching principles [3,15,24].

Beyond direct competition, this section will elaborate on the profound potential for synergy between diffusion models and other generative frameworks [1,5,9,12]. It will discuss how diffusion models can be integrated with and improve upon existing models by leveraging their respective strengths [15,31]. For instance, the stability of diffusion models can stabilize GAN training [1], while VAEs can provide efficient latent spaces for diffusion processes, as seen in Latent Diffusion Models [4,32]. The insights from diffusion models have also influenced the training methodologies of autoregressive models and provided enhanced expressive capabilities for Normalizing Flows [1,15,24]. Furthermore, the score-matching framework inherent in diffusion models offers a stable and efficient mechanism for density estimation and sample generation in Energy-Based Models [24,26]. The following subsections will delve into specific comparisons and hybrid approaches for each model type, providing a detailed understanding of these interconnections and their implications for future generative modeling research [15].
### 4.1 VAEs
Variational Autoencoders (VAEs) represent a class of generative models designed to learn encoders and decoders that map input data to values within a continuous latent space [28]. They aim to model the real data distribution through a latent vector distribution and are optimized by maximizing the Evidence Lower Bound (ELBO), which serves as a lower bound on the data likelihood, i.e., 
$$\log P(x) \geq \text{ELBO}$$ 
[20,26].

The relationship between diffusion models and VAEs is multifaceted, encompassing significant similarities and notable differences. Fundamentally, both model types are based on variational inference and share a common training objective: maximizing the log-likelihood to ensure the starting distribution and the sampled distribution are as close as possible 
[9,17,27]. This shared objective is reflected in their respective ELBO formulations. For VAEs, the ELBO is typically expressed as:
$$\log p(x) \geq \mathbb{E}_{q(z|x)} - D_{KL}\bigl(q(z|x) \parallel p(z)\bigr)$$
In contrast, the ELBO for diffusion models, which employ a hierarchical latent variable structure, takes a more elaborate form:
$$\log p(x_0) \geq \mathbb{E}_q$$
Despite these distinctions, the underlying variational principle unites them [27].

Specifically, Denoising Diffusion Probabilistic Models (DDPMs) can be regarded as a hierarchical Markovian VAE 
[1,5,15,31]. However, several key structural and functional differences differentiate DDPMs from general VAEs. In DDPMs, the encoder and decoder are characterized by Gaussian distributions and exhibit Markovian properties. Crucially, the dimension of their hidden variables (latent space) is identical to the data dimension, unlike VAEs where reduced latent dimensions are often preferred for efficiency 
[5,15,26,31]. Furthermore, all layers of the decoder in DDPMs share a neural network, contributing to their distinct architectural characteristics 
[15,31].

A significant divergence lies in their handling of latent representations and the forward process. While a VAE's latent representation contains compressed information about the original image, the forward process of diffusion models progressively destroys the data through noise addition, leading to a complete destruction of the original data after the final step 
[26]. Moreover, the mapping to a VAE's latent space is trainable, allowing for adaptive compression. Conversely, the forward process of a diffusion model is fixed and non-trainable, as the latent states are generated by a predefined process of gradually adding Gaussian noise to the original image 
[26].

VAEs commonly face challenges, such as potential defects and quality issues in generated images due to random sampling in their latent space 
[9]. A core pain point in VAEs is the selection of the variational posterior distribution, where a trade-off exists between expressiveness (approximating the real posterior) and computational tractability (difficult optimization if too complex) 
[23]. In this context, diffusion models have emerged as a promising advancement, with some research suggesting they can be viewed as an improvement over the posterior distribution in VAEs and may potentially outperform them in generative tasks 
[18,27].

The complementary strengths of VAEs and diffusion models have led to the development of hybrid approaches. Latent Diffusion Models (LDMs), for instance, leverage a pre-trained VAE as a crucial component. The VAE's encoder compresses images into a lower-dimensional latent space, where the diffusion process then operates, significantly reducing computational cost while boosting visual fidelity 
[4,25]. Subsequently, the VAE's decoder maps the generated latent representations back to the pixel space 
[25]. This integration capitalizes on the VAE's ability for perceptual compression, which is essential for building invariant and robust representations 
[32]. Furthermore, advancements like DiffuseVAE have been introduced to specifically address challenges such as generating poor quality samples and slow sampling speeds often encountered in low-dimensional latent spaces, further bridging the gap and leveraging the strengths of both paradigms 
[19]. The connections between VAEs and diffusion models have been further explored in various surveys and theoretical works, including those providing a unified perspective or focusing on variational and score-based approaches in latent spaces 
[3].
### 4.2 GANs
Generative Adversarial Networks (GANs) comprise two competing neural networks: a generator ($G$) that synthesizes new samples, and a discriminator ($D$) that distinguishes between real and generated data [28]. The theoretical objective of GANs is to reach a Nash equilibrium where the generator’s distribution ($p_g$) matches the real data distribution ($p_{data}$), and the discriminator is unable to differentiate between real and fake samples, i.e., 
$$
D^*(x) = \frac{1}{2}
$$
[38].

Despite their ability to generate high-fidelity samples, GANs often encounter significant training challenges. Their adversarial nature frequently leads to unstable training, convergence issues, and mode collapse [25,26,38]. This instability stems from the low overlap between the distribution of GAN-generated data and the real data distribution in high-dimensional spaces [15,31]. Furthermore, GANs demand the concurrent training of two intricate networks, making the overall optimization process difficult and prone to non-convergence [23,38]. Another common issue is limited diversity in generated samples, as GANs may fail to cover the entire data manifold [38].

In contrast, diffusion models offer a robust alternative to GANs, generally exhibiting more stable training processes and providing greater sample diversity due to their likelihood-based objectives [25,26]. Unlike GANs, which aim for a Nash equilibrium, diffusion models maximize the likelihood of the training data [38]:
$$
\max_\theta \mathbb{E}_{x_0 \sim q(x_0)}
$$
This objective, coupled with their systematic process of adding and removing noise, contributes to their inherent training stability [27]. Research by Yang Song positions score-based generative models, a class of diffusion models, as a viable alternative to GANs, demonstrating comparable or even superior performance on various image datasets without requiring adversarial optimization [24]. Diffusion models have shown potential to outperform GANs in certain applications [18], and their simpler generative modeling approach contrasts with the complexity of GAN training [38].

While diffusion models generally offer more stable training and diverse sample generation, they are often less efficient during inference compared to GANs, typically requiring multiple network evaluations to synthesize a single sample [26]. Additionally, GANs inherently provide perceptual compression by mapping high-dimensional pixel data to a low-dimensional latent space [32], whereas diffusion models often maintain the original data dimensionality, with their latent space frequently modeled as a Gaussian distribution, similar to Variational Autoencoders (VAEs) [26].

Beyond direct competition, diffusion models can also address some of the fundamental limitations of GANs. Specifically, the systematic noise addition process of diffusion models can be leveraged to stabilize GAN training. By introducing noise to both generated and real data before feeding them into the discriminator, the discriminator becomes more robust, effectively mitigating issues of unstable training and non-convergence in GANs [1,15,31].

The synergistic potential between these two generative paradigms has led to the development of hybrid models. Several studies explore combining GANs and diffusion models, such as "Diffusion-GAN: Training GANs with Diffusion" and "Tackling the generative learning trilemma with Denoising Diffusion GANs" [3]. These hybrid approaches aim to leverage the training stability of diffusion models with the perceptual quality and inference efficiency often associated with GANs. For instance, integrating GANs within a diffusion framework can enhance the realism and naturalness of generated outputs, such as audio, by employing GAN’s adversarial training to refine the generated quality [7]. This demonstrates a growing trend towards combining their respective strengths to overcome individual weaknesses, offering a valuable reference for future research [19]. The ongoing research continues to explore whether diffusion models could fully replace GANs or if a combined approach offers the most promising path forward [19].
### 4.3 Normalizing Flows
Normalizing Flows (NFs) are a class of generative models distinguished by their ability to transform simple probability distributions, such as a Gaussian, into complex, high-dimensional data distributions [26,28]. This transformation is achieved through a sequence of invertible and differentiable functions, where the overall transformation from the latent space (Z) to the data space (X) is bijective [15,26,31]. A key characteristic of NFs is the explicit tractability of their log-likelihood, which is essential for training via negative log-likelihood minimization [26]. The exact log-likelihood for NFs is computed as

$$
\log p_X(x) = \log p_Z(z) + \sum_{i=1}^K \log\left|\det\bigl(J_{f_i}(z_{i-1})\bigr)\right|
$$

where z is the latent variable, f_i are the transformations, and J_{f_i} are their Jacobian matrices [27].

However, the requirement for bijective functions and easily computable Jacobian determinants imposes significant architectural constraints on Normalizing Flows, thereby limiting their expressive power and general applicability, particularly preventing the casual adoption of state-of-the-art neural network architectures commonly used in image classification or segmentation tasks [15,23,31].

The relationship between Diffusion Models and Normalizing Flows is profound and multifaceted. A significant connection was established by Song et al. (NeurIPS 2021), demonstrating that score-based diffusion models can be formulated as continuous normalizing flows, which enables the tractable computation of their log-likelihood [1,24].

Furthermore, diffusion models have provided a novel paradigm for enhancing the expressive capabilities of Normalizing Flows. By drawing an analogy to the noise addition process inherent in diffusion models, NFs can be improved through the incorporation of noise into their encoder mechanisms [1,5,15,31]. This approach significantly augments the expressive power of Normalizing Flows and effectively generalizes diffusion models to scenarios where the forward process (the noise addition) is also learnable [5,15,31]. This conceptual linkage has led to the development of hybrid models and theoretical interpretations, including Diffusion Normalizing Flow, methods for interpreting diffusion score matching using normalizing flow, and Maximum Likelihood Training of Implicit Nonlinear Diffusion Models [3]. Unlike the exact likelihood of NFs, diffusion models typically optimize a likelihood lower bound, represented as

$$
\log p(x_0) \geq \mathcal{L} = \mathbb{E}_q
$$

This interplay highlights how insights from one generative modeling paradigm can address the limitations and expand the theoretical and practical scope of another.
### 4.4 Autoregressive Models
Autoregressive Models (ARMs) represent a class of generative models that decompose the joint distribution of data into a product of a series of conditional distributions, leveraging the probability chain rule [28]. This approach enables the generation of new samples by sequentially producing elements, where each new element is conditioned on previously generated ones. For instance, in image generation, ARMs typically represent images as pixel sequences, constructing new images pixel by pixel based on the preceding pixels [26]. Similarly, in time-series contexts, ARMs are built on a time-step basis, where the input for each step transitions from a previous state to the next [9].

Despite their conceptual clarity, traditional ARMs present several challenges. Their sequential generation process often leads to high computational complexity and long run times, as each step is dependent on the completion of the previous one, making them computationally inefficient for feedforward computation [9]. Furthermore, the pixel-by-pixel or step-by-step generative approach can introduce a unidirectional bias, which is a notable limitation in capturing complex data dependencies [26]. A significant hurdle in the deployment of ARMs is the inherent requirement for the data to possess a specific structure, which complicates their design and parameterization [15,31].

The advent and success of diffusion models have notably inspired and influenced the training methodologies of autoregressive models [1,5,15,31]. By adopting specific training methods inspired by diffusion processes, researchers have found ways to circumvent some of the aforementioned design difficulties inherent in traditional ARMs [5,15,31]. This cross-pollination of ideas has led to the development of hybrid models, such as Autoregressive Diffusion Models and Autoregressive Denoising Diffusion Models, particularly explored for tasks like Multivariate Probabilistic Time Series Forecasting [3].

Moreover, efforts have been made to address the computational inefficiencies of autoregressive sampling. A method introduced at ICML 2021 frames autoregressive sampling of models like MADE and PixelCNN++ as solving a system of nonlinear equations, enabling acceleration of feedforward computation through parallel computing techniques [24]. While these advancements enhance the practicality of ARMs, comparative studies, such as those in facial animation, sometimes emphasize the sampling efficiency advantages of other generative approaches over autoregressive diffusion models [21].
### 4.5 Energy-Based Models
Energy-Based Models (EBMs) represent a class of generative models that directly model the distribution of raw data by defining an energy function, where lower energy corresponds to higher probability density [3,15,31]. This energy function provides an estimate of the non-standardized version of the density function [26]. EBMs can be viewed as generative discriminators capable of learning from unlabeled input data [28]. However, the direct modeling approach inherent to EBMs traditionally presents significant challenges in terms of learning and sampling efficiency and stability [3,15,31].

A profound synergy exists between diffusion models and EBMs, primarily rooted in the concept of score matching. Diffusion models, particularly score-based generative models, are intimately connected to EBMs because the score function, $\nabla_x \log p(x)$, is directly proportional to the negative gradient of the energy function, $-\nabla_x E(x)$ [27]. This equivalence signifies that learning the score function in diffusion models is tantamount to learning the score function of an EBM [27]. Consequently, the mathematical formulation of diffusion models can be regarded as a specialized instance of an energy-based framework, particularly when training and sampling necessitate solely the score function [26]. Research has further demonstrated that techniques like sliced score matching can effectively learn deep score estimators for implicit distributions, which is highly relevant to the training of deep EBMs [19,24].

The integration of diffusion processes offers a robust solution to the inherent difficulties in training and sampling EBMs. By leveraging diffusion to restore likelihood, models can first introduce small noise to data samples and subsequently infer the original sample distribution from these slightly noisy samples [1,3,5,15,24,31]. This approach simplifies and stabilizes both the learning and sampling processes, circumventing the direct modeling challenges of traditional EBMs [1,3,5,15,24,31].

The close relationship is further highlighted by the fact that EBMs are sometimes referenced as one of the various naming conventions for diffusion models, alongside terms such as denoising diffusion probabilistic models and score-based generative models [19]. This overlap in nomenclature underscores a shared core ideology and provides a deeper understanding of why diffusion models can be conceptualized as score-matching models [19]. Specific applications demonstrating this synergy include works like "Learning Energy-Based Models by Diffusion Recovery Likelihood" and "Latent Diffusion Energy-Based Model for Interpretable Text Modeling," which exemplify practical implementations of this integrated approach [3,24]. In essence, diffusion models enhance the tractability and performance of EBMs by providing a stable and efficient mechanism for density estimation and sample generation, thereby unlocking their full potential.
## 5. Applications of Generative Diffusion Models

**Scientific and Other Key Applications of Generative Diffusion Models**

| Domain                         | Key Applications / Tasks                           | Examples / Models / Techniques                           | Impact / Benefit                                                |
| :----------------------------- | :------------------------------------------------- | :------------------------------------------------------- | :-------------------------------------------------------------- |
| **Bioinformatics & Comp. Biology** | Protein Design, Drug Discovery, Molecular Engineering | RFdiffusion, Chroma, DiffDock, ConfGF, DGSM                | Generation of novel proteins, molecular structures, ligand prediction |
| **Materials Science**          | Material Design                                    | CDVAE                                                    | Generation of stable materials, exploration of periodic structures |
| **Weather & Environmental Science** | Weather Forecasting, Environmental Modeling        | SwinRDM, DiffESM, SEEDS, seismic processing              | Realistic simulations, precise predictions, data emulation     |
| **Healthcare**                 | Medical Imaging (reconstruction, segmentation), ECG/EEG | (General DMs), Diff-E, DeSCoD-ECG, DiffECG, PPG-to-ECG    | Synthetic medical images, signal denoising, data augmentation |
| **Energy & Electricity Sector**| EV Charging Scenarios, Load Profile Synthesis      | DiffCharge, DiffLoad                                     | Uncertainty quantification, probabilistic forecasting         |
| **Finance**                    | Stock Price Prediction, Tabular Data Generation    | (General DMs), FinDiff                                   | Addressing stochasticity, generating financial data             |
| **AIOps & Networking**         | Performance Anomaly Anticipation, Traffic Generation | Maat, NetDiffus                                          | Enhancing system reliability, network analysis                  |
| **Mathematics & Physics**      | High-fidelity Flow Field Reconstruction            | Physics-informed DMs, DiTTO                              | Solving PDEs, modeling complex physical phenomena               |
| **Data Quality & Augmentation**| Data Purification, Synthetic Data Generation       | (General DMs)                                            | Addressing data scarcity/privacy, improving model generalization |


**Key Computer Vision Applications of Generative Diffusion Models**

| Category           | Sub-Category / Task           | Examples / Key Models / Techniques              | Distinctive Capability / Impact                                                              |
| :----------------- | :---------------------------- | :---------------------------------------------- | :------------------------------------------------------------------------------------------- |
| **Image Generation** | Text-to-Image Synthesis       | Imagen, Stable Diffusion, LDM, DALLE-3          | Producing photorealistic images from text, high fidelity, strong alignment, s.o.t. results |
|                    | Image-to-Image Synthesis      | Stable Diffusion, LDM                           | Transforming images based on other image inputs or text guidance                             |
|                    | High-Resolution Generation    | Diffusion-4K, LDM-BSR                           | Scaling to ultra-high resolutions, versatile upsampling                                      |
| **Image Editing**  | Inpainting                    | RePaint                                         | Filling missing/corrupted regions coherently                                                 |
|                    | Super-Resolution              | SR3, CDM                                        | Upscaling images with detail preservation, state-of-the-art                                  |
|                    | Semantic Image Editing        | SDEdit, DiffusionCLIP, Blended Diffusion, ControlNet | Fine-grained control over content, object/attribute manipulation based on text/labels        |
| **Image Restoration**| Denoising / Deblurring        | (General DM capabilities)                       | Handling various degradations (blur, noise), robust image completion & repair                |
|                    | Multi-modal Image Fusion      | DRMF                                            | Enhancing image quality from varied, degraded sources (e.g., infrared-visible)               |

Generative Diffusion Models have emerged as a transformative paradigm across numerous scientific and technological domains, fundamentally reshaping capabilities in data synthesis and analysis [1,3,9,22,28,33]. Their inherent probabilistic framework, coupled with an iterative denoising process, enables the generation of high-fidelity, diverse, and controllable data. This characteristic often positions them advantageously over traditional generative models, such as Generative Adversarial Networks (GANs), in terms of training stability and the preservation of semantic consistency [28].

This section provides a comprehensive overview of the broad spectrum of applications, detailing how diffusion models are uniquely adapted and deployed to address complex challenges within each field. We delve into their profound impact on core areas such as **Computer Vision**, where they have revolutionized tasks including image generation, editing, and restoration through capabilities like photorealistic synthesis and fine-grained content manipulation. In **Natural Language Processing**, we explore their growing utility in controllable text generation, navigating the inherent complexities associated with processing discrete data. Furthermore, their prowess extends significantly to **Audio and Video Synthesis**, enabling the creation of realistic waveforms and dynamic visual content, despite the high dimensionality and temporal intricacies involved in these modalities.

A particularly impactful area is **Multi-Modal Generation**, where diffusion models excel at synthesizing data conditioned on diverse inputs, effectively integrating information across various modalities such as text, images, and audio to produce coherent and contextually relevant outputs. Beyond these conventional domains, we highlight their significant contributions to **Scientific and Other Applications**, spanning critical fields including bioinformatics (e.g., protein design, drug discovery), materials science, environmental forecasting, healthcare (e.g., medical imaging), finance, and robust data purification.

For each application domain, this survey not only provides examples of successful implementations and their tangible impact but also discusses the specific challenges encountered, the limitations that necessitate further research, and promising avenues for future exploration. This systematic approach aims to offer a thorough understanding of the current landscape and prospective developments in generative diffusion models.
### 5.1 Computer Vision
Diffusion models have emerged as a transformative technology within computer vision, demonstrating exceptional capabilities across a myriad of tasks, notably in image generation, editing, and restoration [4,26,29]. Their inherent probabilistic nature enables the synthesis of high-quality and diverse images, pushing the boundaries of what is achievable with generative models [26].

A prominent application is **image generation**, particularly **text-to-image synthesis**. Models such as Imagen are capable of producing photorealistic images directly from textual descriptions, exhibiting high fidelity and strong alignment with the input text [2]. Latent Diffusion Models (LDMs), including Stable Diffusion, further advance this domain by enabling both text-to-image and image-to-image synthesis [4,32]. LDMs leverage pre-trained components like CLIP to provide robust multi-modal embeddings, facilitating semantic and visual similarity in generated images, and often outperforming style-based Generative Adversarial Networks (GANs) in semantic structure preservation [32]. Furthermore, these models can generalize to produce higher resolutions than encountered during training, even serving as general-purpose upsamplers, with efforts like Diffusion-4K aiming for ultra-high-resolution image generation [4,25].

Beyond generation, diffusion models excel in **image editing** tasks. This includes **image inpainting**, where models can effectively fill missing or corrupted regions within an image, as demonstrated by applications like RePaint [5,31,33]. Another critical editing capability is **super-resolution**, which involves upscaling images while preserving or enhancing detail. Diffusion models have achieved state-of-the-art performance in this area, with methods like SR3 and CDM employing diffusion for image reconstruction, presenting a formidable challenge to previous leading models like GANs [19,28]. **Semantic image editing**, allowing manipulation of objects or attributes based on labels or textual prompts, represents a sophisticated application. Techniques such as SDEdit, DiffusionCLIP, Blended Diffusion, Diffusion Self-Guidance, and Prompt-to-Prompt Image Editing enable fine-grained control over image content, from modifying local regions to altering global styles by adjusting text inputs or attention maps [4,26,29]. The integration of control mechanisms like ControlNet/Lora further enhances practical applications, such as generating model clothes or reconstructing home scenes [33]. Diffusion models are also instrumental in tasks like image translation, where SDEdit uses Stochastic Differential Equations priors to improve the fidelity of translated images [28].

In **image restoration**, diffusion models have shown remarkable prowess in tasks such as denoising and deblurring [26]. They can effectively handle various forms of image degradation, including blur and low-amplitude noise [11]. Their capabilities extend to multi-modal image fusion under degraded conditions, showcasing their utility in enhancing image quality from varied sources, such as infrared-visible fusion [13]. The iterative denoising process inherent to diffusion models allows for robust image completion and repair, making them highly effective for reconstructing corrupted visual data [1,5,31].

When **comparing the performance** of diffusion models, particularly in image restoration, they often demonstrate superior ability in generating diverse and high-quality outputs compared to traditional methods. While GANs have shown promising results in certain generative tasks like super-resolution, diffusion models often provide better semantic structure preservation and a more stable training process [19,32]. The ability of diffusion models to generate a variety of plausible outputs for under-determined inverse problems, such as super-resolution or inpainting, positions them advantageously over deterministic restoration approaches. The field continues to expand, encompassing tasks like semantic segmentation (e.g., SegDiff, DDeP), 3D generation, video generation, anomaly detection, and object detection, underscoring the versatility and growing impact of diffusion models in computer vision [3,21,28,34].
### 5.2 Natural Language Processing
Diffusion models, traditionally recognized for their prowess in continuous data domains such as image generation, are increasingly being adapted for applications within Natural Language Processing (NLP). Their primary utility in NLP centers on text generation, encompassing the production of coherent and natural language outputs [1,12]. A significant aspect of this application is controllable text generation, which allows for directed manipulation of the generated text's attributes or content [30].

To address the unique characteristics of textual data, several specialized diffusion-based models have been developed. Notable examples include D3PM, Diffusion-LM, Analog Bits, and DiffuSeq [3,28]. These methods demonstrate a broad range of applicability within text generation, extending from granular character-level text generation to more expansive language model control and enhanced text generation controllability [28]. These advancements represent critical adaptations of the diffusion paradigm to handle the sequential nature of language.

The application of diffusion models to discrete data, such as sequences of text tokens, introduces inherent methodological challenges, primarily because the foundational principles of diffusion models are rooted in continuous variable modeling. Overcoming the inherent discreteness of text necessitates particular architectural and algorithmic adaptations. While the provided digests enumerate specific models designed for this domain—namely, D3PM, Diffusion-LM, Analog Bits, and DiffuSeq [3,28]—these models implicitly represent solutions that have incorporated mechanisms to handle sequential and discrete data structures, thereby extending the utility of diffusion models beyond their traditional image generation paradigm. These models are engineered to effectively process and generate sequences of tokens, adapting the denoising process to the sequential and discrete nature of linguistic data.

Beyond text generation, machine translation is another potential area for diffusion model application, though the provided digests do not detail specific methodologies or examples for this task. Furthermore, certain related applications demonstrate the broader linkage of diffusion models to NLP. For instance, FaceTalk illustrates an interesting intersection by utilizing audio signals as input to drive neural parametric head models, thus connecting to NLP through its reliance on speech analysis and synthesis, which often involves textual components as intermediaries or outputs [21]. This diversification highlights the expanding scope of diffusion models within the broader NLP ecosystem.
### 5.3 Audio and Video Synthesis
Diffusion models have emerged as a powerful paradigm for generating high-fidelity and diverse content across various modalities, significantly impacting both audio and video synthesis [19]. Their ability to model complex data distributions has led to notable advancements in creating realistic and expressive outputs in these challenging domains.

In the realm of audio synthesis, diffusion models demonstrate exceptional performance, particularly in raw waveform generation. Applications span a wide range, including speech synthesis, music composition, and the generation of diverse sound effects [7,25]. Key architectural developments, such as WaveGrad and DiffWave, have successfully leveraged diffusion models to perform effective waveform generation by introducing conditional models [28,30]. These advancements contribute significantly to waveform signal processing, enabling the creation of natural-sounding speech and original musical compositions [3].

The application of diffusion models to video synthesis presents unique challenges due to the high dimensionality and temporal complexity of video data. Despite these difficulties, significant progress has been made in generating realistic videos of human actions, special effects, and facilitating video editing techniques [29]. Early approaches, such as Video Diffusion Models, attempted to adapt 2D convolutions to 3D convolutions for video generation; however, they often faced limitations due to the scarcity of high-quality video resources [29].

Subsequent innovations have addressed these challenges. Meta AI's Make-A-Video, for instance, has been recognized as a state-of-the-art (SOTA) work in text-to-video generation. This model operates by initially generating images and then interpolating to predict intermediate frames, distinguishing itself through the introduction of pseudo-three-dimensional convolutional and attention layers that have been widely adopted in subsequent research [29,33]. Beyond Make-A-Video, numerous other models have advanced the field, including Control-A-Video, Tune-A-Video (which involves one-shot tuning of image diffusion models for text-to-video generation), VideoFactory, and Align your Latents [29,33]. Furthermore, methods like Structure and Content-Guided Video Synthesis with Diffusion Models have applied stable diffusion techniques to video generation [29]. Other models, such as FDM and RVD, employ distinct strategies, including novel generative model architecture designs and autoregressive video diffusion, to enhance the quality of generated videos [28]. Google Research has also notably contributed to the successful application of diffusion models in video generation [19]. An illustrative example of multi-modal synthesis is FaceTalk, which directly generates video (specifically, 3D head motion) from audio inputs (speech), showcasing the potential for audio-driven visual content creation [21].
### 5.4 Multi-Modal Generation
Conditional diffusion models represent a significant advancement in generative AI, enabling the synthesis of data conditioned on specific inputs or constraints from various modalities. This paradigm facilitates a broad spectrum of multi-modal tasks, extending beyond traditional unconditional generation by integrating diverse information sources, such as text, audio, or other images, to guide the generative process.

These models are extensively employed in tasks such as text-to-image generation and image-to-image translation, among others. Techniques for incorporating conditional information and aligning modalities vary. For instance, some architectures utilize pre-trained models to encode conditional inputs. Imagen, a model excelling in text-to-image synthesis, employs a frozen large language model (T5-XXL) to encode text prompts, subsequently leveraging cascaded diffusion models for high-quality image generation [2]. Similarly, unCLIP (DALLE-2) adopts a two-stage approach: a prior model generates CLIP-based image embeddings from text, which are then used by a diffusion-based decoder to synthesize images [28]. Another strategy involves combining vector-quantized (VQ) data with diffusion techniques to address multi-modal problems, including text-to-image generation, text-to-3D generation, and text-to-image editing [30]. Latent Diffusion Models (LDMs) also exemplify multi-modal capabilities, particularly in text-to-image synthesis, by generating images based on textual prompts within a compressed latent space [4].

The performance of different multi-modal diffusion models varies across tasks, each exhibiting distinct strengths. In text-to-image generation, models like GLIDE [5,31], Imagen [2], and DALLE-3 [33] have demonstrated remarkable capabilities. Imagen is particularly noted for its photorealistic text-to-image outputs, while DALLE-3 is recognized for generating superior images with enhanced caption adherence [2,33]. Blended Diffusion offers a general region-oriented image editing solution guided by natural language, by combining pre-trained DDPM and CLIP models, making it suitable for diverse real-world images [28].

Beyond text-to-image synthesis, diffusion models have been successfully applied to a diverse array of multi-modal challenges. These include text-to-3D generation, scene graph-to-image generation, text-to-audio generation, text-to-motion generation, and text-to-video generation/editing [3]. Specific examples include FaceTalk, which performs multi-modal generation by creating 3D head motion from audio input [21], and models focused on image-to-audio generation [7]. Furthermore, DRMF (Degradation-Robust Multi-Modal Image Fusion) showcases the application of diffusion priors for multi-modal image fusion, effectively combining images from different modalities (e.g., infrared and visible) even when degraded by noise, low light, or low resolution [13]. The primary strength of these models lies in their ability to synthesize high-fidelity, coherent outputs across different data types, often achieving photorealistic quality and precise adherence to complex conditional inputs. However, a common challenge involves the intricate alignment of disparate modalities and the computational intensity often associated with multi-stage or high-resolution generation processes.
### 5.5 Scientific and Other Applications
Diffusion models have rapidly emerged as a transformative technology, demonstrating significant utility across a diverse array of scientific disciplines and specialized applications [18,33]. Their generative capabilities enable the creation of realistic simulations, the design of novel molecules and materials, and advancements in data analysis—particularly within fields requiring complex data generation and intricate modeling [18,33].

In **bioinformatics and computational biology**, diffusion models are pivotal for advancements in protein design, drug discovery, and molecular engineering [1,5,18,25,31]. They facilitate the generation of novel, designable, and diverse protein structures, exemplified by equivariant diffusion models that diffuse oriented residue clouds. Tools such as RFdiffusion enable complex protein-generation tasks, while Chroma—a graph-neural-network-based conditional diffusion model—is utilized for generating large single-chain proteins and complexes with programmable properties [18]. For drug and small-molecule design, diffusion models are applied to molecular graph generation and the prediction of molecular conformations, including through dynamic graph score matching and evolving conformations from thermodynamic noise [18]. Torsional diffusion offers a framework operating on torsion angles to enhance molecular graph modeling, and models like ConfGF and DGSM generate conformations using physics-inspired gradient fields [28,30]. Furthermore, equivariant 3D-conditional diffusion models support molecular linker design and structure-based drug design, with DiffDock and multiscale generative diffusion models assisting in protein–ligand interaction modeling and dynamic-backbone protein–ligand structure prediction [18]. Beyond molecular synthesis, diffusion models contribute to cryo-electron microscopy data analysis through latent space models for cryo-EM structures, and to single-cell data analysis via diffusion-based denoising of genomics data [18].

In the realm of **materials science**, diffusion models contribute to the design and creation of new materials. A notable application is CDVAE, which investigates the generation of stable materials by exploring their periodic structures [30].

For **weather forecasting and environmental science**, diffusion models enable the generation of realistic simulations and precise predictions [33]. Specific instances include SwinRDM for high-resolution weather forecasting, DiffESM for the conditional emulation of Earth System Models, and models for precipitation nowcasting and high-resolution solar forecasts [33]. They also support the emulation of weather forecast ensembles through SEEDS [33]. In broader environmental applications, deep diffusion models are employed for seismic processing [33].

Beyond these molecular and environmental sciences, diffusion models have significant applications in general **healthcare**. They are employed for generating synthetic medical images, assisting in medical image reconstruction (e.g., in CT and MRI scans by score-based generative models), segmentation, and multimodal image fusion [13,24,25,33]. More detailed applications include decoding imagined speech EEG (Diff-E), synthetic EEG data generation, denoising and synthesizing ECG signals (DeScoD-ECG, DiffECG), and PPG-to-ECG translation [33]. They also facilitate the synthesis of realistic Electronic Health Records (EHR) data—encompassing mixed-type and longitudinal variables (EHRDiff, MedDiff)—alongside data augmentation for seizure prediction and modeling brain dynamics [33].

Diffusion models demonstrate considerable versatility in other specialized domains, particularly in **temporal data modeling** [12]. In the **energy and electricity sector**, they are used for generating EV charging scenarios (DiffCharge), synthesizing customized load profiles for electricity customers, quantifying uncertainty in load forecasting (DiffLoad), and probabilistic energy forecasting [33]. Within **finance**, diffusion models address stochasticity in multi-step regression stock price prediction and generate financial tabular data (FinDiff) [33]. In **AIOps**, applications include performance metric anomaly anticipation (Maat) and network traffic generation via time-series imaging (NetDiffus) [33]. They also support sensor-based human activity recognition [33]. In **mathematics and physics**, diffusion models are applied to high-fidelity flow field reconstruction through physics-informed diffusion models, diffusion-inspired temporal transformer operations (DiTTO), and generative diffusion learning for parametric partial differential equations, extending to infinite-dimensional function spaces [33].

Finally, diffusion models contribute to enhancing data quality and model robustness. They are utilized for **data purification** and the **generation of synthetic data for robust learning**, addressing challenges such as data scarcity and privacy while improving overall model generalization [3]. Their capabilities further extend to **sequential recommendation systems**, with models like RecFusion and DiffuRec demonstrating efficacy in diverse recommendation tasks [33].
## 6. Evaluation Metrics and Benchmarks

**Key Evaluation Metrics and Benchmarks for Generative Diffusion Models**

| Category           | Metric / Type                | Description                                                | Purpose / What it Measures                                | Key Characteristics                                         |
| :----------------- | :--------------------------- | :--------------------------------------------------------- | :-------------------------------------------------------- | :---------------------------------------------------------- |
| **Quantitative**   | **Fréchet Inception Distance (FID)** | Compares statistical aspects of features (Inception v3) between real and generated images. | Quality & Diversity (lower is better)                     | Widely used, correlates well with human perception, sensitive to mode collapse |
|                    | **Inception Score (IS)**     | Measures image quality (clarity) and diversity (entropy of class predictions). | Quality & Diversity (higher is better)                    | Can be less correlated with human perception than FID, less sensitive to mode collapse |
|                    | **Negative Log-Likelihood (NLL)** | Evaluates model's ability to assign high probabilities to real data. | Learning true data distribution (lower is better)         | Direct measure of likelihood, good for density estimation |
|                    | **Clipscore**                | Calculates similarity between generated images and text prompts using CLIP embeddings. | Image-Text Alignment (higher is better)                   | Crucial for multi-modal (e.g., text-to-image) models     |
|                    | **LSE-D (Lip Sync Error Distance)** | Measures lip synchronization accuracy in audio-driven animation. | Specialized for audio-driven animation (lower is better)  | Specific to tasks involving speech and visual motion      |
|                    | **Number of Function Evaluations (NFE)** | Counts the number of model evaluations during sampling. | Sampling Speed & Efficiency (lower is better)             | Direct measure of computational cost during inference     |
| **Qualitative**    | **Human Evaluation**         | Subjective assessment by human raters.                     | Subjective Quality, Realism, Coherence, Aesthetic Appeal  | Can discern nuances, indispensable for creative tasks, but resource-intensive & prone to bias |
| **Benchmarks / Datasets** | **General Image Datasets**   | COCO, ImageNet, CIFAR-10, CelebA-64, FFHQ, LSUN-Churches/Bedrooms | Common ground for model comparison, wide range of complexities | Standardized datasets for robust evaluation                   |
|                    | **Specialized Benchmarks**   | DrawBench                                                | Rigorous testing of text-to-image models' compositionality, spatial relations, etc. | Provides granular understanding of strengths/weaknesses, targets complex prompts |

The comprehensive evaluation of generative diffusion models necessitates the application of various metrics and benchmarks to quantitatively and qualitatively assess the quality, diversity, and fidelity of generated samples. Commonly employed quantitative metrics include the Inception Score (IS), Fréchet Inception Distance (FID), and Negative Log-Likelihood (NLL) [24,30]. These metrics are fundamental in gauging model performance, particularly in image synthesis tasks.

The Fréchet Inception Distance (FID) stands as a prominent metric for evaluating the quality and diversity of generated images. It quantifies the similarity between the distribution of generated samples and that of real-world data by comparing statistical aspects of computer vision features extracted using the Inception v3 model [29]. A lower FID score indicates higher quality and greater similarity to real data. For instance, the Soft Diffusion framework achieved a state-of-the-art FID score of 1.85 on CelebA-64 and a competitive 4.64 on CIFAR-10, demonstrating superior performance in image generation [11]. Similarly, consistency models have reported state-of-the-art FID scores of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation, highlighting advancements in sampling efficiency [24]. Imagen, a notable text-to-image generation model, achieved an impressive FID score of 7.27 on the COCO dataset without explicit training on COCO data, showcasing its robust generalization capabilities [2].

Beyond FID, other metrics serve specific evaluation purposes. The Inception Score (IS) measures both the quality and diversity of generated images, though it is known to sometimes correlate less strongly with human perception than FID. Negative Log-Likelihood (NLL) evaluates the model's ability to assign high probabilities to real data, often used as a proxy for the model's capacity to learn the true data distribution [30]. For multimodal applications, such as text-to-image generation, metrics like Clipscore are crucial, as they calculate the similarity between generated images and their corresponding text prompts, assessing image-text alignment [29]. In specific domains like audio-driven animation, specialized metrics such as LSE-D (Lip Sync Error Distance) are employed for quantitative evaluation of lip synchronization, alongside FID and KID for diversity, and the Number of Function Evaluations (NFE) for measuring sampling time and efficiency [11,21]. The broad categorization of evaluation metrics also facilitates a structured overview of methods for enhanced control in diffusion models, indicating the importance of tailored metrics for controllable generation [22].

While quantitative metrics provide objective comparisons, human evaluation remains paramount for assessing the subjective quality, realism, and coherence of generated data. Human raters can discern nuances in image fidelity, aesthetic appeal, and semantic consistency that automated metrics might miss. For instance, human evaluation revealed that Imagen's samples were on par with real COCO data in terms of image-text alignment [2]. Perceptual evaluation through user studies is critical for evaluating the subjective quality of generative models, as demonstrated in assessments of audio-driven neural parametric head models [21]. The significant preference of human raters for Imagen over other models on benchmarks like DrawBench underscores the indispensable role of human perception in validating model performance, especially in creative and text-to-image domains [2].

The performance of diffusion models is typically benchmarked against established datasets, which serve as common grounds for comparison. Key benchmarks include COCO, ImageNet, CIFAR-10, CelebA-64, CelebA-HQ, FFHQ, LSUN-Churches, and LSUN-Bedrooms [2,4,11]. These datasets cover a wide range of image complexities and categories, enabling a comprehensive assessment of model capabilities. Beyond general image datasets, specialized benchmarks like DrawBench have been introduced to rigorously test text-to-image models. DrawBench systematically evaluates critical aspects such as compositionality, cardinality, spatial relations, long-form text interpretation, handling of rare words, and responses to challenging prompts, providing a more granular understanding of model strengths and weaknesses [2]. The strengths of quantitative metrics lie in their objectivity and reproducibility, enabling consistent comparison across models. However, their weakness can be a potential disconnect from human perception and the inability to fully capture subjective qualities like creativity or subtle artifacts. Conversely, human evaluation excels in capturing subjective quality but can be resource-intensive and prone to individual biases. Benchmarks like DrawBench mitigate some weaknesses of general metrics by offering structured, challenging evaluations, thereby providing a more holistic assessment of model performance in complex generative tasks.
## 7. Challenges and Future Directions

**Generative Diffusion Models: Key Challenges and Future Directions**

| Category             | Key Challenges                                           | Promising Future Directions                               |
| :------------------- | :------------------------------------------------------- | :-------------------------------------------------------- |
| **Computational & Efficiency** | High training/sampling cost, slow inference speed, memory requirements | More efficient sampling algorithms (e.g., DPM-Solver), optimized architectures (LDMs), real-time applications |
| **Generation Quality & Control** | Limited data generalization, lack of explicit content control, training stability/mode collapse, fixed input size for text | Enhanced control mechanisms (e.g., better steering, interpretability of latent vectors), improved likelihood estimation |
| **Ethical & Societal** | Social biases (e.g., in text-to-image models), potential for misuse, dataset bias | More robust social bias evaluation methods, responsible AI development, ensuring safety and fairness |
| **Applicability & Data Types** | Primarily single-input/single-output, challenges with complex data structures (e.g., 3D motion, discrete text) | Extension to new applications (e.g., audio-visual speech, full-body animation), diverse data types, personalized medicine, climate modeling |
| **Theoretical Understanding** | Assumptions (e.g., Gaussian noise conversion), gap between theory and practice, specific classifications/derivations | Re-examine assumptions, bridge theory-practice gap, comprehensive theoretical frameworks, novel generation processes, exploring prior distributions |
| **Model Integration**| How to combine effectively with other generative models | Integration with other generative models (e.g., self-supervised, adversarial), foundational models, understanding optimal task fit |

Despite the remarkable success of generative diffusion models across various domains, several significant challenges persist, demanding continued research and development. A primary limitation is the substantial computational cost associated with both training and sampling processes, along with considerable memory requirements [13,18,29,32]. Training large models, for instance, necessitates immense computational resources [29]. Furthermore, diffusion models, particularly early iterations like DDPMs, are characterized by slow inference speeds due to their iterative, multi-step reverse process, often requiring hundreds or even a thousand steps for sample generation [14,17,20,23,26]. This slow sampling process presents a significant hurdle for real-time applications [7].

Another critical area of concern involves the models' ability to handle complex data distributions and ensure generation quality and controllability. Classical diffusion models exhibit shortcomings in data generalization capabilities and often lack explicit content control [10,17]. While excelling at capturing fine-grained details, they can become trapped in imperceptible complexities of data, and current methods face challenges in effectively controlling and steering the generation process [22,32]. For instance, challenges arise in generating high-fidelity and temporally consistent outputs for complex data types like 3D head motion from audio [21], and their application in text generation is limited by fixed input size requirements [20]. Issues of training stability and potential mode collapse also exist, where models may fail to capture the full diversity of the data distribution, as observed with models like Imagen [2].

Moreover, the ethical implications of diffusion models, particularly concerning potential biases encoded within training data, demand careful attention. Text-to-image models, for example, have demonstrated social biases and stereotypes, such as a tendency to generate images of people with lighter skin tones or align professions with Western gender stereotypes, raising concerns about misuse and the social consequences of dataset bias [2]. There is an urgent need for more robust social bias evaluation methods for these models [2].

Addressing these limitations paves the way for several promising future research directions. A key focus is the development of more efficient sampling algorithms and architectures to mitigate computational burden and accelerate inference. Techniques like Latent Diffusion Models (LDMs), which operate in a compressed latent space, have demonstrated success in reducing computational costs for high-resolution image synthesis [4]. Further optimization in balancing complexity reduction with visual fidelity is a crucial avenue [4]. Breakthroughs such as DPM-Solver have significantly reduced the required sampling steps, achieving good results in as few as 10 to 20 steps [23]. Future work will explore novel architectures and more efficient training methods, alongside improving the efficiency of generation for real-time applications and optimizing speed and memory consumption, particularly for multi-modal tasks [7,9,24].

Enhancing control mechanisms and improving likelihood estimation are critical for greater applicability and model reliability [9,10,25]. This includes refining methods for steering the generation process and improving the interpretability of latent vectors [20,22].

The extension of diffusion models to new applications and data types represents a vast area for exploration. While primarily applied to single-input, single-output scenarios, future research should generalize these models to more complex contexts, such as text-to-audiovisual speech synthesis, full-body animation, and multi-modal generation tasks beyond image and audio [7,15,21]. This expansion includes exploring applications in diverse fields like unsupervised learning, personalized medicine, and climate modeling, and addressing specific challenges for different data structures [28].

Furthermore, foundational theoretical advancements are essential. This encompasses re-examining accepted assumptions, such as the widely held belief that the forward process of diffusion models converts data into a standard Gaussian distribution [1,15]. A deeper understanding of the effectiveness of diffusion models, coupled with efforts to bridge the gap between theoretical conditions and practical operations, is crucial [1,15]. Research into converting discrete-time models to continuous-time models and designing improved discrete methods also holds significant promise [15,31]. Investigating new generation processes and perspectives, exploring prior distributions, transfer kernels, and novel diffusion schemes will further advance the field [15,30].

Finally, fostering responsible AI development is paramount, which includes continuing to address social biases, promoting ethical use, and ensuring the safety and fairness of generated content [2,25]. Future research will also involve exploring latent representations more deeply and investigating how diffusion models can be effectively combined or interfaced with other generative models and foundational models, aiming to harness their complementary strengths and understand when and why diffusion models are most effective for specific tasks [1,28].
## 8. Conclusion
Diffusion models represent a profound advancement in the landscape of generative modeling, fundamentally reshaping capabilities in high-quality data generation and diverse applications [17,25]. Their emergence has marked a significant paradigm shift, offering simplified training processes and exceptional performance across various tasks, often surpassing prior generative adversarial networks (GANs) in aspects like training stability and sample quality [7,23,24,38]. This class of models is characterized by its robust theoretical foundation and a balance of simplicity, stability, and interpretability, contributing to its widespread adoption and ongoing research [7,38].

The impact of diffusion models spans a broad spectrum of domains, demonstrating their versatility and potential to revolutionize various fields [3,18]. In visual computing, they have achieved state-of-the-art results in tasks such as high-resolution image synthesis, inpainting, super-resolution, and text-to-image generation, exemplified by models like Latent Diffusion Models (LDMs) and Stable Diffusion [4,32]. LDMs, in particular, addressed the inherent high computational costs of traditional diffusion models by operating in a latent space, thereby balancing complexity reduction with visual fidelity [4]. Further advancements include models like Imagen, which showcased unprecedented photorealism and language understanding in text-to-image generation, emphasizing the importance of scaling text encoders [2]. Beyond static image generation, diffusion models have shown promise in dynamic applications such as audio-driven 3D facial animation [21] and even complex multi-modal image fusion under various degradations [13]. Their utility extends to semantic segmentation, particularly in scenarios with scarce labeled data [34], and in broader high-dimensional data generation, including cross-modal tasks in bioinformatics and computational biology [7,18].

Despite these significant achievements, the field continues to face challenges, primarily concerning inference speed and the need for further research in specific applications [25,29]. Nonetheless, continuous research efforts are rapidly enhancing the efficiency and applicability of these models [23]. Key advancements include the development of score-based generative models and consistency models, which offer improved sampling speed and training stability, challenging the historical dominance of GANs [24]. Moreover, innovations in modeling more general corruption processes, such as those seen in Soft Diffusion, have led to significant computational advantages and state-of-the-art results [11]. The increasing focus on controllable generation in diffusion models is establishing it as a distinct and crucial subfield, aiming to foster further innovations in reliable and scalable controlled synthesis [22].

The promising future of diffusion models is underscored by the synergistic progress in theoretical insights, architectural innovations, and practical application demands [25]. Future research directions include exploring novel architectures, such as new systematic classifications and detailed comparisons of various diffusion model types, and refining existing models through improved sampling techniques [28,35]. Further advancements will involve developing more comprehensive theoretical frameworks, such as exploring the three equivalent perspectives for training diffusion models (predicting input, noise, or score) and their derivations from the evidence lower bound (ELBO) [20]. Additionally, integrating diffusion models with complementary mechanisms like self-supervised learning and adversarial training is expected to unlock their full potential [7]. Addressing open problems in both algorithmic development and specific applications will be crucial for sustained progress [9]. As the research community continues to address their limitations and explore new frontiers, diffusion models are poised to continue their trajectory as a revolutionary force in artificial intelligence.

## References

[1] 扩散模型(Diffusion Model)综述：方法与应用 [https://zhuanlan.zhihu.com/p/562389931](https://zhuanlan.zhihu.com/p/562389931) 

[2] Imagen: Photorealistic Text-to-Image Generation with Deep Language Understanding [http://imagen.research.google/](http://imagen.research.google/) 

[3] Diffusion Models: Methods, Applications, and Connections to Other Generative Models [https://blog.csdn.net/m0_61899108/article/details/133935794](https://blog.csdn.net/m0_61899108/article/details/133935794) 

[4] Latent Diffusion Models for High-Resolution Image Synthesis (Stable Diffusion) [https://ommer-lab.com/research/latent-diffusion-models/](https://ommer-lab.com/research/latent-diffusion-models/) 

[5] 扩散模型综述：算法、应用与未来方向 [https://baijiahao.baidu.com/s?id=1743911018911016154&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1743911018911016154&wfr=spider&for=pc) 

[6] 扩散模型：扩散匹配与得分匹配 [https://zhuanlan.zhihu.com/p/601960115](https://zhuanlan.zhihu.com/p/601960115) 

[7] 高维数据生成：扩散模型从图像到音频的多模态应用 [https://bbs.huaweicloud.com/blogs/448668](https://bbs.huaweicloud.com/blogs/448668) 

[8] DDPM：去噪扩散概率模型详解 [https://zhuanlan.zhihu.com/p/26347907795](https://zhuanlan.zhihu.com/p/26347907795) 

[9] Diffusion Models: 方法与应用的全面综述 [https://zhuanlan.zhihu.com/p/639699822](https://zhuanlan.zhihu.com/p/639699822) 

[10] 扩散模型综述：方法与应用 [https://zhuanlan.zhihu.com/p/654691590](https://zhuanlan.zhihu.com/p/654691590) 

[11] Soft Diffusion：谷歌提出通用扩散模型新框架，加速图像生成 [http://baijiahao.baidu.com/s?id=1746548059115785830&wfr=spider&for=pc](http://baijiahao.baidu.com/s?id=1746548059115785830&wfr=spider&for=pc) 

[12] Diffusion Models: 方法与应用综述 [https://a434.tongji.edu.cn/info/1010/1791.htm](https://a434.tongji.edu.cn/info/1010/1791.htm) 

[13] DRMF: Degradation-Robust Multi-Modal Image Fusion via Diffusion Priors [https://dl.acm.org/doi/10.1145/3664647.3681064](https://dl.acm.org/doi/10.1145/3664647.3681064) 

[14] 白话DDPM：一文搞懂扩散模型 [https://juejin.cn/post/7420320945297571849](https://juejin.cn/post/7420320945297571849) 

[15] 扩散模型首篇综述：算法、应用与未来方向 [https://baijiahao.baidu.com/s?id=1744106089533388303&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1744106089533388303&wfr=spider&for=pc) 

[16] Diffusion Models 综述总结：理论、改进与应用 [https://zhuanlan.zhihu.com/p/597479167](https://zhuanlan.zhihu.com/p/597479167) 

[17] 扩散模型 (Diffusion Models) 综述：原理、算法及 DDPM 模型 [https://zhuanlan.zhihu.com/p/640138441](https://zhuanlan.zhihu.com/p/640138441) 

[18] Diffusion Models in Bioinformatics and Computational Biology: A Review [https://www.nature.com/articles/s44222-023-00114-9](https://www.nature.com/articles/s44222-023-00114-9) 

[19] Diffusion Models 专栏文章汇总：入门与实战 [https://blog.csdn.net/qq_41895747/article/details/122847060](https://blog.csdn.net/qq_41895747/article/details/122847060) 

[20] 大一统视角理解扩散模型 (Diffusion Models) [https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247552643&idx=5&sn=24285a4da921a874076c82b0b41990bf&chksm=ebb73657dcc0bf41cc33f45cb5ba67466602fb0a44f8cfd70e223a2adf2b83f8c4d20ae7449c&scene=27](https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247552643&idx=5&sn=24285a4da921a874076c82b0b41990bf&chksm=ebb73657dcc0bf41cc33f45cb5ba67466602fb0a44f8cfd70e223a2adf2b83f8c4d20ae7449c&scene=27) 

[21] FaceTalk: 基于音频驱动的神经参数头模型的运动扩散 [https://ar5iv.labs.arxiv.org/html/2312.08459](https://ar5iv.labs.arxiv.org/html/2312.08459) 

[22] A Comprehensive Survey of Multimodal Controllable Diffusion Models [https://jcst.ict.ac.cn/article/doi/10.1007/s11390-024-3814-0](https://jcst.ict.ac.cn/article/doi/10.1007/s11390-024-3814-0) 

[23] Diffusion Model：超越 GAN 的图像生成新星？ [https://www.zhihu.com/question/536012286](https://www.zhihu.com/question/536012286) 

[24] Yang Song: Score-Based Diffusion Models and Generative AI Research [http://yang-song.net/](http://yang-song.net/) 

[25] Diffusion模型：原理、实践与应用全方位教程 [https://cloud.tencent.com.cn/developer/article/2529971](https://cloud.tencent.com.cn/developer/article/2529971) 

[26] 视觉扩散模型：综述与应用 [https://blog.csdn.net/qq_44681809/article/details/131188803](https://blog.csdn.net/qq_44681809/article/details/131188803) 

[27] 扩散模型(Diffusion Models)详解 [https://blog.csdn.net/qq_44648285/article/details/143316237](https://blog.csdn.net/qq_44648285/article/details/143316237) 

[28] Diffusion Models: 方法与应用综合综述 [https://blog.csdn.net/qq_61735602/article/details/134914364](https://blog.csdn.net/qq_61735602/article/details/134914364) 

[29] 扩散模型在视觉计算中的应用：最新技术综述 [https://blog.csdn.net/weixin_44796129/article/details/134561975](https://blog.csdn.net/weixin_44796129/article/details/134561975) 

[30] 扩散模型综述：原理、应用与AIGC新宠 [https://blog.csdn.net/weixin_43424450/article/details/130836178](https://blog.csdn.net/weixin_43424450/article/details/130836178) 

[31] 扩散模型（Diffusion Model）：谷歌&北大首篇综述解读 [https://blog.csdn.net/weixin_44603934/article/details/128215572](https://blog.csdn.net/weixin_44603934/article/details/128215572) 

[32] 扩散模型：图像生成的新突破 [https://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ==&mid=2247616307&idx=1&sn=6cb43349c4b1135850f21e3b08624092&chksm=fa84a9d6fb993d40686cf8d2c92251ae2a70f9a194d77b29698cc14a11e11688fee0505a3e26&scene=27](https://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ==&mid=2247616307&idx=1&sn=6cb43349c4b1135850f21e3b08624092&chksm=fa84a9d6fb993d40686cf8d2c92251ae2a70f9a194d77b29698cc14a11e11688fee0505a3e26&scene=27) 

[33] 扩散模型应用研究进展：医疗、推荐、天气、能源及其他领域 [https://www.zhihu.com/question/579204461](https://www.zhihu.com/question/579204461) 

[34] 扩散模型：标签高效语义分割新方法 [https://zhuanlan.zhihu.com/p/460302946](https://zhuanlan.zhihu.com/p/460302946) 

[35] Diffusion Model 综述：方法与应用 [https://zhuanlan.zhihu.com/p/571451312?utm_id=0](https://zhuanlan.zhihu.com/p/571451312?utm_id=0) 

[36] Score-Based生成模型：公式推导与代码实战 [https://blog.csdn.net/qq_45934285/article/details/129837476](https://blog.csdn.net/qq_45934285/article/details/129837476) 

[37] 统计学学术速递[12.15]: 前沿理论与应用进展 [https://cloud.tencent.com/developer/article/1920741](https://cloud.tencent.com/developer/article/1920741) 

[38] 扩散模型 (Diffusion Models) 详解 [https://blog.csdn.net/weixin_42656681/article/details/128705855](https://blog.csdn.net/weixin_42656681/article/details/128705855) 

[39] 一译文档交互界面 [https://yiyibooks.cn/__src__/arxiv/2312.07409v1/index.html](https://yiyibooks.cn/__src__/arxiv/2312.07409v1/index.html) 

[40] TechBeat：AI技术交流与分享平台 [https://techbeat.net/article-info?id=5016](https://techbeat.net/article-info?id=5016) 

