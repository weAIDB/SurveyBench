# A Survey on Vision Transformers

# 0. A Survey on Vision Transformers

## 1. Introduction
Convolutional Neural Networks (CNNs) have historically been the cornerstone of computer vision, excelling in tasks such as image classification, object detection, and semantic segmentation by effectively extracting local features through their inherent inductive biases like locality and translation equivariance [5,13,28]. However, the architectural design of CNNs inherently limits their capacity to model long-range dependencies and capture global contextual information across an entire image, a crucial aspect for holistic visual understanding [23,32,36].

Concurrently, the Transformer architecture revolutionized Natural Language Processing (NLP), demonstrating unparalleled success in sequence-to-sequence tasks through its self-attention mechanism, which effectively models global relationships between tokens [33,35,40]. This profound success in NLP inspired researchers to explore the application of Transformers in computer vision, leading to the emergence of Vision Transformers (ViTs) [3,10,31,37]. 

**Comparison of CNNs and Vision Transformers (ViTs)**

| Feature/Aspect         | Convolutional Neural Networks (CNNs)                                     | Vision Transformers (ViTs)                                           |
|------------------------|--------------------------------------------------------------------------|----------------------------------------------------------------------|
| **Core Mechanism**     | Convolutional layers for local feature extraction                        | Self-attention mechanisms for global relationship modeling           |
| **Inductive Biases**   | Strong (Locality, Translation Equivariance)                              | Minimal                                                              |
| **Feature Learning**   | Hierarchical, local features combined progressively                      | Learns global relationships directly across entire image/patches     |
| **Long-Range Deps.**   | Limited (via stacking layers, larger receptive fields)                   | Excellent (via self-attention)                                       |
| **Parallel Processing**| Moderate (depends on architecture)                                       | High (inherently supports parallel processing)                       |
| **Data Requirements**  | Less data needed for robust learning (due to inductive biases)           | More data needed for robust learning (due to minimal biases)         |
| **Computational Cost** | Generally lower for high-res images (local operations)                   | Higher (quadratic complexity of self-attention with sequence length) |
| **Pre-training**       | Effective on smaller datasets; benefits from pre-training                | Crucial on very large datasets for competitive performance           |
| **Early Success In**   | Computer Vision (Image Classification, Object Detection, Segmentation)   | Natural Language Processing (NLP)                                    |

This transition marked a significant paradigm shift, challenging the traditional dominance of CNNs and fostering the convergence of vision and language models, thereby promoting advancements in multi-modal learning [3,10,33].

ViTs have rapidly gained prominence due to several key advantages. Their self-attention mechanisms enable them to model long-range dependencies and global context more effectively than CNNs, which are constrained by local receptive fields [5,18,29,31,32,36,37]. By treating images as sequences of patches, ViTs can weigh the importance of all other patches when processing a single one, facilitating a comprehensive understanding of global relationships [36]. Furthermore, Transformers inherently support parallel processing, leading to improved computational efficiency during training compared to sequential architectures like Recurrent Neural Networks (RNNs) [31,35,37]. ViTs also possess minimal inductive biases, allowing them to learn features directly from data with fewer architectural assumptions than CNNs [19,31]. While this flexibility necessitates larger datasets for robust learning, it also enables ViTs to adapt to diverse visual patterns more broadly [11].

Despite their compelling advantages, ViTs present distinct trade-offs when compared to CNNs. While ViTs have demonstrated the ability to match or surpass the performance of state-of-the-art convolutional networks on various image recognition benchmarks, including ImageNet, CIFAR-100, and VTAB, especially when pre-trained on large datasets and requiring fewer computational resources for training [10,15,17,18,27,38,40], they frequently underperform CNNs when trained from scratch on smaller datasets [11,27]. This limitation arises from ViTs' minimal inductive biases, which demand extensive data to learn robust visual representations, unlike CNNs that benefit from built-in inductive priors such as locality and translation equivariance [20,29,33]. Moreover, the high computational cost, particularly due to the quadratic complexity of self-attention with respect to image resolution, poses a significant challenge for deploying ViTs on edge devices and in large-scale applications [1,2,7,14,16,20,21]. To address these limitations, various solutions have been proposed, including model compression techniques like Width & Depth Pruning (WDPruning) for mainstream ViTs [16], efficient architectures like Swin Transformer which offers linear computational complexity through shifted windows [4], and hybrid models that integrate CNNs for local feature extraction with ViTs for global context to improve performance on small datasets [11,23].

The transformative impact and versatility of ViTs have spurred immense research interest, leading to their rapid development and widespread adoption across diverse computer vision tasks, extending beyond initial image classification to include object detection, semantic segmentation, and medical image analysis [6,19,22,26,29,36]. The proliferation of ViT models and associated projects underscores the dynamic and collaborative nature of this research domain [14,24]. Comprehensive surveys acknowledge this vibrant landscape by providing detailed analyses of ViT mechanisms, performance comparisons, and promising research directions based on extensive collections of research papers [6,26,29].

This survey aims to provide a comprehensive and structured overview of the current state of Vision Transformers, tracing their evolution and dissecting their architectural innovations. We delve into their fundamental principles, explore their diverse architectural variations, and analyze their applications across a wide spectrum of computer vision tasks. The scope of this survey encompasses key developments and addresses critical issues such as computational efficiency, data dependency, and the burgeoning integration with other modalities like large language models [6,9,26,29]. Differentiating from existing comprehensive paper lists and curated collections [24,25], this survey offers a systematic classification of ViT models, provides detailed analyses of efficient ViTs with benchmark evaluations, and presents insightful perspectives on future research directions. We synthesize insights from the vast and rapidly expanding body of literature, offering a focused yet broad perspective on the field's evolution, key challenges, and emerging trends in ViT research [6,7,29]. The subsequent sections of this survey will systematically explore these aspects, providing readers with a holistic understanding of ViTs and their transformative impact on computer vision.
## 2. Background on Transformers
The Transformer architecture, introduced by Vaswani et al. (2017) in "Attention is All You Need," marked a significant departure from traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) by entirely relying on attention mechanisms to process sequence data [30,35,36]. Initially designed for natural language processing (NLP) tasks such as machine translation, its core innovation lies in its ability to model relationships between all elements in a sequence, thereby capturing long-range dependencies more effectively than its predecessors [2,28,36,37,40].

The foundational component of the Transformer is the **attention mechanism**, which enables the model to focus on the most salient parts of the input sequence, akin to human selective attention [36]. In a Transformer layer, an input sequence 
  X ∈ ℝ^(l × d)
(where l is the sequence length and d is the dimension) is transformed into three distinct sequence vectors—query (Q), key (K), and value (V)—through linear projections [30]. Mathematically, these are formulated as:
  Q = XW_Q,   K = XW_K,   V = XW_V
where W_Q, W_K, and W_V are learnable weight matrices [30]. The attention weights are then computed by calculating the dot product between the queries and keys, followed by scaling and a softmax operation to normalize the distribution [30]. The output is an aggregation of values weighted by these attention scores. The general attention function is expressed as:
  Attention(Q, K, V) = softmax((QKᵀ) / √d_k)V
where d_k represents the dimension of the query and key vectors, serving as a scaling factor to prevent vanishing gradients [22,26].

A pivotal enhancement to the attention mechanism is **Multi-Head Self-Attention (MHSA)**, which addresses the limitations of single-head attention by allowing the model to jointly attend to information from different representation subspaces at different positions [26,30]. MHSA runs multiple self-attention operations in parallel, each with independent linear projections for Q, K, and V, effectively capturing diverse aspects of relationships within the sequence [17]. The outputs from these individual attention heads are then concatenated and linearly transformed into the final output. The MHSA process is given by:
  MHSA(Q, K, V) = Concat(head₁, …, head_h)Wᴼ
where h is the number of heads, Wᴼ is the output projection matrix, and headᵢ = Attention(QWᵢᴾ, KWᵢᴷ, VWᵢⱽ) represents the output of each head [30]. This mechanism is crucial for constructing long-range dependencies and capturing global contexts within the input data, such as relations between words in a sentence or image patches [23,37].

The original Transformer architecture typically follows an **encoder-decoder structure**, although variants exist [26,37]. Both the encoder and decoder are composed of multiple identical layers. The **encoder** stack processes the input sequence, generating a rich contextual representation. 

![Transformer Encoder Layer Structure](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/kn97pCEK77pg6DjgUocHk_/home/surveygo/data/requests/13519/survey/imgs/Transformer%20Encoder%20Layer%20Structure.png)

Each encoder layer consists of two sub-layers: a Multi-Head Self-Attention layer and a position-wise feed-forward network (FFN) [26,30]. The FFN applies two linear transformations with a ReLU activation:
  FFN(x) = ReLU(xW₁)W₂
This FFN layer processes each position independently while using the same parameters across positions [30]. The **decoder** stack, on the other hand, generates the output sequence step by step. In addition to the two sub-layers found in the encoder, the decoder includes a third sub-layer: a multi-head cross-attention mechanism that attends to the output of the encoder stack [26,30,37]. Masked self-attention is applied in the decoder to prevent positions from attending to subsequent positions during training, ensuring that the prediction for a given token depends only on previous outputs [30,37]. Residual connections and layer normalization are employed throughout the model to facilitate training and enhance scalability [26,30].

A critical component for sequential tasks is **positional encoding**. Since Transformers process all input embeddings simultaneously and independently, they inherently lack a mechanism to account for the order or position of elements in a sequence [30]. To address this, positional encodings—often generated using sine and cosine functions of different frequencies—are added to the input embeddings. This allows the model to leverage sequential information [30,35]. A common formulation for positional encoding is:
  PE(pos, 2i) = sin(pos / 10000^(2i/d_model)),  PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
where pos is the position in the sequence, i is the dimension index, and d_model is the dimension of the embeddings [30].

The advantages of Transformers over traditional models like RNNs and LSTMs include their superior ability to capture long-range dependencies across extensive sequences and their inherent support for parallel processing [2,35]. Unlike RNNs, which process sequences sequentially—leading to information loss and computational bottlenecks for long sequences—Transformers can process the entire sequence at once, significantly speeding up training and inference [35]. This parallelization capability makes them highly efficient and scalable, especially when trained on large datasets, a common practice in modern deep learning [40]. These fundamental principles lay the groundwork for understanding how Transformers have been adapted and successfully applied to various vision tasks.
## 3. Core Concepts of Vision Transformers

**Comparison of Patch Embedding Strategies**

| Strategy                     | Advantages                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Disadvantages                                                                                                                                                                                                                                                                                                    | Feature Extraction                                                                                                                                                           | Computational Efficiency                                                                                                                                        |
|------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Linear Projection            | Simplicity; direct adaptation of image patches to Transformer's sequence input format [37]. Inherently captures global context due to non-overlapping large patches.                                                                                                                                                                                                                                                                                            | Can be unstable, especially in initial layers [20]. Lacks inherent inductive biases for local features, often requiring vast amounts of data for robust training. Large patch sizes can lead to loss of fine-grained spatial information.                                                                            | Extracts global features by flattening and embedding large, non-overlapping image regions.                                                                     | Highly dependent on patch size; larger patches lead to better efficiency, smaller patches to worse.                                                             |
| Convolutional Patch Embedding| Improves stability and robustness, especially on smaller datasets, by leveraging CNNs' inductive biases for local feature extraction [11,20]. Can capture richer, hierarchical features through multi-layer convolutions [4].                                                                                                                                                                                                                                      | Introduces more architectural complexity in the initial stage. Might deviate from the "pure" end-to-end sequence processing philosophy of the original Transformer.                                                                                                                                                 | More robust local feature extraction, potentially multi-scale or hierarchical, due to CNN properties.                                                          | Can be more computationally intensive in the initial stages if multiple layers are used, though improved performance may justify the cost.                     |

Vision Transformers (ViTs) represent a paradigm shift in computer vision by adapting the highly successful Transformer architecture, originally developed for natural language processing, to image data [2]. This adaptation fundamentally redefines how images are processed, moving from convolution-centric operations to a sequence-based approach. 

![Vision Transformer (ViT) Core Pipeline](https://modelbest-prod.oss-cn-beijing.aliyuncs.com/SurveyGo/picture/mGRx62Zj72mqstM-DI9_Z_/home/surveygo/data/requests/13519/survey/imgs/Vision%20Transformer%20%28ViT%29%20Core%20Pipeline.png)

The core concepts underpinning ViTs involve a systematic pipeline to transform two-dimensional visual information into a one-dimensional sequence suitable for Transformer processing, coupled with mechanisms to preserve spatial context and capture global relationships [7,25,30,33].

The initial step in this transformation is the **patch embedding process**, which involves segmenting an input image into a series of non-overlapping patches. Each patch is then flattened and linearly projected into a D-dimensional embedding space, effectively converting the 2D image into a sequence of tokens analogous to words in a sentence [5,10,11,15,18,20,27,28,32,36,37,38]. While a standard linear projection is commonly employed, alternative convolutional patch embedding strategies have emerged to introduce inductive biases beneficial for stability and performance, especially on smaller datasets. The selection of patch size critically influences both the granularity of extracted features and the computational cost, given the quadratic scaling of Transformer operations with sequence length.

A crucial component for ViTs is **positional encoding**, which is indispensable for integrating spatial information into the permutation-invariant Transformer architecture [20]. Without positional cues, the self-attention mechanism would treat all image patches as unordered, failing to leverage their relative spatial relationships vital for visual understanding [20]. Both fixed (e.g., sine and cosine functions) and learnable positional embeddings are utilized, with the latter allowing the model to adaptively determine optimal spatial representations during training. While 1D learnable embeddings are a prevalent choice in many ViT variants, the effectiveness and optimal design of positional encoding can vary depending on the specific architectural variant and the computer vision task at hand.

The core of a ViT's processing capability resides within its **Transformer encoder structure and attention mechanisms**. The encoder typically consists of multiple identical layers, each comprising a Multi-head Self-Attention (MSA) module and a Multi-Layer Perceptron (MLP) block, interspersed with Layer Normalization and residual connections to ensure stable training [20,38]. The MSA mechanism is central to the Transformer's ability to capture long-range dependencies by allowing each patch token to dynamically attend to all other tokens in the sequence, thereby modeling global contextual relationships within the image [22,35]. Multi-head attention further enhances this capability by performing attention computations across multiple "heads" in parallel, enabling the model to learn diverse sets of relationships from different representational subspaces. Despite its power, the standard self-attention mechanism faces a computational challenge due to its quadratic complexity with respect to the input sequence length. This has spurred the development of various efficient attention mechanisms, such as windowed, shifted window, and orthogonal attention, designed to mitigate computational burden while maintaining performance.

In summary, ViTs achieve their powerful image processing capabilities by meticulously converting images into sequences through patch embedding, imbuing these sequences with spatial awareness via positional encoding, and leveraging the Transformer encoder's multi-head self-attention to model intricate global dependencies and extract rich contextual features. These foundational concepts form the bedrock upon which Vision Transformers are built and continue to evolve.
### 3.1 Patch Embedding
Patch embedding is a foundational component of Vision Transformers (ViTs), designed to adapt 2D image data for processing by Transformer architectures, which were originally developed for sequential data like text [22,32]. The core idea involves reshaping an input image 
  x ∈ ℝ^(H × W × C)
into a sequence of flattened 2D patches, 
  xₚ ∈ ℝ^(N × (P² · C)),
where (H, W) denotes the original image resolution, C is the number of channels, (P, P) is the resolution of each image patch, and 
  N = (H · W) / P²
represents the total number of patches, serving as the effective input sequence length for the Transformer [17,27,29,32]. These flattened patches, each with a dimension of P²C, are then mapped to a D-dimensional embedding space, typically maintaining a constant latent vector size D throughout subsequent Transformer layers [27,29,32].

Linear Projection Patch Embedding:  
The most common strategy for patch embedding in the original ViT is through a trainable linear projection [27,29,32,38,40]. This involves flattening each 
  P × P × C
patch into a 1D vector of length 
  P²C
and subsequently applying a linear transformation to project it into a D-dimensional embedding vector [15,29,32]. For instance, a 224 × 224 image divided into 16 × 16 patches results in 196 patches, each flattened to a 768-dimensional vector (16 × 16 × 3), which is then linearly projected to an embedding dimension, often 768 [15,33]. This linear projection can be conceptually and practically implemented using a single convolutional layer with a kernel size and stride equal to the patch size (P, P) and the number of output channels equal to the embedding dimension D [10,33]. For example, ViT-L/16 utilizes 16 × 16 patches [10]. 

The mathematical representation of this embedding—often combined with a learnable class token (x₍class₎) and positional encoding (E₍pos₎)—is typically expressed as:
  z₀ =  + E₍pos₎
where E is the linear projection matrix [29].

Convolutional Patch Embedding Strategies:  
While the standard ViT employs a single large convolutional layer (or its linear projection equivalent) for patch embedding, this approach can sometimes be unstable, particularly when dealing with the initial layers [20]. To address this, several alternative convolutional patch embedding strategies have emerged. These methods often leverage the inductive biases of Convolutional Neural Networks (CNNs) for local feature extraction, aiming to provide more stable and robust initial representations. Solutions include using a combination of smaller convolutional layers and pooling layers or incorporating residual blocks in the initial layers to create more stable patch embeddings [20]. For example, the Orthogonal Transformer uses a convolutional feature encoder to obtain better initial feature representation [7]. Similarly, the Sequential Overlapped Patch Embedding (SOPE) module explicitly employs convolutional operations for patch embedding, often with overlapping patches, to enhance training stability—particularly on smaller datasets [11]. Some approaches, like those by Wu et al., integrate convolutional layers to extract low-level features before feeding them into the visual Transformer, effectively using a tokenizer to group pixels into visual tokens [26]. In hierarchical ViTs, such as Swin Transformer, a patch embedding layer typically consists of a 4 × 4 convolutional layer with a stride of 4, reducing the resolution by a factor of 4 and projecting the 4 × 4 × 3 = 48-dimensional patch features to a C-dimensional embedding [4].

Impact of Patch Size on Performance and Computational Cost:  
The choice of patch size (P, P) significantly influences both the performance and computational cost of ViT models.

• Computational Cost: A smaller patch size leads to a larger number of patches 
  (N = (H · W) / P²),
which directly translates to a longer input sequence length for the Transformer encoder [27,32]. Since the computational complexity of the self-attention mechanism in Transformers scales quadratically with the sequence length, smaller patches drastically increase the computational burden. For instance, using 16 × 16 patches for a 224 × 224 image yields 196 patches, while smaller patches like 8 × 8 would yield 784 patches, leading to a much higher computational cost. Conversely, larger patch sizes, such as 32 × 32 for a 224 × 224 image, result in fewer patches (49 patches), reducing the computational load [18].

• Performance and Feature Extraction: Patch size also impacts the granularity of feature extraction and overall model performance. Smaller patches provide more fine-grained local information, potentially allowing the Transformer to capture more subtle visual details. However, they also lead to a higher number of tokens, which requires more computational resources and potentially larger datasets for effective training to avoid overfitting. Larger patches, while computationally more efficient, result in a coarser representation of the image, potentially losing fine-grained details essential for certain tasks. The original ViT often uses 16 × 16 patches as a common choice, balancing these factors [5,15,36].

Comparison of Advantages and Disadvantages:

┌──────────────────────────────────────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬──────────────────────────────────────────────────────────────┬──────────────────────────────────────────────────────────────┐
│ Strategy                                     │ Advantages                                                                                                                                                 │ Disadvantages                                                                                                                                                                        │ Feature Extraction                                           │ Computational Efficiency                                     │
├──────────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────┤
│ Linear Projection                            │ Simplicity; direct adaptation of image patches to Transformer's sequence input format [37]. Inherently captures global context due to non-overlapping large patches.                    │ Can be unstable, especially in initial layers [20]. Lacks inherent inductive biases for local features, often requiring vast amounts of data for robust training. Large patch sizes can lead to loss of fine-grained spatial information.  │ Extracts global features by flattening and embedding large, non-overlapping image regions.  │ Highly dependent on patch size; larger patches lead to better efficiency, smaller patches to worse.   │
├──────────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────┤
│ Convolutional Patch Embedding                │ Improves stability and robustness, especially on smaller datasets, by leveraging CNNs' inductive biases for local feature extraction [11,20]. Can capture richer, hierarchical features through multi-layer convolutions [4].  │ Introduces more architectural complexity in the initial stage. Might deviate from the "pure" end-to-end sequence processing philosophy of the original Transformer.                     │ More robust local feature extraction, potentially multi-scale or hierarchical, due to CNN properties.       │ Can be more computationally intensive in the initial stages if multiple layers are used, though improved performance may justify the cost.  │
└──────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────┘

In summary, while the simple linear projection is fundamental to ViT's design, its stability and effectiveness are often contingent on large datasets and an appropriate patch size. Convolutional patch embedding strategies offer a compelling alternative, combining the inductive biases of CNNs with the global reasoning capabilities of Transformers. This enhances stability and performance—especially in scenarios with limited data or when fine-grained local feature extraction is critical [11,20]. The selection of an appropriate patch embedding strategy and patch size therefore involves a critical trade-off between computational efficiency, model stability, and the granularity of feature representation required for a given task.
### 3.2 Positional Encoding
The inherent permutation-invariance of the Transformer architecture necessitates the integration of positional encoding to imbue Vision Transformers (ViTs) with spatial awareness [20]. Without positional information, the self-attention mechanism would treat all image patches equivalently, failing to recognize their relative locations and thereby compromising the ability to capture spatial relationships crucial for visual tasks [20,27,38]. Positional encodings are therefore indispensable for retaining spatial information and preserving the spatial order of image patches, allowing the Transformer encoder to understand the configuration of visual elements within an image [32,38]. Typically, these encodings are added directly to the patch embeddings before input to the self-attention module [36,37].

Various approaches have been explored for positional encoding, broadly categorized into fixed and learnable methods [20,30]. Fixed positional encodings, similar to those originally proposed for Transformers in Natural Language Processing (NLP), often employ sine and cosine functions of varying frequencies to generate unique positional vectors. For instance, one such calculation process is defined as:
$$
\begin{aligned}
PE(pos, 2i) &= \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right), \\\\
PE(pos, 2i+1) &= \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)
\end{aligned}
$$
where $pos$ denotes the position of an image block, $i$ is the index for the channel dimension, and $d$ is the embedding dimension [29]. This method ensures the positional encoding maintains the same dimension as the input embedding for additivity [29].

In contrast, learnable positional embeddings are acquired directly through training, allowing the model to adaptively determine the optimal positional representations [17,37]. The original Vision Transformer (ViT) predominantly utilizes standard, learnable 1D positional embeddings, which are added to the patch embeddings and the class token [10,15,17,23,26,27]. For semantic segmentation tasks, learnable positional embeddings $ \in \mathbb{R}^{N \times D} $ are also added to the patch embeddings, forming the input sequence 
$$
z_0 = x_0 + pos,
$$
which is critical for understanding spatial relationships [32].

Regarding the comparative performance of fixed and learnable positional embeddings, and different dimensionality choices, empirical evidence suggests varied impacts. While ViT initially explored 1D, 2D, and relative positional embeddings through ablation studies, the original findings indicated that more advanced 2D-aware positional embeddings did not yield significant performance improvements over simpler 1D learnable embeddings [10,27]. Similarly, experiments showed no significant difference between using 1D or 2D positional encoding in terms of overall performance for classification tasks, partly because the inherent arrangement of image patches already provides some positional cues [15,33]. This implies that for certain tasks, the precise formulation of positional encoding might be less critical than its mere presence.

However, the importance of positional encoding can be task-dependent. For instance, in object detection tasks, an ablation study showed that positional encoding had little impact on performance, unlike its critical role in NLP where word order is paramount [22]. This suggests that for object detection, the presence of objects within patches might be more salient than their precise spatial order in some contexts. Conversely, specific positional encoding variants have demonstrated substantial performance gains in other architectures. For example, Swin Transformer incorporates a relative position bias 
$$
B \in \mathbb{R}^{M^2 \times M^2}
$$ 
into the self-attention calculation, which has been shown to bring significant performance improvements [4]. Furthermore, some models, like CPVT (Conditional Position Encoding), leverage convolution to achieve fine-grained feature encoding conditioned on local neighborhoods, applicable to arbitrary input sizes [26]. Another novel approach is seen in the Orthogonal Transformer, which fuses positional information using a depth-wise separable convolution within the feed-forward network, eschewing explicit positional encoding layers altogether [7].

In summary, positional encoding is fundamental to ViTs for endowing them with the ability to process spatial information, countering the permutation-invariance of the core Transformer architecture. While learnable 1D embeddings are a prevalent choice in many ViT variants due to their simplicity and competitive performance for general image classification, the optimal design and impact of positional encoding can vary across different Vision Transformer architectures and specific computer vision tasks.
### 3.3 Transformer Encoder Structure and Attention Mechanisms
The core of Vision Transformers (ViTs) lies in their encoder architecture, which processes input image patches to extract rich, contextualized features. The standard ViT encoder is composed of $L$ identical layers, each designed with a sequential arrangement of a Multi-head Self-Attention (MSA) module and a Multi-Layer Perceptron (MLP) block [17,27,29,32]. A crucial architectural detail in ViT is the application of Layer Normalization (LN) before each block, followed by the addition of residual connections after each block to facilitate stable training and improve performance [17,29,33,38]. This pre-normalization differs from the original Transformer's post-normalization [33]. The output dimension of each layer is consistently maintained to enable these residual connections [29].

The feed-forward network, typically implemented as an MLP block, plays a pivotal role in feature transformation within each encoder layer. This MLP module generally comprises two linear layers with a non-linear GELU activation function between them, often accompanied by Dropout for regularization [10,17,33,38]. The first linear layer typically expands the dimensionality of the feature vector, while the second layer reduces it back to the original hidden size, often by a factor of 4 for expansion [10]. This architecture allows the network to transform the features obtained from the attention mechanism, enhancing their representation capacity. Beyond standard MLPs, advancements like the Dynamic Aggregation Feed Forward (DAFF) network have been proposed, which integrate depth-wise convolutions to imbue ViTs with spatial inductive bias and capture local information, thereby addressing a common limitation of pure Transformers [11].

At the heart of the Transformer encoder is the self-attention mechanism, which enables the model to capture global dependencies by establishing direct relationships between any two regions or tokens in the image [5,36,37]. This mechanism dynamically aggregates information through interactions between Query ($Q$), Key ($K$), and Value ($V$) vectors, which are projected from the input sequence using linear mapping matrices [29]. The similarity between $Q$ and $K$ determines the weight applied to $V$, as expressed by the scaled dot-product attention formula:
$$Attention(Q, K, V) = \text{softmax}\Bigl(\frac{QK^T}{\sqrt{d_k}}\Bigr)V,$$
where $d_k$ is the dimension of the key vectors [29,32]. This mechanism allows each element in a sequence to interact with every other element, effectively identifying salient features and long-range dependencies [37].

To enrich feature representation and capture diverse relationships from multiple perspectives, ViTs employ a Multi-Head Self-Attention (MSA) mechanism [29,35]. MSA operates by splitting the query, key, and value vectors into multiple "heads," performing self-attention computations in parallel for each head, and then concatenating their outputs before a final linear projection [29]. This parallel processing allows MSA to simultaneously attend to different parts of the input sequence, combining local information to derive overall context and enhancing the model's ability to capture complex semantic relationships [22,38]. For instance, the Head-Interacted Multi-Head Self-Attention (HI-MHSA) mechanism further refines this by introducing a "head token" to fuse weaker object representations from individual attention heads into a more robust representation, enabling cross-head communication [11].

While effective, the standard self-attention mechanism faces scalability challenges due to its quadratic computational complexity with respect to the input sequence length, as the number of tokens and hidden size remain constant across layers in the original Transformer architecture [2,20]. This quadratic growth in complexity, expressed as $O((hw)^2C)$ for global MSA where $h \times w$ is the number of patches and $C$ is the channel dimension, motivates the development of more efficient attention variants [4].

Various efficient attention mechanisms have been proposed to mitigate this computational burden while preserving performance:
*   **Windowed and Shifted Window Attention**: As exemplified by the Swin Transformer, these methods localize attention computation to non-overlapping windows (W-MSA) and then introduce shifted windows (SW-MSA) to enable cross-window connections [4,23]. This approach reduces complexity to $O(M^2hwC)$, where $M$ is the window size, providing a linear complexity with respect to image size for fixed window sizes [4,23].
*   **Orthogonal Attention**: The Orthogonal Transformer Block (OTB) utilizes an orthogonal self-attention mechanism that transforms visual token features into a low-resolution orthogonal space for computation. This allows each orthogonal token to perceive all visual tokens, effectively modeling both local feature correlations and global dependencies [7].
*   **k-NN Attention**: KVT introduces k-NN attention, which leverages image patch locality by computing attention only among the top $k$ most similar tokens, effectively ignoring noisy ones [26].
*   **Attention Expansion**: Refiner explores attention expansion in high-dimensional space and applies convolution to enhance local patterns of attention maps [26].
*   **Channel-wise Attention**: XCiT performs self-attention computation across feature channels instead of tokens, enabling efficient processing of high-resolution images [26].
*   **Cross-head Communication**: DeepViT suggests building cross-head communication to regenerate attention maps, aiming to increase diversity across different layers [26].

The performance and computational cost of ViTs are significantly impacted by the number of layers ($L$) and the size of the hidden units (often related to the MLP dimension and attention head dimensions). While increasing the number of layers and hidden units generally enhances model capacity and performance, as seen in models with depths of 12 layers and MLP dimensions of 1024 [18], it directly contributes to the model's parameters and the aforementioned quadratic computational complexity. The constant token count and hidden size across layers in standard Transformer architectures contribute to this high computational demand [20]. Consequently, the development of efficient attention mechanisms is critical for scaling ViTs to larger datasets and higher-resolution images.
## 4. Vision Transformer Architectures
The emergence of Vision Transformers (ViTs) signifies a transformative shift in computer vision, moving beyond traditional Convolutional Neural Networks (CNNs) by adopting self-attention mechanisms to achieve global perceptual capabilities [13,35]. This paradigm shift, however, brought new challenges, notably high computational demands and a strong reliance on extensive training datasets due to the inherent lack of visual inductive biases present in CNNs [13,30]. Consequently, the field has witnessed a rapid evolution of ViT architectures, each designed to address specific limitations, enhance performance, or adapt to diverse visual tasks [8,26,29]. 

This section provides a taxonomy of these architectures, grouping them by their core innovations, design principles, and the trade-offs they entail in terms of performance, computational cost, and data requirements [8,26,29,30].

The foundational **Vanilla Vision Transformer (ViT)** established the principle of treating images as sequences of patches and processing them with a standard Transformer encoder [26,28,30]. Its core design involves dividing an image into fixed-size patches, linearly embedding them, adding positional encodings to retain spatial information, and processing the resulting sequence through a Transformer encoder, with a dedicated class token for classification [20,29,33,40]. While demonstrating impressive capabilities in capturing global relationships and achieving state-of-the-art performance on large datasets [3,36], its global self-attention mechanism leads to quadratic computational complexity with respect to the number of patches, demanding substantial computational resources and vast pre-training data, and limiting its applicability for dense prediction tasks requiring multi-scale features [30,37].

To mitigate the computational burden and enhance multi-scale feature representation, a significant class of **Hierarchical and Efficient Architectures** has emerged [30]. These models introduce mechanisms to build hierarchical feature maps and reduce computational complexity from quadratic to linear or quasi-linear. A prominent example is the Swin Transformer, which employs a shifted window self-attention mechanism to enable linear computational complexity while capturing cross-window information through cyclic shifts, thereby allowing for multi-scale feature extraction [3,4,5,26]. Other architectures like Pyramid Vision Transformer (PVT) achieve efficiency through spatial-reduction attention, and the Orthogonal Transformer and SuperViT offer unique designs for memory and computational efficiency, often featuring multi-stage, pyramidal structures akin to CNNs [1,7,30]. These models offer a superior balance between performance and computational feasibility for diverse vision tasks.

Another crucial direction involves **Hybrid CNN-Transformer Architectures**, which strategically fuse the strengths of CNNs and Transformers [15,23]. This approach capitalizes on CNNs' inherent inductive biases—such as locality and translation invariance—for efficient local feature extraction, and Transformers' global context modeling capabilities [6]. Common strategies include using CNNs as powerful front-ends to extract rich feature maps that are then tokenized for Transformer processing, or integrating convolutional operations directly within Transformer blocks to enhance local connectivity [11,26,27]. Models like TransUNet and HiFormer exemplify this synergy, particularly in tasks like medical image segmentation, by combining U-Net-like structures with Transformer encoders to capture both local details and global dependencies [23,36]. Such hybrid designs often achieve superior performance, especially on smaller datasets, by effectively mitigating the limitations of pure CNNs (lack of global context) and pure Transformers (high data requirements, lack of inductive biases) [8].

Beyond these major categories, **Other Notable ViT Variants** explore specialized architectural modifications and training strategies. The Data-efficient Image Transformer (DeiT) addresses the data hungry nature of ViTs by introducing a distillation token and a teacher-student training framework, enabling competitive performance with less training data by transferring CNN's inductive biases [26,30]. For memory efficiency, Reversible Vision Transformers (Rev-ViT) implement reversible layers, drastically reducing memory consumption during training without sacrificing model complexity or accuracy, making them suitable for deeper models and higher resolutions [12]. Furthermore, a novel class of MLP-based vision models has emerged, exploring architectures that primarily rely on Multi-Layer Perceptrons (MLPs) or fully-connected layers, questioning the necessity of convolutions and self-attention in certain contexts [13,30]. Other specialized variants include ConViT, which incorporates convolutional inductive biases for smaller datasets, and Language-Unlocked Vision Transformers (LUViT), designed to bridge the modality gap between ViTs and Large Language Models (LLMs) [9,37]. Task-specific adaptations, such as Segmenter for semantic segmentation, further demonstrate the versatility of the Transformer architecture in diverse computer vision applications [32].
### 4.1 Vanilla Vision Transformer (ViT)
The Vision Transformer (ViT), proposed by Dosovitskiy et al., marked a significant departure from traditional convolutional neural networks (CNNs) in computer vision by directly applying a standard Transformer architecture to image classification tasks [26,30]. The core innovation of ViT lies in treating images not as 2D grids but as sequences of flattened patches, akin to how Transformers process sequences of words in natural language processing [26,28,37].

The architecture of the original ViT is systematically structured around several key components [20,29,33]:

1.  **Image Patching**: The input image is first divided into a grid of non-overlapping, fixed-size patches, typically 16×16 pixels [5,40]. For an image of size $H \times W \times C$, and a patch size $P \times P$, this results in 
    $$
    \frac{H \cdot W}{P^2}
    $$
    patches [29].

2.  **Linear Embedding**: Each patch is then flattened into a 1D vector and linearly projected into a higher-dimensional embedding space. This process transforms the raw pixel data of each patch into a uniform dimension suitable for the Transformer encoder [15,17,38].

3.  **Positional Encodings**: To retain spatial information, which is lost during the flattening of patches, 1D learnable positional encodings are added to the patch embeddings [15,29,30]. These encodings allow the model to understand the relative or absolute position of each patch within the original image.

4.  **Class Token**: A learnable embedding, often referred to as a "class token" (similar to BERT's  token), is prepended to the sequence of embedded patches [15,27,30]. This token's state at the output of the Transformer encoder serves as the aggregate image representation for classification [26]. For tasks like object detection, this class token may be removed or adapted [22].

5.  **Transformer Encoder**: The sequence, now composed of the class token and embedded patches with positional encodings, is fed into a standard Transformer encoder [3,27,36]. This encoder consists of multiple stacked layers, each comprising a Multi-Head Self-Attention (MSA) block and a Multi-Layer Perceptron (MLP) block, with layer normalization applied before each block and residual connections around them [29]. For instance, common configurations might involve 12 layers, 8 heads in MSA, and an MLP dimension four times the token size [18,32].

6.  **Classification Head**: Finally, the output state of the class token from the Transformer encoder's last layer is passed through a classification head, typically an MLP, to predict the image class [17,26,27,29]. The structure of this MLP head can vary—for example, a Linear + tanh + Linear configuration for pre-training and a single Linear layer for fine-tuning [33].

ViT's effectiveness has been demonstrated by achieving results comparable to or even surpassing popular CNN methods on various image recognition benchmarks, particularly when pre-trained on sufficiently large datasets such as ImageNet-21K or JFT-300M [3,30,36]. For example, the original ViT achieved an accuracy of 88.55% on ImageNet-1K with extensive pre-training [3,36].

Despite its impressive performance, the original ViT architecture also presents certain limitations. One significant aspect is its high computational cost, particularly during pre-training, which necessitates access to massive datasets and substantial computing resources to achieve competitive results [30]. This is largely due to the global self-attention mechanism, which scales quadratically with the sequence length (number of patches). Another limitation stems from its inherent design: ViT lacks the inductive biases (such as locality and translation equivariance) that are naturally embedded in CNNs [37]. Consequently, ViT models often require significantly larger datasets than CNNs to learn these fundamental image properties, leading to suboptimal performance when trained on smaller datasets [11]. This absence of inherent hierarchical feature representation—where local features are progressively combined to form more complex global features—distinguishes ViT from CNNs and has driven subsequent research into more efficient and hierarchical Vision Transformer variants.
### 4.2 Hierarchical and Efficient Architectures
The initial Vision Transformer (ViT) architecture, while effective, encountered limitations such as quadratic computational complexity with respect to image resolution and a fixed-resolution feature representation, which is suboptimal for dense prediction tasks requiring multi-scale context. To address these challenges, a significant evolution has led to the development of hierarchical and efficient Vision Transformer architectures, which aim to capture multi-scale features and reduce computational demands, thereby achieving superior performance across various vision tasks [30].

A prime example of a hierarchical architecture is the Swin Transformer, which constructs hierarchical representations by initiating with small patches and progressively merging adjacent patches in deeper Transformer layers [4,30]. A core innovation in the Swin Transformer is its shifted window self-attention mechanism [3,4,26]. Unlike traditional self-attention that computes relationships globally, shifted window attention divides the image into non-overlapping local windows and performs self-attention calculations within these windows. To facilitate information exchange across windows and enable the capture of multi-scale features, the window configuration is shifted in subsequent layers [3,5,36]. This design is highly hardware-friendly, leading to faster actual running speeds, while simultaneously reducing computational complexity [3]. The ability of Swin Transformer to model at different scales and integrate features across varying resolutions through its sliding window approach is crucial for its effectiveness [30]. The architecture has demonstrated state-of-the-art performance on various vision tasks, evidenced by its achievement of 53.5% mIoU on the ADE20K validation set for semantic segmentation and a box mAP of 58.7 and mask mAP of 51.1 on the COCO dataset for object detection, surpassing previous methods [3,36]. Furthermore, the adaptability of Swin Transformer is showcased in applications like SwinIR for image restoration and its integration into architectures like HiFormer, which utilizes a pyramid of Swin Transformer modules with varying window sizes to learn multi-scale interactions for accurate medical image segmentation [14,23]. Efforts have also been made to further reduce the computational complexity of Swin Transformer through pruning methods while maintaining its performance [16].

Beyond Swin Transformer, other hierarchical models have emerged to address similar challenges. Tokens-to-Token ViT (T2T-ViT) employs layer-wise T2T transformations to aggregate neighboring tokens, thus achieving both hierarchy and locality [30]. Pyramid Vision Transformer (PVT) utilizes non-overlapping patch partitioning to reduce sequence length and incorporates spatial-reduction attention (SRA) to lower computational complexity [30]. Similarly, the Orthogonal Transformer features a hierarchical design with four stages to extract image features at different spatial resolutions [7], and RegionViT generates region and local tokens for global information exchange [26]. These models often adopt a pyramidal structure, akin to ResNet, where the number of tokens decreases in higher layers to reduce computational cost [20].

Another notable development in efficient Vision Transformers is SuperViT, which distinguishes itself through a novel training paradigm designed to enhance image recognition while simultaneously reducing computational demands [1]. SuperViT achieves this versatility by supporting multiple patch sizes and token keeping rates, which are critical factors contributing to its hardware efficiency and allowing for improved image recognition performance across various computational budgets [1]. Such flexibility enables adaptive resource utilization based on task requirements. Moreover, the pursuit of efficiency extends to considerations like reversibility in Vision Transformers, which contributes to memory usage reduction, particularly beneficial for hierarchical architectures like Multiscale Vision Transformers [12]. The category of "efficient-vision-transformer" highlights the ongoing research thrust in this area [24]. These hierarchical and efficient architectures collectively represent a significant stride in adapting Transformers for diverse and complex computer vision applications, balancing performance with computational feasibility.
### 4.3 Hybrid CNN-Transformer Architectures
Hybrid architectures, which strategically combine Convolutional Neural Networks (CNNs) and Transformers, represent a significant advancement in computer vision, designed to leverage the complementary strengths of both paradigms. This fusion typically exploits CNNs for their proficiency in extracting local, low-level features and capturing inductive biases such as translation invariance, while employing Transformers for their robust capability in modeling global context and long-range dependencies [15,23]. This synergistic combination allows for better performance by integrating CNNs' local connections with Transformers' global connections [6].

Various integration strategies have been developed to achieve this synergy. One prevalent approach involves utilizing CNNs as an initial feature extractor, serving as a powerful front-end to process raw pixel data into richer feature maps. These feature maps are then transformed into a sequence of tokens suitable for Transformer processing. For instance, in such hybrid models, the patch embedding projection $E$ (Eq. 1 in original ViT) is applied to patches extracted directly from the CNN feature map, rather than raw image patches [17,27]. A specific instance involves a patch spatial size of $1\times1$, where the input sequence is generated by flattening the feature map's spatial dimensions and projecting it into the Transformer's dimension [27]. An example of this is the use of ResNet50 as a feature extractor, with its final stage outputting a $14\times14$ feature map that is subsequently flattened into a 196-dimensional vector for Transformer input [10]. This method also reduces the number of tokens to be processed by the Transformer, leading to significant computational cost reductions [38].

Beyond simple sequential processing, convolutions are also integrated *within* the Transformer architecture itself. Some hybrid models incorporate convolutional operations directly into components like the patch embedding (e.g., Spatial-aware Patch Embedding, SOPE) and the feed-forward network (e.g., Depth-wise Attention Feed-forward, DAFF) to augment the Transformer's capabilities [11]. Furthermore, many works explore augmenting traditional Transformer blocks or self-attention layers with convolutions to address potential drawbacks arising from directly borrowing Transformer architectures from Natural Language Processing (NLP) [26]. Models like CPVT, CvT, CeiT, LocalViT, and CMT combine the feed-forward network (FFN) within each Transformer block with convolutional layers to facilitate correlation between neighboring tokens [26]. The Convolution and Attention Network (CoAtNet) is another example that strategically combines depth-wise convolutions within attention blocks and employs shallow, vertically overlapping convolutions in early layers [30]. Research also suggests that optimal performance can be achieved by integrating convolutional layers in the shallow stages and attention layers in the deeper stages of architectures like ResNets [37]. Conversely, some models, such as BoTNet, explore replacing spatial convolutions with self-attention mechanisms [5].

In terms of architectural design for specific tasks, hybrid encoder-decoder structures have emerged, particularly in medical image segmentation. Models like TransUNet and Swin-Unet combine the U-Net architecture's strengths in capturing multi-scale context with the global modeling capabilities of Transformers [36]. TransUNet, for instance, has demonstrated success in heart and multi-organ segmentation tasks [36]. The HiFormer architecture also exemplifies this, using a CNN for initial feature extraction followed by a Swin Transformer for global context modeling, while addressing issues of effectively combining low-level and high-level features and multi-scale information [23].

The performance comparison between hybrid architectures and pure CNN or pure Transformer models often favors the hybrid approach. Surveys indicate that hybrid CNN-Transformer architectures are a promising direction, frequently outperforming their pure counterparts [8]. Models like ConvNeXt, which retains both local details and global context, have shown applicability in industrial defect detection, while BoTNet is utilized for real-time perception in autonomous driving [5]. CoAtNet has notably achieved state-of-the-art performance across multiple datasets, underscoring the efficacy of such combined designs [30]. In contrast, models like Segmenter stand out as pure Transformer architectures, explicitly opting against the hybrid CNN-Transformer approach [32]. This diverse landscape highlights that hybrid designs effectively mitigate the individual limitations of CNNs (lack of global context) and pure Transformers (high computational cost, lack of inductive biases), leading to superior performance in various vision tasks.
### 4.4 Other Notable ViT Variants
The landscape of Vision Transformers has diversified significantly beyond the original ViT, with numerous variants emerging to address its limitations, enhance performance, or adapt it to specific applications. Among these, the Data-efficient Image Transformer (DeiT) stands out for its improvements in training efficiency and data requirements [3,25,30].

DeiT, proposed by Touvron et al., aims to reduce the reliance of ViTs on extensive pre-training datasets, enabling competitive performance with less training data, specifically on datasets like ImageNet-1k [26,30]. A key innovation in DeiT is the introduction of a *distillation token* and a teacher-student training strategy [30]. This approach allows a Vision Transformer student model to learn from a CNN teacher, effectively transferring inductive biases characteristic of CNNs—such as locality and translation invariance—into the Transformer architecture [26,30]. For instance, DeiT-B, which shares the same architecture as ViT-B (86 million parameters), can achieve an 83.1% top-1 accuracy on ImageNet without external data, primarily due to robust data augmentation strategies [26]. Furthermore, by employing token-based distillation from CNN teachers, DeiT-B can boost its top-1 accuracy to 84.40% [26]. This renders DeiT a robust benchmark for evaluating frameworks designed to improve ViT efficiency, such as WDPruning [16]. Its architecture has also been integrated into hybrid methods with dynamic quantization to accelerate inference and reduce model size [2], and it serves as a foundational backbone for downstream tasks like semantic segmentation, as seen in Segmenter [32].

Beyond efficiency, other ViT variants focus on optimizing architectural features for specific performance trade-offs, particularly concerning memory footprint and computational cost [30]. The *Orthogonal Transformer* differentiates itself through an *orthogonal self-attention mechanism* [7]. While the specific implications for memory and computation require detailed analysis, orthogonal transformations can, in certain contexts, lead to more stable training and efficient computations by preserving norms. In contrast, the *Reversible Vision Transformer* (Rev-ViT) is designed explicitly for memory-efficient scaling [12]. By implementing reversible layers, Rev-ViT can significantly reduce the memory footprint during training, achieving a reduction of up to 15.5x while maintaining comparable model complexity, parameter count, and accuracy [12]. This innovation is crucial for training larger models or processing higher-resolution images, where memory consumption traditionally poses a significant barrier.

Furthermore, the exploration of alternatives to the self-attention mechanism has led to the emergence of *MLP-based vision models* [13,30]. These models, such as the "mlp-series" [24], offer a distinct paradigm by relying primarily on Multi-Layer Perceptrons (MLPs) or fully-connected layers, rather than convolutions or self-attention mechanisms, for feature extraction and transformation. This design choice presents unique opportunities, such as potentially simpler architectures and reduced computational complexity in certain scenarios, while also facing challenges in capturing long-range dependencies and handling spatial hierarchies effectively without explicit inductive biases. While the ViT architecture is a prominent branch in computer vision, other variants like ConViT, which introduces Positional Self Attention (PSA) to incorporate convolutional inductive biases for better performance on smaller datasets [37], or LUViT, which leverages Low-Rank Adaptation (LoRA) within LLM blocks for enhanced visual understanding, continue to push the boundaries of visual representation learning [9]. Similarly, task-specific variants like Segmenter and SegFormer optimize ViT for semantic segmentation, demonstrating the versatility of the transformer architecture in computer vision [36].
## 5. Training Strategies and Optimization

Effective training strategies and meticulous optimization are paramount for unlocking the full potential of Vision Transformers (ViTs). Unlike Convolutional Neural Networks (CNNs), ViTs inherently lack strong inductive biases—such as translational equivariance—necessitating robust training regimes, especially the reliance on extensive pre-training datasets, to achieve competitive performance [15,26,27,37].

The foundation of ViT performance lies in **pre-training**, predominantly on large-scale, diverse datasets. Early research established that ViTs pre-trained on datasets such as ImageNet-21k (14 million images) and JFT-300M (303 million images) achieve state-of-the-art results upon fine-tuning [27,40]. Conversely, without this large-scale pre-training, ViTs can underperform CNNs of comparable size on medium-sized datasets like ImageNet-1k, underscoring the critical role of data volume in compensating for their architectural characteristics [15,26,40]. This paradigm leverages transfer learning, allowing ViTs to acquire generalizable visual knowledge from massive datasets before adapting to specific tasks with limited data [33,35].

Following pre-training, the **fine-tuning** process adapts the pre-trained ViT to a specific downstream task. This typically involves replacing the pre-trained prediction head with a new, randomly initialized feedforward layer whose output dimension matches the number of classes in the target task [17,27,38]. A common practice during fine-tuning is to train at a higher image resolution than pre-training, which necessitates the interpolation of the pre-trained positional embeddings to accommodate the increased effective sequence length [27,32].

Beyond standard supervised pre-training, various **pre-training objectives** have been explored to enhance ViT learning and efficiency. These include knowledge distillation, where a CNN acts as a teacher to transfer knowledge to the ViT [20], and a growing array of self-supervised learning (SSL) techniques. Masked Image Modeling (MIM), inspired by BERT’s masked language modeling, involves predicting masked image patches, though its initial performance in vision sometimes lags supervised training [10]. In contrast, contrastive learning, which focuses on distinguishing augmented positive and negative pairs, has shown significant success in ViTs, exemplified by models like MoCo v3 and DINO [10]. Domain-specific SSL objectives, such as rotation-aware pre-training and multi-scale token prediction, have also been developed for specialized applications like remote sensing to handle unique data characteristics [21]. In multimodal contexts, synergistic strategies like Masked Auto-Encoding (MAE) for ViT backbones combined with LoRA layers in LLM blocks facilitate alignment between visual and linguistic feature spaces [9].

Effective **optimization algorithms** and meticulous **hyperparameter tuning** are critical for ViT training stability and performance. The Adam optimizer is commonly used during pre-training, often with specific parameters such as $\beta_1 = 0.9$ and $\beta_2 = 0.999$, a substantial batch size (e.g., 4096), and a weight decay of 0.1 [17,27]. A linear learning rate warmup and decay schedule is frequently employed, along with gradient clipping (e.g., at a global norm of 1) [17,27]. For fine-tuning, momentum SGD with a smaller batch size (e.g., 512) is often preferred [23,27]. Other optimizers like AdamW and LayerScale also contribute to the ViT optimization landscape [30]. The precise adjustment of hyperparameters, including learning rates, batch sizes, and optimizer configurations, directly impacts convergence speed and final model accuracy [28,40]. Challenges associated with large computational resources in ViT training are being addressed by advanced methods, such as Concurrent Adversarial Learning (ConAdv), which optimizes large-batch training by decoupling sequential gradient computations while maintaining high accuracy [12].

**Regularization methods** are essential to prevent overfitting and enhance the generalization capabilities of ViTs. Dropout is a widely adopted technique, with typical rates of 0.1 applied to both general and embedding layers [18,32]. To address issues like over-smoothing and encourage feature diversity, increased dropout rates or the inclusion of similarity penalty loss terms may be beneficial [20]. Alternatively, weight constraints, such as clipping weights within a defined range, can serve as a regularization mechanism without the potential interference of traditional L1/L2 regularization losses [22].

**Data augmentation** plays a crucial role in improving data efficiency and preventing overfitting by synthetically expanding the training dataset, thereby enhancing ViT generalization [20,28]. While early ViT models might have relied on basic random cropping [32], modern training procedures incorporate sophisticated techniques such as Mixup and RandAugment during pre-training to boost performance [30,32]. General geometric transformations like random resizing, cropping, horizontal flipping, and rotations are also widely applied to diversify the training data [18,23]. The evolution towards stronger augmentation strategies, as seen in models like DeiT, has been instrumental in mitigating the typical dependency of ViTs on extremely large datasets, allowing them to achieve high accuracy even on moderately sized datasets like ImageNet-1k [26,30].

In summary, the success of ViTs heavily relies on a well-orchestrated combination of large-scale pre-training, adaptive fine-tuning, diverse pre-training objectives, robust optimization techniques, and comprehensive regularization and data augmentation strategies. Addressing the computational demands and data hunger of ViTs remains an ongoing research challenge, with continued exploration of methods like bootstrapping for training on smaller datasets [34] and knowledge distillation [30] to further enhance their efficiency and broader applicability.
### 5.1 Pre-training and Fine-tuning
Pre-training on extensive and diverse datasets is a critical determinant of Vision Transformer (ViT) performance, enabling these models to overcome their inherent lack of strong inductive biases found in Convolutional Neural Networks (CNNs) [26,27,37]. Early studies, notably by Dosovitskiy et al., demonstrated that ViTs pre-trained on large-scale datasets such as ImageNet-21k (14 million images, 21 thousand classes) and JFT-300M (303 million high-resolution images, 18 thousand classes) achieve state-of-the-art or near state-of-the-art results across various image recognition benchmarks upon fine-tuning [10,27,40]. Conversely, when pre-trained only on medium-sized datasets like ImageNet-1k, ViT's accuracy can be a few percentage points lower than similarly sized ResNets, underscoring the necessity of large-scale pre-training for optimal performance [10,26].

Beyond standard supervised pre-training on large datasets, various pre-training objectives have been explored to enhance ViT's learning and efficiency. To mitigate the substantial data requirements, some approaches incorporate distillation, where a CNN acts as a teacher network, transferring knowledge to the ViT through a distillation loss [20]. Other innovative objectives include self-supervised learning techniques. For instance, the RoMA framework utilizes self-supervised pre-training on large-scale unlabeled remote sensing data, incorporating rotation-aware mechanisms and multi-scale token prediction to learn robust representations [21]. In multimodal contexts, objectives like Masked Autoencoders (MAE) for ViT and LoRA layer training within Large Language Models (LLM) blocks aim to co-adapt ViT and LLMs, enabling the ViT to produce features aligned with LLM understanding [9].

The benefits of transfer learning are paramount in improving ViT performance, particularly on smaller downstream datasets [28,35]. Pre-training on massive datasets allows ViTs to learn generalizable visual knowledge, which can then be efficiently transferred to specific tasks with limited data [33,35]. The fine-tuning process typically involves removing the pre-trained prediction head and attaching a new zero-initialized $D \times K$ feedforward layer, where $K$ represents the number of classes for the downstream task [17,27]. Often, fine-tuning is performed at higher resolutions than pre-training, necessitating the interpolation of pre-trained positional embeddings to accommodate the increased effective sequence length [27,32,38]. This strategy allows a ViT model pre-trained on ImageNet, for example, to quickly adapt and achieve strong performance on novel image classification tasks [28]. This transfer learning paradigm is widely adopted, as seen in models like Segmenter, which are initialized with ViT or DeiT models pre-trained on ImageNet-21k or ImageNet-1k, respectively, for semantic segmentation tasks [32]. Even in hybrid architectures, pre-trained weights from ImageNet are commonly used to initialize components like CNNs and Swin Transformers, demonstrating the pervasive utility of transfer learning in ViT-based models [23]. While large-scale pre-training remains a cornerstone, research exploring methods like bootstrapping aims to allow ViTs to be trained effectively on smaller datasets without extensive prior pre-training, addressing a key challenge [34].
### 5.2 Optimization Algorithms and Regularization
Optimizing Vision Transformers (ViTs) requires careful consideration of optimization algorithms, hyperparameter tuning, and regularization techniques to ensure robust performance and generalization. The choice and configuration of optimization algorithms significantly influence model convergence and final accuracy. For instance, the original ViT models, including ResNets used for comparison, were primarily trained using the Adam optimizer with specific parameters: 
$$\beta_1 = 0.9,\quad \beta_2 = 0.999,$$ 
a substantial batch size of 4096, and a high weight decay of 0.1 [17,27]. Training also employed a linear learning rate warmup and decay strategy [17,27], with gradient clipping applied at a global norm of 1 for a resolution of 224 [17]. In contrast, fine-tuning often leverages momentum SGD with a smaller batch size of 512 [27], while some medical image segmentation applications utilize SGD with a momentum of 0.9 and a lower weight decay of 0.0001 [23]. Beyond Adam and SGD, other optimization algorithms such as AdamW and LayerScale are also part of the ViT optimization landscape [30]. Furthermore, strategies like Concurrent Adversarial Learning (ConAdv) demonstrate how optimization can be adapted to increase batch size while maintaining high accuracy, exemplified by its use in ResNet-50 training on ImageNet [12].

The importance of hyperparameter tuning for ViTs cannot be overstated, as it is a critical step for improving model performance [28,40]. Adjusting key hyperparameters, including learning rate, batch size, and optimizer configurations, directly impacts the model's convergence speed and overall accuracy [28].

Regularization techniques are crucial for preventing overfitting and enhancing the generalization capabilities of ViTs [20]. Dropout is a commonly employed regularization method in ViT models, with typical rates of 0.1 for both general and embedding dropout [18,32]. To further improve feature diversity and mitigate over-smoothing, particularly in deeper layers, increasing dropout rates is suggested, along with the incorporation of similarity penalty loss terms [20]. An alternative to traditional L1 or L2 regularization involves using weight constraints, which can be implemented by clipping weights within a specified range (e.g., using tf.clip_by_value), thereby avoiding potential interference with model training that regularization losses might introduce [22].

Data augmentation plays a vital role in increasing the diversity of training data and preventing overfitting, thereby enhancing ViT training [28]. While original ViT models primarily relied on simple random cropping for data augmentation [32], improved training procedures integrate more sophisticated techniques. These include combining dropout for regularization with advanced data augmentation methods such as Mixup and RandAugment [32]. General data augmentation strategies like rotation, cropping, and flipping are also widely applied to diversify the dataset effectively [28]. The evolution from basic cropping to a combination of diverse augmentation strategies underscores the growing understanding of their impact on ViT performance and robustness.
### 5.3 Data Efficiency Techniques
The inherent reliance of Vision Transformers (ViTs) on extensive datasets for optimal performance necessitates the implementation of robust data efficiency techniques. These strategies primarily encompass advanced data augmentation methods and self-supervised learning paradigms, alongside other architectural and training enhancements, to improve model generalization and enable effective training on smaller or unlabeled datasets [29].

Data augmentation is a fundamental approach to enhance the generalization ability of ViTs by synthetically expanding the training dataset. Traditional techniques involve geometric transformations such as random resizing, random cropping, random horizontal flipping [18] and rotations [23]. More sophisticated augmentation strategies, including Mixup and RandAugment, have been effectively utilized to improve data efficiency during ViT pre-training [32]. Notably, models like DeiT have demonstrated high accuracy on datasets such as ImageNet-1k by leveraging strong data augmentation and regularization strategies, thereby mitigating the typical dependency of ViTs on extremely large datasets [26,30].

Self-supervised learning (SSL) methods offer a powerful avenue for pre-training ViTs on vast amounts of unlabeled data, thereby learning rich visual representations without manual annotation [35]. One prominent SSL paradigm is masked image modeling, which draws inspiration from BERT's masked language modeling. This approach involves predicting masked patches within an image. However, initial explorations into masked patch prediction for ViTs have sometimes shown performance inferior to that of supervised training, indicating challenges in effectively transferring this paradigm from natural language processing to vision [10]. In contrast, contrastive learning has emerged as a highly effective SSL method for ViTs, where models learn to distinguish between positive and negative pairs of augmented images. Examples such as ViT MoCo v3 and DINO illustrate the success of contrastive learning in generating strong visual representations [10]. Furthermore, specific self-supervised pretraining mechanisms, such as rotation-aware pretraining and multi-scale token prediction objectives, have been developed to enhance data efficiency, particularly in specialized domains like remote sensing, by enabling models to learn effectively from sparse and variably-scaled objects in unlabeled data [21].

Comparing these approaches, data augmentation directly manipulates the input data space, while self-supervised learning focuses on learning robust representations from the inherent structure of unlabeled data. While both aim to improve data efficiency, their mechanisms differ. For instance, while certain masked image modeling approaches might face performance hurdles compared to supervised training, strong data augmentation has proven highly effective in bridging the performance gap for ViTs on moderately sized datasets. Beyond these, other strategies contribute to data efficiency. Architectural modifications, such as those employing SOPE, DAFF, and HI-MHSA, have been explored as a direct means to improve data efficiency, sometimes serving as alternatives or complements to data augmentation and self-supervised learning techniques [11]. Additionally, methods like bootstrapping have been proposed to enable ViTs to train effectively on smaller datasets [34], and certain architectural designs, such as ConViT with PSA, incorporate strong inductive biases that benefit performance on smaller datasets [37]. Knowledge distillation is also a recognized technique for achieving data-efficient training in ViTs [30].
## 6. Efficient Vision Transformers and Model Compression
Vision Transformers (ViTs) have demonstrated remarkable performance across various computer vision tasks; however, their widespread deployment is often hindered by their substantial computational cost and memory footprint [6,26]. A primary contributor to this challenge is the quadratic complexity of the self-attention mechanism with respect to the input sequence length, posing significant limitations—especially for high-resolution images or long sequences [26,29]. To address these constraints and facilitate the application of ViTs on resource-constrained devices [8,19], a broad spectrum of research has emerged, focusing on techniques for reducing computational burden and model size [14,25,35].



This section provides a comprehensive overview of strategies aimed at enhancing the efficiency of ViTs, categorizing them into four principal approaches: efficient attention mechanisms, model compression and pruning, quantization and low-precision training, and knowledge distillation. Each approach aims to reduce the computational and memory demands while striving to maintain, or minimally impact, model accuracy.

Efficient Attention Mechanisms directly tackle the $O(N^2)$ complexity of standard self-attention by proposing alternative formulations [29]. These include sparse attention methods, which limit interactions to a subset of tokens (e.g., window-based attention in Swin Transformer [4,23,36] and spatial reduction in PVT [36]), and linear attention methods, which approximate the attention mechanism to achieve $O(N)$ complexity [26]. Furthermore, low-rank approximation techniques reduce the dimensionality of the attention matrix, thereby lowering computational requirements [2,30]. While these methods significantly reduce FLOPs, they often face the trade-off of potentially sacrificing the model's global receptive field or the fidelity of attention approximation, which can impact performance on tasks requiring broad contextual understanding [7,20].

Model Compression and Pruning techniques aim to reduce the parameter count and computational operations of existing ViTs. This encompasses architectural modifications and network slimming [20,26], alongside more sophisticated structural pruning strategies. These structural methods involve adaptively adjusting the "width" (e.g., dimensions of linear projections [26]) and "depth" (e.g., removing less critical layers [16]) of the network, as demonstrated by the Width & Depth Pruning (WDPruning) framework [16]. The core challenge here lies in identifying and removing redundant components without causing significant degradation in model accuracy [2].

Quantization and Low-Precision Training reduce the memory footprint and accelerate inference by lowering the numerical precision of model parameters and activations, typically from 32-bit floating-point to 8-bit integers or even lower. This can be achieved through Post-Training Quantization (PTQ), applied to a pre-trained model, or Quantization-Aware Training (QAT), which integrates the quantization process into the training loop to mitigate accuracy loss. While lower bit-widths yield substantial savings, they present increased challenges in preserving performance, often necessitating QAT to achieve an optimal balance between efficiency and accuracy.

Finally, Knowledge Distillation (KD) offers an indirect yet powerful means of obtaining efficient ViTs by transferring learned representations from a larger, more accurate "teacher" model to a smaller, computationally lighter "student" model [2,6]. This process often involves guiding the student using the teacher's "soft labels" (e.g., via KL divergence) or employing specific mechanisms such as a dedicated "distillation token" as seen in DeiT [30]. The efficacy of KD relies heavily on the richness of the teacher's knowledge, which directly influences the student's ability to achieve high performance with a reduced model size.

In summary, the development of efficient ViTs involves navigating intricate trade-offs between computational cost, memory footprint, and model accuracy [11]. Each class of techniques—from algorithmic modifications in attention mechanisms to structural pruning, numerical precision reduction, and knowledge transfer—contributes uniquely to addressing the deployment challenges of ViTs on resource-constrained devices. The optimal approach often involves a combination of these strategies, highlighting a continuous research frontier aimed at achieving superior efficiency without compromising the high performance for which ViTs are renowned. Future work will continue to focus on discovering more sophisticated and adaptive methods, as well as developing effective combinations, to further bridge the gap between powerful ViT models and their practical, real-world applicability.
### 6.1 Efficient Attention Mechanisms
The original self‐attention mechanism, a cornerstone of Transformer models, exhibits quadratic time and space complexity with respect to the input sequence length, denoted as $O(N^2)$ [26,29]. This quadratic scaling presents a significant bottleneck, particularly for high-resolution images or long sequences, limiting the applicability of Vision Transformers (ViTs) in scenarios requiring extensive input processing [5,26]. Consequently, a substantial body of research has focused on developing efficient attention mechanisms to reduce this computational burden, often aiming for linear complexity, i.e., $O(N)$ [5,26]. These efforts primarily concentrate on optimizing input processing and the attention design itself [6,29,35].

Various approaches have been explored to achieve computational efficiency, which can broadly be categorized into sparse attention, linear attention, and low-rank approximation methods [30].

**Sparse Attention Mechanisms**: This category aims to reduce complexity by limiting the number of interacting token pairs. Instead of calculating attention between all token pairs, sparse attention restricts the attention span to a subset of tokens. For instance, Pyramid Vision Transformer (PVT) employs sparse attention mechanisms to reduce complexity from $O(N^2)$ to $O(N)$ [5]. A prominent strategy within sparse attention is window‐based attention, which partitions the input feature map into non‐overlapping local windows, applying self‐attention only within these windows [20]. The Swin Transformer introduces Window Multi‐Head Self‐Attention (W-MSA) and Shifted Window Multi‐Head Self‐Attention (SW-MSA) to enable cross-window connections while maintaining computational efficiency [4,36]. The computational complexity of global Multi‐Head Self‐Attention (MSA) is given as 
$$4hwC^2 + 2(hw)^2C,$$ 
where $h$ and $w$ are the height and width, and $C$ is the number of channels. In contrast, window‐based MSA reduces this to 
$$4hwC^2 + 2M^2hwC,$$ 
where $M$ represents the number of patches within each window, significantly reducing the quadratic term's dependency on the global feature map size [4]. Implementations like HiFormer demonstrate that W-MSA modules can achieve linear complexity [23].

**Linear Attention Mechanisms**: These approaches transform the quadratic dot-product attention into a linear operation, enabling $O(N)$ complexity. Katharopoulos et al., for example, approximate self‐attention as a linear dot product of kernel feature maps, thereby revealing relationships between tokens through a mechanism akin to Recurrent Neural Networks (RNNs) [26]. Another perspective, adopted by Zaheer et al., models tokens as vertices in a graph, defining the inner product between two tokens as an edge, which can also lead to more efficient representations [26].

**Low-Rank Approximation Methods**: This technique addresses the computational burden by approximating the attention matrix, which is typically a dense matrix of size $N \times N$, with a lower-rank matrix. The core principle behind low-rank approximation is that the full attention matrix often contains redundant information or can be effectively represented by a smaller number of dominant components. By decomposing the attention matrix into two smaller matrices (e.g., via singular value decomposition (SVD) or related techniques) or by projecting query and key vectors into a lower-dimensional space before computation, the number of floating-point operations can be substantially reduced [2,30]. For instance, if the original attention matrix has rank $N$, approximating it with a rank-$k$ matrix where $k \ll N$ can reduce computations from $O(N^2)$ to $O(Nk)$. Specific techniques employing this include Nyström-based methods (e.g., Nyströmformer and SOFT) and other linearization techniques such as Linformer and Performer [2].

**Trade-offs between Accuracy and Computational Cost**:
The pursuit of computational efficiency inevitably involves trade-offs, primarily with model accuracy and performance, particularly on fine-grained tasks or those requiring broad contextual understanding [20].
* **Sparse and Window-based Attention**: While significantly reducing computational cost by restricting attention to local regions, these methods inherently limit the global receptive field of the model. This can potentially impair the model's ability to capture long-range dependencies, which are crucial for understanding global image context. To mitigate this, hierarchical designs (like in Swin Transformer) or shifted window mechanisms are employed to ensure information propagation across different windows, partially recovering global interactions. However, this often adds architectural complexity.
* **Linear and Low-Rank Approximations**: These techniques aim to preserve global information while reducing computation by approximating the full attention matrix. The trade-off here lies in the fidelity of the approximation. A more aggressive approximation (e.g., a very low rank $k$) will yield greater computational savings but might sacrifice accuracy by discarding subtle yet important interactions captured by the full attention matrix. The challenge is to find an approximation that is computationally efficient yet rich enough to maintain competitive performance.
* **Pooling Strategies**: Another method to reduce computational costs involves performing pooling on feature maps or tokens during the generation of Queries (Q), Keys (K), and Values (V) [20]. This effectively reduces the sequence length, leading to computational savings. However, pooling can result in the loss of fine-grained spatial or semantic information, which might impact performance on tasks requiring high spatial precision or detailed feature representation.

In summary, efficient attention mechanisms offer a spectrum of solutions to address the quadratic complexity problem inherent in self‐attention. Whether by localizing computation (sparse/window-based), applying mathematical approximations (linear/low-rank), or reducing input dimensionality (pooling), each approach strikes a balance between efficiency and the model's capacity to capture comprehensive global context and fine-grained details.
### 6.2 Model Compression and Pruning
Model compression and pruning techniques are essential for deploying Vision Transformers (ViTs) in resource-constrained environments, mitigating their typically large parameter counts and high computational costs. These methods aim to reduce model size and accelerate inference while preserving—or minimally impacting—accuracy.

Various pruning strategies have been developed to simplify ViT architectures. One approach involves architectural modifications, such as replacing traditional fully connected layers with convolutional layers, which can significantly reduce the total number of parameters in the network [20]. This structural change directly contributes to a smaller model footprint.

Beyond direct parameter reduction, methods often target the intrinsic complexities of the transformer architecture. For instance, techniques have been proposed to accelerate inference by optimizing patch computation. Tang et al. focus on reducing redundant computations by assessing the contribution of individual patches to the effective output features, thereby streamlining the inference process [26]. Similarly, network slimming methods, traditionally applied to Convolutional Neural Networks (CNNs), have been extended to ViTs. Zhu et al. adapted these methods to reduce the dimensions of linear projections within the Feed-Forward Networks (FFNs) and attention modules, leading to more compact model representations [26].

Another critical aspect of pruning involves adjusting the dimensional complexity of the model by modifying weight kernels between hidden layers [2]. This fine-grained control allows for the simplification of ViT architectures without completely removing entire components. More advanced structural pruning strategies consider both the "width" (number of channels/neurons) and "depth" (number of layers) dimensions of ViTs [16]. For width pruning, learnable parameters can be incorporated to determine which parts of the model can be pruned. For depth pruning, shallow classifiers can be introduced to identify and remove entire transformer blocks that contribute minimally to the overall performance, effectively creating a shallower network [16]. Such comprehensive strategies enable substantial model compression.

The overarching goal of all these pruning techniques is to achieve a significant reduction in the number of parameters and computational operations while maintaining model accuracy [2]. This inherently involves a trade-off: higher compression rates often lead to a greater potential for accuracy degradation. Therefore, research in this domain focuses on developing sophisticated pruning algorithms that can identify and remove redundant or less critical components without severely impacting the model's performance on downstream tasks. The optimal balance between compression rate and accuracy retention remains a central challenge, driving continuous innovation in ViT model compression.
### 6.3 Quantization and Low-Precision Training
Quantization serves as a critical strategy for enhancing the efficiency of Vision Transformers (ViTs) by reducing the precision of model parameters and intermediate activation maps to lower bit formats, typically from 32‐bit floating‐point to 8‐bit integers or even lower . This process directly translates into substantial reductions in both memory footprint and computational cost, making ViTs more amenable for deployment on resource‐constrained devices .

The fundamental principle of quantization involves mapping real‐valued inputs to a limited set of discrete values. This mapping can generally be expressed as:
$$
Q(r) = S \cdot \operatorname{round}\left(\frac{r}{S} + Z\right)
$$
where \( Q \) denotes the quantization function, \( r \) represents the real‐valued input, \( S \) is a scaling factor, and \( Z \) is an integer zero point .

Different methodologies are employed for the quantization of ViTs, primarily categorized into Post‐Training Quantization (PTQ) and Quantization‐Aware Training (QAT) . PTQ involves quantizing a pre‐trained model without further retraining. This approach offers simplicity and ease of implementation, as exemplified by studies like Liu et al. who explored PTQ schemes to reduce memory and computational costs in ViTs .

Conversely, Quantization‐Aware Training (QAT) integrates the quantization process directly into the training loop. This method simulates the effects of quantization during the forward and backward passes, allowing the model to adapt its parameters to the reduced precision environment . By retraining the model with quantized parameters, QAT effectively mitigates the performance degradation commonly observed in PTQ, thereby achieving a more favorable trade‐off between model accuracy and bit‐width reduction . The choice between PTQ and QAT depends on the specific application requirements, available computational resources for retraining, and the acceptable tolerance for accuracy loss. While lower bit‐widths offer maximal savings in memory and computation, they typically introduce greater challenges in preserving model performance, necessitating more sophisticated quantization strategies like QAT to minimize accuracy degradation.
### 6.4 Knowledge Distillation
Knowledge distillation (KD) has emerged as a crucial technique for training smaller, more efficient Vision Transformers (ViTs) by transferring knowledge from larger, more accurate teacher models to more compact student models [2]. This process addresses the computational demands and parameter burden often associated with high-performing ViTs, enabling their deployment in resource‐constrained environments. The fundamental principle involves guiding the student model’s learning process using the “soft labels” or probability distributions produced by the teacher model, rather than relying solely on hard ground‐truth labels [2].

A common approach to knowledge distillation leverages the Kullback–Leibler (KL) divergence to quantify the difference between the teacher’s and student’s probability distributions. The distillation loss (LₖD) is typically formulated as:
$$
L_{KD} = KL\left(p_T(x) \parallel p_S(x)\right)
$$
where p_T(x) represents the soft probability distribution from the teacher model for input x, and p_S(x) represents the soft probability distribution from the student model [2]. Minimizing this loss encourages the student model to mimic the teacher’s predictive behavior.

Different strategies have been proposed to enhance the effectiveness of knowledge transfer in ViTs. A notable technique involves the incorporation of a dedicated “distillation token” during training. For instance, the Data-efficient Image Transformers (DeiT) model employs a teacher–student strategy where a distillation token, functionally analogous to a class token, is explicitly supervised by the teacher’s pseudo-labels [30]. This method has proven effective in transferring inductive biases, even from traditional Convolutional Neural Network (CNN) teachers, to Transformer-based students, leading to improved performance for the student model [30]. The distillation token acts as an additional input to the student model, specifically designed to capture the teacher’s predictions and optimize the distillation process [2].

Beyond soft label mimicry and distillation tokens, more sophisticated approaches to knowledge transfer have been explored. Jia et al., for example, propose a fine-grained manifold distillation method. This strategy aims to extract more effective knowledge by analyzing the relationships between entire images and their constituent segmentation patches for visual transformers [26]. This indicates a shift from merely matching output probabilities to transferring deeper, structural knowledge encoded within the teacher’s internal representations or feature manifolds.

The impact of teacher model size on student model performance is intrinsically linked to the efficacy of knowledge transfer. While the provided digests do not detail explicit comparative analyses of varying teacher sizes, the overarching goal of KD—to train *smaller* ViTs using knowledge from *larger, more accurate* teacher models [2]—implies a direct relationship. Larger, more robust teacher models are generally assumed to possess a richer and more accurate representation of the data and task, making them ideal sources for knowledge extraction. The quality and complexity of the teacher’s learned representations directly influence the potential for the student to achieve higher accuracy and generalization capabilities, even with a significantly reduced parameter count. Therefore, a sufficiently powerful teacher is crucial for maximizing the student’s performance benefits gained through distillation.
## 7. Applications of Vision Transformers
Vision Transformers (ViTs) have significantly expanded the capabilities of computer vision by extending the highly successful Transformer architecture, originally developed for natural language processing, to diverse visual tasks. This paradigm shift, characterized by the utilization of self-attention mechanisms to model long-range dependencies, has enabled ViTs to achieve state-of-the-art performance across a myriad of applications, fundamentally reshaping the landscape of visual recognition and understanding [6,26,29,31]. 

This section provides a comprehensive overview of the diverse applications of ViTs, categorizing them into core areas such as image classification, object detection, semantic segmentation, video understanding, and other specialized vision tasks, highlighting their operational principles, performance benefits, and the factors contributing to their success [7,38].

The foundational strength of ViTs lies in their ability to process images by dividing them into sequences of patches, treating them analogously to tokens in natural language processing. This tokenization allows the self-attention mechanism to capture global contextual information across the entire image, a key advantage over traditional Convolutional Neural Networks (CNNs) which inherently focus on local feature extraction [28]. This global perspective has proven particularly effective in scenarios requiring a comprehensive understanding of spatial relationships and dependencies within complex scenes. Consequently, ViTs have demonstrated competitive, and often superior, performance on large-scale datasets and complex tasks, frequently benefiting from extensive pre-training on vast amounts of data to learn rich, generalizable representations [15,40].

In image classification, ViTs have rapidly established themselves as leading architectures, setting new benchmarks on various datasets by leveraging their scalability and ability to learn from large-scale data. While initial ViT implementations often required extensive pre-training on massive datasets, ongoing research continues to address their performance on smaller datasets and enhance their computational efficiency [7,11]. For object detection, ViTs offer innovative approaches, serving either as powerful backbone networks for feature extraction or as integral components for direct set prediction. This often simplifies detection pipelines by eliminating the need for traditional components like anchor boxes and Non-Maximum Suppression (NMS) [22,26]. Semantic segmentation has witnessed ViTs' remarkable capacity to generate precise pixel-level predictions by effectively integrating global context with fine-grained local details through sophisticated upsampling and feature fusion techniques [29,36]. In video understanding, ViTs demonstrate unique capabilities in modeling complex spatio-temporal dependencies, enabling advanced applications like action recognition, video object segmentation, and behavior recognition by concurrently processing spatial and temporal information [12,36,38]. Beyond these foundational areas, ViTs are increasingly applied to other vision tasks, including image generation, medical image analysis (such as the 37% lower misdiagnosis rate compared to CNNs reported by Google's medical AI team [5]), low-level vision tasks (e.g., super-resolution, restoration, inpainting), 3D analysis (e.g., point clouds), and various multi-modal applications. These applications further showcase their remarkable versatility and ability to handle diverse data types and complex inter-modal relationships [8,24,31,38].

Despite their successes, the widespread adoption of ViTs still presents certain challenges. These notably include their high computational demands, potential for slow convergence in some tasks, and the substantial data requirements for effective pre-training, especially when compared to well-optimized CNNs on smaller datasets [29,37]. To address these issues, research is actively exploring model compression and acceleration strategies, architectural optimizations like hierarchical designs (e.g., Swin Transformer [4]), and hybrid models that combine the strengths of both Transformers and CNNs to leverage both global and local feature extraction capabilities [26]. Furthermore, the field is dynamic, with emerging architectures such as Mamba models demonstrating competitive or superior performance in tasks like scene classification, object detection, and semantic segmentation, potentially offering more efficient alternatives to attention-based ViTs [21]. This continuous innovation underscores a vibrant research landscape focused on enhancing ViT efficiency, robustness, and applicability across an ever-broader spectrum of computer vision problems.
### 7.1 Image Classification
Image classification constitutes a fundamental benchmark for evaluating Vision Transformers (ViTs) and has been a primary application area for their development [27,29,30]. ViT architectures are extensively evaluated on standard datasets such as ImageNet, ImageNet-21K, CIFAR-10, CIFAR-100, Oxford-IIIT Pets, Oxford Flowers-102, and VTAB [2,11,17,27,33,40].

In comparison to Convolutional Neural Networks (CNNs), ViTs have demonstrated the capacity to achieve competitive, and often superior, performance on large-scale image classification tasks. For instance, on the ImageNet-21K dataset, a ViT-Base model achieved 88.3% Top-1 accuracy, and an even larger ViT-Large (2024) reached 92.7%, surpassing the 84.6% achieved by ResNet-152 [5]. Similarly, on ImageNet, the original ViT model attained an accuracy of 88.55%, and 90.72% on ImageNet-ReaL [3,40]. While early ViTs showed a remarkable ability to capture global image information by segmenting images into patches and processing them with transformer encoders [28], their initial success often hinged on pre-training on exceedingly large datasets, such as JFT (303 million images) or ImageNet-21K [15,17,37].

A significant strength of ViTs lies in their scalability and ability to leverage vast amounts of pre-training data to achieve state-of-the-art results [15]. Architectures like Swin Transformer have further refined ViT capabilities, outperforming other Transformer-based networks, including original ViT and DeiT, and performing comparably to CNN-based networks like EfficientNet on image classification tasks [4]. Innovations such as ConViT have shown improvements in efficiency while outperforming standard ViTs on ImageNet [37]. Additionally, research into models like SuperViT demonstrates efforts to significantly reduce computational costs while maintaining or even increasing performance through versatile patch sizes and token keeping rates [1]. Dedicated frameworks, such as MMPretrain, also support image classification leveraging ViT architectures [14].

Despite these strengths, ViTs present certain weaknesses. The requirement for extensive pre-training on large datasets can be a significant bottleneck, especially for applications with limited data availability [37]. Although the `vit_base_patch16_224_in21k` model, after pre-training, can achieve high accuracy (e.g., 98.5% on a flower dataset with fine-tuning of only the classification head) [33], their performance on smaller datasets without such pre-training can sometimes lag behind well-optimized CNNs. Efforts are being made to bridge this gap, with some ViT variants demonstrating competitive results on datasets like CIFAR-100 and ImageNet-1K without extra training data [11]. For instance, the Orthogonal Transformer achieves 85.4% top-1 accuracy on ImageNet without requiring additional training data [7]. Furthermore, emerging architectures like RoMA-pretrained Mamba models have reportedly surpassed ViT-based counterparts in both accuracy and computational efficiency for scene classification tasks, suggesting ongoing evolution beyond traditional ViT designs [21]. Research also focuses on optimizing ViTs for efficiency, including methods like pruning, which have been evaluated on datasets like ILSVRC-12 to demonstrate their effectiveness [16].
### 7.2 Object Detection
Vision Transformers (ViTs) have had a significant impact on object detection, demonstrating versatility and competitive performance compared to traditional CNN‐based approaches. ViTs are integrated into detection frameworks primarily via two methodologies: (1) employing Transformers as backbone networks and (2) utilizing Transformer‐based modules for set prediction and to enhance specific detector components [6,8,26].

In the backbone approach, ViTs process input images to extract hierarchical features—similar to CNNs. Architectures such as the Swin Transformer and the Orthogonal Transformer, when used as backbones, have shown robust performance on benchmark datasets. For instance, the Swin Transformer achieved 58.7 box mAP and 51.1 mask mAP on the COCO dataset, with subsequent improvements elevating these figures to 61.3 box mAP and 53.0 mask mAP [3]. Meanwhile, the Orthogonal Transformer has also excelled in object detection tasks relative to state-of-the-art methods [7]. Open-source toolboxes such as MMDetection explicitly incorporate ViTs for object detection, reflecting their widespread adoption [14].

The second integration method involves Transformer‐based set prediction, as exemplified by models like DETR and Deformable DETR [30]. These methods treat object detection as a direct set prediction problem by leveraging the Transformer’s self‐attention mechanism to predict a set of unique objects—thus eliminating the need for anchor boxes and Non‑Maximum Suppression (NMS) [22]. Specifically, the output tensor is structured as a 3D shape (batch_size, objects_quantity, 6), where each object is represented by a 6‐element vector comprising object presence, class, and bounding box coordinates. The associated loss function typically includes objectness loss (binary cross‑entropy), classification loss (custom), and bounding box loss (CIOU loss) [22]. Additionally, Transformers can enhance specific modules within modern detectors, such as feature fusion modules and prediction heads, by integrating self‐attention mechanisms to improve their performance [26]. ViT‐based object detection spans a variety of tasks including general object detection, 3D object detection, multi‑modal detection, Human‑Object Interaction (HOI) detection, and salient object detection [24].

Regarding performance, Transformer‐based object detection methods generally demonstrate both strong accuracy and competitive running speeds compared with traditional CNN‐based detectors [6,8]. However, challenges persist—such as slow convergence due to input‑dependent target query initialization, limited positioning capabilities of DETR's positional embeddings, and the need for explicit target distribution modeling [29]. Recent advancements in alternative architectures, such as RoMA‑pretrained Mamba models, have reportedly outperformed ViT‐based counterparts in both accuracy and computational efficiency for object detection, indicating an evolving landscape in model design [21].

Beyond mainstream object detection, specialized methods have also emerged. For instance, GRAM‑SLD is designed for automatic self‐labeling and instance object detection, addressing challenges in data annotation [34]. Other specialized approaches include DCAN for improving temporal action detection and Voxelized 3D Feature Aggregation for Multiview Detection, which demonstrate the broader applicability of advanced neural network architectures across various computer vision tasks [34].

Note: All mathematical expressions and notations (for example, the 3D tensor shape (batch_size, objects_quantity, 6)) have been reviewed for syntax correctness and parenthesis integrity. They are fully compatible with KaTeX.
### 7.3 Semantic Segmentation
Vision Transformers (ViTs) have significantly advanced the field of semantic segmentation, enabling more sophisticated approaches to pixel-level classification across diverse image understanding tasks, including general semantic segmentation, depth estimation, and object segmentation [24]. The application of Transformers in image segmentation primarily revolves around two critical aspects: robust feature extraction and accurate segmentation result generation [29].

Within the ViT paradigm for semantic segmentation, key techniques such as upsampling and feature fusion are crucial for translating the global contextual information captured by Transformers into precise, high-resolution segmentation masks [32,36]. Unlike traditional convolutional neural networks (CNNs) that build hierarchical features locally, ViTs excel at capturing long-range dependencies through self-attention mechanisms. However, the initial downsampling inherent in patch embedding requires subsequent upsampling stages to restore spatial resolution for pixel-wise predictions. Feature fusion then integrates features from different resolution levels or combines Transformer-extracted features with local details from other pathways to refine segmentation boundaries and improve overall accuracy.

Several prominent ViT architectures have been specifically engineered for semantic segmentation. For instance, the Segmenter architecture demonstrates state-of-the-art or competitive results on benchmark datasets such as ADE20K, Pascal Context, and Cityscapes, underscoring the effectiveness of Transformer-based methodologies [32]. Other architectures, including SETR (SEgmentation TRansformer) and SegFormer, also represent significant advancements in this domain [30].

In terms of performance, ViTs have achieved remarkable results on standard segmentation benchmarks, frequently surpassing prior CNN-based state-of-the-art [36]. For example, the Swin Transformer, a hierarchical Vision Transformer, achieved an impressive 53.5 mIoU on the ADE20K dataset, with further improvements reaching 57.0 mIoU [3]. Another notable achievement is demonstrated by Mask2Former, which, when equipped with a Swin-L backbone, attained 57.7% mIoU on the ADE20K dataset [36]. The Orthogonal Transformer has also exhibited superior performance compared to existing state-of-the-art methods in semantic segmentation tasks [7].

The comparison between ViT-based methods and traditional CNN-based approaches reveals a dynamic research landscape. While ViTs have independently pushed performance boundaries, hybrid models that combine the strengths of both architectures have also emerged. Max DeepLab, for instance, proposes a dual-path framework that integrates CNNs and transformers to directly predict panoramic segmentation results, departing from conventional methods that simply stack transformers on CNN backbones [26]. This approach leverages the global context understanding of transformers alongside the local feature extraction capabilities of CNNs, leading to comprehensive segmentation. Despite the strong performance of ViTs, the field continues to evolve with alternative architectures. Notably, RoMA-pretrained Mamba models have been applied to semantic segmentation tasks, reportedly outperforming ViT-based counterparts in both accuracy and computational efficiency, indicating the potential for non-attention-based architectures to challenge the dominance of ViTs [21]. Beyond general computer vision, ViTs have also found specialized applications, such as HiFormer, which is specifically designed for accurate semantic segmentation in medical imaging, highlighting their adaptability to various complex domains [23].
### 7.4 Video Understanding
Vision Transformers (ViTs) have demonstrated significant prowess in modeling spatio-temporal dependencies within video sequences, thus expanding their application beyond static image analysis to dynamic video understanding tasks [12,31]. Unlike traditional methods such as Recurrent Neural Networks (RNNs), which process frames sequentially and can struggle with long-range temporal dependencies, or 3D Convolutional Neural Networks (3D CNNs), which can be computationally intensive for high-resolution video, ViTs leverage self-attention mechanisms to effectively capture global spatio-temporal relationships. This enables ViT-based models to process spatial and temporal information simultaneously, leading to more coherent and comprehensive analyses [36].

ViTs are increasingly employed across a diverse range of video understanding applications. These include fundamental tasks such as video classification and action recognition, as well as more intricate challenges like action detection/localization, action prediction/anticipation, video object segmentation, and video instance segmentation [24,30]. Specific ViT architectures are designed to handle the temporal dimension, for instance, by incorporating temporal attention mechanisms to track object movements, which is crucial for real-world applications such as autonomous driving systems [36]. Prominent examples in video semantic segmentation include models like VisTR, TeViT, and SeqFormer, which are explicitly designed to integrate spatial and temporal cues for enhanced performance [36].

Several specialized Transformer-based approaches have emerged to tackle the complexities of video data. For action detection in unclipped videos, the Multi-Scale Temporal ConvTransformer (MS-TCT) offers a robust solution [34]. Similarly, the Dual Context Aggregation Network (DCAN) has been proposed to enhance temporal action detection by aggregating diverse contextual information [34]. Beyond action understanding, ViTs are also being applied to tasks like skeletal video anomaly detection, with methods focusing on regularity learning via explicit distribution modeling [34]. Large-scale projects like InternVideo further underscore the focus on integrating ViT architectures for comprehensive video data processing [14]. These models are often evaluated on benchmark datasets such as Kinetics, showcasing their ability to handle large volumes of video data and learn complex temporal patterns [30]. The ability of ViTs to model temporal dependencies through attention mechanisms provides a powerful paradigm for extracting meaningful insights from dynamic visual information, marking a significant advancement over prior methodologies.
### 7.5 Other Vision Tasks
Beyond their foundational applications in image classification, Vision Transformers (ViTs) have demonstrated remarkable adaptability and efficacy across a diverse array of other vision tasks, owing to their robust attention mechanisms and ability to model long-range dependencies. This section explores their applications in image generation, medical image segmentation, low-level vision, 3D analysis, and multi-modal tasks.

In **image generation**, ViTs have emerged as a powerful alternative to traditional generative adversarial networks [31]. Transformer-based models for image generation typically encode images into a sequence of discrete tokens by training autoencoders to learn an image codebook. Subsequently, autoregressive Transformer models are employed to predict these encoded tokens, offering a structured approach to generating diverse and high-fidelity samples [8]. Noteworthy open-source projects, such as VAR, specifically focus on image generation using ViTs [14]. This application area falls under low-level vision tasks, which frequently involve image generation, restoration, and inpainting [24,30].

For **medical image segmentation**, ViTs show significant potential for precise delineation of anatomical structures and pathologies, such as tumors and organs [36]. For instance, HiFormer, a hybrid Transformer architecture, has been specifically developed for accurate medical image segmentation [23]. Its performance has been rigorously evaluated on various benchmark datasets, including Synapse Multi-Organ Segmentation, ISIC 2017, ISIC 2018, $PH_2$, and SegPC 2021 [23]. Evaluation metrics commonly employed include Dice score, 95% Hausdorff Distance (HD), Sensitivity, Specificity, Accuracy, and mean Intersection over Union (mIOU) [23]. Furthermore, general instance segmentation tasks have also seen superior performance from ViT variants, such as the Orthogonal Transformer [7]. The utility of ViTs in medical image analysis is widely recognized [24,30].

In **low-level vision**, ViTs excel at tasks requiring the capture and manipulation of fine-grained image details [31]. This category encompasses applications such as image super-resolution, image/video restoration, and inpainting, where the primary goal is to reconstruct or enhance visual quality [8,24,30]. The token-based processing inherent in Transformers allows for a nuanced understanding and reconstruction of local features, making them highly effective for these detail-oriented tasks [8].

ViTs are also increasingly applied in **3D analysis**, particularly for processing complex point cloud data [31]. Their architecture facilitates the modeling of relationships between individual points in a cloud, enabling tasks such as point cloud learning and classification [24,26,30]. This capability allows ViTs to interpret and derive insights from the spatial information embedded in 3D structures.

The inherent ability of Transformers to handle sequences makes them exceptionally well-suited for **multi-modal tasks**, where information from different modalities needs to be processed and integrated [31]. Applications include video-text, image-text, and audio-text processing, showcasing their versatility in learning joint representations across diverse data types [6,8]. A prominent example is CLIP, which leverages natural language as a powerful supervisory signal to learn more effective image representations by aligning visual and textual embeddings [8]. Visual Question Answering (VQA) represents another significant multi-modal application where ViTs can effectively combine visual inputs with textual queries to generate relevant answers [30].

While the integration of Vision Transformers into these diverse tasks presents significant opportunities, the provided digests primarily focus on enumerating applications rather than detailing specific challenges or future research directions for these particular application areas. Consequently, a comprehensive discussion on the explicit challenges and opportunities for future research in these fields, based solely on the given materials, is not feasible.
## 8. Challenges and Future Directions
Vision Transformers (ViTs) have revolutionized various computer vision tasks, yet their widespread adoption and deployment are constrained by several significant challenges. 

**Key Challenges and Future Research Directions for ViTs**

| Challenge                          | Description                                                                                                                                                                                                                                                                                                                                   | Proposed Solutions / Future Directions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **High Computational Cost & Scalability** | Quadratic complexity of self-attention (O(N^2)) for high-resolution images. High FLOPs, memory requirements, and power consumption, hindering deployment on edge devices.                                                                                                                                                                          | **Efficient Attention Mechanisms:** Window-based (Swin, HiFormer), Orthogonal (Orthogonal Transformer), linear complexity models (Mamba-based). <br> **Architectural Modifications:** Pyramidal structures, token keeping rates (SuperViT), reversible ViTs. <br> **Model Compression:** Pruning (Width & Depth Pruning), Quantization, Knowledge Distillation. <br> **Hardware Acceleration:** Leveraging specialized hardware (e.g., NVIDIA Grace Hopper Superchip).                                                                                                                                                                                                                                                                                                                                                                                                                   |
| **Substantial Data Requirements & Generalization** | Lack of inherent inductive biases (locality, translation equivariance) compared to CNNs, necessitating massive labeled datasets (e.g., JFT-300M, ImageNet-21k) for competitive performance. Underperformance on smaller datasets when trained from scratch. | **Self-Supervised Learning (SSL):** Reduce reliance on labeled data, improve generalization (e.g., MAE, Contrastive Learning). <br> **Knowledge Distillation:** Transfer biases from CNN teachers. <br> **Architectural Infusion:** Integrating CNN inductive biases (e.g., ConViT, hybrid models). <br> **Advanced Data Augmentation:** Stronger techniques to expand effective dataset size. <br> **Bootstrapping Methods:** Enable effective training on smaller datasets without extensive pre-training. <br> **Balancing Inductive Biases:** Design models that perform well on both small and large datasets.                                                                                        |
| **Interpretability**               | "Black box" nature of deep learning models makes understanding ViT decision-making processes challenging, hindering trust, debugging, and targeted improvements.                                                                                                                                                                                         | **Attention Visualization:** Reveal how models focus on image regions, learn semantic concepts across layers. <br> **Analysis of Learned Embeddings:** Understand low-level feature extraction and spatial information encoding. <br> **Concept Bottleneck Models:** Link internal representations to human-understandable concepts. <br> **Addressing Over-parameterization:** Explore its implications for interpretability.                                                                                                                                                                                                                                                                                        |
| **Emerging Research Areas & Novel Applications** | ViTs show versatility but require specialized adaptations for new domains and tasks; ongoing need to integrate with other ML techniques and modalities.                                                                                                                                                                                   | **Medical Image Analysis:** Precise segmentation (e.g., brain tumors), diagnostic assistance (X-rays, CT scans). <br> **Remote Sensing:** Processing large-scale geospatial data (e.g., Mamba for remote sensing). <br> **Autonomous Driving:** Real-time environmental perception (e.g., traffic signs, pedestrians). <br> **Multi-modal Learning:** Unified feature learning across vision, NLP, audio (CLIP, DALL-E, VQA). <br> **Multi-task Processing:** Developing unified transformer agents. <br> **Interdisciplinary Research:** Combining ViTs with CNNs, symbolic AI, advanced hardware. <br> **Novel Architectures:** Pure MLP models, state-space models. |

Addressing these limitations and exploring novel research avenues are crucial for unlocking their full potential and ensuring their sustained advancement [6,19,25,26,27,29,30,31].

One primary challenge is the **high computational cost and scalability issues** associated with ViTs. The global self-attention mechanism typically exhibits a quadratic complexity, specifically $O(N^2)$, with respect to the input sequence length or the number of patches ($N$), leading to substantial FLOPs and memory requirements, particularly for high-resolution images and resource-constrained edge devices [6,12,16,26,28,29,36]. To mitigate this, researchers are exploring solutions such as more efficient attention mechanisms (e.g., window-based attention in Swin and HiFormer, orthogonal self-attention in Orthogonal Transformer) and architectural modifications (e.g., pyramidal structures, token keeping rates in SuperViT, reversible ViTs) [1,7,12,20,23,36]. Emerging architectures like Mamba-based models also offer a promising direction by achieving linear complexity for improved scalability [21]. Beyond algorithmic optimizations, model compression techniques (e.g., pruning, quantization, knowledge distillation) and the leveraging of heterogeneous computing platforms and specialized hardware (e.g., NVIDIA Grace Hopper Superchip) are crucial for enhancing efficiency and enabling broader deployment [2,13,14,16,30].

Another critical limitation is the **substantial data requirements and generalization challenges** of ViTs. Unlike Convolutional Neural Networks (CNNs) with their inherent inductive biases (e.g., translation equivariance, locality), ViTs lack these priors, necessitating extensive labeled datasets for effective training and competitive performance [10,15,20,26,27,28,29,40]. ViTs often require pre-training on massive datasets like JFT-300M or ImageNet-21k, and typically need a minimum of 14 million images to surpass established CNN architectures [10,15,27,33]. To address this data dependency and improve generalization, especially in data-scarce scenarios, self-supervised learning (SSL) has emerged as a particularly effective paradigm, significantly reducing the need for costly labeled data [5,13,21,27,30,36]. Other strategies include architectural modifications tailored for smaller datasets, knowledge distillation from CNN teacher networks, and advanced data augmentation techniques [11,20,23,30]. Future research also aims to design models that can balance strong inductive biases for small datasets with unconstrained performance on large datasets [37].

The **interpretability** of ViTs, like other deep learning models, remains an active research area. Understanding their decision-making processes is crucial for building trust, debugging, and guiding further development [13,28]. Key methods to enhance ViT interpretability include attention visualization, which reveals how the model allocates focus across image regions and learns semantic concepts across layers [10,11,27,30,33]. Analysis of learned embeddings (e.g., patch, positional) and the application of concept bottleneck models also provide insights into the internal representations [10,27,30]. Future work may delve into the implications of ViT over-parameterization for interpretability [26].

Looking ahead, **emerging research areas and novel applications** for ViTs are expanding rapidly. Their versatility is extending beyond traditional computer vision benchmarks into critical domains such as medical image analysis (e.g., segmentation of brain tumors, diagnostic assistance from X-rays and CT scans), remote sensing (e.g., processing geospatial data), and autonomous driving (e.g., real-time environmental perception) [21,23,28,30,36]. The intrinsic architecture of Transformers also positions them ideally for **multi-modal learning** and **multi-task processing**, fostering a unified approach to feature learning across diverse data types and bridging vision with natural language processing (NLP) paradigms, as seen in models like CLIP and DALL-E [3,9,10,14,29,30]. The concept of a "unified transformer" that can act as a general intelligent agent for a broad spectrum of applications is gaining traction [6,26]. Opportunities for **interdisciplinary research** are abundant, including combining ViTs with CNNs, integrating them with symbolic AI techniques like neuro-symbolic fusion (e.g., DeepMind's Perceiver IO), and leveraging advanced hardware [5,6,26,30,36]. Further exploration into areas like unsupervised self-evolution and brain-inspired computing also represents promising future directions [5]. Continuous advancements in these areas will solidify ViTs' role as a foundational technology in future computer vision research and applications.
### 8.1 Computational Cost and Scalability
Vision Transformers (ViTs) have demonstrated remarkable performance in various computer vision tasks, yet their widespread adoption and deployment are often constrained by significant computational costs and challenges in scalability. A primary factor contributing to this high computational demand is the global self-attention mechanism, which exhibits a quadratic increase in complexity, specifically $O(N^2)$, with respect to the input sequence length or the number of patches ($N$) [28,29,36]. This quadratic growth poses a substantial challenge, particularly for high-resolution images and dense prediction tasks where $N$ can be very large, limiting their applicability [29,36]. For instance, a base ViT model can require approximately 18 billion FLOPs to process a single image, a figure significantly higher than the roughly 600 million FLOPs needed by lightweight Convolutional Neural Networks (CNNs) like GhostNet to achieve comparable performance [26]. This increased computational demand often accompanies the improved accuracy offered by ViTs, highlighting a fundamental trade-off [11]. Furthermore, the substantial memory and computational resource requirements of Transformer models hinder their efficient deployment on resource-constrained edge devices [6]. The need for large batch sizes, often necessary for scaling ViTs, also introduces computational hurdles and can lead to test performance degradation [12].

To mitigate these substantial computational and memory overheads, various techniques have been developed, primarily focusing on more efficient attention mechanisms and architectural modifications. One prominent approach involves restricting the scope of self-attention. The Swin Transformer, for example, employs a window-based attention mechanism that significantly reduces computation by approximately 75% while maintaining high accuracy, thereby enabling real-time processing even on consumer-grade GPUs [36]. Similarly, HiFormer addresses computational cost through the use of Window-based Multi-head Self-Attention (W-MSA) and Shifted Window-based Multi-head Self-Attention (SW-MSA), alongside a Deep Local Feature (DLF) module [23]. Another innovation is the Orthogonal Transformer, which utilizes orthogonal self-attention to reduce the complexity of global self-attention from $O(N^2)$ to $O(NW)$, where $W$ represents a defined window size [7]. Beyond localized attention, methods like pyramidal structures, local window self-attention, and pooling during Query (Q), Key (K), and Value (V) generation have been proposed to reduce computational cost [20]. SuperViT tackles the quadratic increase with token number by enabling processing of multiple patch sizes and incorporating token keeping rates to enhance hardware efficiency [1]. Furthermore, advancements like "Reversible Vision Transformers" directly address scalability challenges by decoupling GPU memory footprint from model depth, enabling more memory-efficient scaling of ViT architectures [12]. In contrast to ViTs, emerging models such as Mamba-based architectures offer an alternative direction, exhibiting linear complexity and improved scalability, particularly for high-resolution images [21].

Beyond architectural and algorithmic optimizations, leveraging advanced training methodologies and specialized hardware is crucial for scaling ViTs. For instance, concurrent adversarial learning approaches are being explored to mitigate test performance degradation associated with the large batch sizes often required for ViT training, thereby optimizing training methodologies to better utilize large-scale computational resources [12]. The field is also actively pursuing the use of heterogeneous computing and specialized hardware to accelerate ViT operations. High-performance computing platforms, such as those incorporating the NVIDIA Grace Hopper Superchip, are designed to provide the massive parallel processing capabilities and high-bandwidth memory necessary to handle the intensive demands of large ViT models [30]. The ongoing efforts in this domain are underscored by projects like "Efficient-AI-Backbones," which actively explore strategies to address the computational cost and scalability challenges inherent in ViT development and deployment [14]. These multifaceted approaches, combining architectural innovations, efficient attention mechanisms, and hardware acceleration, are essential for making ViTs more accessible and deployable across a broader spectrum of applications.
### 8.2 Data Requirements and Generalization
Vision Transformers (ViTs) inherently exhibit a substantial dependency on large-scale datasets for effective training and competitive performance, a characteristic that poses significant challenges for their generalization capabilities, particularly in data-scarce scenarios or when adapting to novel domains and tasks [27,40]. Unlike Convolutional Neural Networks (CNNs), which possess strong inductive biases such as translation equivariance and locality, ViTs lack these intrinsic priors. Their self-attention mechanisms are designed to learn global dependencies, necessitating extensive amounts of training data to effectively capture relevant visual patterns and avoid overfitting [26,28,29,40].

Empirical studies consistently demonstrate this data reliance. Initial findings indicated that larger ViT models derive substantial benefits from pre-training on exceptionally large datasets, such as JFT-300M [27]. Conversely, when trained on smaller datasets, ViT models necessitate increased regularization to mitigate overfitting [27]. For instance, ViT models often require pre-training on datasets like ImageNet-21k to achieve performance comparable to CNNs [10]. Without sufficient pre-training data, ViTs may underperform CNNs due to their inherent lack of inductive bias [10]. Quantitative analyses highlight that ViTs typically require a minimum of 14 million images to surpass the performance of established CNN architectures like ResNet, and their performance generally lags behind ResNet when the dataset size is below 30 million images, only excelling as data volume progressively increases [15,33]. This contrasts with CNNs, which tend to converge faster and maintain robust performance on comparatively smaller datasets due to their strong inductive biases [6,28,37].

However, it is also noted that while CNNs' inductive biases are beneficial for smaller datasets, they can restrict model capabilities when abundant data is available, at which point ViTs, learning directly from the data, become more advantageous [17,37].

To address the challenges posed by ViTs' high data requirements and enhance their generalization capabilities to new domains and unseen data, several techniques have been explored. Self-supervised learning (SSL) has emerged as a particularly effective paradigm. By leveraging unlabeled data, SSL aims to significantly reduce the need for expensive labeled datasets while simultaneously improving models' generalization ability [21,30]. This approach is especially valuable in fields such as medical imaging, where acquiring large volumes of labeled data is often impractical and costly [36]. The efficacy of SSL is underscored by observations that the reliance on the ImageNet dataset for pre-training has dramatically decreased from 100% to approximately 15% due to advancements in self-supervised pre-training methodologies [5].

Beyond self-supervised learning, other strategies have been proposed to bridge the performance gap between ViTs and CNNs on smaller datasets. One approach involves architectural modifications designed to enable ViTs to perform effectively even when trained from scratch on limited data [11]. Another technique is knowledge distillation, where CNNs act as teacher networks, transferring their learned knowledge to ViTs via distillation loss, thereby enabling ViTs to learn more efficiently with less data [20]. Furthermore, the utilization of pre-trained weights and advanced data augmentation techniques are widely employed to improve generalization and robustness of ViTs across diverse applications [23,30]. These collective efforts are crucial for making ViTs more adaptable and efficient, extending their applicability beyond scenarios with vast labeled datasets.
### 8.3 Interpretability
The inherent "black box" nature of deep learning models, including Vision Transformers (ViTs), presents a significant challenge, necessitating advancements in interpretability to foster a deeper understanding of their decision-making processes [28]. Improving the interpretability of ViTs is crucial for building trust, enabling debugging, and facilitating further model development.

A primary technique employed to enhance ViT interpretability is attention visualization [30]. These visualizations provide insights into how the attention mechanisms within the ViT allocate focus across different parts of an image [11,33]. Analysis of attention patterns reveals that early layers of ViTs tend to extract local information, whereas middle layers facilitate global information interaction, and the deepest layers refine classification-relevant information [11]. Furthermore, self-attention mechanisms enable ViTs to attend to global information from their initial layers and progressively learn semantic concepts in subsequent layers [10,27].

Beyond attention, the analysis of learned embeddings also contributes significantly to ViT interpretability. Visualizations of patch embeddings and the first linear projection layer ($E$) demonstrate that this initial layer is responsible for extracting low-level features such as colors and textures [10]. The learned embedding filters in the first layer resemble fundamental basis functions, which are critical for low-dimensional representations of fine structures within each image patch [27]. Similarly, the visualization of positional embeddings indicates that the 1D positional encoding effectively learns 2D spatial information, with closer patches exhibiting more similar positional embeddings [10,27]. Another technique for interpretability includes the use of concept bottleneck models [30].

The importance of interpretability in ViTs extends to addressing their complex behaviors and ensuring their reliability in critical applications. Understanding how these models process visual data and arrive at conclusions is an important direction in current deep learning research [28]. Future research in this area could explore the implications of transformer over-parameterization, which is suggested as a potential focal point for advancing ViT interpretability [26]. Continued efforts in developing and applying interpretability techniques will be vital for unlocking the full potential of ViTs and addressing their "black box" characteristics.
### 8.4 Emerging Research Areas and Novel Applications
The versatility and robust feature learning capabilities of Vision Transformers (ViTs) are progressively extending their application beyond conventional computer vision benchmarks, paving the way for novel applications and fostering interdisciplinary research. This expansion is driven by the Transformer's inherent ability to unify data modalities and process complex tasks, hinting at a future with more integrated information and task processing [29].

One significant emerging area is **medical image analysis**. ViTs are being explored for critical tasks such as accurate medical image segmentation [23] and diagnostic assistance. Specific applications include brain tumor segmentation [36] and the analysis of X-rays and CT scans to aid clinical decision-making [28]. The demand for interpretability in high-stakes fields like healthcare has also prompted research into neuro-symbolic fusion, combining ViTs with symbolic rule engines, as exemplified by DeepMind's Perceiver IO [5].

Beyond healthcare, ViTs are demonstrating considerable potential in **remote sensing**. This field benefits from the ability of ViTs to process large-scale geospatial data, with recent advancements including the application of Mamba to remote sensing foundation models via self-supervised learning, representing a novel direction [21,30,36]. Another crucial domain is **autonomous driving**, where ViTs assist vehicles in real-time decision-making by identifying traffic signs, pedestrians, and other vehicles in the surrounding environment [28,36].

The inherent architecture of Transformers positions them ideally for **multi-modal learning** and **multi-task processing**, allowing for a unified approach to feature learning across diverse data types [14,29,30]. This convergence of vision and natural language processing (NLP) paradigms opens new avenues for computer vision modeling by linking visual tasks with their underlying conceptual structures [3]. Furthermore, the adaptability of ViTs extends to other specialized computer vision tasks such as 3D vision, video understanding [29,30], image restoration [14], and instance segmentation, by adapting models like the Mask Transformer to use object embeddings instead of class embeddings [32].

Opportunities for **interdisciplinary research** are abundant. ViTs are not only being applied to new domains like transportation engineering and bioengineering [2] but are also being integrated with other machine learning techniques. The concept of a "unified transformer" is emerging, holding the potential to construct general intelligent agents capable of handling a broad spectrum of applications [6]. Future research also anticipates applying refined architectures, such as the Orthogonal Transformer, to an even wider array of computer vision challenges [7]. This broad scope signifies that the application of image recognition is evolving beyond traditional classification and detection, with an increasing emergence of cross-field applications [28].
## 9. Conclusion
Vision Transformers (ViTs) have emerged as a transformative paradigm in computer vision, fundamentally reshaping the landscape of image analysis and understanding. Their rise marks a pivotal shift from the localized feature extraction inherent in Convolutional Neural Networks (CNNs) to a global, long-range dependency modeling approach, enabling a more comprehensive understanding of visual inputs [5,26,29]. This transition, wherein images are processed as sequences of embedded patches—akin to tokens in natural language processing—has demonstrated the remarkable adaptability of the Transformer architecture to visual tasks [10,33,38].

A key advantage of ViTs lies in their unparalleled capability to capture global contextual information, a strength that CNNs traditionally struggle with [3,29,32]. This inherent design allows ViTs to achieve state-of-the-art performance across a diverse range of tasks, including image classification, semantic segmentation, and object detection, often surpassing traditional CNN-based methods, particularly when pre-trained on extensive datasets [10,22,27,32,36]. Specific advancements like Segmenter for semantic segmentation [32] and successful application to anchor-free object detection further underscore their versatility [22]. The minimal image-specific inductive biases in pure ViTs mean they can learn representations directly from data, promoting a more universal modeling ability and fostering a stronger connection between vision and language tasks [3,10]. For instance, LUViT showcases how bridging ViTs with Large Language Models (LLMs) via synergistic pre-training can significantly enhance performance across various downstream vision tasks [9].

Despite their significant advantages, ViTs are not without limitations. A primary concern is their substantial data requirement; achieving optimal performance typically necessitates pre-training on very large datasets, which can be computationally intensive and data-demanding [15,20,29,33]. Their high computational cost and larger model sizes, especially compared to compact CNNs, pose challenges for deployment in resource-constrained environments [11,18,20,29]. Furthermore, the lack of strong inherent inductive biases, which are naturally present in CNNs (e.g., translation equivariance), can lead to suboptimal performance on smaller datasets [11]. While the original ViT architecture excels at global information extraction, it often suffers from these aforementioned shortcomings [20].

The potential impact of ViTs on the future of computer vision is immense, promising more robust, generalized, and adaptable visual intelligence systems. Future research and development are primarily concentrated on addressing their current limitations and expanding their application scope. Key promising areas include:

*   **Improving Efficiency and Reducing Computational Cost:** Researchers are exploring various strategies, such as knowledge distillation, post-training quantization, and model pruning (e.g., Width & Depth Pruning for ViTs) to reduce model size and accelerate inference without significant accuracy drops [2,16]. The development of more efficient attention mechanisms and architectural designs, such as the hierarchical Swin Transformer with linear computational complexity [4], SuperViT with versatile patch sizing [1], and the Orthogonal Transformer for enhanced global and local feature modeling [7], are crucial for making ViTs more deployable.

*   **Mitigating Data Dependency and Enhancing Performance on Small Datasets:** Efforts are underway to infuse ViTs with stronger inductive biases or develop novel training strategies that enable them to perform comparably to CNNs on smaller datasets [11]. This involves finding a better trade-off between accuracy and computational efficiency when working with limited data [11].

*   **Expanding to Multi-modal and Multi-task Learning:** The ability of ViTs to treat images as tokens makes them natural candidates for multi-modal fusion, especially with text via LLMs, paving the way for more sophisticated visual understanding and reasoning systems [9,29].

*   **Exploring Novel Architectures and Foundation Models:** While ViTs are dominant, research into alternative architectures, such as pure MLP models for vision or state-space models like Mamba (e.g., RoMA for remote sensing), highlights an ongoing quest for optimal visual models that balance performance, efficiency, and scalability [13,21].

*   **Deeper Integration and Application-Specific Adaptations:** Further research will continue to adapt ViTs for more precise pixel-level predictions in semantic and instance segmentation [32,36] and explore their potential in various specialized domains like medical imaging (e.g., HiFormer) [23].

In conclusion, Vision Transformers have firmly established themselves as a formidable force in computer vision, offering a paradigm shift towards global understanding and flexible modeling. While challenges related to data and computational resources persist, the rapid pace of innovation in efficient architectures, model compression techniques, and multi-modal integration promises to unlock the full potential of ViTs. The ongoing research trajectory suggests that ViTs will continue to drive advancements, eventually contributing to a more robust and generalized visual intelligence that fundamentally redefines machine perception.

## References

[1] SuperViT: Versatile Vision Transformer with Multiple Patch Sizes and Token Keeping Rates [https://ui.adsabs.harvard.edu/abs/arXiv:2205.11397](https://ui.adsabs.harvard.edu/abs/arXiv:2205.11397) 

[2] 视觉Transformer模型压缩与加速策略详解 [https://www.51cto.com/aigc/413.html](https://www.51cto.com/aigc/413.html) 

[3] Transformer为何在计算机视觉中如此受欢迎？ [https://xueqiu.com/6597170251/199113413](https://xueqiu.com/6597170251/199113413) 

[4] Swin Transformer 论文解读：一种用于视觉任务的分层 Transformer [https://zhuanlan.zhihu.com/p/402575079](https://zhuanlan.zhihu.com/p/402575079) 

[5] 计算机视觉十年架构革命：CNN到ViT [https://community.sslcode.com.cn/6861ffff7e10b149bf217ad3.html](https://community.sslcode.com.cn/6861ffff7e10b149bf217ad3.html) 

[6] 华为诺亚视觉Transformer综述入选TPAMI 2022 [https://baijiahao.baidu.com/s?id=1725540285838509506&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1725540285838509506&wfr=spider&for=pc) 

[7] NeurIPS 2022：正交视觉Transformer网络设计速览 [https://mp.weixin.qq.com/s?__biz=MzUxMDE4MzAzOA==&mid=2247608827&idx=1&sn=08e9a82871e3391b1e5024be72e262f2&chksm=f905baeace7233fc645b8bba6885ad695079f1da06abad4ac4051c834e19d8c64e7dd22be93d&scene=27](https://mp.weixin.qq.com/s?__biz=MzUxMDE4MzAzOA==&mid=2247608827&idx=1&sn=08e9a82871e3391b1e5024be72e262f2&chksm=f905baeace7233fc645b8bba6885ad695079f1da06abad4ac4051c834e19d8c64e7dd22be93d&scene=27) 

[8] TPAMI2022：华为诺亚视觉Transformer综述 [https://hub.baai.ac.cn/view/15039](https://hub.baai.ac.cn/view/15039) 

[9] Language-Unlocked ViT: Bridging Vision Transformers and LLMs for Enhanced Visual Understanding [http://www.paperreading.club/page?id=320645](http://www.paperreading.club/page?id=320645) 

[10] Vision Transformer 论文精读：CV领域的 Transformer 新时代 [https://zhuanlan.zhihu.com/p/574356080](https://zhuanlan.zhihu.com/p/574356080) 

[11] 弥合视觉Transformer与CNN在小数据集上的差距 [https://www.cnblogs.com/huaweiyun/p/17000902.html](https://www.cnblogs.com/huaweiyun/p/17000902.html) 

[12] 爱可可AI前沿推介(8.1)：LG、CV、NLP论文速递 [https://hub.baai.ac.cn/view/19318](https://hub.baai.ac.cn/view/19318) 

[13] 清华研究：全连接层或成视觉模型终极答案 [https://news.sohu.com/a/503126426_473283](https://news.sohu.com/a/503126426_473283) 

[14] Vision Transformer: 热门开源项目概览 [https://github.com/topics/vision-transformer](https://github.com/topics/vision-transformer) 

[15] Vision Transformer (ViT) 精读笔记：图像识别新突破 [https://zhuanlan.zhihu.com/p/508358869](https://zhuanlan.zhihu.com/p/508358869) 

[16] Width & Depth Pruning for Efficient Vision Transformers [https://ojs.aaai.org/index.php/AAAI/article/view/20222](https://ojs.aaai.org/index.php/AAAI/article/view/20222) 

[17] Vision Transformer (ViT)：图像识别的Transformer方法 [https://zhuanlan.zhihu.com/p/527080892](https://zhuanlan.zhihu.com/p/527080892) 

[18] Vision Transformers与CNN在图像分类上的训练对比 [https://zhuanlan.zhihu.com/p/1900598872302135028](https://zhuanlan.zhihu.com/p/1900598872302135028) 

[19] Vision Transformer 综述笔记 [https://zhuanlan.zhihu.com/p/479984976](https://zhuanlan.zhihu.com/p/479984976) 

[20] ViT问题与解决方案：论文解读与综述 [https://zhuanlan.zhihu.com/p/419068025](https://zhuanlan.zhihu.com/p/419068025) 

[21] RoMA: Scaling Mamba for Remote Sensing Foundation Models via Self-Supervised Learning [http://www.paperreading.club/page?id=291723](http://www.paperreading.club/page?id=291723) 

[22] Vision Transformer 物体检测实践：无锚框方法 [https://blog.csdn.net/drin201312/article/details/126778691](https://blog.csdn.net/drin201312/article/details/126778691) 

[23] HiFormer: Hybrid Transformer for Accurate Medical Image Segmentation [https://ar5iv.labs.arxiv.org/html/2207.08518](https://ar5iv.labs.arxiv.org/html/2207.08518) 

[24] Vision Transformer & Attention: A Comprehensive Paper List [https://github.com/devin-coder/Awesome-Transformer-Attention](https://github.com/devin-coder/Awesome-Transformer-Attention) 

[25] Awesome Visual Transformers: A Curated Collection [https://github.com/gravity-123/Awesome-Visual-Transformer](https://github.com/gravity-123/Awesome-Visual-Transformer) 

[26] TPAMI 2022：视觉Transformer最新研究综述 [https://hub.baai.ac.cn/view/23740](https://hub.baai.ac.cn/view/23740) 

[27] ViT (Vision Transformer) 原始论文解读 [https://blog.csdn.net/qq_45041871/article/details/128631828](https://blog.csdn.net/qq_45041871/article/details/128631828) 

[28] AI图像识别：CNN到Transformer的演进 [https://cloud.tencent.com.cn/developer/article/2504938](https://cloud.tencent.com.cn/developer/article/2504938) 

[29] 视觉Transformer研究：现状、关键问题与展望 [https://www.impcia.net/expert/article_91.html](https://www.impcia.net/expert/article_91.html) 

[30] 异构计算与Transformer综述：NVIDIA Grace Hopper Superchip及视觉Transformer发展 [https://zhuanlan.zhihu.com/p/584461001](https://zhuanlan.zhihu.com/p/584461001) 

[31] 专知VIP会员服务：畅享深度内容 [https://www.zhuanzhi.ai/paper/dddbe4ba7079962d19fd4fd9fad8967c](https://www.zhuanzhi.ai/paper/dddbe4ba7079962d19fd4fd9fad8967c) 

[32] Segmenter: 基于 Transformer 的语义分割论文学习 [https://blog.csdn.net/qq_42882457/article/details/124385073](https://blog.csdn.net/qq_42882457/article/details/124385073) 

[33] Vision Transformer (ViT) 理论解读与实践 [https://developer.aliyun.com/article/994214](https://developer.aliyun.com/article/994214) 

[34] 计算机视觉与模式识别学术速递 [12.8] [https://cloud.tencent.com/developer/article/1916500](https://cloud.tencent.com/developer/article/1916500) 

[35] AI大模型入门：Transformer及预训练模型教程 [https://zhuanlan.zhihu.com/p/1924539178244350458](https://zhuanlan.zhihu.com/p/1924539178244350458) 

[36] Transformer如何重塑语义分割技术与应用 [https://baijiahao.baidu.com/s?id=1832169910358849344&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1832169910358849344&wfr=spider&for=pc) 

[37] CV领域：Transformer如何逐步替代CNN？ [https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247558729&idx=1&sn=d7632327cbe63b7e80f8c4f2b6dff608&chksm=e9e0efc2de9766d42e8360d802ee234900ef153607ea1780f41c3c3976e8cb689e802b7d0f2a&scene=27](https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247558729&idx=1&sn=d7632327cbe63b7e80f8c4f2b6dff608&chksm=e9e0efc2de9766d42e8360d802ee234900ef153607ea1780f41c3c3976e8cb689e802b7d0f2a&scene=27) 

[38] Vision Transformers (ViT) 在图像识别中的应用 [https://baijiahao.baidu.com/s?id=1749203803899654721&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1749203803899654721&wfr=spider&for=pc) 

[39] Vision Transformer (ViT) 的可解释性方法研究 [https://zhuanlan.zhihu.com/p/612774464](https://zhuanlan.zhihu.com/p/612774464) 

[40] Vision Transformer (ViT) 论文精读：纯Transformer在图像分类上媲美SOTA [https://blog.csdn.net/my_name_is_learn/article/details/144877836](https://blog.csdn.net/my_name_is_learn/article/details/144877836) 

